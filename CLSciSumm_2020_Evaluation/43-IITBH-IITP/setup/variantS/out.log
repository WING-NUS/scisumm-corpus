parsing: input/ref/Task1/W99-0613_swastika.csv
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
<S sid="159" ssid="26">To prevent this we &amp;quot;smooth&amp;quot; the confidence by adding a small value, e, to both W+ and W_, giving at = Plugging the value of at from Equ.</S>
original cit marker offset is 0
new cit marker offset is 0



['159']
159
['159']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S>
original cit marker offset is 0
new cit marker offset is 0



['137']
137
['137']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['method_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
 <S sid="250" ssid="1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S>
original cit marker offset is 0
new cit marker offset is 0



['250']
250
['250']
parsed_discourse_facet ['result_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
    <S sid="36" ssid="30">Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.</S>
original cit marker offset is 0
new cit marker offset is 0



['36']
36
['36']
parsed_discourse_facet ['result_citation']
<S sid="29" ssid="23">Unfortunately, Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective function.</S>
original cit marker offset is 0
new cit marker offset is 0



['29']
29
['29']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
    <S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['method_citation']
    <S sid="95" ssid="28">(Specifically, the limit n starts at 5 and increases by 5 at each iteration.)</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['method_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="1" ssid = "1">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="9" ssid = "3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="136" ssid = "3">We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'250'", "'136'", "'163'"]
'1'
'9'
'250'
'136'
'163'
['1', '9', '250', '136', '163']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.</S><S sid ="17" ssid = "11">At first glance  the problem seems quite complex: a large number of rules is needed to cover the domain  suggesting that a large number of labeled examples is required to train an accurate classifier.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="105" ssid = "38">Thus the method makes the fairly strong assumption that the features can be partitioned into two types such that each type alone is sufficient for classification.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'17'", "'250'", "'105'", "'248'"]
'2'
'17'
'250'
'105'
'248'
['2', '17', '250', '105', '248']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid ="92" ssid = "25">Thus an explicit assumption about the redundancy of the features — that either the spelling or context alone should be sufficient to build a classifier — has been built into the algorithm.</S><S sid ="121" ssid = "54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).</S><S sid ="58" ssid = "12">Having found (spelling  context) pairs in the parsed data  a number of features are extracted.</S><S sid ="232" ssid = "11">For example  the independence assumptions mean that the model fails to capture the dependence between specific and more general features (for example the fact that the feature full.-string=New_York is always seen with the features contains (New) and The baseline method tags all entities as the most frequent class type (organization). contains (York) and is never seen with a feature such as contains (Group) ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'92'", "'121'", "'58'", "'232'"]
'3'
'92'
'121'
'58'
'232'
['3', '92', '121', '58', '232']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "20">We present two algorithms.</S><S sid ="4" ssid = "4">We present two algorithms.</S><S sid ="68" ssid = "1">The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S><S sid ="88" ssid = "21">We can now compare this algorithm to that of (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'4'", "'68'", "'163'", "'88'"]
'26'
'4'
'68'
'163'
'88'
['26', '4', '68', '163', '88']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid ="6" ssid = "6">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="68" ssid = "1">The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'31'", "'6'", "'97'", "'68'"]
'5'
'31'
'6'
'97'
'68'
['5', '31', '6', '97', '68']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "6">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid ="5" ssid = "5">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="30" ssid = "24">(Blum and Mitchell 98) offer a promising formulation of redundancy  also prove some results about how the use of unlabeled examples can help classification  and suggest an objective function when training with unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'31'", "'5'", "'204'", "'30'"]
'6'
'31'
'5'
'204'
'30'
['6', '31', '5', '204', '30']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision  in the form of labeled training examples.</S><S sid ="2" ssid = "2">A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="3" ssid = "3">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid ="130" ssid = "63">So the success of the algorithm may well be due to its success in maximizing the number of unlabeled examples on which the two decision lists agree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'2'", "'250'", "'3'", "'130'"]
'7'
'2'
'250'
'3'
'130'
['7', '2', '250', '3', '130']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "2">Recent results (e.g.  (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S><S sid ="27" ssid = "21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S><S sid ="120" ssid = "53">(Blum and Mitchell 98) go on to give PAC results for learning in the cotraining case.</S><S sid ="40" ssid = "34">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid ="88" ssid = "21">We can now compare this algorithm to that of (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'27'", "'120'", "'40'", "'88'"]
'8'
'27'
'120'
'40'
'88'
['8', '27', '120', '40', '88']
parsed_discourse_facet ['results_citation']
<S sid ="1" ssid = "1">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="9" ssid = "3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="136" ssid = "3">We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'250'", "'136'", "'163'"]
'1'
'9'
'250'
'136'
'163'
['1', '9', '250', '136', '163']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "4">The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.</S><S sid ="147" ssid = "14">The weak learner for two-class problems computes a weak hypothesis h from the input space into the reals (h : 2x -4 R)  where the sign4 of h(x) is interpreted as the predicted label and the magnitude I h(x)I is the confidence in the prediction: large numbers for I h(x)I indicate high confidence in the prediction  and numbers close to zero indicate low confidence.</S><S sid ="35" ssid = "29">AdaBoost finds a weighted combination of simple (weak) classifiers  where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.</S><S sid ="224" ssid = "3">The model was parameterized such that the joint probability of a (label  feature-set) pair P(yi  xi) is written as The model assumes that (y  x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).</S><S sid ="74" ssid = "7">Output of the learning algorithm: a function h:Xxy [0  1] where h(x  y) is an estimate of the conditional probability p(y1x) of seeing label y given that feature x is present.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'147'", "'35'", "'224'", "'74'"]
'10'
'147'
'35'
'224'
'74'
['10', '147', '35', '224', '74']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "5">For example  a good classifier would identify Mrs. Frank as a person  Steptoe & Johnson as a company  and Honduras as a location.</S><S sid ="40" ssid = "34">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid ="23" ssid = "17">For example  in ..  says Mr. Cooper  a vice president of.. both a spelling feature (that the string contains Mr.) and a contextual feature (that president modifies the string) are strong indications that Mr. Cooper is of type Person.</S><S sid ="48" ssid = "2">For example  take ...  says Maury Cooper  a vice president at S.&P.</S><S sid ="55" ssid = "9">In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'40'", "'23'", "'48'", "'55'"]
'11'
'40'
'23'
'48'
'55'
['11', '40', '23', '48', '55']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "6">The approach uses both spelling and contextual rules.</S><S sid ="58" ssid = "12">Having found (spelling  context) pairs in the parsed data  a number of features are extracted.</S><S sid ="22" ssid = "16">In many cases  inspection of either the spelling or context alone is sufficient to classify an example.</S><S sid ="165" ssid = "32">In the namedentity problem each example is a (spelling context) pair.</S><S sid ="55" ssid = "9">In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'58'", "'22'", "'165'", "'55'"]
'12'
'58'
'22'
'165'
'55'
['12', '58', '22', '165', '55']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "7">A spelling rule might be a simple look-up for the string (e.g.  a rule that Honduras is a location) or a rule that looks at words within a string (e.g.  a rule that any string containing Mr. is a person).</S><S sid ="14" ssid = "8">A contextual rule considers words surrounding the string in the sentence in which it appears (e.g.  a rule that any proper name modified by an appositive whose head is president is a person).</S><S sid ="61" ssid = "15">The following features were used: full-string=x The full string (e.g.  for Maury Cooper  full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word  this feature applies for any words that the string contains (e.g.  Maury Cooper contributes two such features  contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g.  IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods  and contains at least one period.</S><S sid ="22" ssid = "16">In many cases  inspection of either the spelling or context alone is sufficient to classify an example.</S><S sid ="165" ssid = "32">In the namedentity problem each example is a (spelling context) pair.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'14'", "'61'", "'22'", "'165'"]
'13'
'14'
'61'
'22'
'165'
['13', '14', '61', '22', '165']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/E03-1005_swastika.csv
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
    <S sid="41" ssid="38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



['41']
41
['41']
parsed_discourse_facet ['aim_citation']
<S sid="80" ssid="32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S>
original cit marker offset is 0
new cit marker offset is 0



['80']
80
['80']
parsed_discourse_facet ['result_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



['143']
143
['143']
parsed_discourse_facet ['method_citation']
<S sid="146" ssid="11">This paper also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic depen dencies (as e.g. in Collins 1999 and Charniak 2000).</S>
original cit marker offset is 0
new cit marker offset is 0



['146']
146
['146']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
<S sid="100" ssid="3">In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['result_citation']
    <S sid="30" ssid="27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S>
original cit marker offset is 0
new cit marker offset is 0



['30']
30
['30']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
    <S sid="30" ssid="27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S>
original cit marker offset is 0
new cit marker offset is 0



['30']
30
['30']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S sid ="1" ssid = "1">Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.</S><S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S><S sid ="76" ssid = "28">Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.</S><S sid ="26" ssid = "23">Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation  which in some cases is a reasonable approximation of the most probable parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'108'", "'28'", "'76'", "'26'"]
'1'
'108'
'28'
'76'
'26'
['1', '108', '28', '76', '26']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">This paper proposes an integration of the two models which outperforms each of them separately.</S><S sid ="107" ssid = "10">This suggests that a model which combines these two notions of best parse may boost the accuracy.</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S><S sid ="94" ssid = "46">This paper presents the first published results with this estimator on the WSJ.</S><S sid ="56" ssid = "8">Here we will only sketch this PCFG-reduction  which is heavily based on Goodman (2002).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'107'", "'45'", "'94'", "'56'"]
'2'
'107'
'45'
'94'
'56'
['2', '107', '45', '94', '56']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models  and an average processing time of 3.6 seconds per WSJ sentence.</S><S sid ="38" ssid = "35">Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task  the parsing time was reported to be over 200 seconds per sentence (Bod 2003).</S><S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="92" ssid = "44">Tested on the OVIS corpus  Bonnema et al. 's proposal obtains results that are comparable to Sima'an (1999) -- see Bonnema et al.</S><S sid ="41" ssid = "38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'38'", "'130'", "'92'", "'41'"]
'3'
'38'
'130'
'92'
'41'
['3', '38', '130', '92', '41']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S><S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="6" ssid = "3">To appreciate these innovations  it should be noted that the model was radically different from all other statistical parsing models at the time.</S><S sid ="143" ssid = "8">While SL-DOP and LS-DOP have been compared before in Bod (2002)  especially in the context of musical parsing  this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules  and (2) the use of arbitrarily large fragments rather than restricted ones.</S><S sid ="1" ssid = "1">Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'108'", "'6'", "'143'", "'1'"]
'4'
'108'
'6'
'143'
'1'
['4', '108', '6', '143', '1']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "2">Fragments include  for instance  subtrees of depth 1 (corresponding to context-free rules) as well as entire trees.</S><S sid ="62" ssid = "14">Let a represent the number of subtrees headed by nodes with nonterminal A  that is a =Ej aj.</S><S sid ="23" ssid = "20">Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.</S><S sid ="104" ssid = "7">Next  the rank of each (shortest) derivation is computed as the sum of the ranks of the subtrees involved.</S><S sid ="61" ssid = "13">Let aj represent the number of subtrees headed by the node A@j.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'62'", "'23'", "'104'", "'61'"]
'5'
'62'
'23'
'104'
'61'
['5', '62', '23', '104', '61']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "3">To appreciate these innovations  it should be noted that the model was radically different from all other statistical parsing models at the time.</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="106" ssid = "9">What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.</S><S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S><S sid ="76" ssid = "28">Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'79'", "'106'", "'4'", "'76'"]
'6'
'79'
'106'
'4'
'76'
['6', '79', '106', '4', '76']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="128" ssid = "9">Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.</S><S sid ="23" ssid = "20">Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="14" ssid = "11">This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'128'", "'23'", "'15'", "'14'"]
'7'
'128'
'23'
'15'
'14'
['7', '128', '23', '15', '14']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">in Fujisaki et al. 1989; Black et al.</S><S sid ="10" ssid = "7">Waegner 1992; Pereira and Schabes 1992).</S><S sid ="98" ssid = "1">Most DOP models  such as in Bod (1993)  Goodman (1996)  Bonnema et al. (1997)  Sima'an (2000) and Collins & Duffy (2002)  use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.</S><S sid ="18" ssid = "15">Collins 1999; Charniak 2000; Goodman 1998).</S><S sid ="120" ssid = "1">For our experiments we used the standard division of the WSJ (Marcus et al. 1993)  with sections 2 through 21 for training (approx.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'10'", "'98'", "'18'", "'120'"]
'8'
'10'
'98'
'18'
'120'
['8', '10', '98', '18', '120']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "6">1992  1993; Briscoe and I Thanks to Ivan Sag for this pun.</S><S sid ="134" ssid = "15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.</S><S sid ="29" ssid = "26">Johnson (1998b  2002) showed that DOP1's subtree estimation method is statistically biased and inconsistent.</S><S sid ="35" ssid = "32">Bonnema et al. (1999) observed that another problem with DOP1's subtree-estimation method is that it provides more probability to nodes with more subtrees  and therefore more probability to larger subtrees.</S><S sid ="23" ssid = "20">Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'134'", "'29'", "'35'", "'23'"]
'9'
'134'
'29'
'35'
'23'
['9', '134', '29', '35', '23']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">Waegner 1992; Pereira and Schabes 1992).</S><S sid ="18" ssid = "15">Collins 1999; Charniak 2000; Goodman 1998).</S><S sid ="129" ssid = "10">Collins 1996  Charniak 1997  Collins 1999 and Charniak 2000).</S><S sid ="8" ssid = "5">in Fujisaki et al. 1989; Black et al.</S><S sid ="98" ssid = "1">Most DOP models  such as in Bod (1993)  Goodman (1996)  Bonnema et al. (1997)  Sima'an (2000) and Collins & Duffy (2002)  use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'18'", "'129'", "'8'", "'98'"]
'10'
'18'
'129'
'8'
'98'
['10', '18', '129', '8', '98']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "8">The DOP model  on the other hand  was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus  but to directly use corpus fragments as a grammar.</S><S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="143" ssid = "8">While SL-DOP and LS-DOP have been compared before in Bod (2002)  especially in the context of musical parsing  this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules  and (2) the use of arbitrarily large fragments rather than restricted ones.</S><S sid ="110" ssid = "13">We will refer to the first combination (which selects the simplest among the n likeliest trees) as Simplicity-Likelihood-DOP or SL-DOP  and to the second combination (which selects the likeliest among the n simplest trees) as Likelihood-Simplicity-DOP or LS-DOP.</S><S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'13'", "'143'", "'110'", "'4'"]
'11'
'13'
'143'
'110'
'4'
['11', '13', '143', '110', '4']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "9">This approach has now gained wide usage  as exemplified by the work of Collins (1996  1999)  Charniak (1996  1997)  Johnson (1998)  Chiang (2000)  and many others.</S><S sid ="98" ssid = "1">Most DOP models  such as in Bod (1993)  Goodman (1996)  Bonnema et al. (1997)  Sima'an (2000) and Collins & Duffy (2002)  use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.</S><S sid ="143" ssid = "8">While SL-DOP and LS-DOP have been compared before in Bod (2002)  especially in the context of musical parsing  this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules  and (2) the use of arbitrarily large fragments rather than restricted ones.</S><S sid ="16" ssid = "13">While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words  later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a).</S><S sid ="41" ssid = "38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'98'", "'143'", "'16'", "'41'"]
'12'
'98'
'143'
'16'
'41'
['12', '98', '143', '16', '41']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="54" ssid = "6">A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account.</S><S sid ="20" ssid = "17">Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage  and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).</S><S sid ="110" ssid = "13">We will refer to the first combination (which selects the simplest among the n likeliest trees) as Simplicity-Likelihood-DOP or SL-DOP  and to the second combination (which selects the likeliest among the n simplest trees) as Likelihood-Simplicity-DOP or LS-DOP.</S><S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'54'", "'20'", "'110'", "'4'"]
'13'
'54'
'20'
'110'
'4'
['13', '54', '20', '110', '4']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "11">This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="36" ssid = "33">As an alternative  Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'15'", "'7'", "'36'", "'82'"]
'14'
'15'
'7'
'36'
'82'
['14', '15', '7', '36', '82']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="14" ssid = "11">This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.</S><S sid ="134" ssid = "15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'45'", "'82'", "'14'", "'134'"]
'15'
'45'
'82'
'14'
'134'
['15', '45', '82', '14', '134']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



E03-1005
P11-1086
0
method_citation
['aim_citation']
IGNORE THIS: Key error 5
parsing: input/ref/Task1/A97-1014_swastika.csv
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



['151']
151
['151']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
<S sid="47" ssid="37">Argument structure can be represented in terms of unordered trees (with crossing branches).</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



['167']
167
['167']
parsed_discourse_facet ['method_citation']
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



['160']
160
['160']
parsed_discourse_facet ['method_citation']
<S sid="127" ssid="8">As the need for certain functionalities becomes obvious with growing annotation experience, we have decided to implement the tool in two stages.</S>
original cit marker offset is 0
new cit marker offset is 0



['127']
127
['127']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



['39']
39
['39']
parsed_discourse_facet ['method_citation']
<S sid="72" ssid="17">In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.</S>
original cit marker offset is 0
new cit marker offset is 0



['72']
72
['72']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/A97-1014.csv
<S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'162'", "'11'", "'5'", "'4'"]
'1'
'162'
'11'
'5'
'4'
['1', '162', '11', '5', '4']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Since the requirements for such a formalism differ from those posited for configurational languages  several features have been added  influencing the architecture of the scheme.</S><S sid ="3" ssid = "3">The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.</S><S sid ="31" ssid = "21">Due to the substantial differences between existing models of constituent structure  the question arises of how the theory independencf requirement  can be satisfied.</S><S sid ="13" ssid = "3">The data-drivenness of this approach presents a clear advantage over the traditional  idealised notion of competence grammar.</S><S sid ="165" ssid = "7">In addition the approach provides empirical material for psycholinguistic investigation  since preferences for the choice of certain syntactic constructions  linea.rizations  and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'3'", "'31'", "'13'", "'165'"]
'2'
'3'
'31'
'13'
'165'
['2', '3', '31', '13', '165']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.</S><S sid ="31" ssid = "21">Due to the substantial differences between existing models of constituent structure  the question arises of how the theory independencf requirement  can be satisfied.</S><S sid ="113" ssid = "26">Since a precise structural description of non-constituent coordination would require a. rich inventory of incomplete phrase types  we have agreed on a sort of unde.rspe.cified representations: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links.</S><S sid ="2" ssid = "2">Since the requirements for such a formalism differ from those posited for configurational languages  several features have been added  influencing the architecture of the scheme.</S><S sid ="41" ssid = "31">Apart from this rather technical problem  two further arguments speak against phrase structure as the structural pivot of the annotation scheme: Finally  the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies  which has proved tedious.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'31'", "'113'", "'2'", "'41'"]
'3'
'31'
'113'
'2'
'41'
['3', '31', '113', '2', '41']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'1'", "'162'", "'74'", "'11'"]
'4'
'1'
'162'
'74'
'11'
['4', '1', '162', '74', '11']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'162'", "'1'", "'6'", "'74'"]
'5'
'162'
'1'
'6'
'74'
['5', '162', '1', '6', '74']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="9" ssid = "6">Section 4 deals with the treatment of selected phenomena.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="160" ssid = "2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'9'", "'5'", "'1'", "'160'"]
'6'
'9'
'5'
'1'
'160'
['6', '9', '5', '1', '160']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">On the basis of these considerations  we formulate several additional requirements.</S><S sid ="37" ssid = "27">In light of these facts  serious difficulties can be expected arising from the structural component of the existing formalisms.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="104" ssid = "17">The head of the phrase can be determined in a similar way according to theory-specific assumptions.</S><S sid ="127" ssid = "8">As the need for certain functionalities becomes obvious with growing annotation experience  we have decided to implement the tool in two stages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'37'", "'5'", "'104'", "'127'"]
'7'
'37'
'5'
'104'
'127'
['7', '37', '5', '104', '127']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">A formalism complying with these requirements is described in section 3.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="9" ssid = "6">Section 4 deals with the treatment of selected phenomena.</S><S sid ="10" ssid = "7">For a description of the annotation tool see section 5.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'6'", "'9'", "'10'", "'5'"]
'8'
'6'
'9'
'10'
'5'
['8', '6', '9', '10', '5']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "6">Section 4 deals with the treatment of selected phenomena.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="8" ssid = "5">A formalism complying with these requirements is described in section 3.</S><S sid ="128" ssid = "9">In the first phase  the main functionality for building and displaying unordered trees is supplied.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'6'", "'5'", "'8'", "'128'"]
'9'
'6'
'5'
'8'
'128'
['9', '6', '5', '8', '128']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">For a description of the annotation tool see section 5.</S><S sid ="128" ssid = "9">In the first phase  the main functionality for building and displaying unordered trees is supplied.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="160" ssid = "2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'128'", "'1'", "'160'", "'6'"]
'10'
'128'
'1'
'160'
'6'
['10', '128', '1', '160', '6']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="12" ssid = "2">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="170" ssid = "12">Our annotation tool supplies efficient manipulation and immediate visualization of argument structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'12'", "'162'", "'1'", "'170'"]
'11'
'12'
'162'
'1'
'170'
['11', '12', '162', '1', '170']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "2">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="143" ssid = "24">Sentences annotated in previous steps are used as training material for further processing.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'11'", "'143'", "'162'", "'1'"]
'12'
'11'
'143'
'162'
'1'
['12', '11', '143', '162', '1']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "3">The data-drivenness of this approach presents a clear advantage over the traditional  idealised notion of competence grammar.</S><S sid ="54" ssid = "44">Such a word order independent representation has the advantage of all structural information being encoded in a single data structure.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="43" ssid = "33">An alternative solution is to make argument structure the main structural component of the formalism.</S><S sid ="44" ssid = "34">This assumption underlies a growing number of recent syntactic theories which give up the context-free constituent backbone  cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'54'", "'162'", "'43'", "'44'"]
'13'
'54'
'162'
'43'
'44'
['13', '54', '162', '43', '44']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "4">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S><S sid ="12" ssid = "2">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="166" ssid = "8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'12'", "'162'", "'4'", "'166'"]
'14'
'12'
'162'
'4'
'166'
['14', '12', '162', '4', '166']
parsed_discourse_facet ['method_citation']



A97-1014
D07-1066
0
aim_citation
['method_citation']
parsing: input/ref/Task1/A00-2030_sweta.csv
<S sid="18" ssid="1">Almost all approaches to information extraction &#8212; even at the sentence level &#8212; are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="100" ssid="5">Given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a While our focus throughout the project was on TE and TR, we became curious about how well the model did at part-of-speech tagging, syntactic parsing, and at name finding.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="15">Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
 <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="1">Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.<
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="43" ssid = "3">Thus  we did not consider simply adding semantic labels to the existing Penn TREEBANK  which is drawn from a single source — the Wall Street Journal — and is impoverished in articles about rocket launches.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'10'", "'5'", "'43'", "'104'"]
'1'
'10'
'5'
'43'
'104'
['1', '10', '5', '43', '104']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S><S sid ="26" ssid = "9">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993)  and more recently  had begun using a generative statistical model for name finding (Bikel et al.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'28'", "'96'", "'26'"]
'2'
'6'
'28'
'96'
'26'
['2', '6', '28', '96', '26']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "1">Since 1995  a few statistical parsing algorithms (Magerman  1995; Collins  1996 and 1997; Charniak  1997; Rathnaparki  1997) demonstrated a breakthrough in parsing accuracy  as measured against the University of Pennsylvania TREEBANK as a gold standard.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'1'", "'104'", "'5'", "'96'"]
'3'
'1'
'104'
'5'
'96'
['3', '1', '104', '5', '96']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "2">Yet  relatively few have embedded one of these algorithms in a task.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="108" ssid = "5">This simple semantic annotation was the only source of task knowledge used to configure the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'105'", "'23'", "'28'", "'108'"]
'4'
'105'
'23'
'28'
'108'
['4', '105', '23', '28', '108']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'106'", "'1'", "'28'", "'10'"]
'5'
'106'
'1'
'28'
'10'
['5', '106', '1', '28', '10']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S><S sid ="2" ssid = "2">In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'104'", "'2'", "'51'", "'1'"]
'6'
'104'
'2'
'51'
'1'
['6', '104', '2', '51', '1']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "5">The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.</S><S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="50" ssid = "10">Figure 4 shows an example of the semantic annotation  which was the only type of manual annotation we performed.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'11'", "'28'", "'50'", "'107'"]
'7'
'11'
'28'
'50'
'107'
['7', '11', '28', '50', '107']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "6">Several technical challenges confronted us and were solved: TREEBANK on Wall Street Journal adequately train the algorithm for New York Times newswire  which includes dozens of newspapers?</S><S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'10'", "'5'", "'51'", "'1'"]
'8'
'10'
'5'
'51'
'1'
['8', '10', '5', '51', '1']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "7">Manually creating sourcespecific training data for syntax was not required.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="108" ssid = "5">This simple semantic annotation was the only source of task knowledge used to configure the model.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S><S sid ="41" ssid = "1">To train our integrated model  we required a large corpus of augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'106'", "'108'", "'107'", "'41'"]
'9'
'106'
'108'
'107'
'41'
['9', '106', '108', '107', '41']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="43" ssid = "3">Thus  we did not consider simply adding semantic labels to the existing Penn TREEBANK  which is drawn from a single source — the Wall Street Journal — and is impoverished in articles about rocket launches.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'1'", "'43'", "'51'", "'5'"]
'10'
'1'
'43'
'51'
'5'
['10', '1', '43', '51', '5']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S><S sid ="7" ssid = "5">The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'7'", "'101'", "'5'", "'96'"]
'11'
'7'
'101'
'5'
'96'
['11', '7', '101', '5', '96']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "2">The Template Element (TE) task identifies organizations  persons  locations  and some artifacts (rocket and airplane-related artifacts).</S><S sid ="98" ssid = "3">In both Template Entity (TE) and Template Relation (TR)  our system finished in second place among all entrants.</S><S sid ="16" ssid = "6">For the following example  the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S><S sid ="78" ssid = "19">If we generalize the tree components (constituent labels  words  tags  etc.) and treat them all as simply elements  e  and treat all the conditioning factors as the history  h  we can write:</S><S sid ="34" ssid = "2">In these trees  the standard TREEBANK structures are augmented to convey semantic information  that is  entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'98'", "'16'", "'78'", "'34'"]
'12'
'98'
'16'
'78'
'34'
['12', '98', '16', '78', '34']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="14" ssid = "4">For each person  one must find all of the person's names within the document  his/her type (civilian or military)  and any significant descriptions (e.g.  titles).</S><S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="58" ssid = "4">Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree  semantic pointer labels are attached to all of the intermediate nodes.</S><S sid ="100" ssid = "5">Given multiple constituents that cover identical spans in the chart  only those constituents with probabilities within a While our focus throughout the project was on TE and TR  we became curious about how well the model did at part-of-speech tagging  syntactic parsing  and at name finding.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'14'", "'15'", "'58'", "'100'"]
'13'
'14'
'15'
'58'
'100'
['13', '14', '15', '58', '100']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "4">For each person  one must find all of the person's names within the document  his/her type (civilian or military)  and any significant descriptions (e.g.  titles).</S><S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S><S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'13'", "'74'", "'15'", "'36'"]
'14'
'13'
'74'
'15'
'36'
['14', '13', '74', '15', '36']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="78" ssid = "19">If we generalize the tree components (constituent labels  words  tags  etc.) and treat them all as simply elements  e  and treat all the conditioning factors as the history  h  we can write:</S><S sid ="91" ssid = "10">The probability of generating a constituent of the specified category  starting at the topmost node.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="55" ssid = "1">syntactic modifier of the other  the inserted node serves to indicate the relation as well as the argument.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'78'", "'91'", "'82'", "'55'"]
'15'
'78'
'91'
'82'
'55'
['15', '78', '91', '82', '55']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "6">For the following example  the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S><S sid ="98" ssid = "3">In both Template Entity (TE) and Template Relation (TR)  our system finished in second place among all entrants.</S><S sid ="12" ssid = "2">The Template Element (TE) task identifies organizations  persons  locations  and some artifacts (rocket and airplane-related artifacts).</S><S sid ="94" ssid = "13">Given a new sentence  the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'74'", "'98'", "'12'", "'94'"]
'16'
'74'
'98'
'12'
'94'
['16', '74', '98', '12', '94']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "7">For the following example  the template relation in Figure 2 was to be generated: &quot;Donald M. Goldstein  a historian at the University of Pittsburgh who helped write...&quot;</S><S sid ="50" ssid = "10">Figure 4 shows an example of the semantic annotation  which was the only type of manual annotation we performed.</S><S sid ="35" ssid = "3">An example of an augmented parse tree is shown in Figure 3.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'50'", "'35'", "'82'", "'56'"]
'17'
'50'
'35'
'82'
'56'
['17', '50', '35', '82', '56']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "1">Almost all approaches to information extraction — even at the sentence level — are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.</S><S sid ="19" ssid = "2">Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S><S sid ="29" ssid = "12">Thus  each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="84" ssid = "3">Although mathematically the model predicts tree elements in a top-down fashion  we search the space bottom-up using a chartbased search.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'19'", "'29'", "'5'", "'84'"]
'18'
'19'
'29'
'5'
'84'
['18', '19', '29', '5', '84']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "2">Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S><S sid ="29" ssid = "12">Thus  each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.</S><S sid ="24" ssid = "7">For this reason  we focused on designing an integrated model in which tagging  namefinding  parsing  and semantic interpretation decisions all have the opportunity to mutually influence each other.</S><S sid ="90" ssid = "9">Thus  the scores used in pruning can be considered as the product of: 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'96'", "'29'", "'24'", "'90'"]
'19'
'96'
'29'
'24'
'90'
['19', '96', '29', '24', '90']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1



A00-2030
W01-0510
0
method_citation
['method_citation']



A00-2030
P04-1054
0
method_citation
['aim_citation']
parsing: input/ref/Task1/P08-1028_aakansha.csv
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="185" ssid="19">The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.</S>
    <S sid="186" ssid="20">The combined model is best overall with &#961; = 0.19.</S>
    <S sid="187" ssid="21">However, the difference between the two models is not statistically significant.</S>
original cit marker offset is 0
new cit marker offset is 0



["'185'", "'186'"]
'185'
'186'
['185', '186']
parsed_discourse_facet ['result_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="21">The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'"]
'48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="53" ssid="1">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.</S><S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'53'", "'57'"]
'53'
'57'
['53', '57']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="16">Although the composition model in (5) is commonly used in the literature, from a linguistic perspective, the model in (6) is more appealing.</S>
    <S sid="69" ssid="17">Simply adding the vectors u and v lumps their contents together rather than allowing the content of one vector to pick out the relevant content of the other.</S>
    <S sid="70" ssid="18">Instead, it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v, and vice versa.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'69'", "'70'"]
'68'
'69'
'70'
['68', '69', '70']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
    <S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'", "'25'"]
'24'
'25'
['24', '25']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="12">Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'"]
'64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="176" ssid="10">The multiplicative and combined models yield means closer to the human ratings.</S>
    <S sid="177" ssid="11">The difference between High and Low similarity values estimated by these models are statistically significant (p &lt; 0.01 using the Wilcoxon rank sum test).</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'", "'177'"]
'176'
'177'
['176', '177']
parsed_discourse_facet ['method_citation']
<S sid="191" ssid="3">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'191'"]
'191'
['191']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1028.csv
<S sid ="1" ssid = "1">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid ="189" ssid = "1">In this paper we presented a general framework for vector-based semantic composition.</S><S sid ="43" ssid = "16">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid ="24" ssid = "20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'189'", "'43'", "'24'", "'34'"]
'1'
'189'
'43'
'24'
'34'
['1', '189', '43', '24', '34']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S><S sid ="190" ssid = "2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="83" ssid = "31">Although we have presented multiplicative and additive models separately  there is nothing inherent in our formulation that disallows their combination.</S><S sid ="26" ssid = "22">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'190'", "'25'", "'83'", "'26'"]
'2'
'190'
'25'
'83'
'26'
['2', '190', '25', '83', '26']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="26" ssid = "22">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S><S sid ="52" ssid = "25">Under this framework  we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="24" ssid = "20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'26'", "'52'", "'25'", "'24'"]
'3'
'26'
'52'
'25'
'24'
['3', '26', '52', '25', '24']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S><S sid ="27" ssid = "23">Our results show that the multiplicative models are superior and correlate significantly with behavioral data.</S><S sid ="176" ssid = "10">The multiplicative and combined models yield means closer to the human ratings.</S><S sid ="26" ssid = "22">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S><S sid ="174" ssid = "8">The simple additive model fails to distinguish between High and Low Similarity items.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'27'", "'176'", "'26'", "'174'"]
'4'
'27'
'176'
'26'
'174'
['4', '27', '176', '26', '174']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.</S><S sid ="9" ssid = "5">In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S><S sid ="19" ssid = "15">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid ="8" ssid = "4">Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch¨utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'9'", "'20'", "'19'", "'8'"]
'5'
'9'
'20'
'19'
'8'
['5', '9', '20', '19', '8']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).</S><S sid ="21" ssid = "17">Central in these models is the notion of compositionality — the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S><S sid ="138" ssid = "51">Model Parameters Irrespectively of their form  all composition models discussed here are based on a semantic space for representing the meanings of individual words.</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S><S sid ="29" ssid = "2">While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'21'", "'138'", "'20'", "'29'"]
'6'
'21'
'138'
'20'
'29'
['6', '21', '138', '20', '29']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">A variety of NLP tasks have made good use of vector-based models.</S><S sid ="43" ssid = "16">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid ="200" ssid = "12">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid ="1" ssid = "1">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'43'", "'200'", "'1'", "'42'"]
'7'
'43'
'200'
'1'
'42'
['7', '43', '200', '1', '42']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch¨utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).</S><S sid ="19" ssid = "15">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid ="5" ssid = "1">Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.</S><S sid ="193" ssid = "5">Previous applications of vector addition to document indexing (Deerwester et al.  1990) or essay grading (Landauer et al.  1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'19'", "'5'", "'193'", "'20'"]
'8'
'19'
'5'
'193'
'20'
['8', '19', '5', '193', '20']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).</S><S sid ="5" ssid = "1">Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.</S><S sid ="19" ssid = "15">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid ="148" ssid = "61">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid ="193" ssid = "5">Previous applications of vector addition to document indexing (Deerwester et al.  1990) or essay grading (Landauer et al.  1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'5'", "'19'", "'148'", "'193'"]
'9'
'5'
'19'
'148'
'193'
['9', '5', '19', '148', '193']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S><S sid ="141" ssid = "54">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S><S sid ="163" ssid = "76">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S><S sid ="191" ssid = "3">Despite the popularity of additive models  our experimental results showed the superiority of models utilizing multiplicative combinations  at least for the sentence similarity task attempted here.</S><S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'141'", "'163'", "'191'", "'108'"]
'10'
'141'
'163'
'191'
'108'
['10', '141', '163', '191', '108']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">Despite their widespread use  vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature.</S><S sid ="33" ssid = "6">In contrast  symbolic representations can naturally handle the binding of constituents to their roles  in a systematic manner that avoids both these problems.</S><S sid ="6" ssid = "2">The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).</S><S sid ="29" ssid = "2">While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S><S sid ="95" ssid = "8">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'33'", "'6'", "'29'", "'95'"]
'11'
'33'
'6'
'29'
'95'
['11', '33', '6', '29', '95']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">In fact  the commonest method for combining the vectors is to average them.</S><S sid ="77" ssid = "25">For additive models  a natural way to achieve this is to include further vectors into the summation.</S><S sid ="43" ssid = "16">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'77'", "'43'", "'34'", "'39'"]
'12'
'77'
'43'
'34'
'39'
['12', '77', '43', '34', '39']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">Vector averaging is unfortunately insensitive to word order  and more generally syntactic structure  giving the same representation to any constructions that happen to share the same vocabulary.</S><S sid ="6" ssid = "2">The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).</S><S sid ="46" ssid = "19">However  since it is order independent  it cannot capture meaning differences that are modulated by differences in syntactic structure.</S><S sid ="93" ssid = "6">The idea here is that an appropriate composition model when applied to horse and ran will yield a vector closer to the landmark gallop than dissolve.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'6'", "'46'", "'93'", "'194'"]
'13'
'6'
'46'
'93'
'194'
['13', '6', '46', '93', '194']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">This is illustrated in the example below taken from Landauer et al. (1997).</S><S sid ="175" ssid = "9">We observe a similar pattern for the non compositional baseline model  the weighted additive model and Kintsch (2001).</S><S sid ="179" ssid = "13">The results of our correlation analysis are also given in Table 2.</S><S sid ="55" ssid = "3">A hypothetical semantic space is illustrated in Figure 1.</S><S sid ="88" ssid = "1">We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'175'", "'179'", "'55'", "'88'"]
'14'
'175'
'179'
'55'
'88'
['14', '175', '179', '55', '88']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">Sentences (1-a) and (1-b) contain exactly the same set of words but their meaning is entirely different.</S><S sid ="22" ssid = "18">Here  semantic analysis is guided by syntactic structure  and therefore sentences (1-a) and (1-b) receive distinct representations.</S><S sid ="56" ssid = "4">Here  the space has only five dimensions  and the matrix cells denote the co-occurrence of the target words (horse and run) with the context words animal  stable  and so on.</S><S sid ="79" ssid = "27">When modeling predicate-argument structures  Kintsch (2001) proposes including one or more distributional neighbors  n  of the predicate: Note that considerable latitude is allowed in selecting the appropriate neighbors.</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'22'", "'56'", "'79'", "'20'"]
'15'
'22'
'56'
'79'
'20'
['15', '22', '56', '79', '20']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">(1) a.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="15" ssid = "11">Sentences (1-a) and (1-b) contain exactly the same set of words but their meaning is entirely different.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="22" ssid = "18">Here  semantic analysis is guided by syntactic structure  and therefore sentences (1-a) and (1-b) receive distinct representations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'150'", "'15'", "'34'", "'22'"]
'16'
'150'
'15'
'34'
'22'
['16', '150', '15', '34', '22']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">It was not the sales manager who hit the bottle that day  but the office worker with the serious drinking problem. b.</S><S sid ="18" ssid = "14">That day the office manager  who was drinking  hit the problem sales worker with a bottle  but it was not serious.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S><S sid ="174" ssid = "8">The simple additive model fails to distinguish between High and Low Similarity items.</S><S sid ="122" ssid = "35">The opposite is the case for the reference The face glowed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'18'", "'164'", "'174'", "'122'"]
'17'
'18'
'164'
'174'
'122'
['17', '18', '164', '174', '122']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P05-1013_swastika.csv
<S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



['49']
49
['49']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="6">In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
<S sid="38" ssid="9">Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="6">In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
Error in Reference Offset
S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
Error in Reference Offset
    <S sid="86" ssid="13">As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
 <S sid="51" ssid="22">In the second scheme, Head+Path, we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p&#8595;.</S>
original cit marker offset is 0
new cit marker offset is 0



['51']
51
['51']
parsed_discourse_facet ['method_citation']
<S sid="99" ssid="10">This may seem surprising, given the experiments reported in section 4, but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift, where the encoding of path information is often redundant.</S>
original cit marker offset is 0
new cit marker offset is 0



['99']
99
['99']
parsed_discourse_facet ['result_citation']
<S sid="7" ssid="3">From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



['2']
2
['2']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/P05-1013.csv
<S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="13" ssid = "9">The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="16" ssid = "12">Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.</S><S sid ="80" ssid = "7">As shown in Table 3  the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'13'", "'7'", "'16'", "'80'"]
'1'
'13'
'7'
'16'
'80'
['1', '13', '7', '16', '80']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="44" ssid = "15">Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="30" ssid = "1">We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.</S><S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'44'", "'36'", "'30'", "'8'"]
'2'
'44'
'36'
'30'
'8'
['2', '44', '36', '30', '8']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy.</S><S sid ="28" ssid = "24">First  in section 4  we evaluate the graph transformation techniques in themselves  with data from the Prague Dependency Treebank and the Danish Dependency Treebank.</S><S sid ="29" ssid = "25">In section 5  we then evaluate the entire parsing system by training and evaluating on data from the Prague Dependency Treebank.</S><S sid ="62" ssid = "1">In the experiments below  we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs 3 previously tested on Swedish (Nivre et al.  2004) and English (Nivre and Scholz  2004).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'28'", "'29'", "'62'", "'75'"]
'3'
'28'
'29'
'62'
'75'
['3', '28', '29', '62', '75']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'104'", "'106'", "'78'", "'8'"]
'4'
'104'
'106'
'78'
'8'
['4', '104', '106', '78', '8']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order  where discontinuous syntactic constructions are more common than in languages like English (Mel’ˇcuk  1988; Covington  1990).</S><S sid ="94" ssid = "5">The first thing to note is that projectivizing helps in itself  even if no encoding is used  as seen from the fact that the projective baseline outperforms the non-projective training condition by more than half a percentage point on attachment score  although the gain is much smaller with respect to exact match.</S><S sid ="99" ssid = "10">This may seem surprising  given the experiments reported in section 4  but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift  where the encoding of path information is often redundant.</S><S sid ="9" ssid = "5">This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).</S><S sid ="47" ssid = "18">In practice  we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'94'", "'99'", "'9'", "'47'"]
'5'
'94'
'99'
'9'
'47'
['5', '94', '99', '9', '47']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.</S><S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="44" ssid = "15">Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.</S><S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'2'", "'36'", "'44'", "'89'"]
'6'
'2'
'36'
'44'
'89'
['6', '2', '36', '44', '89']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="16" ssid = "12">Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.</S><S sid ="19" ssid = "15">Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).</S><S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="47" ssid = "18">In practice  we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'16'", "'19'", "'1'", "'47'"]
'7'
'16'
'19'
'1'
'47'
['7', '16', '19', '1', '47']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S><S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="30" ssid = "1">We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'2'", "'104'", "'4'", "'30'"]
'8'
'2'
'104'
'4'
'30'
['8', '2', '104', '4', '30']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).</S><S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="17" ssid = "13">There exist a few robust broad-coverage parsers that produce non-projective dependency structures  notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English  Foth et al. (2004) for German  and Holan (2004) for Czech.</S><S sid ="105" ssid = "16">Although the best published results for the Collins parser is 80% UAS (Collins  1999)  this parser reaches 82% when trained on the entire training data set  and an adapted version of Charniak’s parser (Charniak  2000) performs at 84% (Jan Hajiˇc  pers. comm.).</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'10'", "'17'", "'105'", "'24'"]
'9'
'10'
'17'
'105'
'24'
['9', '10', '17', '105', '24']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="42" ssid = "13">Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="12" ssid = "8">Prague Dependency Treebank (Hajiˇc et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'42'", "'36'", "'4'", "'12'"]
'10'
'42'
'36'
'4'
'12'
['10', '42', '36', '4', '12']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="6" ssid = "2">However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'6'", "'78'", "'10'", "'36'"]
'11'
'6'
'78'
'10'
'36'
['11', '6', '78', '10', '36']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Prague Dependency Treebank (Hajiˇc et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S><S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="42" ssid = "13">Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'10'", "'36'", "'42'", "'78'"]
'12'
'10'
'36'
'42'
'78'
['12', '10', '36', '42', '78']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.</S><S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'2'", "'95'", "'1'", "'104'"]
'13'
'2'
'95'
'1'
'104'
['13', '2', '95', '1', '104']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 15–25%  the total proportion of non-projective arcs is normally only 1–2%.</S><S sid ="80" ssid = "7">As shown in Table 3  the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S><S sid ="100" ssid = "11">It is likely that the more complex cases  where path information could make a difference  are beyond the reach of the parser in most cases.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'80'", "'106'", "'81'", "'100'"]
'14'
'80'
'106'
'81'
'100'
['14', '80', '106', '81', '100']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">As long as the main evaluation metric is dependency accuracy per word  with state-of-the-art accuracy mostly below 90%  the penalty for not handling non-projective constructions is almost negligible.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="101" ssid = "12">However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.</S><S sid ="110" ssid = "2">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="105" ssid = "16">Although the best published results for the Collins parser is 80% UAS (Collins  1999)  this parser reaches 82% when trained on the entire training data set  and an adapted version of Charniak’s parser (Charniak  2000) performs at 84% (Jan Hajiˇc  pers. comm.).</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'106'", "'101'", "'110'", "'105'"]
'15'
'106'
'101'
'110'
'105'
['15', '106', '101', '110', '105']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="110" ssid = "2">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="19" ssid = "15">Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'7'", "'1'", "'110'", "'19'"]
'16'
'7'
'1'
'110'
'19'
['16', '7', '1', '110', '19']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'104'", "'106'", "'78'", "'8'"]
'4'
'104'
'106'
'78'
'8'
['4', '104', '106', '78', '8']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">In addition  there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington  1990; Kahane et al.  1998; Duchier and Debusmann  2001; Holan et al.  2001; Hellwig  2003).</S><S sid ="17" ssid = "13">There exist a few robust broad-coverage parsers that produce non-projective dependency structures  notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English  Foth et al. (2004) for German  and Holan (2004) for Czech.</S><S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S><S sid ="100" ssid = "11">It is likely that the more complex cases  where path information could make a difference  are beyond the reach of the parser in most cases.</S><S sid ="109" ssid = "1">We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'17'", "'20'", "'100'", "'109'"]
'18'
'17'
'20'
'100'
'109'
['18', '17', '20', '100', '109']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "15">Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="13" ssid = "9">The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.</S><S sid ="107" ssid = "18">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'7'", "'95'", "'13'", "'107'"]
'19'
'7'
'95'
'13'
'107'
['19', '7', '95', '13', '107']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S><S sid ="109" ssid = "1">We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'109'", "'24'", "'60'", "'95'"]
'20'
'109'
'24'
'60'
'95'
['20', '109', '24', '60', '95']
parsed_discourse_facet ['aim_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/A00-2018_vardha.csv
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'91'"]
'91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="8">In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'"]
'39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="162" ssid="53">Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'"]
'162'
['162']
parsed_discourse_facet ['method_citation']
 <S sid="87" ssid="56">In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17].</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'"]
'87'
['87']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'91'"]
'91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="176" ssid="3">That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'"]
'176'
['176']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['method_citation']
 <S sid="101" ssid="12">In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning).</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'"]
'101'
['101']
parsed_discourse_facet ['method_citation']
<S sid="155" ssid="46">For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
  <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="162" ssid="53">Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'"]
'162'
['162']
parsed_discourse_facet ['method_citation']
 <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
 <S sid="79" ssid="48">We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
    <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
    <S sid="175" ssid="2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["'175'"]
'175'
['175']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2018.csv
<S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'5'", "'2'", "'6'", "'125'"]
'1'
'5'
'2'
'6'
'125'
['1', '5', '2', '6', '125']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="175" ssid = "2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'1'", "'125'", "'175'"]
'2'
'6'
'1'
'125'
'175'
['2', '6', '1', '125', '175']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.</S><S sid ="182" ssid = "9">As noted above  the main methodological innovation presented here is our &quot;maximumentropy-inspired&quot; model for conditioning and smoothing.</S><S sid ="128" ssid = "19">The model labeled &quot;Old&quot; attempts to recreate the Char97 system using the current program.</S><S sid ="118" ssid = "9">First  it uses a clustering scheme on words to give the system a &quot;soft&quot; clustering of heads and sub-heads.</S><S sid ="116" ssid = "7">This is as opposed to the &quot;Markovgrammar&quot; approach used in the current parser.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'182'", "'128'", "'118'", "'116'"]
'3'
'182'
'128'
'118'
'116'
['3', '182', '128', '118', '116']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid ="139" ssid = "30">In Figure 2 we show that this one factor improves performance by nearly 2%.</S><S sid ="164" ssid = "55">When we do so using our maximum-entropy-inspired conditioning  we get another 0.45% improvement in average precision/recall  as indicated in Figure 2 on the line labeled &quot;MaxEnt-Inspired'.</S><S sid ="17" ssid = "6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S><S sid ="94" ssid = "5">We use the gathered statistics for all observed words  even those with very low counts  though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'139'", "'164'", "'17'", "'94'"]
'4'
'139'
'164'
'17'
'94'
['4', '139', '164', '17', '94']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'2'", "'6'", "'7'"]
'5'
'1'
'2'
'6'
'7'
['5', '1', '2', '6', '7']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="175" ssid = "2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'1'", "'125'", "'175'"]
'2'
'6'
'1'
'125'
'175'
['2', '6', '1', '125', '175']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid ="171" ssid = "62">As already noted  our best model uses a Markov-grammar approach.</S><S sid ="90" ssid = "1">We created a parser based upon the maximumentropy-inspired model of the last section  smoothed using standard deleted interpolation.</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'171'", "'90'", "'1'", "'5'"]
'7'
'171'
'90'
'1'
'5'
['7', '171', '90', '1', '5']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">That is  for all sentences s and all parses 7r  the parser assigns a probability p(s   7r) = p(r)  the equality holding when we restrict consideration to 7r whose yield * This research was supported in part by NSF grant LIS SBR 9720368.</S><S sid ="12" ssid = "1">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).</S><S sid ="10" ssid = "6">That is  the parser implements the function arg maxrp(7r s) = arg maxirp(7r  s) = arg maxrp(w).</S><S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S><S sid ="147" ssid = "38">For example  when we first guess the lexical head we can move from computing p(r I 1 1p  h) to p(r I /  t  /p  h).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'12'", "'10'", "'41'", "'147'"]
'8'
'12'
'10'
'41'
'147'
['8', '12', '10', '41', '147']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.</S><S sid ="185" ssid = "12">We expect that as we experiment with other  more semantic conditioning information  the importance of this aspect of the model will increase.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="171" ssid = "62">As already noted  our best model uses a Markov-grammar approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'185'", "'5'", "'1'", "'171'"]
'9'
'185'
'5'
'1'
'171'
['9', '185', '5', '1', '171']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">That is  the parser implements the function arg maxrp(7r s) = arg maxirp(7r  s) = arg maxrp(w).</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid ="83" ssid = "52">In the simple (content-free) form (Equation 6)  it is clear that Z(H) = 1.</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="56" ssid = "25">In max-entropy models one can simply include features for all three events f1 (a  b  c)  f2 (a  b)  and f3(a  c) and combine them in the model according to Equation 3  or equivalently  Equation 4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'13'", "'83'", "'54'", "'56'"]
'10'
'13'
'83'
'54'
'56'
['10', '13', '83', '54', '56']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">What fundamentally distinguishes probabilistic generative parsers is how they compute p(r)  and it is to that topic we turn next.</S><S sid ="53" ssid = "22">That is  suppose we want to compute a conditional probability p(a b  c)  but we are not sure that we have enough examples of the conditioning event b  c in the training corpus to ensure that the empirically obtained probability P (a I b  c) is accurate.</S><S sid ="82" ssid = "51">The major advantage of using Equation 7 is that one can generally get away without computing the partition function Z(H).</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="188" ssid = "15">Ultimately it is this flexibility that let us try the various conditioning events  to move on to a Markov grammar approach  and to try several Markov grammars of different orders  without significant programming.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'53'", "'82'", "'54'", "'188'"]
'11'
'53'
'82'
'54'
'188'
['11', '53', '82', '54', '188']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "1">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).</S><S sid ="18" ssid = "7">The method that gives the best results  however  uses a Markov grammar — a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6 10 15].</S><S sid ="134" ssid = "25">As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.</S><S sid ="147" ssid = "38">For example  when we first guess the lexical head we can move from computing p(r I 1 1p  h) to p(r I /  t  /p  h).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'18'", "'134'", "'147'", "'33'"]
'12'
'18'
'134'
'147'
'33'
['12', '18', '134', '147', '33']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid ="16" ssid = "5">In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.</S><S sid ="45" ssid = "14">The function Z(H)  called the partition function  is a normalizing constant (for fixed H)  so the probabilities over all a sum to one.</S><S sid ="83" ssid = "52">In the simple (content-free) form (Equation 6)  it is clear that Z(H) = 1.</S><S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'16'", "'45'", "'83'", "'41'"]
'13'
'16'
'45'
'83'
'41'
['13', '16', '45', '83', '41']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "3">Much of the interesting work is determining what goes into H (c).</S><S sid ="73" ssid = "42">We make one more point on the connection of Equation 7 to a maximum entropy formulation.</S><S sid ="60" ssid = "29">But let us look at how it works for a particular case in our parsing scheme.</S><S sid ="159" ssid = "50">Something very much like this is done in [15].</S><S sid ="151" ssid = "42">We have already noted the importance of conditioning on the parent label /p.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'73'", "'60'", "'159'", "'151'"]
'14'
'73'
'60'
'159'
'151'
['14', '73', '60', '159', '151']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "4">Whenever it is clear to which constituent we are referring we omit the (c) in  e.g.  h(c).</S><S sid ="67" ssid = "36">For example  it does not seem to make much sense to condition on  say  hp without first conditioning on ti .</S><S sid ="151" ssid = "42">We have already noted the importance of conditioning on the parent label /p.</S><S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="140" ssid = "31">It may not be obvious why this should make so great a difference  since most words are effectively unambiguous.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'67'", "'151'", "'157'", "'140'"]
'15'
'67'
'151'
'157'
'140'
['15', '67', '151', '157', '140']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "5">In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.</S><S sid ="65" ssid = "34">We can now rewrite this in the form of Equation 5 as follows: Here we have sequentially conditioned on steadily increasing portions of c's history.</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid ="58" ssid = "27">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid ="46" ssid = "15">Now for our purposes it is useful to rewrite this as a sequence of multiplicative functions gi(a  H) for 0 < i < j: Here go(a  H) = 11Z (H) and gi(a  H) = eAi(a H) fi(° 11).</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'65'", "'13'", "'58'", "'46'"]
'16'
'65'
'13'
'58'
'46'
['16', '65', '13', '58', '46']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



A00-2018
N10-1002
0
method_citation
['aim_citation']
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P11-1060_swastika.csv
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['aim_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['aim_citation']
    <S sid="148" ssid="33">This bootstrapping behavior occurs naturally: The &#8220;easy&#8221; examples are processed first, where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
    <S sid="13" ssid="9">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['result_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="47" ssid="23">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
S sid="44" ssid="20">The recurrence is as follows: At each node, we compute the set of tuples v consistent with the predicate at that node (v &#8712; w(p)), and S(x)}, where a set of pairs S is treated as a set-valued function S(x) = {y : (x, y) &#8712; S} with domain S1 = {x : (x, y) &#8712; S}.</S>
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
Error in Reference Offset
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



['106']
106
['106']
parsed_discourse_facet ['aim_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
    <S sid="172" ssid="57">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms, we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus, and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S>
original cit marker offset is 0
new cit marker offset is 0



['172']
172
['172']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



['171']
171
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S sid ="1" ssid = "1">Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="2" ssid = "2">In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'141'", "'15'", "'6'", "'2'"]
'1'
'141'
'15'
'6'
'2'
['1', '141', '15', '6', '2']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="117" ssid = "2">In each dataset  each sentence x is annotated with a Prolog logical form  which we use only to evaluate and get an answer y.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="149" ssid = "34">Think of DCS as a higher-level Our system learns lexical associations between programming language tailored to natural language  words and predicates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'3'", "'117'", "'100'", "'149'"]
'2'
'3'
'117'
'100'
'149'
['2', '3', '117', '100', '149']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="48" ssid = "24">In addition  trees enable efficient computation  thereby establishing a new connection between dependency syntax and efficient semantic evaluation.</S><S sid ="149" ssid = "34">Think of DCS as a higher-level Our system learns lexical associations between programming language tailored to natural language  words and predicates.</S><S sid ="2" ssid = "2">In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.</S><S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parent’s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'48'", "'149'", "'2'", "'51'"]
'3'
'48'
'149'
'2'
'51'
['3', '48', '149', '2', '51']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">On two stansemantic parsing benchmarks our system obtains the highest published accuracies  despite requiring no annotated logical forms.</S><S sid ="24" ssid = "20">Our system outperforms all existing systems despite using no annotated logical forms.</S><S sid ="136" ssid = "21">In fact  DCS performs comparably to even the version of SEMRESP trained using logical forms.</S><S sid ="139" ssid = "24">All other systems require logical forms as training data  whereas ours does not.</S><S sid ="135" ssid = "20">SEMRESP requires a lexicon of 1.42 words per non-value predicate  WordNet features  and syntactic parse trees; DCS requires only words for the domain-independent predicates (overall  around 0.5 words per non-value predicate)  POS tags  and very simple indicator features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'24'", "'136'", "'139'", "'135'"]
'4'
'24'
'136'
'139'
'135'
['4', '24', '136', '139', '135']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">What is the total population of the ten largest capitals in the US?</S><S sid ="52" ssid = "28">The denotation of the middle node is {s}  where s is all major cities.</S><S sid ="105" ssid = "81">The feature vector φ(x  z) is defined by sums of five simple indicator feature templates: (F1) a word triggers a predicate (e.g.  [city  city]); (F2) a word is under a relation (e.g.  [that  11]); (F3) a word is under a trace predicate (e.g.  [in  loc]); (F4) two predicates are linked via a relation in the left or right direction (e.g.  [city 11  loc  RIGHT]); and (F5) a predicate has a child relation (e.g.  [city  11]).</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S><S sid ="50" ssid = "26">For example  consider the phrase number of major cities  and suppose that number corresponds to the count predicate.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'52'", "'105'", "'62'", "'50'"]
'5'
'52'
'105'
'62'
'50'
['5', '52', '105', '62', '50']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="48" ssid = "24">In addition  trees enable efficient computation  thereby establishing a new connection between dependency syntax and efficient semantic evaluation.</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'169'", "'58'", "'48'", "'165'"]
'6'
'169'
'58'
'48'
'165'
['6', '169', '58', '48', '165']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Supervised semantic parsers (Zelle and Mooney  1996; Tang and Mooney  2001; Ge and Mooney  2005; Zettlemoyer and Collins  2005; Kate and Mooney  2007; Zettlemoyer and Collins  2007; Wong and Mooney  2007; Kwiatkowski et al.  2010) rely on manual annotation of logical forms  which is expensive.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="166" ssid = "51">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid ="125" ssid = "10">We used the same training-test splits as Zettlemoyer and Collins (2005) (600+280 for GEO and 500+140 for JOBS).</S><S sid ="161" ssid = "46">CCG (Steedman  2000)  in which semantic pars- The integration of natural language with denotaing is driven from the lexicon.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'8'", "'166'", "'125'", "'161'"]
'7'
'8'
'166'
'125'
'161'
['7', '8', '166', '125', '161']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="138" ssid = "23">Next  we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S><S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid ="18" ssid = "14">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'138'", "'9'", "'165'", "'18'"]
'8'
'138'
'9'
'165'
'18'
['8', '138', '9', '165', '18']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="20" ssid = "16">At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'58'", "'165'", "'26'", "'20'"]
'9'
'58'
'165'
'26'
'20'
['9', '58', '165', '26', '20']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="37" ssid = "13">The basic version of DCS restricts R to join and aggregate relations (see Table 1).</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'58'", "'169'", "'37'", "'26'"]
'10'
'58'
'169'
'37'
'26'
['10', '58', '169', '37', '26']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts)  producing an answer y.</S><S sid ="118" ssid = "3">This evaluation is done with respect to a world w. Recall that a world w maps each predicate p ∈ P to a set of tuples w(p).</S><S sid ="31" ssid = "7">Conceptually  a world is a relational database where each predicate is a relation (possibly infinite).</S><S sid ="38" ssid = "14">Let us start by considering a DCS tree z with only join relations.</S><S sid ="29" ssid = "5">Let P be a set of predicates (e.g.  state  count ∈ P)  which are just symbols.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'118'", "'31'", "'38'", "'29'"]
'11'
'118'
'31'
'38'
'29'
['11', '118', '31', '38', '29']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="45" ssid = "21">The logical forms in DCS are called DCS trees  where nodes are labeled with predicates  and edges are labeled with relations.</S><S sid ="32" ssid = "8">Define a special predicate ø with w(ø) = V. We represent functions by a set of inputoutput pairs  e.g.  w(count) = {(S  n) : n = |S|}.</S><S sid ="85" ssid = "61">Extraction allows us to return the set of consistent values of a marked non-root node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'169'", "'45'", "'32'", "'85'"]
'12'
'169'
'45'
'32'
'85'
['12', '169', '45', '32', '85']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x  y)  which is much cheaper to obtain than (x  z) pairs.</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S><S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="103" ssid = "79">To define the features  we technically need to augment each tree z ∈ ZL(x) with alignment information—namely  for each predicate in z  the span in x (if any) that triggered it.</S><S sid ="108" ssid = "84">However  in order to learn  we need to sum over {z ∈ ZL(x) : JzKw = y}  and unfortunately  the additional constraint JzKw = y does not factorize.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'62'", "'12'", "'103'", "'108'"]
'13'
'62'
'12'
'103'
'108'
['13', '62', '12', '103', '108']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.</S><S sid ="43" ssid = "19">Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter  2003).</S><S sid ="107" ssid = "83">Our model is arc-factored  so we can sum over all DCS trees in ZL(x) using dynamic programming.</S><S sid ="47" ssid = "23">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.</S><S sid ="59" ssid = "35">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'43'", "'107'", "'47'", "'59'"]
'14'
'43'
'107'
'47'
'59'
['14', '43', '107', '47', '59']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="142" ssid = "27">This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.</S><S sid ="163" ssid = "48">Feedback from the context; for example  the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x  y)  which means (Schuler  2003) and semantic parsing (Popescu et borders looks right for the first argument and left al.  2003; Clarke et al.  2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'100'", "'141'", "'142'", "'163'"]
'15'
'100'
'141'
'142'
'163'
['15', '100', '141', '142', '163']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Which one should we use?</S><S sid ="144" ssid = "29">Intuitions How is our system learning?</S><S sid ="142" ssid = "27">This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.</S><S sid ="95" ssid = "71">Our first question is: given notation of the DCS tree before execution is an utterance x  what trees z ∈ Z are permissible?</S><S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'144'", "'142'", "'95'", "'15'"]
'16'
'144'
'142'
'95'
'15'
['16', '144', '142', '95', '15']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">The dominant paradigm in compositional semantics is Montague semantics  which constructs lambda calculus forms in a bottom-up manner.</S><S sid ="36" ssid = "12">It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.</S><S sid ="37" ssid = "13">The basic version of DCS restricts R to join and aggregate relations (see Table 1).</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="25" ssid = "1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS)  which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'36'", "'37'", "'58'", "'25'"]
'17'
'36'
'37'
'58'
'25'
['17', '36', '37', '58', '25']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="20" ssid = "16">At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.</S><S sid ="167" ssid = "52">In DCS  we start with lexical triggers  which are 6 Conclusion more basic than CCG lexical entries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'165'", "'26'", "'20'", "'167'"]
'18'
'165'
'26'
'20'
'167'
['18', '165', '26', '20', '167']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "15">However  the logical forms there can become quite complex  and in the context of program induction  this would lead to an unwieldy search space.</S><S sid ="122" ssid = "7">For JOBS  if we use the standard Jobs database  close to half the y’s are empty  which makes it uninteresting.</S><S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parent’s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid ="14" ssid = "10">The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.</S><S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'122'", "'51'", "'14'", "'9'"]
'19'
'122'
'51'
'14'
'9'
['19', '122', '51', '14', '9']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P08-1028_sweta.csv
<S sid="65" ssid="13">So, if we assume that only the ith components of u and v contribute to the ith component of p, that these components are not dependent on i, and that the function is symmetric with regard to the interchange of u and v, we obtain a simpler instantiation of an additive model: Analogously, under the same assumptions, we obtain the following simpler multiplicative model: only the ith components of u and v contribute to the ith component of p. Another class of models can be derived by relaxing this constraint.</S>
original cit marker offset is 0
new cit marker offset is 0



["65'"]
65'
['65']
parsed_discourse_facet ['method_citation']
 <S sid="75" ssid="23">An extreme form of this differential in the contribution of constituents is where one of the vectors, say u, contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic, however it can serve as a simple baseline against which to compare more sophisticated models.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["42'"]
42'
['42']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="17">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="195" ssid="7">The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately.</S>
original cit marker offset is 0
new cit marker offset is 0



["195'"]
195'
['195']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["57'"]
57'
['57']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["190'"]
190'
['190']
parsed_discourse_facet ['method_citation']
 <S sid="29" ssid="2">While neural networks can readily represent single distinct objects, in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>
original cit marker offset is 0
new cit marker offset is 0



["29'"]
29'
['29']
parsed_discourse_facet ['method_citation']
    <S sid="38" ssid="11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="12">Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="163" ssid="76">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["163'"]
163'
['163']
parsed_discourse_facet ['method_citation']
<S sid="185" ssid="19">The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.</S>
original cit marker offset is 0
new cit marker offset is 0



["185'"]
185'
['185']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["190'"]
190'
['190']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1028.csv
<S sid ="1" ssid = "1">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid ="189" ssid = "1">In this paper we presented a general framework for vector-based semantic composition.</S><S sid ="43" ssid = "16">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid ="24" ssid = "20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'189'", "'43'", "'24'", "'34'"]
'1'
'189'
'43'
'24'
'34'
['1', '189', '43', '24', '34']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S><S sid ="190" ssid = "2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="83" ssid = "31">Although we have presented multiplicative and additive models separately  there is nothing inherent in our formulation that disallows their combination.</S><S sid ="26" ssid = "22">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'190'", "'25'", "'83'", "'26'"]
'2'
'190'
'25'
'83'
'26'
['2', '190', '25', '83', '26']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="26" ssid = "22">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S><S sid ="52" ssid = "25">Under this framework  we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="24" ssid = "20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'26'", "'52'", "'25'", "'24'"]
'3'
'26'
'52'
'25'
'24'
['3', '26', '52', '25', '24']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S><S sid ="27" ssid = "23">Our results show that the multiplicative models are superior and correlate significantly with behavioral data.</S><S sid ="176" ssid = "10">The multiplicative and combined models yield means closer to the human ratings.</S><S sid ="26" ssid = "22">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S><S sid ="174" ssid = "8">The simple additive model fails to distinguish between High and Low Similarity items.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'27'", "'176'", "'26'", "'174'"]
'4'
'27'
'176'
'26'
'174'
['4', '27', '176', '26', '174']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.</S><S sid ="9" ssid = "5">In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S><S sid ="19" ssid = "15">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid ="8" ssid = "4">Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch¨utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'9'", "'20'", "'19'", "'8'"]
'5'
'9'
'20'
'19'
'8'
['5', '9', '20', '19', '8']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).</S><S sid ="21" ssid = "17">Central in these models is the notion of compositionality — the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S><S sid ="138" ssid = "51">Model Parameters Irrespectively of their form  all composition models discussed here are based on a semantic space for representing the meanings of individual words.</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S><S sid ="29" ssid = "2">While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'21'", "'138'", "'20'", "'29'"]
'6'
'21'
'138'
'20'
'29'
['6', '21', '138', '20', '29']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">A variety of NLP tasks have made good use of vector-based models.</S><S sid ="43" ssid = "16">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid ="200" ssid = "12">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid ="1" ssid = "1">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'43'", "'200'", "'1'", "'42'"]
'7'
'43'
'200'
'1'
'42'
['7', '43', '200', '1', '42']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch¨utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).</S><S sid ="19" ssid = "15">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid ="5" ssid = "1">Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.</S><S sid ="193" ssid = "5">Previous applications of vector addition to document indexing (Deerwester et al.  1990) or essay grading (Landauer et al.  1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'19'", "'5'", "'193'", "'20'"]
'8'
'19'
'5'
'193'
'20'
['8', '19', '5', '193', '20']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).</S><S sid ="5" ssid = "1">Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.</S><S sid ="19" ssid = "15">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid ="148" ssid = "61">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid ="193" ssid = "5">Previous applications of vector addition to document indexing (Deerwester et al.  1990) or essay grading (Landauer et al.  1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'5'", "'19'", "'148'", "'193'"]
'9'
'5'
'19'
'148'
'193'
['9', '5', '19', '148', '193']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S><S sid ="141" ssid = "54">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S><S sid ="163" ssid = "76">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S><S sid ="191" ssid = "3">Despite the popularity of additive models  our experimental results showed the superiority of models utilizing multiplicative combinations  at least for the sentence similarity task attempted here.</S><S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'141'", "'163'", "'191'", "'108'"]
'10'
'141'
'163'
'191'
'108'
['10', '141', '163', '191', '108']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">Despite their widespread use  vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature.</S><S sid ="33" ssid = "6">In contrast  symbolic representations can naturally handle the binding of constituents to their roles  in a systematic manner that avoids both these problems.</S><S sid ="6" ssid = "2">The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).</S><S sid ="29" ssid = "2">While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S><S sid ="95" ssid = "8">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'33'", "'6'", "'29'", "'95'"]
'11'
'33'
'6'
'29'
'95'
['11', '33', '6', '29', '95']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">In fact  the commonest method for combining the vectors is to average them.</S><S sid ="77" ssid = "25">For additive models  a natural way to achieve this is to include further vectors into the summation.</S><S sid ="43" ssid = "16">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'77'", "'43'", "'34'", "'39'"]
'12'
'77'
'43'
'34'
'39'
['12', '77', '43', '34', '39']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">Vector averaging is unfortunately insensitive to word order  and more generally syntactic structure  giving the same representation to any constructions that happen to share the same vocabulary.</S><S sid ="6" ssid = "2">The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).</S><S sid ="46" ssid = "19">However  since it is order independent  it cannot capture meaning differences that are modulated by differences in syntactic structure.</S><S sid ="93" ssid = "6">The idea here is that an appropriate composition model when applied to horse and ran will yield a vector closer to the landmark gallop than dissolve.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'6'", "'46'", "'93'", "'194'"]
'13'
'6'
'46'
'93'
'194'
['13', '6', '46', '93', '194']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">This is illustrated in the example below taken from Landauer et al. (1997).</S><S sid ="175" ssid = "9">We observe a similar pattern for the non compositional baseline model  the weighted additive model and Kintsch (2001).</S><S sid ="179" ssid = "13">The results of our correlation analysis are also given in Table 2.</S><S sid ="55" ssid = "3">A hypothetical semantic space is illustrated in Figure 1.</S><S sid ="88" ssid = "1">We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'175'", "'179'", "'55'", "'88'"]
'14'
'175'
'179'
'55'
'88'
['14', '175', '179', '55', '88']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">Sentences (1-a) and (1-b) contain exactly the same set of words but their meaning is entirely different.</S><S sid ="22" ssid = "18">Here  semantic analysis is guided by syntactic structure  and therefore sentences (1-a) and (1-b) receive distinct representations.</S><S sid ="56" ssid = "4">Here  the space has only five dimensions  and the matrix cells denote the co-occurrence of the target words (horse and run) with the context words animal  stable  and so on.</S><S sid ="79" ssid = "27">When modeling predicate-argument structures  Kintsch (2001) proposes including one or more distributional neighbors  n  of the predicate: Note that considerable latitude is allowed in selecting the appropriate neighbors.</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'22'", "'56'", "'79'", "'20'"]
'15'
'22'
'56'
'79'
'20'
['15', '22', '56', '79', '20']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">(1) a.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="15" ssid = "11">Sentences (1-a) and (1-b) contain exactly the same set of words but their meaning is entirely different.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="22" ssid = "18">Here  semantic analysis is guided by syntactic structure  and therefore sentences (1-a) and (1-b) receive distinct representations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'150'", "'15'", "'34'", "'22'"]
'16'
'150'
'15'
'34'
'22'
['16', '150', '15', '34', '22']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">It was not the sales manager who hit the bottle that day  but the office worker with the serious drinking problem. b.</S><S sid ="18" ssid = "14">That day the office manager  who was drinking  hit the problem sales worker with a bottle  but it was not serious.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S><S sid ="174" ssid = "8">The simple additive model fails to distinguish between High and Low Similarity items.</S><S sid ="122" ssid = "35">The opposite is the case for the reference The face glowed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'18'", "'164'", "'174'", "'122'"]
'17'
'18'
'164'
'174'
'122'
['17', '18', '164', '174', '122']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/D09-1092_vardha.csv
<S sid="17" ssid="13">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
 <S sid="20" ssid="16">We also explore how the characteristics of different languages affect topic model performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
    <S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
   <S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'"]
'55'
['55']
parsed_discourse_facet ['method_citation']
 sid="9" ssid="5">In this paper, we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
Error in Reference Offset
<S sid="118" ssid="67">We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
 <S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'"]
'55'
['55']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="51">In contrast, PLTM assigns a significant number of tokens to almost all 800 topics, in very similar proportions in both languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'"]
'102'
['102']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="4">This is unlike LDA, in which each document is assumed to have its own document-specific distribution over topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'"]
'38'
['38']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="105">We use both Jensen-Shannon divergence and cosine distance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'"]
'156'
['156']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="2">Each tuple is a set of documents that are loosely equivalent to each other, but written in different languages, e.g., corresponding Wikipedia articles in French, English and German.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
  <S sid="170" ssid="4">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
    <S sid="29" ssid="5">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
  <S sid="170" ssid="4">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="168" ssid="2">However, the growth of the web, and in particular Wikipedia, has made comparable text corpora &#8211; documents that are topically similar but are not direct translations of one another &#8211; considerably more abundant than true parallel corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'168'"]
'168'
['168']
parsed_discourse_facet ['method_citation']
    <S sid="29" ssid="5">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
 <S sid="110" ssid="59">Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'"]
'110'
['110']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="105" ssid = "54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S><S sid ="194" ssid = "3">We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'25'", "'105'", "'194'", "'126'"]
'1'
'25'
'105'
'194'
'126'
['1', '25', '105', '194', '126']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Meanwhile  massive collections of interlinked documents in dozens of languages  such as Wikipedia  are now widely available  calling for tools that can characterize content in many languages.</S><S sid ="16" ssid = "12">However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.</S><S sid ="168" ssid = "2">However  the growth of the web  and in particular Wikipedia  has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid ="196" ssid = "5">When applied to comparable document collections such as Wikipedia  PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S><S sid ="105" ssid = "54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'16'", "'168'", "'196'", "'105'"]
'2'
'16'
'168'
'196'
'105'
['2', '16', '168', '196', '105']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="192" ssid = "1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="20" ssid = "16">We also explore how the characteristics of different languages affect topic model performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'18'", "'31'", "'20'"]
'3'
'192'
'18'
'31'
'20'
['3', '192', '18', '31', '20']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="22" ssid = "18">We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages  and to detect differences in topic emphasis between languages.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="10" ssid = "6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'22'", "'19'", "'10'", "'126'"]
'4'
'22'
'19'
'10'
'126'
['4', '22', '19', '10', '126']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Statistical topic models have emerged as an increasingly useful analysis tool for large text collections.</S><S sid ="15" ssid = "11">Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="6" ssid = "2">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'15'", "'18'", "'3'", "'6'"]
'5'
'15'
'18'
'3'
'6'
['5', '15', '18', '3', '6']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S><S sid ="26" ssid = "2">Tam  Lane and Schultz (Tam et al.  2007) also show improvements in machine translation using bilingual topic models.</S><S sid ="5" ssid = "1">Statistical topic models have emerged as an increasingly useful analysis tool for large text collections.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="29" ssid = "5">A recent extended abstract  developed concurrently by Ni et al. (Ni et al.  2009)  discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'26'", "'5'", "'18'", "'29'"]
'6'
'26'
'5'
'18'
'29'
['6', '26', '5', '18', '29']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Much of this work  however  has occurred in monolingual contexts.</S><S sid ="15" ssid = "11">Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'15'", "'31'", "'18'", "'161'"]
'7'
'15'
'31'
'18'
'161'
['7', '15', '31', '18', '161']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">In an increasingly connected world  the ability to access documents in many languages has become both a strategic asset and a personally enriching experience.</S><S sid ="16" ssid = "12">However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.</S><S sid ="168" ssid = "2">However  the growth of the web  and in particular Wikipedia  has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="108" ssid = "57">If we wish to perform topic-based bibliometric analysis  it is vital to be able to track the same topics across all languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'16'", "'168'", "'23'", "'108'"]
'8'
'16'
'168'
'23'
'108'
['8', '16', '168', '23', '108']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">In this paper  we present the polylingual topic model (PLTM).</S><S sid ="192" ssid = "1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="169" ssid = "3">In this section  we explore two questions relating to comparable text corpora and polylingual topic modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'192'", "'18'", "'3'", "'169'"]
'9'
'192'
'18'
'3'
'169'
['9', '192', '18', '3', '169']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="22" ssid = "18">We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages  and to detect differences in topic emphasis between languages.</S><S sid ="111" ssid = "60">In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts – i.e.  we put each of these documents in a single-document tuple.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'19'", "'22'", "'111'"]
'10'
'4'
'19'
'22'
'111'
['10', '4', '19', '22', '111']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">There are many potential applications for polylingual topic models.</S><S sid ="169" ssid = "3">In this section  we explore two questions relating to comparable text corpora and polylingual topic modeling.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'169'", "'3'", "'18'", "'31'"]
'11'
'169'
'3'
'18'
'31'
['11', '169', '3', '18', '31']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Although research literature is typically written in English  bibliographic databases often contain substantial quantities of work in other languages.</S><S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S><S sid ="174" ssid = "8">These versions of Wikipedia were selected to provide a diverse range of language families  geographic areas  and quantities of text.</S><S sid ="7" ssid = "3">Much of this work  however  has occurred in monolingual contexts.</S><S sid ="14" ssid = "10">Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'161'", "'174'", "'7'", "'14'"]
'12'
'161'
'174'
'7'
'14'
['12', '161', '174', '7', '14']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">To perform topic-based bibliometric analysis on these collections  it is necessary to have topic models that are aligned across languages.</S><S sid ="108" ssid = "57">If we wish to perform topic-based bibliometric analysis  it is vital to be able to track the same topics across all languages.</S><S sid ="196" ssid = "5">When applied to comparable document collections such as Wikipedia  PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S><S sid ="193" ssid = "2">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'108'", "'196'", "'126'", "'193'"]
'13'
'108'
'196'
'126'
'193'
['13', '108', '196', '126', '193']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.</S><S sid ="20" ssid = "16">We also explore how the characteristics of different languages affect topic model performance.</S><S sid ="170" ssid = "4">First  we explore whether comparable document tuples support the alignment of fine-grained topics  as demonstrated earlier using parallel documents.</S><S sid ="53" ssid = "2">In this case  we can be confident that the topic distribution is genuinely shared across all languages.</S><S sid ="68" ssid = "17">The third topic demonstrates differences in inflectional variation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'20'", "'170'", "'53'", "'68'"]
'14'
'20'
'170'
'53'
'68'
['14', '20', '170', '53', '68']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.</S><S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'52'", "'31'", "'3'", "'18'"]
'15'
'52'
'31'
'3'
'18'
['15', '52', '31', '3', '18']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.</S><S sid ="168" ssid = "2">However  the growth of the web  and in particular Wikipedia  has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid ="2" ssid = "2">Meanwhile  massive collections of interlinked documents in dozens of languages  such as Wikipedia  are now widely available  calling for tools that can characterize content in many languages.</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="21" ssid = "17">The second corpus  Wikipedia articles in twelve languages  contains sets of documents that are not translations of one another  but are very likely to be about similar concepts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'168'", "'2'", "'54'", "'21'"]
'16'
'168'
'2'
'54'
'21'
['16', '168', '2', '54', '21']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S><S sid ="194" ssid = "3">We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.</S><S sid ="105" ssid = "54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S><S sid ="13" ssid = "9">To perform topic-based bibliometric analysis on these collections  it is necessary to have topic models that are aligned across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'126'", "'194'", "'105'", "'13'"]
'17'
'126'
'194'
'105'
'13'
['17', '126', '194', '105', '13']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="169" ssid = "3">In this section  we explore two questions relating to comparable text corpora and polylingual topic modeling.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="9" ssid = "5">In this paper  we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'169'", "'3'", "'31'", "'9'"]
'18'
'169'
'3'
'31'
'9'
['18', '169', '3', '31', '9']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'148'", "'131'", "'4'", "'126'"]
'19'
'148'
'131'
'4'
'126'
['19', '148', '131', '4', '126']
parsed_discourse_facet ['method_citation']



D09-1092
E12-1014
0
method_citation
['aim_citation']



D09-1092
D10-1025
0
method_citation
['method_citation']
parsing: input/ref/Task1/P08-1102_sweta.csv
<S sid="21" ssid="17">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="24">A subsequence of boundary-POS labelling result indicates a word with POS t only if the boundary tag sequence composed of its boundary part conforms to s or bm*e style, and all POS tags in its POS part equal to t. For example, a tag sequence b NN m NN e NN represents a threecharacter word with POS tag NN.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="43" ssid="15">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["43'"]
43'
['43']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["92'"]
92'
['92']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="5">To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&amp;T).</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="5">In following subsections, we describe the feature templates and the perceptron training algorithm.</S>
    <S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["33'", "'34'"]
33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="4">By maintaining a stack of size N at each position i of the sequence, we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S>
original cit marker offset is 0
new cit marker offset is 0



["79'"]
79'
['79']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="7">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["96'"]
96'
['96']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="16" ssid="12">Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="7">To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="130" ssid = "1">We proposed a cascaded linear model for Chinese Joint S&T.</S><S sid ="76" ssid = "1">Sequence segmentation and labelling problem can be solved through a viterbi style decoding procedure.</S><S sid ="24" ssid = "20">It is a better idea to perform segmentation and POS tagging jointly in a uniform framework.</S><S sid ="4" ssid = "4">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'130'", "'76'", "'24'", "'4'"]
'1'
'130'
'76'
'24'
'4'
['1', '130', '76', '24', '4']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="64" ssid = "15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'16'", "'133'", "'64'", "'134'"]
'2'
'16'
'133'
'64'
'134'
['2', '16', '133', '64', '134']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid ="21" ssid = "17">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S sid ="106" ssid = "17">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.</S><S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="4" ssid = "4">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'21'", "'106'", "'1'", "'4'"]
'3'
'21'
'106'
'1'
'4'
['3', '21', '106', '1', '4']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S><S sid ="127" ssid = "38">Finally  the word count penalty gives improvement to the cascaded model  0.13 points on segmentation and 0.16 points on Joint S&T.</S><S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="123" ssid = "34">Without it  the F-measure on segmentation and Joint S&T both suffer a decrement of 0.2 points.</S><S sid ="118" ssid = "29">We find that the cascaded model achieves a F-measure increment of about 0.5 points on segmentation and about 0.9 points on Joint S&T  over the perceptron-only model POS+.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'127'", "'1'", "'123'", "'118'"]
'4'
'127'
'1'
'123'
'118'
['4', '127', '1', '123', '118']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages.</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="51" ssid = "2">Additional features most widely used are related to word or POS ngrams.</S><S sid ="114" ssid = "25">We used SRI Language Modelling Toolkit (Stolcke and Andreas  2002) to train a 3gram word LM with modified Kneser-Ney smoothing (Chen and Goodman  1998)  and a 4-gram POS LM with Witten-Bell smoothing  and we trained a word-POS co-occurrence model simply by MLE without smoothing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'31'", "'1'", "'51'", "'114'"]
'5'
'31'
'1'
'51'
'114'
['5', '31', '1', '51', '114']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Several models were introduced for these problems  for example  the Hidden Markov Model (HMM) (Rabiner  1989)  Maximum Entropy Model (ME) (Ratnaparkhi and Adwait  1996)  and Conditional Random Fields (CRFs) (Lafferty et al.  2001).</S><S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="15" ssid = "11">To cope with this problem  we propose a cascaded linear model inspired by the log-linear model (Och and Ney  2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'8'", "'134'", "'15'", "'45'"]
'6'
'8'
'134'
'15'
'45'
['6', '8', '134', '15', '45']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S><S sid ="100" ssid = "11">Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS  CityU and MSR).</S><S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'8'", "'100'", "'14'", "'31'"]
'7'
'8'
'100'
'14'
'31'
['7', '8', '100', '14', '31']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="29" ssid = "1">The perceptron algorithm introduced into NLP by Collins (2002)  is a simple but effective discriminative training method.</S><S sid ="115" ssid = "26">To obtain their corresponding weights  we adapted the minimum-error-rate training algorithm (Och  2003) to train the outside-layer model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'45'", "'49'", "'29'", "'115'"]
'8'
'45'
'49'
'29'
'115'
['8', '45', '49', '29', '115']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid ="23" ssid = "19">We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag.</S><S sid ="11" ssid = "7">Compared to performing segmentation and POS tagging one at a time  Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low  2004).</S><S sid ="80" ssid = "5">At each position i  we enumerate all possible word-POS pairs by assigning each POS to each possible word formed from the character subsequence spanning length l = L. min(i  K) (K is assigned 20 in all our experiments) and ending at position i  then we derive all candidate results by attaching each word-POS pair p (of length l) to the tail of each candidate result at the prior position of p (position i−l)  and select for position i a N-best list of candidate results from all these candidates.</S><S sid ="81" ssid = "6">When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p  we calculate the scores of the word LM  the POS LM  the labelling probability and the generating probability  Algorithm 2 Decoding algorithm. as well as the score of the perceptron model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'23'", "'11'", "'80'", "'81'"]
'9'
'23'
'11'
'80'
'81'
['9', '23', '11', '80', '81']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen  2003)  Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low  2004).</S><S sid ="25" ssid = "21">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S><S sid ="23" ssid = "19">We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="106" ssid = "17">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'25'", "'23'", "'7'", "'106'"]
'10'
'25'
'23'
'7'
'106'
['10', '25', '23', '7', '106']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">Compared to performing segmentation and POS tagging one at a time  Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low  2004).</S><S sid ="9" ssid = "5">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid ="3" ssid = "3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid ="26" ssid = "22">In order to perform POS tagging at the same time  we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004).</S><S sid ="106" ssid = "17">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'9'", "'3'", "'26'", "'106'"]
'11'
'9'
'3'
'26'
'106'
['11', '9', '3', '26', '106']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Besides the usual character-based features  additional features dependent on POS’s or words can also be employed to improve the performance.</S><S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid ="54" ssid = "5">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S><S sid ="51" ssid = "2">Additional features most widely used are related to word or POS ngrams.</S><S sid ="82" ssid = "7">In addition  we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'14'", "'54'", "'51'", "'82'"]
'12'
'14'
'54'
'51'
'82'
['12', '14', '54', '51', '82']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">However  as such features are generated dynamically during the decoding procedure  two limitation arise: on the one hand  the amount of parameters increases rapidly  which is apt to overfit on training corpus; on the other hand  exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.</S><S sid ="52" ssid = "3">However  such features are generated dynamically during the decoding procedure so that the feature space enlarges much more rapidly.</S><S sid ="55" ssid = "6">In addition  even though these higher grams were managed to be used  there still remains another problem: as the current predication relies on the results of prior ones  the decoding procedure has to resort to approximate inference by maintaining a list of N-best candidates at each predication position  which evokes a potential risk to depress the training.</S><S sid ="131" ssid = "2">Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.</S><S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'52'", "'55'", "'131'", "'14'"]
'13'
'52'
'55'
'131'
'14'
['13', '52', '55', '131', '14']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid ="12" ssid = "8">Besides the usual character-based features  additional features dependent on POS’s or words can also be employed to improve the performance.</S><S sid ="131" ssid = "2">Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.</S><S sid ="132" ssid = "3">This is a substitute method to use both local and non-local features  and it would be especially useful when the training corpus is very large.</S><S sid ="54" ssid = "5">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'12'", "'131'", "'132'", "'54'"]
'14'
'12'
'131'
'132'
'54'
['14', '12', '131', '132', '54']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">To cope with this problem  we propose a cascaded linear model inspired by the log-linear model (Och and Ney  2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.</S><S sid ="56" ssid = "7">To alleviate the drawbacks  we propose a cascaded linear model.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'56'", "'49'", "'45'", "'8'"]
'15'
'56'
'49'
'45'
'8'
['15', '56', '49', '45', '8']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="65" ssid = "16">As shown in Figure 1  the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.</S><S sid ="57" ssid = "8">It has a two-layer architecture  with a perceptron as the core and another linear model as the outside-layer.</S><S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="78" ssid = "3">Given a Chinese character sequence C1:n  the decoding procedure can proceed in a left-right fashion with a dynamic programming approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'65'", "'57'", "'2'", "'78'"]
'16'
'65'
'57'
'2'
'78'
['16', '65', '57', '2', '78']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/A97-1014_vardha.csv
<S sid="72" ssid="17">In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
 <S sid="144" ssid="25">We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="4">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
  <S sid="36" ssid="26">As for free word order languages, the following features may cause problems: sition between the two poles.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
 <S sid="24" ssid="14">The typical treebank architecture is as follows: Structures: A context-free backbone is augmented with trace-filler representations of non-local dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
 <S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'"]
'151'
['151']
parsed_discourse_facet ['method_citation']
 <S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
 <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["'167'"]
'167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'"]
'39'
['39']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'"]
'151'
['151']
parsed_discourse_facet ['method_citation']
<S sid="71" ssid="16">However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'71'"]
'71'
['71']
parsed_discourse_facet ['method_citation']
 <S sid="144" ssid="25">We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A97-1014.csv
<S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'162'", "'11'", "'5'", "'4'"]
'1'
'162'
'11'
'5'
'4'
['1', '162', '11', '5', '4']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Since the requirements for such a formalism differ from those posited for configurational languages  several features have been added  influencing the architecture of the scheme.</S><S sid ="3" ssid = "3">The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.</S><S sid ="31" ssid = "21">Due to the substantial differences between existing models of constituent structure  the question arises of how the theory independencf requirement  can be satisfied.</S><S sid ="13" ssid = "3">The data-drivenness of this approach presents a clear advantage over the traditional  idealised notion of competence grammar.</S><S sid ="165" ssid = "7">In addition the approach provides empirical material for psycholinguistic investigation  since preferences for the choice of certain syntactic constructions  linea.rizations  and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'3'", "'31'", "'13'", "'165'"]
'2'
'3'
'31'
'13'
'165'
['2', '3', '31', '13', '165']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.</S><S sid ="31" ssid = "21">Due to the substantial differences between existing models of constituent structure  the question arises of how the theory independencf requirement  can be satisfied.</S><S sid ="113" ssid = "26">Since a precise structural description of non-constituent coordination would require a. rich inventory of incomplete phrase types  we have agreed on a sort of unde.rspe.cified representations: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links.</S><S sid ="2" ssid = "2">Since the requirements for such a formalism differ from those posited for configurational languages  several features have been added  influencing the architecture of the scheme.</S><S sid ="41" ssid = "31">Apart from this rather technical problem  two further arguments speak against phrase structure as the structural pivot of the annotation scheme: Finally  the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies  which has proved tedious.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'31'", "'113'", "'2'", "'41'"]
'3'
'31'
'113'
'2'
'41'
['3', '31', '113', '2', '41']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'1'", "'162'", "'74'", "'11'"]
'4'
'1'
'162'
'74'
'11'
['4', '1', '162', '74', '11']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'162'", "'1'", "'6'", "'74'"]
'5'
'162'
'1'
'6'
'74'
['5', '162', '1', '6', '74']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="9" ssid = "6">Section 4 deals with the treatment of selected phenomena.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="160" ssid = "2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'9'", "'5'", "'1'", "'160'"]
'6'
'9'
'5'
'1'
'160'
['6', '9', '5', '1', '160']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">On the basis of these considerations  we formulate several additional requirements.</S><S sid ="37" ssid = "27">In light of these facts  serious difficulties can be expected arising from the structural component of the existing formalisms.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="104" ssid = "17">The head of the phrase can be determined in a similar way according to theory-specific assumptions.</S><S sid ="127" ssid = "8">As the need for certain functionalities becomes obvious with growing annotation experience  we have decided to implement the tool in two stages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'37'", "'5'", "'104'", "'127'"]
'7'
'37'
'5'
'104'
'127'
['7', '37', '5', '104', '127']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">A formalism complying with these requirements is described in section 3.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="9" ssid = "6">Section 4 deals with the treatment of selected phenomena.</S><S sid ="10" ssid = "7">For a description of the annotation tool see section 5.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'6'", "'9'", "'10'", "'5'"]
'8'
'6'
'9'
'10'
'5'
['8', '6', '9', '10', '5']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "6">Section 4 deals with the treatment of selected phenomena.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="8" ssid = "5">A formalism complying with these requirements is described in section 3.</S><S sid ="128" ssid = "9">In the first phase  the main functionality for building and displaying unordered trees is supplied.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'6'", "'5'", "'8'", "'128'"]
'9'
'6'
'5'
'8'
'128'
['9', '6', '5', '8', '128']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">For a description of the annotation tool see section 5.</S><S sid ="128" ssid = "9">In the first phase  the main functionality for building and displaying unordered trees is supplied.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="160" ssid = "2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'128'", "'1'", "'160'", "'6'"]
'10'
'128'
'1'
'160'
'6'
['10', '128', '1', '160', '6']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="12" ssid = "2">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="170" ssid = "12">Our annotation tool supplies efficient manipulation and immediate visualization of argument structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'12'", "'162'", "'1'", "'170'"]
'11'
'12'
'162'
'1'
'170'
['11', '12', '162', '1', '170']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "2">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="143" ssid = "24">Sentences annotated in previous steps are used as training material for further processing.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'11'", "'143'", "'162'", "'1'"]
'12'
'11'
'143'
'162'
'1'
['12', '11', '143', '162', '1']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "3">The data-drivenness of this approach presents a clear advantage over the traditional  idealised notion of competence grammar.</S><S sid ="54" ssid = "44">Such a word order independent representation has the advantage of all structural information being encoded in a single data structure.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="43" ssid = "33">An alternative solution is to make argument structure the main structural component of the formalism.</S><S sid ="44" ssid = "34">This assumption underlies a growing number of recent syntactic theories which give up the context-free constituent backbone  cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'54'", "'162'", "'43'", "'44'"]
'13'
'54'
'162'
'43'
'44'
['13', '54', '162', '43', '44']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "4">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S><S sid ="12" ssid = "2">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="166" ssid = "8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'12'", "'162'", "'4'", "'166'"]
'14'
'12'
'162'
'4'
'166'
['14', '12', '162', '4', '166']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1



A97-1014
C10-1061
0
method_citation
['method_citation']
parsing: input/ref/Task1/P08-1102_aakansha.csv
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="14">As predications generated from such templates depend on the current character, we name these templates lexical-target.</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="121" ssid="32">Among other features, the 4-gram POS LM plays the most important role, removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'121'"]
'121'
['121']
parsed_discourse_facet ['method_citation']
<S sid="37" ssid="9">C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'"]
'37'
['37']
parsed_discourse_facet ['method_citation']
<S sid="73" ssid="24">For instance, if the word w appears N times in training corpus and is labelled as POS t for n times, the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'73'"]
'73'
['73']
parsed_discourse_facet ['method_citation']
<S sid="46" ssid="18">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) &#8712; X &#215; Y to a feature vector 4)(x, y) &#8712; Rd, and a parameter vector &#945;&#65533; &#8712; Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'"]
'46'
['46']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="130" ssid = "1">We proposed a cascaded linear model for Chinese Joint S&T.</S><S sid ="76" ssid = "1">Sequence segmentation and labelling problem can be solved through a viterbi style decoding procedure.</S><S sid ="24" ssid = "20">It is a better idea to perform segmentation and POS tagging jointly in a uniform framework.</S><S sid ="4" ssid = "4">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'130'", "'76'", "'24'", "'4'"]
'1'
'130'
'76'
'24'
'4'
['1', '130', '76', '24', '4']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="64" ssid = "15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'16'", "'133'", "'64'", "'134'"]
'2'
'16'
'133'
'64'
'134'
['2', '16', '133', '64', '134']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid ="21" ssid = "17">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S sid ="106" ssid = "17">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.</S><S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="4" ssid = "4">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'21'", "'106'", "'1'", "'4'"]
'3'
'21'
'106'
'1'
'4'
['3', '21', '106', '1', '4']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S><S sid ="127" ssid = "38">Finally  the word count penalty gives improvement to the cascaded model  0.13 points on segmentation and 0.16 points on Joint S&T.</S><S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="123" ssid = "34">Without it  the F-measure on segmentation and Joint S&T both suffer a decrement of 0.2 points.</S><S sid ="118" ssid = "29">We find that the cascaded model achieves a F-measure increment of about 0.5 points on segmentation and about 0.9 points on Joint S&T  over the perceptron-only model POS+.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'127'", "'1'", "'123'", "'118'"]
'4'
'127'
'1'
'123'
'118'
['4', '127', '1', '123', '118']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages.</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="51" ssid = "2">Additional features most widely used are related to word or POS ngrams.</S><S sid ="114" ssid = "25">We used SRI Language Modelling Toolkit (Stolcke and Andreas  2002) to train a 3gram word LM with modified Kneser-Ney smoothing (Chen and Goodman  1998)  and a 4-gram POS LM with Witten-Bell smoothing  and we trained a word-POS co-occurrence model simply by MLE without smoothing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'31'", "'1'", "'51'", "'114'"]
'5'
'31'
'1'
'51'
'114'
['5', '31', '1', '51', '114']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Several models were introduced for these problems  for example  the Hidden Markov Model (HMM) (Rabiner  1989)  Maximum Entropy Model (ME) (Ratnaparkhi and Adwait  1996)  and Conditional Random Fields (CRFs) (Lafferty et al.  2001).</S><S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="15" ssid = "11">To cope with this problem  we propose a cascaded linear model inspired by the log-linear model (Och and Ney  2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'8'", "'134'", "'15'", "'45'"]
'6'
'8'
'134'
'15'
'45'
['6', '8', '134', '15', '45']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S><S sid ="100" ssid = "11">Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS  CityU and MSR).</S><S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'8'", "'100'", "'14'", "'31'"]
'7'
'8'
'100'
'14'
'31'
['7', '8', '100', '14', '31']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="29" ssid = "1">The perceptron algorithm introduced into NLP by Collins (2002)  is a simple but effective discriminative training method.</S><S sid ="115" ssid = "26">To obtain their corresponding weights  we adapted the minimum-error-rate training algorithm (Och  2003) to train the outside-layer model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'45'", "'49'", "'29'", "'115'"]
'8'
'45'
'49'
'29'
'115'
['8', '45', '49', '29', '115']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid ="23" ssid = "19">We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag.</S><S sid ="11" ssid = "7">Compared to performing segmentation and POS tagging one at a time  Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low  2004).</S><S sid ="80" ssid = "5">At each position i  we enumerate all possible word-POS pairs by assigning each POS to each possible word formed from the character subsequence spanning length l = L. min(i  K) (K is assigned 20 in all our experiments) and ending at position i  then we derive all candidate results by attaching each word-POS pair p (of length l) to the tail of each candidate result at the prior position of p (position i−l)  and select for position i a N-best list of candidate results from all these candidates.</S><S sid ="81" ssid = "6">When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p  we calculate the scores of the word LM  the POS LM  the labelling probability and the generating probability  Algorithm 2 Decoding algorithm. as well as the score of the perceptron model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'23'", "'11'", "'80'", "'81'"]
'9'
'23'
'11'
'80'
'81'
['9', '23', '11', '80', '81']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen  2003)  Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low  2004).</S><S sid ="25" ssid = "21">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S><S sid ="23" ssid = "19">We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="106" ssid = "17">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'25'", "'23'", "'7'", "'106'"]
'10'
'25'
'23'
'7'
'106'
['10', '25', '23', '7', '106']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">Compared to performing segmentation and POS tagging one at a time  Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low  2004).</S><S sid ="9" ssid = "5">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid ="3" ssid = "3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid ="26" ssid = "22">In order to perform POS tagging at the same time  we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004).</S><S sid ="106" ssid = "17">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'9'", "'3'", "'26'", "'106'"]
'11'
'9'
'3'
'26'
'106'
['11', '9', '3', '26', '106']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Besides the usual character-based features  additional features dependent on POS’s or words can also be employed to improve the performance.</S><S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid ="54" ssid = "5">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S><S sid ="51" ssid = "2">Additional features most widely used are related to word or POS ngrams.</S><S sid ="82" ssid = "7">In addition  we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'14'", "'54'", "'51'", "'82'"]
'12'
'14'
'54'
'51'
'82'
['12', '14', '54', '51', '82']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">However  as such features are generated dynamically during the decoding procedure  two limitation arise: on the one hand  the amount of parameters increases rapidly  which is apt to overfit on training corpus; on the other hand  exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.</S><S sid ="52" ssid = "3">However  such features are generated dynamically during the decoding procedure so that the feature space enlarges much more rapidly.</S><S sid ="55" ssid = "6">In addition  even though these higher grams were managed to be used  there still remains another problem: as the current predication relies on the results of prior ones  the decoding procedure has to resort to approximate inference by maintaining a list of N-best candidates at each predication position  which evokes a potential risk to depress the training.</S><S sid ="131" ssid = "2">Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.</S><S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'52'", "'55'", "'131'", "'14'"]
'13'
'52'
'55'
'131'
'14'
['13', '52', '55', '131', '14']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid ="12" ssid = "8">Besides the usual character-based features  additional features dependent on POS’s or words can also be employed to improve the performance.</S><S sid ="131" ssid = "2">Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.</S><S sid ="132" ssid = "3">This is a substitute method to use both local and non-local features  and it would be especially useful when the training corpus is very large.</S><S sid ="54" ssid = "5">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'12'", "'131'", "'132'", "'54'"]
'14'
'12'
'131'
'132'
'54'
['14', '12', '131', '132', '54']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">To cope with this problem  we propose a cascaded linear model inspired by the log-linear model (Och and Ney  2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.</S><S sid ="56" ssid = "7">To alleviate the drawbacks  we propose a cascaded linear model.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'56'", "'49'", "'45'", "'8'"]
'15'
'56'
'49'
'45'
'8'
['15', '56', '49', '45', '8']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="65" ssid = "16">As shown in Figure 1  the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.</S><S sid ="57" ssid = "8">It has a two-layer architecture  with a perceptron as the core and another linear model as the outside-layer.</S><S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="78" ssid = "3">Given a Chinese character sequence C1:n  the decoding procedure can proceed in a left-right fashion with a dynamic programming approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'65'", "'57'", "'2'", "'78'"]
'16'
'65'
'57'
'2'
'78'
['16', '65', '57', '2', '78']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/A97-1014_sweta.csv
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["168'"]
168'
['168']
parsed_discourse_facet ['method_citation']
<S sid="165" ssid="7">In addition the approach provides empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linea.rizations, and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.</S>
original cit marker offset is 0
new cit marker offset is 0



["165'"]
165'
['165']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="26">This amount of data suffices as training material to reliably assign the grammatical functions if the user determines the elements of a phrase and its type (step 1 of the list above).</S>
original cit marker offset is 0
new cit marker offset is 0



["145'"]
145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.<
original cit marker offset is 0
new cit marker offset is 0



["15'"]
15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="31">Apart from this rather technical problem, two further arguments speak against phrase structure as the structural pivot of the annotation scheme: Finally, the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies, which has proved tedious.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
<S sid="47" ssid="37">Argument structure can be represented in terms of unordered trees (with crossing branches).</S>
original cit marker offset is 0
new cit marker offset is 0



["47'"]
47'
['47']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
 <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["160'"]
160'
['160']
parsed_discourse_facet ['method_citation']
<S sid="166" ssid="8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
<S sid="166" ssid="8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="24">Sentences annotated in previous steps are used as training material for further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
  <S sid="71" ssid="16">However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["71'"]
71'
['71']
parsed_discourse_facet ['method_citation']
  <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A97-1014.csv
<S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'162'", "'11'", "'5'", "'4'"]
'1'
'162'
'11'
'5'
'4'
['1', '162', '11', '5', '4']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Since the requirements for such a formalism differ from those posited for configurational languages  several features have been added  influencing the architecture of the scheme.</S><S sid ="3" ssid = "3">The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.</S><S sid ="31" ssid = "21">Due to the substantial differences between existing models of constituent structure  the question arises of how the theory independencf requirement  can be satisfied.</S><S sid ="13" ssid = "3">The data-drivenness of this approach presents a clear advantage over the traditional  idealised notion of competence grammar.</S><S sid ="165" ssid = "7">In addition the approach provides empirical material for psycholinguistic investigation  since preferences for the choice of certain syntactic constructions  linea.rizations  and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'3'", "'31'", "'13'", "'165'"]
'2'
'3'
'31'
'13'
'165'
['2', '3', '31', '13', '165']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.</S><S sid ="31" ssid = "21">Due to the substantial differences between existing models of constituent structure  the question arises of how the theory independencf requirement  can be satisfied.</S><S sid ="113" ssid = "26">Since a precise structural description of non-constituent coordination would require a. rich inventory of incomplete phrase types  we have agreed on a sort of unde.rspe.cified representations: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links.</S><S sid ="2" ssid = "2">Since the requirements for such a formalism differ from those posited for configurational languages  several features have been added  influencing the architecture of the scheme.</S><S sid ="41" ssid = "31">Apart from this rather technical problem  two further arguments speak against phrase structure as the structural pivot of the annotation scheme: Finally  the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies  which has proved tedious.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'31'", "'113'", "'2'", "'41'"]
'3'
'31'
'113'
'2'
'41'
['3', '31', '113', '2', '41']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'1'", "'162'", "'74'", "'11'"]
'4'
'1'
'162'
'74'
'11'
['4', '1', '162', '74', '11']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'162'", "'1'", "'6'", "'74'"]
'5'
'162'
'1'
'6'
'74'
['5', '162', '1', '6', '74']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="9" ssid = "6">Section 4 deals with the treatment of selected phenomena.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="160" ssid = "2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'9'", "'5'", "'1'", "'160'"]
'6'
'9'
'5'
'1'
'160'
['6', '9', '5', '1', '160']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">On the basis of these considerations  we formulate several additional requirements.</S><S sid ="37" ssid = "27">In light of these facts  serious difficulties can be expected arising from the structural component of the existing formalisms.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="104" ssid = "17">The head of the phrase can be determined in a similar way according to theory-specific assumptions.</S><S sid ="127" ssid = "8">As the need for certain functionalities becomes obvious with growing annotation experience  we have decided to implement the tool in two stages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'37'", "'5'", "'104'", "'127'"]
'7'
'37'
'5'
'104'
'127'
['7', '37', '5', '104', '127']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">A formalism complying with these requirements is described in section 3.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="9" ssid = "6">Section 4 deals with the treatment of selected phenomena.</S><S sid ="10" ssid = "7">For a description of the annotation tool see section 5.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'6'", "'9'", "'10'", "'5'"]
'8'
'6'
'9'
'10'
'5'
['8', '6', '9', '10', '5']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "6">Section 4 deals with the treatment of selected phenomena.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="8" ssid = "5">A formalism complying with these requirements is described in section 3.</S><S sid ="128" ssid = "9">In the first phase  the main functionality for building and displaying unordered trees is supplied.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'6'", "'5'", "'8'", "'128'"]
'9'
'6'
'5'
'8'
'128'
['9', '6', '5', '8', '128']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">For a description of the annotation tool see section 5.</S><S sid ="128" ssid = "9">In the first phase  the main functionality for building and displaying unordered trees is supplied.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="160" ssid = "2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'128'", "'1'", "'160'", "'6'"]
'10'
'128'
'1'
'160'
'6'
['10', '128', '1', '160', '6']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="12" ssid = "2">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="170" ssid = "12">Our annotation tool supplies efficient manipulation and immediate visualization of argument structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'12'", "'162'", "'1'", "'170'"]
'11'
'12'
'162'
'1'
'170'
['11', '12', '162', '1', '170']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "2">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="143" ssid = "24">Sentences annotated in previous steps are used as training material for further processing.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'11'", "'143'", "'162'", "'1'"]
'12'
'11'
'143'
'162'
'1'
['12', '11', '143', '162', '1']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "3">The data-drivenness of this approach presents a clear advantage over the traditional  idealised notion of competence grammar.</S><S sid ="54" ssid = "44">Such a word order independent representation has the advantage of all structural information being encoded in a single data structure.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="43" ssid = "33">An alternative solution is to make argument structure the main structural component of the formalism.</S><S sid ="44" ssid = "34">This assumption underlies a growing number of recent syntactic theories which give up the context-free constituent backbone  cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'54'", "'162'", "'43'", "'44'"]
'13'
'54'
'162'
'43'
'44'
['13', '54', '162', '43', '44']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "4">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S><S sid ="12" ssid = "2">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="166" ssid = "8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'12'", "'162'", "'4'", "'166'"]
'14'
'12'
'162'
'4'
'166'
['14', '12', '162', '4', '166']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1



A97-1014
P11-2067
0
method_citation
['aim_citation']



A97-1014
E99-1016
0
method_citation
['method_citation']
parsing: input/ref/Task1/A00-2030_vardha.csv
 <S sid="11" ssid="1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="10">Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'"]
'50'
['50']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
 <S sid="106" ssid="3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
 <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
 <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="2">The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
 <S sid="3" ssid="1">Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'"]
'3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="15">Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'"]
'32'
['32']
parsed_discourse_facet ['method_citation']
 <S sid="10" ssid="8">Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="13">Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
    <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
 <S sid="26" ssid="9">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'"]
'26'
['26']
parsed_discourse_facet ['method_citation']
    <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="2">Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="43" ssid = "3">Thus  we did not consider simply adding semantic labels to the existing Penn TREEBANK  which is drawn from a single source — the Wall Street Journal — and is impoverished in articles about rocket launches.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'10'", "'5'", "'43'", "'104'"]
'1'
'10'
'5'
'43'
'104'
['1', '10', '5', '43', '104']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S><S sid ="26" ssid = "9">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993)  and more recently  had begun using a generative statistical model for name finding (Bikel et al.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'28'", "'96'", "'26'"]
'2'
'6'
'28'
'96'
'26'
['2', '6', '28', '96', '26']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "1">Since 1995  a few statistical parsing algorithms (Magerman  1995; Collins  1996 and 1997; Charniak  1997; Rathnaparki  1997) demonstrated a breakthrough in parsing accuracy  as measured against the University of Pennsylvania TREEBANK as a gold standard.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'1'", "'104'", "'5'", "'96'"]
'3'
'1'
'104'
'5'
'96'
['3', '1', '104', '5', '96']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "2">Yet  relatively few have embedded one of these algorithms in a task.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="108" ssid = "5">This simple semantic annotation was the only source of task knowledge used to configure the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'105'", "'23'", "'28'", "'108'"]
'4'
'105'
'23'
'28'
'108'
['4', '105', '23', '28', '108']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'106'", "'1'", "'28'", "'10'"]
'5'
'106'
'1'
'28'
'10'
['5', '106', '1', '28', '10']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S><S sid ="2" ssid = "2">In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'104'", "'2'", "'51'", "'1'"]
'6'
'104'
'2'
'51'
'1'
['6', '104', '2', '51', '1']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "5">The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.</S><S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="50" ssid = "10">Figure 4 shows an example of the semantic annotation  which was the only type of manual annotation we performed.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'11'", "'28'", "'50'", "'107'"]
'7'
'11'
'28'
'50'
'107'
['7', '11', '28', '50', '107']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "6">Several technical challenges confronted us and were solved: TREEBANK on Wall Street Journal adequately train the algorithm for New York Times newswire  which includes dozens of newspapers?</S><S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'10'", "'5'", "'51'", "'1'"]
'8'
'10'
'5'
'51'
'1'
['8', '10', '5', '51', '1']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "7">Manually creating sourcespecific training data for syntax was not required.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="108" ssid = "5">This simple semantic annotation was the only source of task knowledge used to configure the model.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S><S sid ="41" ssid = "1">To train our integrated model  we required a large corpus of augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'106'", "'108'", "'107'", "'41'"]
'9'
'106'
'108'
'107'
'41'
['9', '106', '108', '107', '41']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="43" ssid = "3">Thus  we did not consider simply adding semantic labels to the existing Penn TREEBANK  which is drawn from a single source — the Wall Street Journal — and is impoverished in articles about rocket launches.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'1'", "'43'", "'51'", "'5'"]
'10'
'1'
'43'
'51'
'5'
['10', '1', '43', '51', '5']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S><S sid ="7" ssid = "5">The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'7'", "'101'", "'5'", "'96'"]
'11'
'7'
'101'
'5'
'96'
['11', '7', '101', '5', '96']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "2">The Template Element (TE) task identifies organizations  persons  locations  and some artifacts (rocket and airplane-related artifacts).</S><S sid ="98" ssid = "3">In both Template Entity (TE) and Template Relation (TR)  our system finished in second place among all entrants.</S><S sid ="16" ssid = "6">For the following example  the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S><S sid ="78" ssid = "19">If we generalize the tree components (constituent labels  words  tags  etc.) and treat them all as simply elements  e  and treat all the conditioning factors as the history  h  we can write:</S><S sid ="34" ssid = "2">In these trees  the standard TREEBANK structures are augmented to convey semantic information  that is  entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'98'", "'16'", "'78'", "'34'"]
'12'
'98'
'16'
'78'
'34'
['12', '98', '16', '78', '34']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="14" ssid = "4">For each person  one must find all of the person's names within the document  his/her type (civilian or military)  and any significant descriptions (e.g.  titles).</S><S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="58" ssid = "4">Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree  semantic pointer labels are attached to all of the intermediate nodes.</S><S sid ="100" ssid = "5">Given multiple constituents that cover identical spans in the chart  only those constituents with probabilities within a While our focus throughout the project was on TE and TR  we became curious about how well the model did at part-of-speech tagging  syntactic parsing  and at name finding.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'14'", "'15'", "'58'", "'100'"]
'13'
'14'
'15'
'58'
'100'
['13', '14', '15', '58', '100']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "4">For each person  one must find all of the person's names within the document  his/her type (civilian or military)  and any significant descriptions (e.g.  titles).</S><S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S><S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'13'", "'74'", "'15'", "'36'"]
'14'
'13'
'74'
'15'
'36'
['14', '13', '74', '15', '36']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="78" ssid = "19">If we generalize the tree components (constituent labels  words  tags  etc.) and treat them all as simply elements  e  and treat all the conditioning factors as the history  h  we can write:</S><S sid ="91" ssid = "10">The probability of generating a constituent of the specified category  starting at the topmost node.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="55" ssid = "1">syntactic modifier of the other  the inserted node serves to indicate the relation as well as the argument.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'78'", "'91'", "'82'", "'55'"]
'15'
'78'
'91'
'82'
'55'
['15', '78', '91', '82', '55']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "6">For the following example  the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S><S sid ="98" ssid = "3">In both Template Entity (TE) and Template Relation (TR)  our system finished in second place among all entrants.</S><S sid ="12" ssid = "2">The Template Element (TE) task identifies organizations  persons  locations  and some artifacts (rocket and airplane-related artifacts).</S><S sid ="94" ssid = "13">Given a new sentence  the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'74'", "'98'", "'12'", "'94'"]
'16'
'74'
'98'
'12'
'94'
['16', '74', '98', '12', '94']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "7">For the following example  the template relation in Figure 2 was to be generated: &quot;Donald M. Goldstein  a historian at the University of Pittsburgh who helped write...&quot;</S><S sid ="50" ssid = "10">Figure 4 shows an example of the semantic annotation  which was the only type of manual annotation we performed.</S><S sid ="35" ssid = "3">An example of an augmented parse tree is shown in Figure 3.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'50'", "'35'", "'82'", "'56'"]
'17'
'50'
'35'
'82'
'56'
['17', '50', '35', '82', '56']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "1">Almost all approaches to information extraction — even at the sentence level — are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.</S><S sid ="19" ssid = "2">Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S><S sid ="29" ssid = "12">Thus  each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="84" ssid = "3">Although mathematically the model predicts tree elements in a top-down fashion  we search the space bottom-up using a chartbased search.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'19'", "'29'", "'5'", "'84'"]
'18'
'19'
'29'
'5'
'84'
['18', '19', '29', '5', '84']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "2">Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S><S sid ="29" ssid = "12">Thus  each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.</S><S sid ="24" ssid = "7">For this reason  we focused on designing an integrated model in which tagging  namefinding  parsing  and semantic interpretation decisions all have the opportunity to mutually influence each other.</S><S sid ="90" ssid = "9">Thus  the scores used in pruning can be considered as the product of: 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'96'", "'29'", "'24'", "'90'"]
'19'
'96'
'29'
'24'
'90'
['19', '96', '29', '24', '90']
parsed_discourse_facet ['method_citation']



A00-2030
W01-0510
0
method_citation
['method_citation']



A00-2030
P14-1078
0
method_citation
['method_citation']



A00-2030
N06-1037
0
method_citation
['method_citation']
parsing: input/ref/Task1/P11-1061_swastika.csv
    <S sid="144" ssid="7">For comparison, the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%, and goes up to 88.7% with a treebank dictionary.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
    <S sid="44" ssid="10">Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.</S
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
parsed_discourse_facet ['method_citation']
    <S sid="16" ssid="12">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (&#167;3), and then use graph label propagation to project syntactic information from English to the foreign language (&#167;4).</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="44" ssid="10">Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.</S
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
parsed_discourse_facet ['method_citation']
<S sid="110" ssid="10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S>
original cit marker offset is 0
new cit marker offset is 0



['110']
110
['110']
parsed_discourse_facet ['aim_citation']
    <S sid="115" ssid="15">The taggers were trained on datasets labeled with the universal tags.</S>
original cit marker offset is 0
new cit marker offset is 0



['115']
115
['115']
parsed_discourse_facet ['method_citation']
    <S sid="115" ssid="15">The taggers were trained on datasets labeled with the universal tags.</S>
original cit marker offset is 0
new cit marker offset is 0



['115']
115
['115']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['158']
158
['158']
parsed_discourse_facet ['result_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



['23']
23
['23']
parsed_discourse_facet ['result_citation']
<S sid="24" ssid="1">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['24']
24
['24']
parsed_discourse_facet ['aim_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['???']
???
['???']
parsed_discourse_facet ['method_citation']
    <S sid="16" ssid="12">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (&#167;3), and then use graph label propagation to project syntactic information from English to the foreign language (&#167;4).</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



['23']
23
['23']
parsed_discourse_facet ['result_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['158']
158
['158']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
<S sid="56" ssid="22">To define a similarity function between the English and the foreign vertices, we rely on high-confidence word alignments.</S>
original cit marker offset is 0
new cit marker offset is 0



['56']
56
['56']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



['161']
161
['161']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P11-1061.csv
<S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S><S sid ="8" ssid = "4">Unsupervised learning approaches appear to be a natural solution to this problem  as they require only unannotated text for training models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'160'", "'3'", "'13'", "'8'"]
'1'
'160'
'3'
'13'
'8'
['1', '160', '3', '13', '8']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed)  making it applicable to a wide array of resource-poor languages.</S><S sid ="9" ssid = "5">Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.</S><S sid ="126" ssid = "26">Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.</S><S sid ="41" ssid = "7">Because the information flow in our graph is asymmetric (from English to the foreign language)  we use different types of vertices for each language.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'9'", "'126'", "'41'", "'107'"]
'2'
'9'
'126'
'41'
'107'
['2', '9', '126', '41', '107']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="33" ssid = "10">The POS distributions over the foreign trigram types are used as features to learn a better unsupervised POS tagger (§5).</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="60" ssid = "26">To initialize the graph for label propagation we use a supervised English tagger to label the English side of the bitext.7 We then simply count the individual labels of the English tokens and normalize the counts to produce tag distributions over English word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'39'", "'33'", "'1'", "'60'"]
'3'
'39'
'33'
'1'
'60'
['3', '39', '33', '1', '60']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S><S sid ="23" ssid = "19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%)  and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S sid ="143" ssid = "6">Overall  it gives improvements ranging from 1.1% for German to 14.7% for Italian  for an average improvement of 8.3% over the unsupervised feature-HMM model.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S><S sid ="144" ssid = "7">For comparison  the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%  and goes up to 88.7% with a treebank dictionary.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'23'", "'143'", "'135'", "'144'"]
'4'
'23'
'143'
'135'
'144'
['4', '23', '143', '135', '144']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing  resulting in highly accurate systems.</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S><S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S><S sid ="15" ssid = "11">First  we use a novel graph-based framework for projecting syntactic information across language boundaries.</S><S sid ="18" ssid = "14">To make the projection practical  we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'158'", "'4'", "'15'", "'18'"]
'5'
'158'
'4'
'15'
'18'
['5', '158', '4', '15', '18']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Supervised part-of-speech (POS) taggers  for example  approach the level of inter-annotator agreement (Shen et al.  2007  97.3% accuracy for English).</S><S sid ="47" ssid = "13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S><S sid ="9" ssid = "5">Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S><S sid ="20" ssid = "16">Because there might be some controversy about the exact definitions of such universals  this set of coarse-grained POS categories is defined operationally  by collapsing language (or treebank) specific distinctions to a set of categories that exists across all languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'47'", "'9'", "'158'", "'20'"]
'6'
'47'
'9'
'158'
'20'
['6', '47', '9', '158', '20']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S><S sid ="27" ssid = "4">Graph construction does not require any labeled data  but makes use of two similarity functions.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'27'", "'107'", "'56'", "'37'"]
'7'
'27'
'107'
'56'
'37'
['7', '27', '107', '56', '37']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Unsupervised learning approaches appear to be a natural solution to this problem  as they require only unannotated text for training models.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'160'", "'1'", "'13'", "'3'"]
'8'
'160'
'1'
'13'
'3'
['8', '160', '1', '13', '3']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.</S><S sid ="47" ssid = "13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S><S sid ="126" ssid = "26">Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.</S><S sid ="2" ssid = "2">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed)  making it applicable to a wide array of resource-poor languages.</S><S sid ="57" ssid = "23">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'47'", "'126'", "'2'", "'57'"]
'9'
'47'
'126'
'2'
'57'
['9', '47', '126', '2', '57']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'39'", "'16'", "'26'", "'13'"]
'10'
'39'
'16'
'26'
'13'
['10', '39', '16', '26', '13']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This scenario is applicable to a large set of languages and has been considered by a number of authors in the past (Alshawi et al.  2000; Xi and Hwa  2005; Ganchev et al.  2009).</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="94" ssid = "25">We adopted this state-of-the-art model because it makes it easy to experiment with various ways of incorporating our novel constraint feature into the log-linear emission model.</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'19'", "'26'", "'94'", "'13'"]
'11'
'19'
'26'
'94'
'13'
['11', '19', '26', '94', '13']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Naseem et al. (2009) and Snyder et al.</S><S sid ="38" ssid = "4">More recently  Subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model.</S><S sid ="93" ssid = "24">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S><S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="123" ssid = "23">This can be seen as a rough approximation of Yarowsky and Ngai (2001).</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'38'", "'93'", "'37'", "'123'"]
'12'
'38'
'93'
'37'
'123'
['12', '38', '93', '37', '123']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="24" ssid = "1">The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'39'", "'160'", "'1'", "'24'"]
'13'
'39'
'160'
'1'
'24'
['13', '39', '160', '1', '24']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">Our work is closest to that of Yarowsky and Ngai (2001)  but differs in two important ways.</S><S sid ="123" ssid = "23">This can be seen as a rough approximation of Yarowsky and Ngai (2001).</S><S sid ="40" ssid = "6">We extend Subramanya et al.’s intuitions to our bilingual setup.</S><S sid ="88" ssid = "19">In our experiments  we used the same set of features as BergKirkpatrick et al. (2010): an indicator feature based In a traditional Markov model  the emission distribution PΘ(Xi = xi  |Zi = zi) is a set of multinomials.</S><S sid ="85" ssid = "16">We develop our POS induction model based on the feature-based HMM of Berg-Kirkpatrick et al. (2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'123'", "'40'", "'88'", "'85'"]
'14'
'123'
'40'
'88'
'85'
['14', '123', '40', '88', '85']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">First  we use a novel graph-based framework for projecting syntactic information across language boundaries.</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'158'", "'29'", "'56'", "'25'"]
'15'
'158'
'29'
'56'
'25'
['15', '158', '29', '56', '25']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="57" ssid = "23">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid ="70" ssid = "1">Given the bilingual graph described in the previous section  we can use label propagation to project the English POS labels to the foreign language.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'57'", "'70'", "'29'", "'26'"]
'16'
'57'
'70'
'29'
'26'
['16', '57', '70', '29', '26']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">Second  we treat the projected labels as features in an unsupervised model (§5)  rather than using them directly for supervised training.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="71" ssid = "2">We use label propagation in two stages to generate soft labels on all the vertices in the graph.</S><S sid ="126" ssid = "26">Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.</S><S sid ="52" ssid = "18">This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'97'", "'71'", "'126'", "'52'"]
'17'
'97'
'71'
'126'
'52'
['17', '97', '71', '126', '52']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">To make the projection practical  we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="111" ssid = "11">We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns)  VERB (verbs)  ADJ (adjectives)  ADV (adverbs)  PRON (pronouns)  DET (determiners)  ADP (prepositions or postpositions)  NUM (numerals)  CONJ (conjunctions)  PRT (particles)  PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).</S><S sid ="85" ssid = "16">We develop our POS induction model based on the feature-based HMM of Berg-Kirkpatrick et al. (2010).</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'56'", "'111'", "'85'", "'158'"]
'18'
'56'
'111'
'85'
'158'
['18', '56', '111', '85', '158']
parsed_discourse_facet ['method_citation']



P11-1061
D11-1006
0
method_citation
['method_citation']
parsing: input/ref/Task1/W99-0623_vardha.csv
    <S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="117" ssid="46">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'117'"]
'117'
['117']
parsed_discourse_facet ['method_citation']
<S sid="72" ssid="1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
    <S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'"]
'120'
['120']
parsed_discourse_facet ['method_citation']
  <S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'139'"]
'139'
['139']
parsed_discourse_facet ['method_citation']
 <S sid="84" ssid="13">The first shows how constituent features and context do not help in deciding which parser to trust.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'"]
'84'
['84']
parsed_discourse_facet ['method_citation']
    <S sid="76" ssid="5">The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'"]
'76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'"]
'38'
['38']
parsed_discourse_facet ['method_citation']
  <S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
 <S sid="72" ssid="1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
    <S sid="87" ssid="16">It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'"]
'87'
['87']
parsed_discourse_facet ['method_citation']
    <S sid="51" ssid="37">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
    <S sid="79" ssid="8">Precision is the portion of hypothesized constituents that are correct and recall is the portion of the Treebank constituents that are hypothesized.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
  <S sid="27" ssid="13">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'"]
'27'
['27']
parsed_discourse_facet ['method_citation']
 <S sid="77" ssid="6">Each parse is converted into a set of constituents represented as a tuples: (label, start, end).</S>
original cit marker offset is 0
new cit marker offset is 0



["'77'"]
'77'
['77']
parsed_discourse_facet ['method_citation']
  <S sid="11" ssid="7">Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
  <S sid="116" ssid="45">The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'116'"]
'116'
['116']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0623.csv
<S sid ="1" ssid = "1">Three state-of-the-art statistical parsers are combined to produce more accurate parses  as well as new bounds on achievable Treebank parsing accuracy.</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="129" ssid = "58">In the interest of testing the robustness of these combining techniques  we added a fourth  simple nonlexicalized PCFG parser.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'72'", "'38'", "'129'", "'130'"]
'1'
'72'
'38'
'129'
'130'
['1', '72', '38', '129', '130']
Error in Discourse Facet
<S sid ="2" ssid = "2">Two general approaches are presented and two combination techniques are described for each approach.</S><S sid ="59" ssid = "45">Once again we present both a non-parametric and a parametric technique for this task.</S><S sid ="3" ssid = "3">Both parametric and non-parametric models are explored.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'59'", "'3'", "'85'", "'139'"]
'2'
'59'
'3'
'85'
'139'
['2', '59', '3', '85', '139']
Error in Discourse Facet
<S sid ="3" ssid = "3">Both parametric and non-parametric models are explored.</S><S sid ="59" ssid = "45">Once again we present both a non-parametric and a parametric technique for this task.</S><S sid ="2" ssid = "2">Two general approaches are presented and two combination techniques are described for each approach.</S><S sid ="138" ssid = "67">Surprisingly  the non-parametric switching technique also exhibited robust behaviour in this situation.</S><S sid ="136" ssid = "65">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'59'", "'2'", "'138'", "'136'"]
'3'
'59'
'2'
'138'
'136'
['3', '59', '2', '138', '136']
Error in Discourse Facet
<S sid ="4" ssid = "4">The resulting parsers surpass the best previously published performance results for the Penn Treebank.</S><S sid ="76" ssid = "5">The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="134" ssid = "63">As seen by the drop in average individual parser performance baseline  the introduced parser does not perform very well.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'76'", "'85'", "'130'", "'134'"]
'4'
'76'
'85'
'130'
'134'
['4', '76', '85', '130', '134']
Error in Discourse Facet
<S sid ="5" ssid = "1">The natural language processing community is in the strong position of having many available approaches to solving some of its most fundamental problems.</S><S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid ="26" ssid = "12">This technique has the advantage of requiring no training  but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'6'", "'12'", "'13'", "'26'"]
'5'
'6'
'12'
'13'
'26'
['5', '6', '12', '13', '26']
Error in Discourse Facet
<S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="108" ssid = "37">From this we see that a finer-grained model for parser combination  at least for the features we have examined  will not give us any additional power.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'11'", "'9'", "'12'", "'108'"]
'6'
'11'
'9'
'12'
'108'
['6', '11', '9', '12', '108']
Error in Discourse Facet
<S sid ="7" ssid = "3">Their theoretical finding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent  accurate classifiers.</S><S sid ="30" ssid = "16">This is equivalent to the assumption used in probability estimation for naïve Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.</S><S sid ="116" ssid = "45">The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="41" ssid = "27">IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'30'", "'116'", "'38'", "'41'"]
'7'
'30'
'116'
'38'
'41'
['7', '30', '116', '38', '41']
Error in Discourse Facet
<S sid ="8" ssid = "4">The theory has also been validated empirically.</S><S sid ="36" ssid = "22">The estimation of the probabilities in the model is carried out as shown in Equation 4.</S><S sid ="83" ssid = "12">We performed three experiments to evaluate our techniques.</S><S sid ="121" ssid = "50">Table 3 contains the results for evaluating our systems on the test set (section 22).</S><S sid ="109" ssid = "38">The results in Table 2 were achieved on the development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'36'", "'83'", "'121'", "'109'"]
'8'
'36'
'83'
'121'
'109'
['8', '36', '83', '121', '109']
Error in Discourse Facet
<S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="88" ssid = "17">For example  one parser could be more accurate at predicting noun phrases than the other parsers.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'55'", "'88'", "'21'", "'139'"]
'9'
'55'
'88'
'21'
'139'
['9', '55', '88', '21', '139']
Error in Discourse Facet
<S sid ="10" ssid = "6">In both cases the investigators were able to achieve significant improvements over the previous best tagging results.</S><S sid ="136" ssid = "65">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="141" ssid = "3">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid ="134" ssid = "63">As seen by the drop in average individual parser performance baseline  the introduced parser does not perform very well.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'136'", "'85'", "'141'", "'134'"]
'10'
'136'
'85'
'141'
'134'
['10', '136', '85', '141', '134']
Error in Discourse Facet
<S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'9'", "'6'", "'12'", "'13'"]
'11'
'9'
'6'
'12'
'13'
['11', '9', '6', '12', '13']
Error in Discourse Facet
<S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'13'", "'11'", "'6'", "'72'"]
'12'
'13'
'11'
'6'
'72'
['12', '13', '11', '6', '72']
Error in Discourse Facet
<S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="4" ssid = "4">The resulting parsers surpass the best previously published performance results for the Penn Treebank.</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'12'", "'4'", "'72'", "'130'"]
'13'
'12'
'4'
'72'
'130'
['13', '12', '4', '72', '130']
Error in Discourse Facet
<S sid ="14" ssid = "10">We used these three parsers to explore parser combination techniques.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="126" ssid = "55">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S><S sid ="140" ssid = "2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'139'", "'85'", "'126'", "'140'"]
'14'
'139'
'85'
'126'
'140'
['14', '139', '85', '126', '140']
Error in Discourse Facet
<S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="88" ssid = "17">For example  one parser could be more accurate at predicting noun phrases than the other parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'55'", "'27'", "'85'", "'88'"]
'15'
'55'
'27'
'85'
'88'
['15', '55', '27', '85', '88']
Error in Discourse Facet
<S sid ="16" ssid = "2">We call this approach parse hybridization.</S><S sid ="58" ssid = "44">We call this approach parser switching.</S><S sid ="23" ssid = "9">We call this technique constituent voting.</S><S sid ="96" ssid = "25">We call such a constituent an isolated constituent.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'58'", "'23'", "'96'", "'139'"]
'16'
'58'
'23'
'96'
'139'
['16', '58', '23', '96', '139']
Error in Discourse Facet
<S sid ="17" ssid = "3">The substructures that are unanimously hypothesized by the parsers should be preserved after combination  and the combination technique should not foolishly create substructures for which there is no supporting evidence.</S><S sid ="49" ssid = "35">In general  the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.</S><S sid ="117" ssid = "46">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S><S sid ="125" ssid = "54">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S><S sid ="95" ssid = "24">One side of the decision making process is when we choose to believe a constituent should be in the parse  even though only one parser suggests it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'49'", "'117'", "'125'", "'95'"]
'17'
'49'
'117'
'125'
'95'
['17', '49', '117', '125', '95']
Error in Discourse Facet
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
parsing: input/ref/Task1/D09-1092_swastika.csv
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="114" ssid="63">Ideally, the &#8220;glue&#8221; documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.</S>
original cit marker offset is 0
new cit marker offset is 0



['114']
114
['114']
parsed_discourse_facet ['aim_citation']
<S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



['138']
138
['138']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



['55']
55
['55']
parsed_discourse_facet ['method_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['192']
192
['192']
parsed_discourse_facet ['result_citation']
<S sid="119" ssid="68">The lower the divergence, the more similar the distributions are to each other.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['result_citation']
<S sid="148" ssid="97">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="163" ssid="112">Performance continues to improve with longer documents, most likely due to better topic inference.</S>
original cit marker offset is 0
new cit marker offset is 0



['163']
163
['163']
parsed_discourse_facet ['result_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="148" ssid="97">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['192']
192
['192']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['method_citation']
    <S sid="10" ssid="6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="105" ssid = "54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S><S sid ="194" ssid = "3">We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'25'", "'105'", "'194'", "'126'"]
'1'
'25'
'105'
'194'
'126'
['1', '25', '105', '194', '126']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Meanwhile  massive collections of interlinked documents in dozens of languages  such as Wikipedia  are now widely available  calling for tools that can characterize content in many languages.</S><S sid ="16" ssid = "12">However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.</S><S sid ="168" ssid = "2">However  the growth of the web  and in particular Wikipedia  has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid ="196" ssid = "5">When applied to comparable document collections such as Wikipedia  PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S><S sid ="105" ssid = "54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'16'", "'168'", "'196'", "'105'"]
'2'
'16'
'168'
'196'
'105'
['2', '16', '168', '196', '105']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="192" ssid = "1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="20" ssid = "16">We also explore how the characteristics of different languages affect topic model performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'18'", "'31'", "'20'"]
'3'
'192'
'18'
'31'
'20'
['3', '192', '18', '31', '20']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="22" ssid = "18">We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages  and to detect differences in topic emphasis between languages.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="10" ssid = "6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'22'", "'19'", "'10'", "'126'"]
'4'
'22'
'19'
'10'
'126'
['4', '22', '19', '10', '126']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Statistical topic models have emerged as an increasingly useful analysis tool for large text collections.</S><S sid ="15" ssid = "11">Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="6" ssid = "2">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'15'", "'18'", "'3'", "'6'"]
'5'
'15'
'18'
'3'
'6'
['5', '15', '18', '3', '6']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S><S sid ="26" ssid = "2">Tam  Lane and Schultz (Tam et al.  2007) also show improvements in machine translation using bilingual topic models.</S><S sid ="5" ssid = "1">Statistical topic models have emerged as an increasingly useful analysis tool for large text collections.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="29" ssid = "5">A recent extended abstract  developed concurrently by Ni et al. (Ni et al.  2009)  discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'26'", "'5'", "'18'", "'29'"]
'6'
'26'
'5'
'18'
'29'
['6', '26', '5', '18', '29']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Much of this work  however  has occurred in monolingual contexts.</S><S sid ="15" ssid = "11">Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'15'", "'31'", "'18'", "'161'"]
'7'
'15'
'31'
'18'
'161'
['7', '15', '31', '18', '161']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">In an increasingly connected world  the ability to access documents in many languages has become both a strategic asset and a personally enriching experience.</S><S sid ="16" ssid = "12">However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.</S><S sid ="168" ssid = "2">However  the growth of the web  and in particular Wikipedia  has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="108" ssid = "57">If we wish to perform topic-based bibliometric analysis  it is vital to be able to track the same topics across all languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'16'", "'168'", "'23'", "'108'"]
'8'
'16'
'168'
'23'
'108'
['8', '16', '168', '23', '108']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">In this paper  we present the polylingual topic model (PLTM).</S><S sid ="192" ssid = "1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="169" ssid = "3">In this section  we explore two questions relating to comparable text corpora and polylingual topic modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'192'", "'18'", "'3'", "'169'"]
'9'
'192'
'18'
'3'
'169'
['9', '192', '18', '3', '169']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="22" ssid = "18">We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages  and to detect differences in topic emphasis between languages.</S><S sid ="111" ssid = "60">In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts – i.e.  we put each of these documents in a single-document tuple.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'19'", "'22'", "'111'"]
'10'
'4'
'19'
'22'
'111'
['10', '4', '19', '22', '111']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">There are many potential applications for polylingual topic models.</S><S sid ="169" ssid = "3">In this section  we explore two questions relating to comparable text corpora and polylingual topic modeling.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'169'", "'3'", "'18'", "'31'"]
'11'
'169'
'3'
'18'
'31'
['11', '169', '3', '18', '31']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Although research literature is typically written in English  bibliographic databases often contain substantial quantities of work in other languages.</S><S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S><S sid ="174" ssid = "8">These versions of Wikipedia were selected to provide a diverse range of language families  geographic areas  and quantities of text.</S><S sid ="7" ssid = "3">Much of this work  however  has occurred in monolingual contexts.</S><S sid ="14" ssid = "10">Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'161'", "'174'", "'7'", "'14'"]
'12'
'161'
'174'
'7'
'14'
['12', '161', '174', '7', '14']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">To perform topic-based bibliometric analysis on these collections  it is necessary to have topic models that are aligned across languages.</S><S sid ="108" ssid = "57">If we wish to perform topic-based bibliometric analysis  it is vital to be able to track the same topics across all languages.</S><S sid ="196" ssid = "5">When applied to comparable document collections such as Wikipedia  PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S><S sid ="193" ssid = "2">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'108'", "'196'", "'126'", "'193'"]
'13'
'108'
'196'
'126'
'193'
['13', '108', '196', '126', '193']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.</S><S sid ="20" ssid = "16">We also explore how the characteristics of different languages affect topic model performance.</S><S sid ="170" ssid = "4">First  we explore whether comparable document tuples support the alignment of fine-grained topics  as demonstrated earlier using parallel documents.</S><S sid ="53" ssid = "2">In this case  we can be confident that the topic distribution is genuinely shared across all languages.</S><S sid ="68" ssid = "17">The third topic demonstrates differences in inflectional variation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'20'", "'170'", "'53'", "'68'"]
'14'
'20'
'170'
'53'
'68'
['14', '20', '170', '53', '68']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.</S><S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'52'", "'31'", "'3'", "'18'"]
'15'
'52'
'31'
'3'
'18'
['15', '52', '31', '3', '18']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.</S><S sid ="168" ssid = "2">However  the growth of the web  and in particular Wikipedia  has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid ="2" ssid = "2">Meanwhile  massive collections of interlinked documents in dozens of languages  such as Wikipedia  are now widely available  calling for tools that can characterize content in many languages.</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="21" ssid = "17">The second corpus  Wikipedia articles in twelve languages  contains sets of documents that are not translations of one another  but are very likely to be about similar concepts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'168'", "'2'", "'54'", "'21'"]
'16'
'168'
'2'
'54'
'21'
['16', '168', '2', '54', '21']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S><S sid ="194" ssid = "3">We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.</S><S sid ="105" ssid = "54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S><S sid ="13" ssid = "9">To perform topic-based bibliometric analysis on these collections  it is necessary to have topic models that are aligned across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'126'", "'194'", "'105'", "'13'"]
'17'
'126'
'194'
'105'
'13'
['17', '126', '194', '105', '13']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="169" ssid = "3">In this section  we explore two questions relating to comparable text corpora and polylingual topic modeling.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="9" ssid = "5">In this paper  we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'169'", "'3'", "'31'", "'9'"]
'18'
'169'
'3'
'31'
'9'
['18', '169', '3', '31', '9']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'148'", "'131'", "'4'", "'126'"]
'19'
'148'
'131'
'4'
'126'
['19', '148', '131', '4', '126']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/D09-1092_sweta.csv
 <S sid="32" ssid="8">Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="5">Additionally, PLTM assumes that each &#8220;topic&#8221; consists of a set of discrete distributions over words&#8212;one for each language l = 1, ... , L. In other words, rather than using a single set of topics &#934; = {&#966;1, ... , &#966;T}, as in LDA, there are L sets of language-specific topics, &#934;1, ... , &#934;L, each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter &#946;l.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



["138'"]
138'
['138']
parsed_discourse_facet ['method_citation']
 <S sid="196" ssid="5">When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>
original cit marker offset is 0
new cit marker offset is 0



["196'"]
196'
['196']
parsed_discourse_facet ['method_citation']
<S sid="111" ssid="60">In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts &#8211; i.e., we put each of these documents in a single-document tuple.</S>
original cit marker offset is 0
new cit marker offset is 0



["111'"]
111'
['111']
parsed_discourse_facet ['method_citation']
<S sid="128" ssid="77">Although the PLTM is clearly not a substitute for a machine translation system&#8212;it has no way to represent syntax or even multi-word phrases&#8212;it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.</S>
original cit marker offset is 0
new cit marker offset is 0



["128'"]
128'
['128']
parsed_discourse_facet ['method_citation']
<S sid="122" ssid="71">Divergence drops significantly when the proportion of &#8220;glue&#8221; tuples increases from 0.01 to 0.25.</S>
original cit marker offset is 0
new cit marker offset is 0



["122'"]
122'
['122']
parsed_discourse_facet ['method_citation']
 <S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
 <S sid="110" ssid="59">Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S>
original cit marker offset is 0
new cit marker offset is 0



["110'"]
110'
['110']
parsed_discourse_facet ['method_citation']
<S sid="30" ssid="6">However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["30'"]
30'
['30']
parsed_discourse_facet ['method_citation']
    <S sid="146" ssid="95">In addition to enhancing lexicons by aligning topic-specific vocabulary, PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["146'"]
146'
['146']
parsed_discourse_facet ['method_citation']
<S sid="77" ssid="26">Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.</
original cit marker offset is 0
new cit marker offset is 0



["77'"]
77'
['77']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="105">We use both Jensen-Shannon divergence and cosine distance.</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="7">They also provide little analysis of the differences between polylingual and single-language topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>
original cit marker offset is 0
new cit marker offset is 0



["19'"]
19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="131" ssid="80">We evaluate sets of high-probability words in each topic and multilingual &#8220;synsets&#8221; by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["131'"]
131'
['131']
parsed_discourse_facet ['method_citation']
<S sid="196" ssid="5">When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>
original cit marker offset is 0
new cit marker offset is 0



["196'"]
196'
['196']
parsed_discourse_facet ['method_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["192'"]
192'
['192']
parsed_discourse_facet ['method_citation']
<S sid="195" ssid="4">Additionally, PLTM can support the creation of bilingual lexica for low resource language pairs, providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["195'"]
195'
['195']
parsed_discourse_facet ['method_citation']
<S sid="6" ssid="2">Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["6'"]
6'
['6']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="105" ssid = "54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S><S sid ="194" ssid = "3">We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'25'", "'105'", "'194'", "'126'"]
'1'
'25'
'105'
'194'
'126'
['1', '25', '105', '194', '126']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Meanwhile  massive collections of interlinked documents in dozens of languages  such as Wikipedia  are now widely available  calling for tools that can characterize content in many languages.</S><S sid ="16" ssid = "12">However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.</S><S sid ="168" ssid = "2">However  the growth of the web  and in particular Wikipedia  has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid ="196" ssid = "5">When applied to comparable document collections such as Wikipedia  PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S><S sid ="105" ssid = "54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'16'", "'168'", "'196'", "'105'"]
'2'
'16'
'168'
'196'
'105'
['2', '16', '168', '196', '105']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="192" ssid = "1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="20" ssid = "16">We also explore how the characteristics of different languages affect topic model performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'18'", "'31'", "'20'"]
'3'
'192'
'18'
'31'
'20'
['3', '192', '18', '31', '20']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="22" ssid = "18">We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages  and to detect differences in topic emphasis between languages.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="10" ssid = "6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'22'", "'19'", "'10'", "'126'"]
'4'
'22'
'19'
'10'
'126'
['4', '22', '19', '10', '126']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Statistical topic models have emerged as an increasingly useful analysis tool for large text collections.</S><S sid ="15" ssid = "11">Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="6" ssid = "2">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'15'", "'18'", "'3'", "'6'"]
'5'
'15'
'18'
'3'
'6'
['5', '15', '18', '3', '6']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S><S sid ="26" ssid = "2">Tam  Lane and Schultz (Tam et al.  2007) also show improvements in machine translation using bilingual topic models.</S><S sid ="5" ssid = "1">Statistical topic models have emerged as an increasingly useful analysis tool for large text collections.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="29" ssid = "5">A recent extended abstract  developed concurrently by Ni et al. (Ni et al.  2009)  discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'26'", "'5'", "'18'", "'29'"]
'6'
'26'
'5'
'18'
'29'
['6', '26', '5', '18', '29']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Much of this work  however  has occurred in monolingual contexts.</S><S sid ="15" ssid = "11">Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'15'", "'31'", "'18'", "'161'"]
'7'
'15'
'31'
'18'
'161'
['7', '15', '31', '18', '161']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">In an increasingly connected world  the ability to access documents in many languages has become both a strategic asset and a personally enriching experience.</S><S sid ="16" ssid = "12">However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.</S><S sid ="168" ssid = "2">However  the growth of the web  and in particular Wikipedia  has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="108" ssid = "57">If we wish to perform topic-based bibliometric analysis  it is vital to be able to track the same topics across all languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'16'", "'168'", "'23'", "'108'"]
'8'
'16'
'168'
'23'
'108'
['8', '16', '168', '23', '108']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">In this paper  we present the polylingual topic model (PLTM).</S><S sid ="192" ssid = "1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="169" ssid = "3">In this section  we explore two questions relating to comparable text corpora and polylingual topic modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'192'", "'18'", "'3'", "'169'"]
'9'
'192'
'18'
'3'
'169'
['9', '192', '18', '3', '169']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="22" ssid = "18">We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages  and to detect differences in topic emphasis between languages.</S><S sid ="111" ssid = "60">In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts – i.e.  we put each of these documents in a single-document tuple.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'19'", "'22'", "'111'"]
'10'
'4'
'19'
'22'
'111'
['10', '4', '19', '22', '111']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">There are many potential applications for polylingual topic models.</S><S sid ="169" ssid = "3">In this section  we explore two questions relating to comparable text corpora and polylingual topic modeling.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'169'", "'3'", "'18'", "'31'"]
'11'
'169'
'3'
'18'
'31'
['11', '169', '3', '18', '31']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Although research literature is typically written in English  bibliographic databases often contain substantial quantities of work in other languages.</S><S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S><S sid ="174" ssid = "8">These versions of Wikipedia were selected to provide a diverse range of language families  geographic areas  and quantities of text.</S><S sid ="7" ssid = "3">Much of this work  however  has occurred in monolingual contexts.</S><S sid ="14" ssid = "10">Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'161'", "'174'", "'7'", "'14'"]
'12'
'161'
'174'
'7'
'14'
['12', '161', '174', '7', '14']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">To perform topic-based bibliometric analysis on these collections  it is necessary to have topic models that are aligned across languages.</S><S sid ="108" ssid = "57">If we wish to perform topic-based bibliometric analysis  it is vital to be able to track the same topics across all languages.</S><S sid ="196" ssid = "5">When applied to comparable document collections such as Wikipedia  PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S><S sid ="193" ssid = "2">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'108'", "'196'", "'126'", "'193'"]
'13'
'108'
'196'
'126'
'193'
['13', '108', '196', '126', '193']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.</S><S sid ="20" ssid = "16">We also explore how the characteristics of different languages affect topic model performance.</S><S sid ="170" ssid = "4">First  we explore whether comparable document tuples support the alignment of fine-grained topics  as demonstrated earlier using parallel documents.</S><S sid ="53" ssid = "2">In this case  we can be confident that the topic distribution is genuinely shared across all languages.</S><S sid ="68" ssid = "17">The third topic demonstrates differences in inflectional variation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'20'", "'170'", "'53'", "'68'"]
'14'
'20'
'170'
'53'
'68'
['14', '20', '170', '53', '68']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.</S><S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'52'", "'31'", "'3'", "'18'"]
'15'
'52'
'31'
'3'
'18'
['15', '52', '31', '3', '18']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.</S><S sid ="168" ssid = "2">However  the growth of the web  and in particular Wikipedia  has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid ="2" ssid = "2">Meanwhile  massive collections of interlinked documents in dozens of languages  such as Wikipedia  are now widely available  calling for tools that can characterize content in many languages.</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="21" ssid = "17">The second corpus  Wikipedia articles in twelve languages  contains sets of documents that are not translations of one another  but are very likely to be about similar concepts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'168'", "'2'", "'54'", "'21'"]
'16'
'168'
'2'
'54'
'21'
['16', '168', '2', '54', '21']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S><S sid ="194" ssid = "3">We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.</S><S sid ="105" ssid = "54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S><S sid ="13" ssid = "9">To perform topic-based bibliometric analysis on these collections  it is necessary to have topic models that are aligned across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'126'", "'194'", "'105'", "'13'"]
'17'
'126'
'194'
'105'
'13'
['17', '126', '194', '105', '13']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="169" ssid = "3">In this section  we explore two questions relating to comparable text corpora and polylingual topic modeling.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="9" ssid = "5">In this paper  we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'169'", "'3'", "'31'", "'9'"]
'18'
'169'
'3'
'31'
'9'
['18', '169', '3', '31', '9']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'148'", "'131'", "'4'", "'126'"]
'19'
'148'
'131'
'4'
'126'
['19', '148', '131', '4', '126']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/W99-0613_vardha.csv
    <S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="35" ssid="29">AdaBoost finds a weighted combination of simple (weak) classifiers, where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
    <S sid="134" ssid="1">This section describes an algorithm based on boosting algorithms, which were previously developed for supervised machine learning problems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'"]
'134'
['134']
parsed_discourse_facet ['method_citation']
    <S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'"]
'236'
['236']
parsed_discourse_facet ['method_citation']
    <S sid="8" ssid="2">Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
    <S sid="42" ssid="36">(Riloff and Shepherd 97) describe a bootstrapping approach for acquiring nouns in particular categories (such as &amp;quot;vehicle&amp;quot; or &amp;quot;weapon&amp;quot; categories).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
    <S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'"]
'236'
['236']
parsed_discourse_facet ['method_citation']
    <S sid="222" ssid="1">The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'222'"]
'222'
['222']
parsed_discourse_facet ['method_citation']
    <S sid="30" ssid="24">(Blum and Mitchell 98) offer a promising formulation of redundancy, also prove some results about how the use of can help classification, and suggest an objective function when training with unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'"]
'30'
['30']
parsed_discourse_facet ['method_citation']
 <S sid="26" ssid="20">We present two algorithms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'"]
'26'
['26']
parsed_discourse_facet ['method_citation']
    <S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="26">The algorithm can be viewed as heuristically optimizing an objective function suggested by (Blum and Mitchell 98); empirically it is shown to be quite successful in optimizing this criterion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'"]
'32'
['32']
parsed_discourse_facet ['method_citation']
    <S sid="47" ssid="1">971,746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'"]
'47'
['47']
parsed_discourse_facet ['method_citation']
    <S sid="127" ssid="60">The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'"]
'127'
['127']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="1" ssid = "1">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="9" ssid = "3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="136" ssid = "3">We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'250'", "'136'", "'163'"]
'1'
'9'
'250'
'136'
'163'
['1', '9', '250', '136', '163']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.</S><S sid ="17" ssid = "11">At first glance  the problem seems quite complex: a large number of rules is needed to cover the domain  suggesting that a large number of labeled examples is required to train an accurate classifier.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="105" ssid = "38">Thus the method makes the fairly strong assumption that the features can be partitioned into two types such that each type alone is sufficient for classification.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'17'", "'250'", "'105'", "'248'"]
'2'
'17'
'250'
'105'
'248'
['2', '17', '250', '105', '248']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid ="92" ssid = "25">Thus an explicit assumption about the redundancy of the features — that either the spelling or context alone should be sufficient to build a classifier — has been built into the algorithm.</S><S sid ="121" ssid = "54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).</S><S sid ="58" ssid = "12">Having found (spelling  context) pairs in the parsed data  a number of features are extracted.</S><S sid ="232" ssid = "11">For example  the independence assumptions mean that the model fails to capture the dependence between specific and more general features (for example the fact that the feature full.-string=New_York is always seen with the features contains (New) and The baseline method tags all entities as the most frequent class type (organization). contains (York) and is never seen with a feature such as contains (Group) ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'92'", "'121'", "'58'", "'232'"]
'3'
'92'
'121'
'58'
'232'
['3', '92', '121', '58', '232']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "20">We present two algorithms.</S><S sid ="4" ssid = "4">We present two algorithms.</S><S sid ="68" ssid = "1">The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S><S sid ="88" ssid = "21">We can now compare this algorithm to that of (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'4'", "'68'", "'163'", "'88'"]
'26'
'4'
'68'
'163'
'88'
['26', '4', '68', '163', '88']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid ="6" ssid = "6">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="68" ssid = "1">The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'31'", "'6'", "'97'", "'68'"]
'5'
'31'
'6'
'97'
'68'
['5', '31', '6', '97', '68']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "6">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid ="5" ssid = "5">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="30" ssid = "24">(Blum and Mitchell 98) offer a promising formulation of redundancy  also prove some results about how the use of unlabeled examples can help classification  and suggest an objective function when training with unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'31'", "'5'", "'204'", "'30'"]
'6'
'31'
'5'
'204'
'30'
['6', '31', '5', '204', '30']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision  in the form of labeled training examples.</S><S sid ="2" ssid = "2">A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="3" ssid = "3">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid ="130" ssid = "63">So the success of the algorithm may well be due to its success in maximizing the number of unlabeled examples on which the two decision lists agree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'2'", "'250'", "'3'", "'130'"]
'7'
'2'
'250'
'3'
'130'
['7', '2', '250', '3', '130']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "2">Recent results (e.g.  (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S><S sid ="27" ssid = "21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S><S sid ="120" ssid = "53">(Blum and Mitchell 98) go on to give PAC results for learning in the cotraining case.</S><S sid ="40" ssid = "34">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid ="88" ssid = "21">We can now compare this algorithm to that of (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'27'", "'120'", "'40'", "'88'"]
'8'
'27'
'120'
'40'
'88'
['8', '27', '120', '40', '88']
parsed_discourse_facet ['results_citation']
<S sid ="1" ssid = "1">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="9" ssid = "3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="136" ssid = "3">We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'250'", "'136'", "'163'"]
'1'
'9'
'250'
'136'
'163'
['1', '9', '250', '136', '163']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "4">The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.</S><S sid ="147" ssid = "14">The weak learner for two-class problems computes a weak hypothesis h from the input space into the reals (h : 2x -4 R)  where the sign4 of h(x) is interpreted as the predicted label and the magnitude I h(x)I is the confidence in the prediction: large numbers for I h(x)I indicate high confidence in the prediction  and numbers close to zero indicate low confidence.</S><S sid ="35" ssid = "29">AdaBoost finds a weighted combination of simple (weak) classifiers  where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.</S><S sid ="224" ssid = "3">The model was parameterized such that the joint probability of a (label  feature-set) pair P(yi  xi) is written as The model assumes that (y  x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).</S><S sid ="74" ssid = "7">Output of the learning algorithm: a function h:Xxy [0  1] where h(x  y) is an estimate of the conditional probability p(y1x) of seeing label y given that feature x is present.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'147'", "'35'", "'224'", "'74'"]
'10'
'147'
'35'
'224'
'74'
['10', '147', '35', '224', '74']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "5">For example  a good classifier would identify Mrs. Frank as a person  Steptoe & Johnson as a company  and Honduras as a location.</S><S sid ="40" ssid = "34">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid ="23" ssid = "17">For example  in ..  says Mr. Cooper  a vice president of.. both a spelling feature (that the string contains Mr.) and a contextual feature (that president modifies the string) are strong indications that Mr. Cooper is of type Person.</S><S sid ="48" ssid = "2">For example  take ...  says Maury Cooper  a vice president at S.&P.</S><S sid ="55" ssid = "9">In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'40'", "'23'", "'48'", "'55'"]
'11'
'40'
'23'
'48'
'55'
['11', '40', '23', '48', '55']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "6">The approach uses both spelling and contextual rules.</S><S sid ="58" ssid = "12">Having found (spelling  context) pairs in the parsed data  a number of features are extracted.</S><S sid ="22" ssid = "16">In many cases  inspection of either the spelling or context alone is sufficient to classify an example.</S><S sid ="165" ssid = "32">In the namedentity problem each example is a (spelling context) pair.</S><S sid ="55" ssid = "9">In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'58'", "'22'", "'165'", "'55'"]
'12'
'58'
'22'
'165'
'55'
['12', '58', '22', '165', '55']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "7">A spelling rule might be a simple look-up for the string (e.g.  a rule that Honduras is a location) or a rule that looks at words within a string (e.g.  a rule that any string containing Mr. is a person).</S><S sid ="14" ssid = "8">A contextual rule considers words surrounding the string in the sentence in which it appears (e.g.  a rule that any proper name modified by an appositive whose head is president is a person).</S><S sid ="61" ssid = "15">The following features were used: full-string=x The full string (e.g.  for Maury Cooper  full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word  this feature applies for any words that the string contains (e.g.  Maury Cooper contributes two such features  contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g.  IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods  and contains at least one period.</S><S sid ="22" ssid = "16">In many cases  inspection of either the spelling or context alone is sufficient to classify an example.</S><S sid ="165" ssid = "32">In the namedentity problem each example is a (spelling context) pair.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'14'", "'61'", "'22'", "'165'"]
'13'
'14'
'61'
'22'
'165'
['13', '14', '61', '22', '165']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W99-0613_sweta.csv
<S sid="121" ssid="54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page, and other pages pointing to the page).</S>
original cit marker offset is 0
new cit marker offset is 0



["121'"]
121'
['121']
parsed_discourse_facet ['method_citation']
<S sid="252" ssid="3">The method uses a &amp;quot;soft&amp;quot; measure of the agreement between two classifiers as an objective function; we described an algorithm which directly optimizes this function.</S>
original cit marker offset is 0
new cit marker offset is 0



["252'"]
252'
['252']
parsed_discourse_facet ['method_citation']
 <S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



["213'"]
213'
['213']
parsed_discourse_facet ['method_citation']
 <S sid="250" ssid="1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S>
original cit marker offset is 0
new cit marker offset is 0



["250'"]
250'
['250']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="33">(Brin 98) ,describes a system for extracting (author, book-title) pairs from the World Wide Web using an approach that bootstraps from an initial seed set of examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="202" ssid="69">The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels, and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.</S>
original cit marker offset is 0
new cit marker offset is 0



["202'"]
202'
['202']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="15">The following features were used: full-string=x The full string (e.g., for Maury Cooper, full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word, this feature applies for any words that the string contains (e.g., Maury Cooper contributes two such features, contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g., IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods, and contains at least one period.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="176" ssid="43">(7) is at 0 when: 1) Vi : sign(gi (xi)) = sign(g2 (xi)); 2) Ig3(xi)l oo; and 3) sign(gi (xi)) = yi for i = 1, , m. In fact, Zco provides a bound on the sum of the classification error of the labeled examples and the number of disagreements between the two classifiers on the unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["176'"]
176'
['176']
parsed_discourse_facet ['method_citation']
 <S sid="108" ssid="41">In the cotraining case, (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples, and (2) must agree with each other on the unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S>
    <S sid="28" ssid="22">(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'", "'28'"]
27'
'28'
['27', '28']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["7'"]
7'
['7']
parsed_discourse_facet ['method_citation']
 <S sid="172" ssid="39">To see this, note thai the first two terms in the above equation correspond to the function that AdaBoost attempts to minimize in the standard supervised setting (Equ.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["85'"]
85'
['85']
parsed_discourse_facet ['method_citation']
 <S sid="214" ssid="81">This modification brings the method closer to the DL-CoTrain algorithm described earlier, and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples, preventing one label from dominating &#8212; this deserves more theoretical investigation.</S>
original cit marker offset is 0
new cit marker offset is 0



["214'"]
214'
['214']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="1" ssid = "1">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="9" ssid = "3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="136" ssid = "3">We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'250'", "'136'", "'163'"]
'1'
'9'
'250'
'136'
'163'
['1', '9', '250', '136', '163']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.</S><S sid ="17" ssid = "11">At first glance  the problem seems quite complex: a large number of rules is needed to cover the domain  suggesting that a large number of labeled examples is required to train an accurate classifier.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="105" ssid = "38">Thus the method makes the fairly strong assumption that the features can be partitioned into two types such that each type alone is sufficient for classification.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'17'", "'250'", "'105'", "'248'"]
'2'
'17'
'250'
'105'
'248'
['2', '17', '250', '105', '248']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid ="92" ssid = "25">Thus an explicit assumption about the redundancy of the features — that either the spelling or context alone should be sufficient to build a classifier — has been built into the algorithm.</S><S sid ="121" ssid = "54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).</S><S sid ="58" ssid = "12">Having found (spelling  context) pairs in the parsed data  a number of features are extracted.</S><S sid ="232" ssid = "11">For example  the independence assumptions mean that the model fails to capture the dependence between specific and more general features (for example the fact that the feature full.-string=New_York is always seen with the features contains (New) and The baseline method tags all entities as the most frequent class type (organization). contains (York) and is never seen with a feature such as contains (Group) ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'92'", "'121'", "'58'", "'232'"]
'3'
'92'
'121'
'58'
'232'
['3', '92', '121', '58', '232']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "20">We present two algorithms.</S><S sid ="4" ssid = "4">We present two algorithms.</S><S sid ="68" ssid = "1">The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S><S sid ="88" ssid = "21">We can now compare this algorithm to that of (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'4'", "'68'", "'163'", "'88'"]
'26'
'4'
'68'
'163'
'88'
['26', '4', '68', '163', '88']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid ="6" ssid = "6">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="68" ssid = "1">The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'31'", "'6'", "'97'", "'68'"]
'5'
'31'
'6'
'97'
'68'
['5', '31', '6', '97', '68']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "6">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid ="5" ssid = "5">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="30" ssid = "24">(Blum and Mitchell 98) offer a promising formulation of redundancy  also prove some results about how the use of unlabeled examples can help classification  and suggest an objective function when training with unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'31'", "'5'", "'204'", "'30'"]
'6'
'31'
'5'
'204'
'30'
['6', '31', '5', '204', '30']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision  in the form of labeled training examples.</S><S sid ="2" ssid = "2">A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="3" ssid = "3">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid ="130" ssid = "63">So the success of the algorithm may well be due to its success in maximizing the number of unlabeled examples on which the two decision lists agree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'2'", "'250'", "'3'", "'130'"]
'7'
'2'
'250'
'3'
'130'
['7', '2', '250', '3', '130']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "2">Recent results (e.g.  (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S><S sid ="27" ssid = "21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S><S sid ="120" ssid = "53">(Blum and Mitchell 98) go on to give PAC results for learning in the cotraining case.</S><S sid ="40" ssid = "34">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid ="88" ssid = "21">We can now compare this algorithm to that of (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'27'", "'120'", "'40'", "'88'"]
'8'
'27'
'120'
'40'
'88'
['8', '27', '120', '40', '88']
parsed_discourse_facet ['results_citation']
<S sid ="1" ssid = "1">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="9" ssid = "3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="136" ssid = "3">We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'250'", "'136'", "'163'"]
'1'
'9'
'250'
'136'
'163'
['1', '9', '250', '136', '163']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "4">The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.</S><S sid ="147" ssid = "14">The weak learner for two-class problems computes a weak hypothesis h from the input space into the reals (h : 2x -4 R)  where the sign4 of h(x) is interpreted as the predicted label and the magnitude I h(x)I is the confidence in the prediction: large numbers for I h(x)I indicate high confidence in the prediction  and numbers close to zero indicate low confidence.</S><S sid ="35" ssid = "29">AdaBoost finds a weighted combination of simple (weak) classifiers  where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.</S><S sid ="224" ssid = "3">The model was parameterized such that the joint probability of a (label  feature-set) pair P(yi  xi) is written as The model assumes that (y  x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).</S><S sid ="74" ssid = "7">Output of the learning algorithm: a function h:Xxy [0  1] where h(x  y) is an estimate of the conditional probability p(y1x) of seeing label y given that feature x is present.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'147'", "'35'", "'224'", "'74'"]
'10'
'147'
'35'
'224'
'74'
['10', '147', '35', '224', '74']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "5">For example  a good classifier would identify Mrs. Frank as a person  Steptoe & Johnson as a company  and Honduras as a location.</S><S sid ="40" ssid = "34">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid ="23" ssid = "17">For example  in ..  says Mr. Cooper  a vice president of.. both a spelling feature (that the string contains Mr.) and a contextual feature (that president modifies the string) are strong indications that Mr. Cooper is of type Person.</S><S sid ="48" ssid = "2">For example  take ...  says Maury Cooper  a vice president at S.&P.</S><S sid ="55" ssid = "9">In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'40'", "'23'", "'48'", "'55'"]
'11'
'40'
'23'
'48'
'55'
['11', '40', '23', '48', '55']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "6">The approach uses both spelling and contextual rules.</S><S sid ="58" ssid = "12">Having found (spelling  context) pairs in the parsed data  a number of features are extracted.</S><S sid ="22" ssid = "16">In many cases  inspection of either the spelling or context alone is sufficient to classify an example.</S><S sid ="165" ssid = "32">In the namedentity problem each example is a (spelling context) pair.</S><S sid ="55" ssid = "9">In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'58'", "'22'", "'165'", "'55'"]
'12'
'58'
'22'
'165'
'55'
['12', '58', '22', '165', '55']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "7">A spelling rule might be a simple look-up for the string (e.g.  a rule that Honduras is a location) or a rule that looks at words within a string (e.g.  a rule that any string containing Mr. is a person).</S><S sid ="14" ssid = "8">A contextual rule considers words surrounding the string in the sentence in which it appears (e.g.  a rule that any proper name modified by an appositive whose head is president is a person).</S><S sid ="61" ssid = "15">The following features were used: full-string=x The full string (e.g.  for Maury Cooper  full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word  this feature applies for any words that the string contains (e.g.  Maury Cooper contributes two such features  contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g.  IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods  and contains at least one period.</S><S sid ="22" ssid = "16">In many cases  inspection of either the spelling or context alone is sufficient to classify an example.</S><S sid ="165" ssid = "32">In the namedentity problem each example is a (spelling context) pair.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'14'", "'61'", "'22'", "'165'"]
'13'
'14'
'61'
'22'
'165'
['13', '14', '61', '22', '165']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
parsing: input/ref/Task1/D10-1044_aakansha.csv
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S>
    <S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'96'"]
'95'
'96'
['95', '96']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'"]
'75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="28">For comparison to information-retrieval inspired baselines, eg (L&#168;u et al., 2007), we select sentences from OUT using language model perplexities from IN.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'"]
'22'
['22']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="23">The 2nd block contains the IR system, which was tuned by selecting text in multiples of the size of the EMEA training corpus, according to dev set performance.</S>
    <S sid="120" ssid="24">This significantly underperforms log-linear combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'", "'120'"]
'119'
'120'
['119', '120']
parsed_discourse_facet ['result_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
    <S sid="24" ssid="21">Sentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="4">We focus here instead on adapting the two most important features: the language model (LM), which estimates the probability p(wIh) of a target word w following an ngram h; and the translation models (TM) p(slt) and p(t1s), which give the probability of source phrase s translating to target phrase t, and vice versa.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D10-1044.csv
<S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="145" ssid = "2">Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="62" ssid = "26">To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'65'", "'145'", "'64'", "'62'"]
'1'
'65'
'145'
'64'
'62'
['1', '65', '145', '64', '62']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S><S sid ="23" ssid = "20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="153" ssid = "10">Finally  we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.</S><S sid ="68" ssid = "5">First  we learn weights on individual phrase pairs rather than sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'23'", "'20'", "'153'", "'68'"]
'2'
'23'
'20'
'153'
'68'
['2', '23', '20', '153', '68']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">We incorporate instance weighting into a mixture-model framework  and find that it yields consistent improvements over a wide range of baselines.</S><S sid ="150" ssid = "7">In both cases  the instanceweighting approach improved over a wide range of baselines  giving gains of over 2 BLEU points over the best non-adapted baseline  and gains of between 0.6 and 1.8 over an equivalent mixture model (with an identical training procedure but without instance weighting).</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'150'", "'65'", "'38'", "'1'"]
'3'
'150'
'65'
'38'
'1'
['3', '150', '65', '38', '1']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'10'", "'6'", "'24'", "'17'"]
'4'
'10'
'6'
'24'
'17'
['4', '10', '6', '24', '17']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "2">Even when there is training data available in the domain of interest  there is often additional data from other domains that could in principle be used to improve performance.</S><S sid ="138" ssid = "7">However  for multinomial models like our LMs and TMs  there is a one to one correspondence between instances and features  eg the correspondence between a phrase pair (s  t) and its conditional multinomial probability p(s1t).</S><S sid ="9" ssid = "6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="39" ssid = "3">Thus  provided at least this amount of IN data is available—as it is in our setting—adapting these weights is straightforward.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'138'", "'9'", "'6'", "'39'"]
'5'
'138'
'9'
'6'
'39'
['5', '138', '9', '6', '39']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'17'", "'24'", "'18'", "'4'"]
'6'
'17'
'24'
'18'
'4'
['6', '17', '24', '18', '4']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="9" ssid = "6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'24'", "'21'", "'4'", "'9'"]
'7'
'24'
'21'
'4'
'9'
['7', '24', '21', '4', '9']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'10'", "'18'", "'6'", "'20'"]
'8'
'10'
'18'
'6'
'20'
['8', '10', '18', '6', '20']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S><S sid ="144" ssid = "1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S><S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="42" ssid = "6">The natural baseline approach is to concatenate data from IN and OUT.</S><S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'144'", "'1'", "'42'", "'2'"]
'9'
'144'
'1'
'42'
'2'
['9', '144', '1', '42', '2']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="29" ssid = "26">This is a simple and effective alternative to setting weights discriminatively to maximize a metric such as BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'18'", "'6'", "'29'"]
'10'
'4'
'18'
'6'
'29'
['10', '4', '18', '6', '29']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="44" ssid = "8">When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.</S><S sid ="92" ssid = "29">6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'18'", "'6'", "'44'", "'92'"]
'11'
'18'
'6'
'44'
'92'
['11', '18', '6', '44', '92']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "9">For simplicity  we assume that OUT is homogeneous.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="92" ssid = "29">6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.</S><S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'18'", "'11'", "'92'", "'16'"]
'12'
'18'
'11'
'92'
'16'
['12', '18', '11', '92', '16']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "10">The techniques we develop can be extended in a relatively straightforward manner to the more general case when OUT consists of multiple sub-domains.</S><S sid ="139" ssid = "8">As mentioned above  it is not obvious how to apply Daum´e’s approach to multinomials  which do not have a mechanism for combining split features.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="82" ssid = "19">This is not unreasonable given the application to phrase pairs from OUT  but it suggests that an interesting alternative might be to use a plain log-linear weighting function exp(Ei Aifi(s  t))  with outputs in [0  oo].</S><S sid ="32" ssid = "29">This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'139'", "'21'", "'82'", "'32'"]
'13'
'139'
'21'
'82'
'32'
['13', '139', '21', '82', '32']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "11">There is a fairly large body of work on SMT adaptation.</S><S sid ="141" ssid = "10">Moving beyond directly related work  major themes in SMT adaptation include the IR (Hildebrand et al.  2005; L¨u et al.  2007; Zhao et al.  2004) and mixture (Finch and Sumita  2008; Foster and Kuhn  2007; Koehn and Schroeder  2007; L¨u et al.  2007) approaches for LMs and TMs described above  as well as methods for exploiting monolingual in-domain text  typically by translating it automatically and then performing self training (Bertoldi and Federico  2009; Ueffing et al.  2007; Schwenk and Senellart  2009).</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="34" ssid = "31">Section 2 describes our baseline techniques for SMT adaptation  and section 3 describes the instance-weighting approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'141'", "'4'", "'10'", "'34'"]
'14'
'141'
'4'
'10'
'34'
['14', '141', '4', '10', '34']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "12">We introduce several new ideas.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="27" ssid = "24">Finally  we make some improvements to baseline approaches.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="88" ssid = "25">We have not explored this strategy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'67'", "'27'", "'20'", "'88'"]
'15'
'67'
'27'
'20'
'88'
['15', '67', '27', '20', '88']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="42" ssid = "6">The natural baseline approach is to concatenate data from IN and OUT.</S><S sid ="68" ssid = "5">First  we learn weights on individual phrase pairs rather than sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'17'", "'24'", "'42'", "'68'"]
'16'
'17'
'24'
'42'
'68'
['16', '17', '24', '42', '68']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S><S sid ="100" ssid = "4">Figure 1 shows sample sentences from these domains  which are widely divergent.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'6'", "'24'", "'16'", "'100'"]
'17'
'6'
'24'
'16'
'100'
['17', '6', '24', '16', '100']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="44" ssid = "8">When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'11'", "'44'", "'6'", "'10'"]
'18'
'11'
'44'
'6'
'10'
['18', '11', '44', '6', '10']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "16">The idea of distinguishing between general and domain-specific examples is due to Daum´e and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daum´e’s (2007) work that splits each feature into domain-specific and general copies.</S><S sid ="73" ssid = "10">Jiang and Zhai (2007) suggest the following derivation  making use of the true OUT distribution po(s  t): where each fi(s  t) is a feature intended to charac- !0ˆ = argmax pf(s  t) log pθ(s|t) (8) terize the usefulness of (s  t)  weighted by Ai. θ s t pf(s  t)po(s  t) log pθ(s|t) The mixing parameters and feature weights (col- != argmax po (s  t) lectively 0) are optimized simultaneously using dev- θ s t pf(s  t)co(s  t) log pθ(s|t)  set maximum likelihood as before: !�argmax po (s  t) ! θ s t �ˆ = argmax ˜p(s  t) log p(s|t; 0).</S><S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'136'", "'73'", "'140'", "'64'"]
'19'
'136'
'73'
'140'
'64'
['19', '136', '73', '140', '64']
parsed_discourse_facet ['method_citation']



D10-1044
E12-1055
0
method_citation
['method_citation']
parsing: input/ref/Task1/P11-1061_aakansha.csv
<S sid="40" ssid="6">We extend Subramanya et al.&#8217;s intuitions to our bilingual setup.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="11">First, we use a novel graph-based framework for projecting syntactic information across language boundaries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="29" ssid="6">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'"]
'158'
['158']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'"]
'158'
['158']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="1">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="17" ssid="13">Second, we treat the projected labels as features in an unsupervised model (&#167;5), rather than using them directly for supervised training.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="111" ssid="11">We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).</S>
original cit marker offset is 0
new cit marker offset is 0



["'111'"]
'111'
['111']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="29" ssid="6">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'161'"]
'161'
['161']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1061.csv
<S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S><S sid ="8" ssid = "4">Unsupervised learning approaches appear to be a natural solution to this problem  as they require only unannotated text for training models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'160'", "'3'", "'13'", "'8'"]
'1'
'160'
'3'
'13'
'8'
['1', '160', '3', '13', '8']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed)  making it applicable to a wide array of resource-poor languages.</S><S sid ="9" ssid = "5">Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.</S><S sid ="126" ssid = "26">Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.</S><S sid ="41" ssid = "7">Because the information flow in our graph is asymmetric (from English to the foreign language)  we use different types of vertices for each language.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'9'", "'126'", "'41'", "'107'"]
'2'
'9'
'126'
'41'
'107'
['2', '9', '126', '41', '107']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="33" ssid = "10">The POS distributions over the foreign trigram types are used as features to learn a better unsupervised POS tagger (§5).</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="60" ssid = "26">To initialize the graph for label propagation we use a supervised English tagger to label the English side of the bitext.7 We then simply count the individual labels of the English tokens and normalize the counts to produce tag distributions over English word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'39'", "'33'", "'1'", "'60'"]
'3'
'39'
'33'
'1'
'60'
['3', '39', '33', '1', '60']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S><S sid ="23" ssid = "19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%)  and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S sid ="143" ssid = "6">Overall  it gives improvements ranging from 1.1% for German to 14.7% for Italian  for an average improvement of 8.3% over the unsupervised feature-HMM model.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S><S sid ="144" ssid = "7">For comparison  the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%  and goes up to 88.7% with a treebank dictionary.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'23'", "'143'", "'135'", "'144'"]
'4'
'23'
'143'
'135'
'144'
['4', '23', '143', '135', '144']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing  resulting in highly accurate systems.</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S><S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S><S sid ="15" ssid = "11">First  we use a novel graph-based framework for projecting syntactic information across language boundaries.</S><S sid ="18" ssid = "14">To make the projection practical  we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'158'", "'4'", "'15'", "'18'"]
'5'
'158'
'4'
'15'
'18'
['5', '158', '4', '15', '18']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Supervised part-of-speech (POS) taggers  for example  approach the level of inter-annotator agreement (Shen et al.  2007  97.3% accuracy for English).</S><S sid ="47" ssid = "13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S><S sid ="9" ssid = "5">Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S><S sid ="20" ssid = "16">Because there might be some controversy about the exact definitions of such universals  this set of coarse-grained POS categories is defined operationally  by collapsing language (or treebank) specific distinctions to a set of categories that exists across all languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'47'", "'9'", "'158'", "'20'"]
'6'
'47'
'9'
'158'
'20'
['6', '47', '9', '158', '20']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S><S sid ="27" ssid = "4">Graph construction does not require any labeled data  but makes use of two similarity functions.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'27'", "'107'", "'56'", "'37'"]
'7'
'27'
'107'
'56'
'37'
['7', '27', '107', '56', '37']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Unsupervised learning approaches appear to be a natural solution to this problem  as they require only unannotated text for training models.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'160'", "'1'", "'13'", "'3'"]
'8'
'160'
'1'
'13'
'3'
['8', '160', '1', '13', '3']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.</S><S sid ="47" ssid = "13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S><S sid ="126" ssid = "26">Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.</S><S sid ="2" ssid = "2">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed)  making it applicable to a wide array of resource-poor languages.</S><S sid ="57" ssid = "23">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'47'", "'126'", "'2'", "'57'"]
'9'
'47'
'126'
'2'
'57'
['9', '47', '126', '2', '57']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'39'", "'16'", "'26'", "'13'"]
'10'
'39'
'16'
'26'
'13'
['10', '39', '16', '26', '13']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This scenario is applicable to a large set of languages and has been considered by a number of authors in the past (Alshawi et al.  2000; Xi and Hwa  2005; Ganchev et al.  2009).</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="94" ssid = "25">We adopted this state-of-the-art model because it makes it easy to experiment with various ways of incorporating our novel constraint feature into the log-linear emission model.</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'19'", "'26'", "'94'", "'13'"]
'11'
'19'
'26'
'94'
'13'
['11', '19', '26', '94', '13']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Naseem et al. (2009) and Snyder et al.</S><S sid ="38" ssid = "4">More recently  Subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model.</S><S sid ="93" ssid = "24">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S><S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="123" ssid = "23">This can be seen as a rough approximation of Yarowsky and Ngai (2001).</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'38'", "'93'", "'37'", "'123'"]
'12'
'38'
'93'
'37'
'123'
['12', '38', '93', '37', '123']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="24" ssid = "1">The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'39'", "'160'", "'1'", "'24'"]
'13'
'39'
'160'
'1'
'24'
['13', '39', '160', '1', '24']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">Our work is closest to that of Yarowsky and Ngai (2001)  but differs in two important ways.</S><S sid ="123" ssid = "23">This can be seen as a rough approximation of Yarowsky and Ngai (2001).</S><S sid ="40" ssid = "6">We extend Subramanya et al.’s intuitions to our bilingual setup.</S><S sid ="88" ssid = "19">In our experiments  we used the same set of features as BergKirkpatrick et al. (2010): an indicator feature based In a traditional Markov model  the emission distribution PΘ(Xi = xi  |Zi = zi) is a set of multinomials.</S><S sid ="85" ssid = "16">We develop our POS induction model based on the feature-based HMM of Berg-Kirkpatrick et al. (2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'123'", "'40'", "'88'", "'85'"]
'14'
'123'
'40'
'88'
'85'
['14', '123', '40', '88', '85']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">First  we use a novel graph-based framework for projecting syntactic information across language boundaries.</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'158'", "'29'", "'56'", "'25'"]
'15'
'158'
'29'
'56'
'25'
['15', '158', '29', '56', '25']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="57" ssid = "23">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid ="70" ssid = "1">Given the bilingual graph described in the previous section  we can use label propagation to project the English POS labels to the foreign language.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'57'", "'70'", "'29'", "'26'"]
'16'
'57'
'70'
'29'
'26'
['16', '57', '70', '29', '26']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">Second  we treat the projected labels as features in an unsupervised model (§5)  rather than using them directly for supervised training.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="71" ssid = "2">We use label propagation in two stages to generate soft labels on all the vertices in the graph.</S><S sid ="126" ssid = "26">Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.</S><S sid ="52" ssid = "18">This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'97'", "'71'", "'126'", "'52'"]
'17'
'97'
'71'
'126'
'52'
['17', '97', '71', '126', '52']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">To make the projection practical  we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="111" ssid = "11">We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns)  VERB (verbs)  ADJ (adjectives)  ADV (adverbs)  PRON (pronouns)  DET (determiners)  ADP (prepositions or postpositions)  NUM (numerals)  CONJ (conjunctions)  PRT (particles)  PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).</S><S sid ="85" ssid = "16">We develop our POS induction model based on the feature-based HMM of Berg-Kirkpatrick et al. (2010).</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'56'", "'111'", "'85'", "'158'"]
'18'
'56'
'111'
'85'
'158'
['18', '56', '111', '85', '158']
parsed_discourse_facet ['method_citation']



P11-1061
P12-3012
0
method_citation
['method_citation']
parsing: input/ref/Task1/P05-1013_aakansha.csv
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="17">However, the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech, with a best performance of 73% UAS (Holan, 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="86" ssid="13">As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'"]
'86'
['86']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="7">As shown in Table 3, the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.</S>
    <S sid="81"  ssid="8">However, the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'81'"]
'80'
'81'
['80', '81']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'"]
'49'
['49']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="10">While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P05-1013.csv
<S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="13" ssid = "9">The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="16" ssid = "12">Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.</S><S sid ="80" ssid = "7">As shown in Table 3  the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'13'", "'7'", "'16'", "'80'"]
'1'
'13'
'7'
'16'
'80'
['1', '13', '7', '16', '80']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="44" ssid = "15">Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="30" ssid = "1">We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.</S><S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'44'", "'36'", "'30'", "'8'"]
'2'
'44'
'36'
'30'
'8'
['2', '44', '36', '30', '8']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy.</S><S sid ="28" ssid = "24">First  in section 4  we evaluate the graph transformation techniques in themselves  with data from the Prague Dependency Treebank and the Danish Dependency Treebank.</S><S sid ="29" ssid = "25">In section 5  we then evaluate the entire parsing system by training and evaluating on data from the Prague Dependency Treebank.</S><S sid ="62" ssid = "1">In the experiments below  we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs 3 previously tested on Swedish (Nivre et al.  2004) and English (Nivre and Scholz  2004).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'28'", "'29'", "'62'", "'75'"]
'3'
'28'
'29'
'62'
'75'
['3', '28', '29', '62', '75']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'104'", "'106'", "'78'", "'8'"]
'4'
'104'
'106'
'78'
'8'
['4', '104', '106', '78', '8']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order  where discontinuous syntactic constructions are more common than in languages like English (Mel’ˇcuk  1988; Covington  1990).</S><S sid ="94" ssid = "5">The first thing to note is that projectivizing helps in itself  even if no encoding is used  as seen from the fact that the projective baseline outperforms the non-projective training condition by more than half a percentage point on attachment score  although the gain is much smaller with respect to exact match.</S><S sid ="99" ssid = "10">This may seem surprising  given the experiments reported in section 4  but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift  where the encoding of path information is often redundant.</S><S sid ="9" ssid = "5">This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).</S><S sid ="47" ssid = "18">In practice  we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'94'", "'99'", "'9'", "'47'"]
'5'
'94'
'99'
'9'
'47'
['5', '94', '99', '9', '47']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.</S><S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="44" ssid = "15">Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.</S><S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'2'", "'36'", "'44'", "'89'"]
'6'
'2'
'36'
'44'
'89'
['6', '2', '36', '44', '89']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="16" ssid = "12">Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.</S><S sid ="19" ssid = "15">Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).</S><S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="47" ssid = "18">In practice  we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'16'", "'19'", "'1'", "'47'"]
'7'
'16'
'19'
'1'
'47'
['7', '16', '19', '1', '47']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S><S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="30" ssid = "1">We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'2'", "'104'", "'4'", "'30'"]
'8'
'2'
'104'
'4'
'30'
['8', '2', '104', '4', '30']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).</S><S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="17" ssid = "13">There exist a few robust broad-coverage parsers that produce non-projective dependency structures  notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English  Foth et al. (2004) for German  and Holan (2004) for Czech.</S><S sid ="105" ssid = "16">Although the best published results for the Collins parser is 80% UAS (Collins  1999)  this parser reaches 82% when trained on the entire training data set  and an adapted version of Charniak’s parser (Charniak  2000) performs at 84% (Jan Hajiˇc  pers. comm.).</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'10'", "'17'", "'105'", "'24'"]
'9'
'10'
'17'
'105'
'24'
['9', '10', '17', '105', '24']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="42" ssid = "13">Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="12" ssid = "8">Prague Dependency Treebank (Hajiˇc et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'42'", "'36'", "'4'", "'12'"]
'10'
'42'
'36'
'4'
'12'
['10', '42', '36', '4', '12']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="6" ssid = "2">However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'6'", "'78'", "'10'", "'36'"]
'11'
'6'
'78'
'10'
'36'
['11', '6', '78', '10', '36']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Prague Dependency Treebank (Hajiˇc et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S><S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="42" ssid = "13">Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'10'", "'36'", "'42'", "'78'"]
'12'
'10'
'36'
'42'
'78'
['12', '10', '36', '42', '78']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.</S><S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'2'", "'95'", "'1'", "'104'"]
'13'
'2'
'95'
'1'
'104'
['13', '2', '95', '1', '104']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 15–25%  the total proportion of non-projective arcs is normally only 1–2%.</S><S sid ="80" ssid = "7">As shown in Table 3  the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S><S sid ="100" ssid = "11">It is likely that the more complex cases  where path information could make a difference  are beyond the reach of the parser in most cases.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'80'", "'106'", "'81'", "'100'"]
'14'
'80'
'106'
'81'
'100'
['14', '80', '106', '81', '100']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">As long as the main evaluation metric is dependency accuracy per word  with state-of-the-art accuracy mostly below 90%  the penalty for not handling non-projective constructions is almost negligible.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="101" ssid = "12">However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.</S><S sid ="110" ssid = "2">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="105" ssid = "16">Although the best published results for the Collins parser is 80% UAS (Collins  1999)  this parser reaches 82% when trained on the entire training data set  and an adapted version of Charniak’s parser (Charniak  2000) performs at 84% (Jan Hajiˇc  pers. comm.).</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'106'", "'101'", "'110'", "'105'"]
'15'
'106'
'101'
'110'
'105'
['15', '106', '101', '110', '105']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="110" ssid = "2">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="19" ssid = "15">Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'7'", "'1'", "'110'", "'19'"]
'16'
'7'
'1'
'110'
'19'
['16', '7', '1', '110', '19']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'104'", "'106'", "'78'", "'8'"]
'4'
'104'
'106'
'78'
'8'
['4', '104', '106', '78', '8']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">In addition  there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington  1990; Kahane et al.  1998; Duchier and Debusmann  2001; Holan et al.  2001; Hellwig  2003).</S><S sid ="17" ssid = "13">There exist a few robust broad-coverage parsers that produce non-projective dependency structures  notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English  Foth et al. (2004) for German  and Holan (2004) for Czech.</S><S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S><S sid ="100" ssid = "11">It is likely that the more complex cases  where path information could make a difference  are beyond the reach of the parser in most cases.</S><S sid ="109" ssid = "1">We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'17'", "'20'", "'100'", "'109'"]
'18'
'17'
'20'
'100'
'109'
['18', '17', '20', '100', '109']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "15">Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="13" ssid = "9">The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.</S><S sid ="107" ssid = "18">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'7'", "'95'", "'13'", "'107'"]
'19'
'7'
'95'
'13'
'107'
['19', '7', '95', '13', '107']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S><S sid ="109" ssid = "1">We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'109'", "'24'", "'60'", "'95'"]
'20'
'109'
'24'
'60'
'95'
['20', '109', '24', '60', '95']
parsed_discourse_facet ['aim_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



P05-1013
D07-1111
0
method_citation
['aim_citation']



P05-1013
D08-1008
0
method_citation
['method_citation']
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P08-1043_aakansha.csv
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="69" ssid="1">We represent all morphological analyses of a given utterance using a lattice structure.</S>
    <S sid="70" ssid="2">Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'70'"]
'69'
'70'
['69', '70']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="17">The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty, 2006) and (Cohen and Smith, 2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="11">Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
    <S sid="134" ssid="12">Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training 	similar to (Cohen and Smith, 2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'134'"]
'133'
'134'
['133', '134']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="17">The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="155" ssid="33">Evaluation We use 8 different measures to evaluate the performance of our system on the joint disambiguation task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S sid ="1" ssid = "1">Morphological processes in Semitic languages deliver space-delimited words which introduce multiple  distinct  syntactic units into the structure of the input sentence.</S><S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="39" ssid = "18">Such ambiguities cause discrepancies between token boundaries (indexed as white spaces) and constituent boundaries (imposed by syntactic categories) with respect to a surface form.</S><S sid ="34" ssid = "13">Lexical and Morphological Ambiguity The rich morphological processes for deriving Hebrew stems give rise to a high degree of ambiguity for Hebrew space-delimited tokens.</S><S sid ="24" ssid = "3">Co-occurrences among the particles themselves are subject to further syntactic and lexical constraints relative to the stem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'14'", "'39'", "'34'", "'24'"]
'1'
'14'
'39'
'34'
'24'
['1', '14', '39', '34', '24']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.</S><S sid ="16" ssid = "12">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S><S sid ="94" ssid = "26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S><S sid ="20" ssid = "16">We claim that no particular morphological segmentation is a-priory more likely for surface forms before exploring the compositional nature of syntactic structures  including manifestations of various long-distance dependencies.</S><S sid ="191" ssid = "5">In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'16'", "'94'", "'20'", "'191'"]
'2'
'16'
'94'
'20'
'191'
['2', '16', '94', '20', '191']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S><S sid ="19" ssid = "15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S sid ="187" ssid = "1">Employing a PCFG-based generative framework to make both syntactic and morphological disambiguation decisions is not only theoretically clean and linguistically justified and but also probabilistically apropriate and empirically sound.</S><S sid ="189" ssid = "3">Better grammars are shown here to improve performance on both morphological and syntactic tasks  providing support for the advantage of a joint framework over pipelined or factorized ones.</S><S sid ="21" ssid = "17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'19'", "'187'", "'189'", "'21'"]
'3'
'19'
'187'
'189'
'21'
['3', '19', '187', '189', '21']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Using a treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined  integrated or factorized systems for Hebrew morphological and syntactic processing  yielding an error reduction of 12% over the best published results so far.</S><S sid ="21" ssid = "17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S><S sid ="185" ssid = "23">In contrast  our morphological probabilities are based on a unigram  lexeme-based model  and all other (local and non-local) contextual considerations are delegated to the PCFG.</S><S sid ="188" ssid = "2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S><S sid ="191" ssid = "5">In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'21'", "'185'", "'188'", "'191'"]
'4'
'21'
'185'
'188'
'191'
['4', '21', '185', '188', '191']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  ‘tokens’) that constitute the unanalyzed surface forms (utterances).</S><S sid ="12" ssid = "8">This leads to word- and constituent-boundaries discrepancy  which breaks the assumptions underlying current state-of-the-art statistical parsers.</S><S sid ="94" ssid = "26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S><S sid ="21" ssid = "17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S><S sid ="16" ssid = "12">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'12'", "'94'", "'21'", "'16'"]
'5'
'12'
'94'
'21'
'16'
['5', '12', '94', '21', '16']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">In Semitic languages the situation is very different.</S><S sid ="78" ssid = "10">A similar structure is used in speech recognition.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="99" ssid = "31">This is done using a simple PCFG which is lexemebased.</S><S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'78'", "'92'", "'99'", "'180'"]
'6'
'78'
'92'
'99'
'180'
['6', '78', '92', '99', '180']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).</S><S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="17" ssid = "13">Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task.</S><S sid ="8" ssid = "4">The Hebrew token ‘bcl’1  for example  stands for the complete prepositional phrase 'We adopt here the transliteration of (Sima’an et al.  2001).</S><S sid ="29" ssid = "8">A less canonical representation of segmental morphology is triggered by a morpho-phonological process of omitting the definite article h when occurring after the particles b or l. This process triggers ambiguity as for the definiteness status of Nouns following these particles.We refer to such cases in which the concatenation of elements does not strictly correspond to the original surface form as super-segmental morphology.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'14'", "'17'", "'8'", "'29'"]
'7'
'14'
'17'
'8'
'29'
['7', '14', '17', '8', '29']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">The Hebrew token ‘bcl’1  for example  stands for the complete prepositional phrase 'We adopt here the transliteration of (Sima’an et al.  2001).</S><S sid ="35" ssid = "14">The form fmnh  for example  can be understood as the verb “lubricated”  the possessed noun “her oil”  the adjective “fat” or the verb “got fat”.</S><S sid ="62" ssid = "9">When a token fmnh is to be interpreted as the lexeme sequence f/REL mnh/VB  the analysis introduces two distinct entities  the relativizer f (“that”) and the verb mnh (“counted”)  and not as the complex entity “that counted”.</S><S sid ="7" ssid = "3">In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).</S><S sid ="15" ssid = "11">The aforementioned surface form bcl  for example  may also stand for the lexical item “onion”  a Noun.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'35'", "'62'", "'7'", "'15'"]
'8'
'35'
'62'
'7'
'15'
['8', '35', '62', '7', '15']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">“in the shadow”.</S><S sid ="184" ssid = "22">Oracle results).</S><S sid ="109" ssid = "41">The latter arcs correspond to OOV words in English.</S><S sid ="113" ssid = "45">The remaining arcs are marked OOV.</S><S sid ="78" ssid = "10">A similar structure is used in speech recognition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'184'", "'109'", "'113'", "'78'"]
'9'
'184'
'109'
'113'
'78'
['9', '184', '109', '113', '78']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">This token may further embed into a larger utterance  e.g.  ‘bcl hneim’ (literally “in-the-shadow the-pleasant”  meaning roughly “in the pleasant shadow”) in which the dominated Noun is modified by a proceeding space-delimited adjective.</S><S sid ="15" ssid = "11">The aforementioned surface form bcl  for example  may also stand for the lexical item “onion”  a Noun.</S><S sid ="76" ssid = "8">Furthermore  some of the arcs represent lexemes not present in the input tokens (e.g. h/DT  fl/POS)  however these are parts of valid analyses of the token (cf. super-segmental morphology section 2).</S><S sid ="58" ssid = "5">Such tag sequences are often treated as “complex tags” (e.g.</S><S sid ="108" ssid = "40">Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p —* (s  p)) > 0  while other segments have never been observed as a lexical event before.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'15'", "'76'", "'58'", "'108'"]
'10'
'15'
'76'
'58'
'108'
['10', '15', '76', '58', '108']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">It should be clear from the onset that the particle b (“in”) in ‘bcl’ may then attach higher than the bare noun cl (“shadow”).</S><S sid ="37" ssid = "16">The same form fmnh can be segmented as f-mnh  f (“that”) functioning as a reletivizer with the form mnh.</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="38" ssid = "17">The form mnh itself can be read as at least three different verbs (“counted”  “appointed”  “was appointed”)  a noun (“a portion”)  and a possessed noun (“her kind”).</S><S sid ="26" ssid = "5">The relativizer f(“that”) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'37'", "'57'", "'38'", "'26'"]
'11'
'37'
'57'
'38'
'26'
['11', '37', '57', '38', '26']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">This leads to word- and constituent-boundaries discrepancy  which breaks the assumptions underlying current state-of-the-art statistical parsers.</S><S sid ="5" ssid = "1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  ‘tokens’) that constitute the unanalyzed surface forms (utterances).</S><S sid ="13" ssid = "9">One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).</S><S sid ="2" ssid = "2">These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.</S><S sid ="19" ssid = "15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'5'", "'13'", "'2'", "'19'"]
'12'
'5'
'13'
'2'
'19'
['12', '5', '13', '2', '19']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).</S><S sid ="33" ssid = "12">The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).</S><S sid ="140" ssid = "18">For these models we limit the options provided for OOV words by not considering the entire token as a valid segmentation in case at least some prefix segmentation exists.</S><S sid ="139" ssid = "17">To control for the effect of the HSPELL-based pruning  we also experimented with a morphological analyzer that does not perform this pruning.</S><S sid ="3" ssid = "3">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'33'", "'140'", "'139'", "'3'"]
'13'
'33'
'140'
'139'
'3'
['13', '33', '140', '139', '3']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="7" ssid = "3">In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).</S><S sid ="140" ssid = "18">For these models we limit the options provided for OOV words by not considering the entire token as a valid segmentation in case at least some prefix segmentation exists.</S><S sid ="17" ssid = "13">Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task.</S><S sid ="121" ssid = "53">In such cases we use the non-pruned lattice including all (possibly ungrammatical) segmentation  and let the statistics (including OOV) decide.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'7'", "'140'", "'17'", "'121'"]
'14'
'7'
'140'
'17'
'121'
['14', '7', '140', '17', '121']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">The aforementioned surface form bcl  for example  may also stand for the lexical item “onion”  a Noun.</S><S sid ="32" ssid = "11">The additional morphological material in such cases appears after the stem and realizes the extended meaning.</S><S sid ="28" ssid = "7">The same argument holds for resolving PP attachment of a prefixed preposition or marking conjunction of elements of any kind.</S><S sid ="10" ssid = "6">This token may further embed into a larger utterance  e.g.  ‘bcl hneim’ (literally “in-the-shadow the-pleasant”  meaning roughly “in the pleasant shadow”) in which the dominated Noun is modified by a proceeding space-delimited adjective.</S><S sid ="30" ssid = "9">An additional case of super-segmental morphology is the case of Pronominal Clitics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'32'", "'28'", "'10'", "'30'"]
'15'
'32'
'28'
'10'
'30'
['15', '32', '28', '10', '30']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S><S sid ="94" ssid = "26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S><S sid ="188" ssid = "2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S><S sid ="13" ssid = "9">One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).</S><S sid ="2" ssid = "2">These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'94'", "'188'", "'13'", "'2'"]
'16'
'94'
'188'
'13'
'2'
['16', '94', '188', '13', '2']
parsed_discourse_facet ['method_citation']



P08-1043
P10-1074
0
method_citation
['method_citation']
parsing: input/ref/Task1/J01-2004_aakansha.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'"]
'372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="17" ssid="5">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



["'302'"]
'302'
['302']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="37">This underspecification of the nonterminal predictions (e.g., VP-VBD in the example in Figure 2, as opposed to NP), allows lexical items to become part of the left context, and so be used to condition production probabilities, even the production probabilities of constituents that dominate them in the unfactored tree.</S>
    <S sid="80" ssid="38">It also brings words further downstream into the look-ahead at the point of specification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'", "'80'"]
'79'
'80'
['79', '80']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="42">Our approach is found to yield very accurate parses efficiently, and, in addition, to lend itself straightforwardly to estimating word probabilities on-line, that is, in a single pass from left to right.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="291" ssid="47">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'291'"]
'291'
['291']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['method_citation']
<S sid="209" ssid="113">This parser is essentially a stochastic version of the top-down parser described in Aho, Sethi, and Ullman (1986).</S>
    <S sid="210" ssid="114">It uses a PCFG with a conditional probability model of the sort defined in the previous section.</S>
original cit marker offset is 0
new cit marker offset is 0



["'209'", "'210'"]
'209'
'210'
['209', '210']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'"]
'372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="8">Two features of our top-down parsing approach will emerge as key to its success.</S>
    <S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S><S sid="32" ssid="20">A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'21'", "'32'"]
'20'
'21'
'32'
['20', '21', '32']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="21">Because the rooted partial derivation is fully connected, all of the conditioning information that might be extracted from the top-down left context has already been specified, and a conditional probability model built on this information will not impose any additional burden on the search.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'"]
'33'
['33']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S sid ="1" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="7" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="209" ssid = "113">This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).</S><S sid ="132" ssid = "36">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid ="277" ssid = "33">The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989 860 words  39 832 sentences) of the Penn Treebank (Marcus  Santorini  and Marcinkiewicz 1993) served as the training data  Section 24 (34 199 words  1 346 sentences) as the held-out data for parameter estimation  and Section 23 (59 100 words  2 416 sentences) as the test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'7'", "'209'", "'132'", "'277'"]
'1'
'7'
'209'
'132'
'277'
['1', '7', '209', '132', '277']
parsed_discourse_facet ['aim_citation']
<S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="41" ssid = "29">There will also be a brief review of previous work using syntactic information for language modeling  before we introduce our model in Section 4.</S><S sid ="18" ssid = "6">Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture long-distance dependencies for language modeling will be reviewed.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'2'", "'41'", "'18'", "'4'"]
'8'
'2'
'41'
'18'
'4'
['8', '2', '41', '18', '4']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="9" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'390'", "'10'", "'4'"]
'3'
'9'
'390'
'10'
'4'
['3', '9', '390', '10', '4']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="100" ssid = "4">The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'100'", "'2'", "'8'"]
'10'
'4'
'100'
'2'
'8'
['10', '4', '100', '2', '8']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="11" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="379" ssid = "135">While the improvements over the trigram model in these trials are modest  they do indicate that our model is robust enough to provide good information even in the face of noisy input.</S><S sid ="300" ssid = "56">The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'11'", "'379'", "'300'", "'404'"]
'5'
'11'
'379'
'300'
'404'
['5', '11', '379', '300', '404']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="6" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="39" ssid = "27">The top-down guidance that is provided makes this approach quite efficient in practice.</S><S sid ="388" ssid = "1">The empirical results presented above are quite encouraging  and the potential of this kind of approach both for parsing and language modeling seems very promising.</S><S sid ="114" ssid = "18">He was able to reduce both sentence and word error rates on the ATIS corpus using this method.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'6'", "'39'", "'388'", "'114'"]
'12'
'6'
'39'
'388'
'114'
['12', '6', '39', '388', '114']
parsed_discourse_facet ['method_citation']
<S sid ="1" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="7" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="209" ssid = "113">This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).</S><S sid ="132" ssid = "36">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid ="277" ssid = "33">The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989 860 words  39 832 sentences) of the Penn Treebank (Marcus  Santorini  and Marcinkiewicz 1993) served as the training data  Section 24 (34 199 words  1 346 sentences) as the held-out data for parameter estimation  and Section 23 (59 100 words  2 416 sentences) as the test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'7'", "'209'", "'132'", "'277'"]
'1'
'7'
'209'
'132'
'277'
['1', '7', '209', '132', '277']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="41" ssid = "29">There will also be a brief review of previous work using syntactic information for language modeling  before we introduce our model in Section 4.</S><S sid ="18" ssid = "6">Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture long-distance dependencies for language modeling will be reviewed.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'2'", "'41'", "'18'", "'4'"]
'8'
'2'
'41'
'18'
'4'
['8', '2', '41', '18', '4']
parsed_discourse_facet ['results_citation']
<S sid ="3" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="9" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'390'", "'10'", "'4'"]
'3'
'9'
'390'
'10'
'4'
['3', '9', '390', '10', '4']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="100" ssid = "4">The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'100'", "'2'", "'8'"]
'10'
'4'
'100'
'2'
'8'
['10', '4', '100', '2', '8']
parsed_discourse_facet ['method_citation']
<S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="11" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="379" ssid = "135">While the improvements over the trigram model in these trials are modest  they do indicate that our model is robust enough to provide good information even in the face of noisy input.</S><S sid ="300" ssid = "56">The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'11'", "'379'", "'300'", "'404'"]
'5'
'11'
'379'
'300'
'404'
['5', '11', '379', '300', '404']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="6" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="39" ssid = "27">The top-down guidance that is provided makes this approach quite efficient in practice.</S><S sid ="388" ssid = "1">The empirical results presented above are quite encouraging  and the potential of this kind of approach both for parsing and language modeling seems very promising.</S><S sid ="114" ssid = "18">He was able to reduce both sentence and word error rates on the ATIS corpus using this method.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'6'", "'39'", "'388'", "'114'"]
'12'
'6'
'39'
'388'
'114'
['12', '6', '39', '388', '114']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "1">With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.</S><S sid ="14" ssid = "2">Perhaps one reason for this is that  until relatively recently  few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.</S><S sid ="15" ssid = "3">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S><S sid ="383" ssid = "139">It was selected with the goal of high parser accuracy; but in this new domain  parser accuracy is a secondary measure of performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'14'", "'15'", "'404'", "'383'"]
'13'
'14'
'15'
'404'
'383'
['13', '14', '15', '404', '383']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "2">Perhaps one reason for this is that  until relatively recently  few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.</S><S sid ="13" ssid = "1">With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.</S><S sid ="300" ssid = "56">The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.</S><S sid ="15" ssid = "3">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid ="319" ssid = "75">The hope  however  is that the improved parsing model provided by our conditional probability model will cause the distribution over structures to be more peaked  thus enabling us to capture more of the total probability mass  and making this a fairly snug upper bound on the perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'13'", "'300'", "'15'", "'319'"]
'14'
'13'
'300'
'15'
'319'
['14', '13', '300', '15', '319']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "3">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid ="97" ssid = "1">There have been attempts to jump over adjacent words to words farther back in the left context  without the use of dependency links or syntactic structure  for example Saul and Pereira (1997) and Rosenfeld (1996  1997).</S><S sid ="13" ssid = "1">With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.</S><S sid ="38" ssid = "26">This has lead us to a formulation of the conditional probability model in terms of values returned from tree-walking functions that themselves are contextually sensitive.</S><S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'97'", "'13'", "'38'", "'5'"]
'15'
'97'
'13'
'38'
'5'
['15', '97', '13', '38', '5']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "4">While language models built around shallow local dependencies are still the standard in state-of-the-art speech recognition systems  there is reason to hope that better language models can and will be developed by computational linguists for this task.</S><S sid ="397" ssid = "10">A top-down parser  in contrast to a standard bottom-up chart parser  has enough information to predict empty categories only where they are likely to occur.</S><S sid ="139" ssid = "43">This on-line characteristic allows our language model to be interpolated on a word-by-word basis with other models  such as the trigram  yielding further improvements.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'397'", "'139'", "'4'", "'10'"]
'16'
'397'
'139'
'4'
'10'
['16', '397', '139', '4', '10']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "5">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S sid ="132" ssid = "36">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid ="88" ssid = "46">This section will briefly introduce language modeling for statistical speech recognition.'</S><S sid ="40" ssid = "28">The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.</S><S sid ="247" ssid = "3">Perplexity is a standard measure within the speech recognition community for comparing language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'132'", "'88'", "'40'", "'247'"]
'17'
'132'
'88'
'40'
'247'
['17', '132', '88', '40', '247']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P08-1102_swastika.csv
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="10">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="8">All feature templates and their instances are shown in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



['36']
36
['36']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
<S sid="97" ssid="8">Figure 3 shows their learning curves depicting the F-measure on the development set after 1 to 10 training iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



['97']
97
['97']
parsed_discourse_facet ['result_citation']
<S sid="91" ssid="2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['aim_citation']
<S sid="16" ssid="12">Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



['34']
34
['34']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="130" ssid = "1">We proposed a cascaded linear model for Chinese Joint S&T.</S><S sid ="76" ssid = "1">Sequence segmentation and labelling problem can be solved through a viterbi style decoding procedure.</S><S sid ="24" ssid = "20">It is a better idea to perform segmentation and POS tagging jointly in a uniform framework.</S><S sid ="4" ssid = "4">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'130'", "'76'", "'24'", "'4'"]
'1'
'130'
'76'
'24'
'4'
['1', '130', '76', '24', '4']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="64" ssid = "15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'16'", "'133'", "'64'", "'134'"]
'2'
'16'
'133'
'64'
'134'
['2', '16', '133', '64', '134']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid ="21" ssid = "17">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S sid ="106" ssid = "17">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.</S><S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="4" ssid = "4">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'21'", "'106'", "'1'", "'4'"]
'3'
'21'
'106'
'1'
'4'
['3', '21', '106', '1', '4']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S><S sid ="127" ssid = "38">Finally  the word count penalty gives improvement to the cascaded model  0.13 points on segmentation and 0.16 points on Joint S&T.</S><S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="123" ssid = "34">Without it  the F-measure on segmentation and Joint S&T both suffer a decrement of 0.2 points.</S><S sid ="118" ssid = "29">We find that the cascaded model achieves a F-measure increment of about 0.5 points on segmentation and about 0.9 points on Joint S&T  over the perceptron-only model POS+.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'127'", "'1'", "'123'", "'118'"]
'4'
'127'
'1'
'123'
'118'
['4', '127', '1', '123', '118']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages.</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="51" ssid = "2">Additional features most widely used are related to word or POS ngrams.</S><S sid ="114" ssid = "25">We used SRI Language Modelling Toolkit (Stolcke and Andreas  2002) to train a 3gram word LM with modified Kneser-Ney smoothing (Chen and Goodman  1998)  and a 4-gram POS LM with Witten-Bell smoothing  and we trained a word-POS co-occurrence model simply by MLE without smoothing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'31'", "'1'", "'51'", "'114'"]
'5'
'31'
'1'
'51'
'114'
['5', '31', '1', '51', '114']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Several models were introduced for these problems  for example  the Hidden Markov Model (HMM) (Rabiner  1989)  Maximum Entropy Model (ME) (Ratnaparkhi and Adwait  1996)  and Conditional Random Fields (CRFs) (Lafferty et al.  2001).</S><S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="15" ssid = "11">To cope with this problem  we propose a cascaded linear model inspired by the log-linear model (Och and Ney  2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'8'", "'134'", "'15'", "'45'"]
'6'
'8'
'134'
'15'
'45'
['6', '8', '134', '15', '45']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S><S sid ="100" ssid = "11">Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS  CityU and MSR).</S><S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'8'", "'100'", "'14'", "'31'"]
'7'
'8'
'100'
'14'
'31'
['7', '8', '100', '14', '31']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="29" ssid = "1">The perceptron algorithm introduced into NLP by Collins (2002)  is a simple but effective discriminative training method.</S><S sid ="115" ssid = "26">To obtain their corresponding weights  we adapted the minimum-error-rate training algorithm (Och  2003) to train the outside-layer model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'45'", "'49'", "'29'", "'115'"]
'8'
'45'
'49'
'29'
'115'
['8', '45', '49', '29', '115']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid ="23" ssid = "19">We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag.</S><S sid ="11" ssid = "7">Compared to performing segmentation and POS tagging one at a time  Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low  2004).</S><S sid ="80" ssid = "5">At each position i  we enumerate all possible word-POS pairs by assigning each POS to each possible word formed from the character subsequence spanning length l = L. min(i  K) (K is assigned 20 in all our experiments) and ending at position i  then we derive all candidate results by attaching each word-POS pair p (of length l) to the tail of each candidate result at the prior position of p (position i−l)  and select for position i a N-best list of candidate results from all these candidates.</S><S sid ="81" ssid = "6">When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p  we calculate the scores of the word LM  the POS LM  the labelling probability and the generating probability  Algorithm 2 Decoding algorithm. as well as the score of the perceptron model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'23'", "'11'", "'80'", "'81'"]
'9'
'23'
'11'
'80'
'81'
['9', '23', '11', '80', '81']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen  2003)  Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low  2004).</S><S sid ="25" ssid = "21">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S><S sid ="23" ssid = "19">We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="106" ssid = "17">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'25'", "'23'", "'7'", "'106'"]
'10'
'25'
'23'
'7'
'106'
['10', '25', '23', '7', '106']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">Compared to performing segmentation and POS tagging one at a time  Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low  2004).</S><S sid ="9" ssid = "5">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid ="3" ssid = "3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid ="26" ssid = "22">In order to perform POS tagging at the same time  we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004).</S><S sid ="106" ssid = "17">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'9'", "'3'", "'26'", "'106'"]
'11'
'9'
'3'
'26'
'106'
['11', '9', '3', '26', '106']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Besides the usual character-based features  additional features dependent on POS’s or words can also be employed to improve the performance.</S><S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid ="54" ssid = "5">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S><S sid ="51" ssid = "2">Additional features most widely used are related to word or POS ngrams.</S><S sid ="82" ssid = "7">In addition  we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'14'", "'54'", "'51'", "'82'"]
'12'
'14'
'54'
'51'
'82'
['12', '14', '54', '51', '82']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">However  as such features are generated dynamically during the decoding procedure  two limitation arise: on the one hand  the amount of parameters increases rapidly  which is apt to overfit on training corpus; on the other hand  exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.</S><S sid ="52" ssid = "3">However  such features are generated dynamically during the decoding procedure so that the feature space enlarges much more rapidly.</S><S sid ="55" ssid = "6">In addition  even though these higher grams were managed to be used  there still remains another problem: as the current predication relies on the results of prior ones  the decoding procedure has to resort to approximate inference by maintaining a list of N-best candidates at each predication position  which evokes a potential risk to depress the training.</S><S sid ="131" ssid = "2">Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.</S><S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'52'", "'55'", "'131'", "'14'"]
'13'
'52'
'55'
'131'
'14'
['13', '52', '55', '131', '14']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid ="12" ssid = "8">Besides the usual character-based features  additional features dependent on POS’s or words can also be employed to improve the performance.</S><S sid ="131" ssid = "2">Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.</S><S sid ="132" ssid = "3">This is a substitute method to use both local and non-local features  and it would be especially useful when the training corpus is very large.</S><S sid ="54" ssid = "5">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'12'", "'131'", "'132'", "'54'"]
'14'
'12'
'131'
'132'
'54'
['14', '12', '131', '132', '54']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">To cope with this problem  we propose a cascaded linear model inspired by the log-linear model (Och and Ney  2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.</S><S sid ="56" ssid = "7">To alleviate the drawbacks  we propose a cascaded linear model.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'56'", "'49'", "'45'", "'8'"]
'15'
'56'
'49'
'45'
'8'
['15', '56', '49', '45', '8']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="65" ssid = "16">As shown in Figure 1  the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.</S><S sid ="57" ssid = "8">It has a two-layer architecture  with a perceptron as the core and another linear model as the outside-layer.</S><S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="78" ssid = "3">Given a Chinese character sequence C1:n  the decoding procedure can proceed in a left-right fashion with a dynamic programming approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'65'", "'57'", "'2'", "'78'"]
'16'
'65'
'57'
'2'
'78'
['16', '65', '57', '2', '78']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W11-2123_aakansha.csv
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="136" ssid="8">We offer a state function s(wn1) = wn&#65533; where substring wn&#65533; is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'"]
'136'
['136']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="3">Queries take the form p(wn|wn&#8722;1 1 ) where wn1 is an n-gram.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="205" ssid="24">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al., 2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'205'"]
'205'
['205']
parsed_discourse_facet ['method_citation']
<S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
<S sid="204" ssid="23">For RandLM, we used the settings in the documentation: 8 bits per value and false positive probability 1 256.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'"]
'204'
['204']
parsed_discourse_facet ['method_citation']
<S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="229" ssid="48">Then we ran binary search to determine the least amount of memory with which it would run.</S>
original cit marker offset is 0
new cit marker offset is 0



["'229'"]
'229'
['229']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="93" ssid="71">The cost of storing these averages, in bits, is Because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.</S>
original cit marker offset is 0
new cit marker offset is 0



["'93'"]
'93'
['93']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W11-2123.csv
<S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid ="7" ssid = "2">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'274'", "'7'", "'259'", "'45'"]
'1'
'274'
'7'
'259'
'45'
['1', '274', '7', '259', '45']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">The structure uses linear probing hash tables and is designed for speed.</S><S sid ="194" ssid = "13">Further  the probing hash table does only one random lookup per query  explaining why it is faster on large data.</S><S sid ="23" ssid = "1">We implement two data structures: PROBING  designed for speed  and TRIE  optimized for memory.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S><S sid ="120" ssid = "24">The compressed variant uses block compression and is rather slow as a result.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'194'", "'23'", "'45'", "'120'"]
'2'
'194'
'23'
'45'
'120'
['2', '194', '23', '45', '120']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S><S sid ="128" ssid = "32">These models generally outperform our memory consumption but are much slower  even when cached.</S><S sid ="161" ssid = "33">In both cases  SRILM walks its trie an additional time to minimize context as mentioned in Section 4.1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'275'", "'276'", "'128'", "'161'"]
'3'
'275'
'276'
'128'
'161'
['3', '275', '276', '128', '161']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Our code is thread-safe  and integrated into the Moses  cdec  and Joshua translation systems.</S><S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="169" ssid = "41">In our case multi-threading is trivial because our data structures are read-only and uncached.</S><S sid ="282" ssid = "3">Adam Pauls provided a pre-release comparison to BerkeleyLM and an initial Java interface.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'279'", "'169'", "'282'", "'266'"]
'4'
'279'
'169'
'282'
'266'
['4', '279', '169', '282', '266']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid ="26" ssid = "4">We use two common techniques  hash tables and sorted arrays  describing each before the model that uses the technique.</S><S sid ="129" ssid = "1">In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'274'", "'26'", "'129'"]
'5'
'1'
'274'
'26'
'129'
['5', '1', '274', '26', '129']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "1">Language models are widely applied in natural language processing  and applications such as machine translation make very frequent queries.</S><S sid ="130" ssid = "2">Applications such as machine translation use language model probability as a feature to assist in choosing between hypotheses.</S><S sid ="184" ssid = "3">Sparse lookup is a key subproblem of language model queries.</S><S sid ="7" ssid = "2">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'130'", "'184'", "'7'", "'259'"]
'6'
'130'
'184'
'7'
'259'
['6', '130', '184', '7', '259']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "2">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'1'", "'274'", "'259'", "'45'"]
'7'
'1'
'274'
'259'
'45'
['7', '1', '274', '259', '45']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "3">Queries take the form p(wn|wn−1 1 ) where wn1 is an n-gram.</S><S sid ="84" ssid = "62">In a model we built with default settings  1.2% of n + 1-grams were missing their ngram suffix.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="72" ssid = "50">We maintain a separate array for each length n containing all n-gram entries sorted in suffix order.</S><S sid ="46" ssid = "24">Unigram lookup is dense so we use an array of probability and backoff values.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'84'", "'88'", "'72'", "'46'"]
'8'
'84'
'88'
'72'
'46'
['8', '84', '88', '72', '46']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "4">Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf   returning where the probability p(wn|wn−1 f ) and backoff penalties b(wn−1 i ) are given by an already-estimated model.</S><S sid ="127" ssid = "31">Minimal perfect hashing is used to find the index at which a quantized probability and possibly backoff are stored.</S><S sid ="136" ssid = "8">We offer a state function s(wn1) = wn� where substring wn� is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.</S><S sid ="78" ssid = "56">Unigram records store probability  backoff  and an index in the bigram table.</S><S sid ="154" ssid = "26">To optimize left-to-right queries  we extend state to store backoff information: where m is the minimal context from Section 4.1 and b is the backoff penalty.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'127'", "'136'", "'78'", "'154'"]
'9'
'127'
'136'
'78'
'154'
['9', '127', '136', '78', '154']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "5">The problem is to store these two values for a large and sparse set of n-grams in a way that makes queries efficient.</S><S sid ="25" ssid = "3">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S><S sid ="24" ssid = "2">The set of n-grams appearing in a model is sparse  and we want to efficiently find their associated probabilities and backoff penalties.</S><S sid ="93" ssid = "71">The cost of storing these averages  in bits  is Because there are comparatively few unigrams  we elected to store them byte-aligned and unquantized  making every query faster.</S><S sid ="85" ssid = "63">This causes a problem for reverse trie implementations  including SRILM itself  because it leaves n+1-grams without an n-gram node pointing to them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'25'", "'24'", "'93'", "'85'"]
'10'
'25'
'24'
'93'
'85'
['10', '25', '24', '93', '85']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "6">Many packages perform language model queries.</S><S sid ="68" ssid = "46">The trie data structure is commonly used for language modeling.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'68'", "'266'", "'259'", "'45'"]
'11'
'68'
'266'
'259'
'45'
['11', '68', '266', '259', '45']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke  2002) is a popular toolkit based on tries used in several decoders.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="97" ssid = "1">SRILM (Stolcke  2002) is widely used within academia.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'13'", "'14'", "'103'", "'97'"]
'12'
'13'
'14'
'103'
'97'
['12', '13', '14', '103', '97']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'103'", "'15'", "'14'", "'262'"]
'13'
'103'
'15'
'14'
'262'
['13', '103', '15', '14', '262']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="97" ssid = "1">SRILM (Stolcke  2002) is widely used within academia.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'15'", "'103'", "'97'", "'266'"]
'14'
'15'
'103'
'97'
'266'
['14', '15', '103', '97', '266']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'14'", "'103'", "'13'", "'262'"]
'15'
'14'
'103'
'13'
'262'
['15', '14', '103', '13', '262']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "11">BerkeleyLM revision 152 (Pauls and Klein  2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.</S><S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="51" ssid = "29">This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'15'", "'51'", "'262'", "'13'"]
'16'
'15'
'51'
'262'
'13'
['16', '15', '51', '262', '13']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "12">Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.</S><S sid ="18" ssid = "13">TPT Germann et al. (2009) describe tries with better locality properties  but did not release code.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="215" ssid = "34">Another option is the closedsource data structures from Sheffield (Guthrie and Hepple  2010).</S><S sid ="114" ssid = "18">Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'18'", "'262'", "'215'", "'114'"]
'17'
'18'
'262'
'215'
'114'
['17', '18', '262', '215', '114']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "13">TPT Germann et al. (2009) describe tries with better locality properties  but did not release code.</S><S sid ="17" ssid = "12">Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.</S><S sid ="232" ssid = "51">Memory usage is likely much lower than ours. fThe original paper (Germann et al.  2009) provided only 2s of query timing and compared with SRI when it exceeded available RAM.</S><S sid ="114" ssid = "18">Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'17'", "'232'", "'114'", "'262'"]
'18'
'17'
'232'
'114'
'262'
['18', '17', '232', '114', '262']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "14">These packages are further described in Section 3.</S><S sid ="129" ssid = "1">In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="5" ssid = "5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S sid ="150" ssid = "22">Section 4.1 explained that state s is stored by applications with partial hypotheses to determine when they can be recombined.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'129'", "'266'", "'5'", "'150'"]
'19'
'129'
'266'
'5'
'150'
['19', '129', '266', '5', '150']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "15">We substantially outperform all of them on query speed and offer lower memory consumption than lossless alternatives.</S><S sid ="128" ssid = "32">These models generally outperform our memory consumption but are much slower  even when cached.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S><S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'128'", "'276'", "'1'", "'275'"]
'20'
'128'
'276'
'1'
'275'
['20', '128', '276', '1', '275']
parsed_discourse_facet ['aim_citation']



W11-2123
P14-2022
0
method_citation
['method_citation']



W11-2123
W11-2138
0
method_citation
['aim_citation']



W11-2123
W11-2139
0
method_citation
['method_citation']
parsing: input/ref/Task1/P87-1015_vardha.csv
<S sid="165" ssid="50">This class of formalisms have the properties that their derivation trees are local sets, and manipulate objects, using a finite number of composition operations that use a finite number of symbols.</S>
original cit marker offset is 0
new cit marker offset is 0



["'165'"]
'165'
['165']
parsed_discourse_facet ['method_citation']
 <S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'"]
'119'
['119']
parsed_discourse_facet ['method_citation']
    <S sid="3" ssid="1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'"]
'3'
['3']
parsed_discourse_facet ['method_citation']
    <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
    <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
    <S sid="133" ssid="18">To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'"]
'133'
['133']
parsed_discourse_facet ['method_citation']
 <S sid="138" ssid="23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
    <S sid="156" ssid="41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'"]
'156'
['156']
parsed_discourse_facet ['method_citation']
 <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
 <S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'"]
'119'
['119']
parsed_discourse_facet ['method_citation']
  <S sid="133" ssid="18">To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'"]
'133'
['133']
parsed_discourse_facet ['method_citation']
 <S sid="164" ssid="49">Although embedding this version of LCFRS's in the framework of ILFP developed by Rounds (1985) is straightforward, our motivation was to capture properties shared by a family of grammatical systems and generalize them defining a class of related formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'164'"]
'164'
['164']
parsed_discourse_facet ['method_citation']
 <S sid="204" ssid="10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's, HG's, TAG's, and MCTAG's are all local sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'"]
'204'
['204']
parsed_discourse_facet ['method_citation']
    <S sid="9" ssid="7">We examine both the complexity of the paths of trees in the tree sets, and the kinds of dependencies that the formalisms can impose between paths.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="28" ssid="13">A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
 <S sid="138" ssid="23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P87-1015.csv
<S sid ="1" ssid = "1">We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.</S><S sid ="9" ssid = "7">We examine both the complexity of the paths of trees in the tree sets  and the kinds of dependencies that the formalisms can impose between paths.</S><S sid ="232" ssid = "38">In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="110" ssid = "16">The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'232'", "'195'", "'110'"]
'1'
'9'
'232'
'195'
'110'
['1', '9', '232', '195', '110']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">In considering the relationship between formalisms  we show that it is useful to abstract away from the details of the formalism  and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation  we describe a class of formalisms which we call Linear Context- Free Rewriting Systems  and show they are recognizable in polynomial time and generate only semilinear languages.</S><S sid ="203" ssid = "9">In order to observe the similarity between these constrained systems  it is crucial to abstract away from the details of the structures and operations used by the system.</S><S sid ="232" ssid = "38">In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.</S><S sid ="11" ssid = "9">By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'203'", "'232'", "'11'", "'116'"]
'2'
'203'
'232'
'11'
'116'
['2', '203', '232', '11', '116']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S><S sid ="48" ssid = "33">There has been recent interest in the application of Indexed Grammars (IG's) to natural languages.</S><S sid ="23" ssid = "8">He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.</S><S sid ="56" ssid = "41">Gazdar (1985) argues that sharing of stacks can be used to give analyses for coordination.</S><S sid ="65" ssid = "50">While the generative power of CG's is greater that of CFG's  it appears to be highly constrained.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'48'", "'23'", "'56'", "'65'"]
'3'
'48'
'23'
'56'
'65'
['3', '48', '23', '56', '65']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "2">Little attention  however  has been paid to the structural descriptions that these formalisms can assign to strings  i.e. their strong generative capacity.</S><S sid ="3" ssid = "1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S><S sid ="66" ssid = "51">Hence  their relationship to formalisms such as HG's and TAG's is of interest.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="48" ssid = "33">There has been recent interest in the application of Indexed Grammars (IG's) to natural languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'3'", "'66'", "'195'", "'48'"]
'4'
'3'
'66'
'195'
'48'
['4', '3', '66', '195', '48']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "3">This aspect of the formalism is both linguistically and computationally important.</S><S sid ="198" ssid = "4">This property reflects an important aspect of the underlying linguistic theory associated with the formalism.</S><S sid ="10" ssid = "8">These two properties of the tree sets are not only linguistically relevant  but also have computational importance.</S><S sid ="123" ssid = "8">A second restriction on the formalisms is that choices during the derivation are independent of the context in the derivation.</S><S sid ="3" ssid = "1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'198'", "'10'", "'123'", "'3'"]
'5'
'198'
'10'
'123'
'3'
['5', '198', '10', '123', '3']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">For example  Gazdar (1985) discusses the applicability of Indexed Grammars (IG's) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar (LFG) and Government and Bindings grammars (GB).</S><S sid ="24" ssid = "9">Head Grammars (HG's)  introduced by Pollard (1984)  is a formalism that manipulates headed strings: i.e.  strings  one of whose symbols is distinguished as the head.</S><S sid ="35" ssid = "20">TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish  for cross serial dependencies in Dutch subordinate clauses  and for the nestings of paired English complementizers.</S><S sid ="64" ssid = "49">Steedman (1986) considers Categorial Grammars in which both the operations of function application and composition may be used  and in which function can specify whether they take their arguments from their right or left.</S><S sid ="22" ssid = "7">Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'24'", "'35'", "'64'", "'22'"]
'6'
'24'
'35'
'64'
'22'
['6', '24', '35', '64', '22']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "5">The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG's and IG's.</S><S sid ="50" ssid = "35">The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.</S><S sid ="16" ssid = "1">From Thatcher's (1973) work  it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S><S sid ="98" ssid = "4">Thatcher (1973) describes a tee pumping lemma for recognizable sets related to the string pumping lemma for regular sets.</S><S sid ="24" ssid = "9">Head Grammars (HG's)  introduced by Pollard (1984)  is a formalism that manipulates headed strings: i.e.  strings  one of whose symbols is distinguished as the head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'50'", "'16'", "'98'", "'24'"]
'7'
'50'
'16'
'98'
'24'
['7', '50', '16', '98', '24']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "6">We consider properties of the tree sets generated by CFG's  Tree Adjoining Grammars (TAG's)  Head Grammars (HG's)  Categorial Grammars (CG's)  and IG's.</S><S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="27" ssid = "12">Thus  the tree sets generated by HG's are similar to those of CFG's  with each node annotated by the operation (concatenation or wrapping) used to combine the headed strings derived by the daughters of Tree Adjoining Grammars  a tree rewriting formalism  was introduced by Joshi  Levy and Takahashi (1975) and Joshi (1983/85).</S><S sid ="94" ssid = "79">The semilinearity of Tree Adjoining Languages (TAL's)  MCTAL's  and Head Languages (HL's) can be proved using this property  with suitable restrictions on the composition operations.</S><S sid ="208" ssid = "14">This family represents an attempt to generalize the properties shared by CFG's  HG's  TAG's  and MCTAG's.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'151'", "'27'", "'94'", "'208'"]
'8'
'151'
'27'
'94'
'208'
['8', '151', '27', '94', '208']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "7">We examine both the complexity of the paths of trees in the tree sets  and the kinds of dependencies that the formalisms can impose between paths.</S><S sid ="1" ssid = "1">We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.</S><S sid ="8" ssid = "6">We consider properties of the tree sets generated by CFG's  Tree Adjoining Grammars (TAG's)  Head Grammars (HG's)  Categorial Grammars (CG's)  and IG's.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="110" ssid = "16">The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'1'", "'8'", "'195'", "'110'"]
'9'
'1'
'8'
'195'
'110'
['9', '1', '8', '195', '110']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "8">These two properties of the tree sets are not only linguistically relevant  but also have computational importance.</S><S sid ="5" ssid = "3">This aspect of the formalism is both linguistically and computationally important.</S><S sid ="198" ssid = "4">This property reflects an important aspect of the underlying linguistic theory associated with the formalism.</S><S sid ="68" ssid = "53">This kind of dependency arises from the use of the composition operation to compose two arbitrarily large categories.</S><S sid ="23" ssid = "8">He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'5'", "'198'", "'68'", "'23'"]
'10'
'5'
'198'
'68'
'23'
['10', '5', '198', '68', '23']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "9">By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.</S><S sid ="2" ssid = "2">In considering the relationship between formalisms  we show that it is useful to abstract away from the details of the formalism  and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation  we describe a class of formalisms which we call Linear Context- Free Rewriting Systems  and show they are recognizable in polynomial time and generate only semilinear languages.</S><S sid ="232" ssid = "38">In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.</S><S sid ="14" ssid = "12">This suggests that by generalizing the notion of context-freeness in CFG's  we can define a class of grammatical formalisms that manipulate more complex structures.</S><S sid ="203" ssid = "9">In order to observe the similarity between these constrained systems  it is crucial to abstract away from the details of the structures and operations used by the system.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'2'", "'232'", "'14'", "'203'"]
'11'
'2'
'232'
'14'
'203'
['11', '2', '232', '14', '203']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "10">We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.</S><S sid ="137" ssid = "22">In considering recognition of LCFRS's  we make further assumption concerning the contribution of each structure to the input string  and how the composition operations combine structures in this respect.</S><S sid ="200" ssid = "6">The importance of this property becomes clear in contrasting theories underlying GPSG (Gazdar  Klein  Pulluna  and Sag  1985)  and GB (as described by Berwick  1984) with those underlying LFG and FUG.</S><S sid ="22" ssid = "7">Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.</S><S sid ="60" ssid = "45">Gazdar (1985) considers a restriction of IG's in which no more than one nonterminal on the right-hand-side of a production can inherit the stack from the left-hand-side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'137'", "'200'", "'22'", "'60'"]
'12'
'137'
'200'
'22'
'60'
['12', '137', '200', '22', '60']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "11">It is striking that from this point of view many formalisms can be grouped together as having identically structured derivation tree sets.</S><S sid ="15" ssid = "13">In this paper  we outline how such family of formalisms can be defined  and show that like CFG's  each member possesses a number of desirable linguistic and computational properties: in particular  the constant growth property and polynomial recognizability.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S><S sid ="202" ssid = "8">As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.</S><S sid ="78" ssid = "63">Each member of a set of trees can be adjoined into distinct nodes of trees in a single elementary tree set  i.e  derivations always involve the adjunction of a derived auxiliary tree set into an elementary tree set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'15'", "'116'", "'202'", "'78'"]
'13'
'15'
'116'
'202'
'78'
['13', '15', '116', '202', '78']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "12">This suggests that by generalizing the notion of context-freeness in CFG's  we can define a class of grammatical formalisms that manipulate more complex structures.</S><S sid ="119" ssid = "4">In defining LCFRS's  we hope to generalize the definition of CFG's to formalisms manipulating any structure  e.g. strings  trees  or graphs.</S><S sid ="53" ssid = "38">We focus on this difference between the tree sets of CFG's and IG's  and formalize the notion of dependence between paths in a tree set in Section 3.</S><S sid ="142" ssid = "27">Although this property is not structural  it depends on the structural property that sentences can be built from a finite set of clauses of bounded structure as noted by Joshi (1983/85).</S><S sid ="159" ssid = "44">Similarly  for all the LCFRS's  discussed in Section 2  we can define the relationship between a structure and the sequence of substrings it spans  and the effect of the composition operations on sequences of substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'119'", "'53'", "'142'", "'159'"]
'14'
'119'
'53'
'142'
'159'
['14', '119', '53', '142', '159']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "13">In this paper  we outline how such family of formalisms can be defined  and show that like CFG's  each member possesses a number of desirable linguistic and computational properties: in particular  the constant growth property and polynomial recognizability.</S><S sid ="202" ssid = "8">As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.</S><S sid ="117" ssid = "2">Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S><S sid ="13" ssid = "11">It is striking that from this point of view many formalisms can be grouped together as having identically structured derivation tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'202'", "'117'", "'116'", "'13'"]
'15'
'202'
'117'
'116'
'13'
['15', '202', '117', '116', '13']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "1">From Thatcher's (1973) work  it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S><S sid ="50" ssid = "35">The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.</S><S sid ="19" ssid = "4">It can be easily shown from Thatcher's result that the path set of every local set is a regular set.</S><S sid ="229" ssid = "35">In addition  the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars  and the hierarchy defined by Weir).</S><S sid ="166" ssid = "51">With the additional assumptions  inspired by Rounds (1985)  we can show that members of this class can be recognized in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'50'", "'19'", "'229'", "'166'"]
'16'
'50'
'19'
'229'
'166'
['16', '50', '19', '229', '166']
parsed_discourse_facet ['method_citation']



P87-1015
W07-2214
0
method_citation
['method_citation']
parsing: input/ref/Task1/A00-2018_akanksha.csv
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'"]
'90'
'91'
['90', '91']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="17">Maximum-entropy models have two benefits for a parser builder.</S>
    <S sid="49" ssid="18">First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &amp;quot;features&amp;quot; suggests that the probability model should be easily changeable &#8212; just change the set of features used.</S>
    <S sid="51" ssid="20">Second, and this is a point we have not yet mentioned, the features used in these models need have no particular independence of one another.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'49'", "'51'"]
'48'
'49'
'51'
['48', '49', '51']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
    <S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
    <S sid="93" ssid="4">This allows the second pass to see expansions not present in the training corpus.</S>
    <S sid="94" ssid="5">We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'", "'92'", "'93'", "'94'"]
'90'
'91'
'92'
'93'
'94'
['90', '91', '92', '93', '94']
parsed_discourse_facet ['method_citation']
NA
original cit marker offset is 0
new cit marker offset is 0



['0']
0
['0']
Error in Reference Offset
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
    <S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
    <S sid="93" ssid="4">This allows the second pass to see expansions not present in the training corpus.</S>
    <S sid="94" ssid="5">We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'", "'92'", "'93'", "'94'"]
'90'
'91'
'92'
'93'
'94'
['90', '91', '92', '93', '94']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="7">To compute a probability in a log-linear model one first defines a set of &amp;quot;features&amp;quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.</S>
    <S sid="39" ssid="8">In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.</S>
    <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'39'", "'40'"]
'38'
'39'
'40'
['38', '39', '40']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['result_citation']
<S sid="85" ssid="54">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models, this simplifies the model significantly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="32">As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &amp;quot;before&amp;quot;), and the label of the grandparent of c (la).</S><S sid="143" ssid="34">The first is simply that if we first guess the pre-terminal, when we go to guess the head the first thing we can condition upon is the pre-terminal, i.e., we compute p(h I t).</S><S sid="146" ssid="37">The second major reason why first guessing the pre-terminal makes so much difference is that it can be used when backing off the lexical head in computing the probability of the rule expansion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'143'", "'146'"]
'63'
'143'
'146'
['63', '143', '146']
parsed_discourse_facet ['method_citation']
???<S sid="78" ssid="47">With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.</S>                                        <S sid="79" ssid="48">We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.</S>
original cit marker offset is 0
new cit marker offset is 0



["'78'", "'79'"]
'78'
'79'
['78', '79']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/A00-2018.csv
<S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'5'", "'2'", "'6'", "'125'"]
'1'
'5'
'2'
'6'
'125'
['1', '5', '2', '6', '125']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="175" ssid = "2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'1'", "'125'", "'175'"]
'2'
'6'
'1'
'125'
'175'
['2', '6', '1', '125', '175']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.</S><S sid ="182" ssid = "9">As noted above  the main methodological innovation presented here is our &quot;maximumentropy-inspired&quot; model for conditioning and smoothing.</S><S sid ="128" ssid = "19">The model labeled &quot;Old&quot; attempts to recreate the Char97 system using the current program.</S><S sid ="118" ssid = "9">First  it uses a clustering scheme on words to give the system a &quot;soft&quot; clustering of heads and sub-heads.</S><S sid ="116" ssid = "7">This is as opposed to the &quot;Markovgrammar&quot; approach used in the current parser.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'182'", "'128'", "'118'", "'116'"]
'3'
'182'
'128'
'118'
'116'
['3', '182', '128', '118', '116']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid ="139" ssid = "30">In Figure 2 we show that this one factor improves performance by nearly 2%.</S><S sid ="164" ssid = "55">When we do so using our maximum-entropy-inspired conditioning  we get another 0.45% improvement in average precision/recall  as indicated in Figure 2 on the line labeled &quot;MaxEnt-Inspired'.</S><S sid ="17" ssid = "6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S><S sid ="94" ssid = "5">We use the gathered statistics for all observed words  even those with very low counts  though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'139'", "'164'", "'17'", "'94'"]
'4'
'139'
'164'
'17'
'94'
['4', '139', '164', '17', '94']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'2'", "'6'", "'7'"]
'5'
'1'
'2'
'6'
'7'
['5', '1', '2', '6', '7']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="175" ssid = "2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'1'", "'125'", "'175'"]
'2'
'6'
'1'
'125'
'175'
['2', '6', '1', '125', '175']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid ="171" ssid = "62">As already noted  our best model uses a Markov-grammar approach.</S><S sid ="90" ssid = "1">We created a parser based upon the maximumentropy-inspired model of the last section  smoothed using standard deleted interpolation.</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'171'", "'90'", "'1'", "'5'"]
'7'
'171'
'90'
'1'
'5'
['7', '171', '90', '1', '5']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">That is  for all sentences s and all parses 7r  the parser assigns a probability p(s   7r) = p(r)  the equality holding when we restrict consideration to 7r whose yield * This research was supported in part by NSF grant LIS SBR 9720368.</S><S sid ="12" ssid = "1">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).</S><S sid ="10" ssid = "6">That is  the parser implements the function arg maxrp(7r s) = arg maxirp(7r  s) = arg maxrp(w).</S><S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S><S sid ="147" ssid = "38">For example  when we first guess the lexical head we can move from computing p(r I 1 1p  h) to p(r I /  t  /p  h).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'12'", "'10'", "'41'", "'147'"]
'8'
'12'
'10'
'41'
'147'
['8', '12', '10', '41', '147']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.</S><S sid ="185" ssid = "12">We expect that as we experiment with other  more semantic conditioning information  the importance of this aspect of the model will increase.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="171" ssid = "62">As already noted  our best model uses a Markov-grammar approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'185'", "'5'", "'1'", "'171'"]
'9'
'185'
'5'
'1'
'171'
['9', '185', '5', '1', '171']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">That is  the parser implements the function arg maxrp(7r s) = arg maxirp(7r  s) = arg maxrp(w).</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid ="83" ssid = "52">In the simple (content-free) form (Equation 6)  it is clear that Z(H) = 1.</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="56" ssid = "25">In max-entropy models one can simply include features for all three events f1 (a  b  c)  f2 (a  b)  and f3(a  c) and combine them in the model according to Equation 3  or equivalently  Equation 4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'13'", "'83'", "'54'", "'56'"]
'10'
'13'
'83'
'54'
'56'
['10', '13', '83', '54', '56']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">What fundamentally distinguishes probabilistic generative parsers is how they compute p(r)  and it is to that topic we turn next.</S><S sid ="53" ssid = "22">That is  suppose we want to compute a conditional probability p(a b  c)  but we are not sure that we have enough examples of the conditioning event b  c in the training corpus to ensure that the empirically obtained probability P (a I b  c) is accurate.</S><S sid ="82" ssid = "51">The major advantage of using Equation 7 is that one can generally get away without computing the partition function Z(H).</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="188" ssid = "15">Ultimately it is this flexibility that let us try the various conditioning events  to move on to a Markov grammar approach  and to try several Markov grammars of different orders  without significant programming.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'53'", "'82'", "'54'", "'188'"]
'11'
'53'
'82'
'54'
'188'
['11', '53', '82', '54', '188']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "1">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).</S><S sid ="18" ssid = "7">The method that gives the best results  however  uses a Markov grammar — a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6 10 15].</S><S sid ="134" ssid = "25">As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.</S><S sid ="147" ssid = "38">For example  when we first guess the lexical head we can move from computing p(r I 1 1p  h) to p(r I /  t  /p  h).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'18'", "'134'", "'147'", "'33'"]
'12'
'18'
'134'
'147'
'33'
['12', '18', '134', '147', '33']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid ="16" ssid = "5">In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.</S><S sid ="45" ssid = "14">The function Z(H)  called the partition function  is a normalizing constant (for fixed H)  so the probabilities over all a sum to one.</S><S sid ="83" ssid = "52">In the simple (content-free) form (Equation 6)  it is clear that Z(H) = 1.</S><S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'16'", "'45'", "'83'", "'41'"]
'13'
'16'
'45'
'83'
'41'
['13', '16', '45', '83', '41']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "3">Much of the interesting work is determining what goes into H (c).</S><S sid ="73" ssid = "42">We make one more point on the connection of Equation 7 to a maximum entropy formulation.</S><S sid ="60" ssid = "29">But let us look at how it works for a particular case in our parsing scheme.</S><S sid ="159" ssid = "50">Something very much like this is done in [15].</S><S sid ="151" ssid = "42">We have already noted the importance of conditioning on the parent label /p.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'73'", "'60'", "'159'", "'151'"]
'14'
'73'
'60'
'159'
'151'
['14', '73', '60', '159', '151']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "4">Whenever it is clear to which constituent we are referring we omit the (c) in  e.g.  h(c).</S><S sid ="67" ssid = "36">For example  it does not seem to make much sense to condition on  say  hp without first conditioning on ti .</S><S sid ="151" ssid = "42">We have already noted the importance of conditioning on the parent label /p.</S><S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="140" ssid = "31">It may not be obvious why this should make so great a difference  since most words are effectively unambiguous.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'67'", "'151'", "'157'", "'140'"]
'15'
'67'
'151'
'157'
'140'
['15', '67', '151', '157', '140']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "5">In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.</S><S sid ="65" ssid = "34">We can now rewrite this in the form of Equation 5 as follows: Here we have sequentially conditioned on steadily increasing portions of c's history.</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid ="58" ssid = "27">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid ="46" ssid = "15">Now for our purposes it is useful to rewrite this as a sequence of multiplicative functions gi(a  H) for 0 < i < j: Here go(a  H) = 11Z (H) and gi(a  H) = eAi(a H) fi(° 11).</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'65'", "'13'", "'58'", "'46'"]
'16'
'65'
'13'
'58'
'46'
['16', '65', '13', '58', '46']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



A00-2018
W05-0638
0
method_citation
['method_citation']
parsing: input/ref/Task1/A00-2030_aakansha.csv
<S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="9">By necessity, we adopted the strategy of hand marking only the semantics.</S>
    <S sid="50" ssid="10">Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'50'"]
'49'
'50'
['49', '50']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="8">Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'"]
'16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S><S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'", "'33'", "'34'"]
'23'
'24'
'33'
'34'
['23', '24', '33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="60" ssid="1">In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).</S>
    <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'61'"]
'60'
'61'
['60', '61']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'"]
'33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).</S>
    <S sid="12" ssid="2">The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).</S><S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'12'", "'16'"]
'11'
'12'
'16'
['11', '12', '16']
parsed_discourse_facet ['method_citation']
<S sid="105" ssid="2">A single model proved capable of performing all necessary sentential processing, both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'"]
'105'
['105']
parsed_discourse_facet ['result_citation']
<S sid="60" ssid="1">In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).</S>
    <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'61'"]
'60'
'61'
['60', '61']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'"]
'16'
['16']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="43" ssid = "3">Thus  we did not consider simply adding semantic labels to the existing Penn TREEBANK  which is drawn from a single source — the Wall Street Journal — and is impoverished in articles about rocket launches.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'10'", "'5'", "'43'", "'104'"]
'1'
'10'
'5'
'43'
'104'
['1', '10', '5', '43', '104']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S><S sid ="26" ssid = "9">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993)  and more recently  had begun using a generative statistical model for name finding (Bikel et al.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'28'", "'96'", "'26'"]
'2'
'6'
'28'
'96'
'26'
['2', '6', '28', '96', '26']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "1">Since 1995  a few statistical parsing algorithms (Magerman  1995; Collins  1996 and 1997; Charniak  1997; Rathnaparki  1997) demonstrated a breakthrough in parsing accuracy  as measured against the University of Pennsylvania TREEBANK as a gold standard.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'1'", "'104'", "'5'", "'96'"]
'3'
'1'
'104'
'5'
'96'
['3', '1', '104', '5', '96']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "2">Yet  relatively few have embedded one of these algorithms in a task.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="108" ssid = "5">This simple semantic annotation was the only source of task knowledge used to configure the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'105'", "'23'", "'28'", "'108'"]
'4'
'105'
'23'
'28'
'108'
['4', '105', '23', '28', '108']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'106'", "'1'", "'28'", "'10'"]
'5'
'106'
'1'
'28'
'10'
['5', '106', '1', '28', '10']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S><S sid ="2" ssid = "2">In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'104'", "'2'", "'51'", "'1'"]
'6'
'104'
'2'
'51'
'1'
['6', '104', '2', '51', '1']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "5">The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.</S><S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="50" ssid = "10">Figure 4 shows an example of the semantic annotation  which was the only type of manual annotation we performed.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'11'", "'28'", "'50'", "'107'"]
'7'
'11'
'28'
'50'
'107'
['7', '11', '28', '50', '107']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "6">Several technical challenges confronted us and were solved: TREEBANK on Wall Street Journal adequately train the algorithm for New York Times newswire  which includes dozens of newspapers?</S><S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'10'", "'5'", "'51'", "'1'"]
'8'
'10'
'5'
'51'
'1'
['8', '10', '5', '51', '1']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "7">Manually creating sourcespecific training data for syntax was not required.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="108" ssid = "5">This simple semantic annotation was the only source of task knowledge used to configure the model.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S><S sid ="41" ssid = "1">To train our integrated model  we required a large corpus of augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'106'", "'108'", "'107'", "'41'"]
'9'
'106'
'108'
'107'
'41'
['9', '106', '108', '107', '41']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="43" ssid = "3">Thus  we did not consider simply adding semantic labels to the existing Penn TREEBANK  which is drawn from a single source — the Wall Street Journal — and is impoverished in articles about rocket launches.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'1'", "'43'", "'51'", "'5'"]
'10'
'1'
'43'
'51'
'5'
['10', '1', '43', '51', '5']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S><S sid ="7" ssid = "5">The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'7'", "'101'", "'5'", "'96'"]
'11'
'7'
'101'
'5'
'96'
['11', '7', '101', '5', '96']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "2">The Template Element (TE) task identifies organizations  persons  locations  and some artifacts (rocket and airplane-related artifacts).</S><S sid ="98" ssid = "3">In both Template Entity (TE) and Template Relation (TR)  our system finished in second place among all entrants.</S><S sid ="16" ssid = "6">For the following example  the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S><S sid ="78" ssid = "19">If we generalize the tree components (constituent labels  words  tags  etc.) and treat them all as simply elements  e  and treat all the conditioning factors as the history  h  we can write:</S><S sid ="34" ssid = "2">In these trees  the standard TREEBANK structures are augmented to convey semantic information  that is  entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'98'", "'16'", "'78'", "'34'"]
'12'
'98'
'16'
'78'
'34'
['12', '98', '16', '78', '34']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="14" ssid = "4">For each person  one must find all of the person's names within the document  his/her type (civilian or military)  and any significant descriptions (e.g.  titles).</S><S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="58" ssid = "4">Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree  semantic pointer labels are attached to all of the intermediate nodes.</S><S sid ="100" ssid = "5">Given multiple constituents that cover identical spans in the chart  only those constituents with probabilities within a While our focus throughout the project was on TE and TR  we became curious about how well the model did at part-of-speech tagging  syntactic parsing  and at name finding.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'14'", "'15'", "'58'", "'100'"]
'13'
'14'
'15'
'58'
'100'
['13', '14', '15', '58', '100']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "4">For each person  one must find all of the person's names within the document  his/her type (civilian or military)  and any significant descriptions (e.g.  titles).</S><S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S><S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'13'", "'74'", "'15'", "'36'"]
'14'
'13'
'74'
'15'
'36'
['14', '13', '74', '15', '36']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="78" ssid = "19">If we generalize the tree components (constituent labels  words  tags  etc.) and treat them all as simply elements  e  and treat all the conditioning factors as the history  h  we can write:</S><S sid ="91" ssid = "10">The probability of generating a constituent of the specified category  starting at the topmost node.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="55" ssid = "1">syntactic modifier of the other  the inserted node serves to indicate the relation as well as the argument.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'78'", "'91'", "'82'", "'55'"]
'15'
'78'
'91'
'82'
'55'
['15', '78', '91', '82', '55']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "6">For the following example  the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S><S sid ="98" ssid = "3">In both Template Entity (TE) and Template Relation (TR)  our system finished in second place among all entrants.</S><S sid ="12" ssid = "2">The Template Element (TE) task identifies organizations  persons  locations  and some artifacts (rocket and airplane-related artifacts).</S><S sid ="94" ssid = "13">Given a new sentence  the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'74'", "'98'", "'12'", "'94'"]
'16'
'74'
'98'
'12'
'94'
['16', '74', '98', '12', '94']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "7">For the following example  the template relation in Figure 2 was to be generated: &quot;Donald M. Goldstein  a historian at the University of Pittsburgh who helped write...&quot;</S><S sid ="50" ssid = "10">Figure 4 shows an example of the semantic annotation  which was the only type of manual annotation we performed.</S><S sid ="35" ssid = "3">An example of an augmented parse tree is shown in Figure 3.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'50'", "'35'", "'82'", "'56'"]
'17'
'50'
'35'
'82'
'56'
['17', '50', '35', '82', '56']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "1">Almost all approaches to information extraction — even at the sentence level — are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.</S><S sid ="19" ssid = "2">Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S><S sid ="29" ssid = "12">Thus  each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="84" ssid = "3">Although mathematically the model predicts tree elements in a top-down fashion  we search the space bottom-up using a chartbased search.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'19'", "'29'", "'5'", "'84'"]
'18'
'19'
'29'
'5'
'84'
['18', '19', '29', '5', '84']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "2">Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S><S sid ="29" ssid = "12">Thus  each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.</S><S sid ="24" ssid = "7">For this reason  we focused on designing an integrated model in which tagging  namefinding  parsing  and semantic interpretation decisions all have the opportunity to mutually influence each other.</S><S sid ="90" ssid = "9">Thus  the scores used in pruning can be considered as the product of: 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'96'", "'29'", "'24'", "'90'"]
'19'
'96'
'29'
'24'
'90'
['19', '96', '29', '24', '90']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W06-3114_sweta.csv
 <S sid="108" ssid="1">The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="27">For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="44">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages, as demonstrated by the results for English-German and English-French.</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
 <S sid="172" ssid="3">Due to many similarly performing systems, we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
 <S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["36'"]
36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="103" ssid="19">Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["103'"]
103'
['103']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="60">One annotator suggested that this was the case for as much as 10% of our test sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["102'"]
102'
['102']
parsed_discourse_facet ['method_citation']
<S sid="123" ssid="16">For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.</S>
original cit marker offset is 0
new cit marker offset is 0



["123'"]
123'
['123']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="27">For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="1">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="19">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["126'"]
126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="173" ssid="4">The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["173'"]
173'
['173']
parsed_discourse_facet ['method_citation']
 <S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["170'"]
170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="84" ssid="23">The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["84'"]
84'
['84']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="143" ssid = "36">For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.</S><S sid ="129" ssid = "22">All systems (except for Systran  which was not tuned to Europarl) did considerably worse on outof-domain training data.</S><S sid ="121" ssid = "14">For the automatic scoring method BLEU  we can distinguish three quarters of the systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'136'", "'143'", "'129'", "'121'"]
'1'
'136'
'143'
'129'
'121'
['1', '136', '143', '129', '121']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S sid ="86" ssid = "2">The average fluency judgement per judge ranged from 2.33 to 3.67  the average adequacy judgement ranged from 2.56 to 4.13.</S><S sid ="150" ssid = "43">For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French)  at least as measured by BLEU.</S><S sid ="95" ssid = "11">The judgement of 4 in the first case will go to a vastly better system output than in the second case.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'86'", "'150'", "'95'", "'136'"]
'2'
'86'
'150'
'95'
'136'
['2', '86', '150', '95', '136']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "1">was done by the participants.</S><S sid ="67" ssid = "6">Participants and other volunteers contributed about 180 hours of labor in the manual evaluation.</S><S sid ="29" ssid = "22">About half of the participants of last year’s shared task participated again.</S><S sid ="78" ssid = "17">Judges where excluded from assessing the quality of MT systems that were submitted by their institution.</S><S sid ="168" ssid = "61">Annotators argued for the importance of having correct and even multiple references.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'67'", "'29'", "'78'", "'168'"]
'3'
'67'
'29'
'78'
'168'
['3', '67', '29', '78', '168']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "2">This revealed interesting clues about the properties of automatic and manual scoring.</S><S sid ="146" ssid = "39">This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.</S><S sid ="168" ssid = "61">Annotators argued for the importance of having correct and even multiple references.</S><S sid ="176" ssid = "7">Human judges also pointed out difficulties with the evaluation of long sentences.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'146'", "'168'", "'176'", "'134'"]
'4'
'146'
'168'
'176'
'134'
['4', '146', '168', '176', '134']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "3">• We evaluated translation from English  in addition to into English.</S><S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S><S sid ="152" ssid = "45">The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'6'", "'152'", "'170'", "'162'"]
'5'
'6'
'152'
'170'
'162'
['5', '6', '152', '170', '162']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S><S sid ="5" ssid = "3">• We evaluated translation from English  in addition to into English.</S><S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="79" ssid = "18">Sentences and systems were randomly selected and randomly shuffled for presentation.</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'5'", "'163'", "'79'", "'162'"]
'6'
'5'
'163'
'79'
'162'
['6', '5', '163', '79', '162']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "5">We dropped  however  one of the languages  Finnish  partly to keep the number of tracks manageable  partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation.</S><S sid ="165" ssid = "58">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid ="66" ssid = "5">In this shared task  we were also confronted with this problem  and since we had no funding for paying human judgements  we asked participants in the evaluation to share the burden.</S><S sid ="47" ssid = "13">Because of this  we retokenized and lowercased submitted output with our own tokenizer  which was also used to prepare the training and test data.</S><S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'165'", "'66'", "'47'", "'125'"]
'7'
'165'
'66'
'47'
'125'
['7', '165', '66', '47', '125']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "1">The evaluation framework for the shared task is similar to the one used in last year’s shared task.</S><S sid ="57" ssid = "23">We are therefore applying a different method  which has been used at the 2005 DARPA/NIST evaluation.</S><S sid ="29" ssid = "22">About half of the participants of last year’s shared task participated again.</S><S sid ="31" ssid = "24">Compared to last year’s shared task  the participants represent more long-term research efforts.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'57'", "'29'", "'31'", "'28'"]
'8'
'57'
'29'
'31'
'28'
['8', '57', '29', '31', '28']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "2">Training and testing is based on the Europarl corpus.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="143" ssid = "36">For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.</S><S sid ="21" ssid = "14">The out-of-domain test set differs from the Europarl data in various ways.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'28'", "'170'", "'143'", "'21'"]
'9'
'28'
'170'
'143'
'21'
['9', '28', '170', '143', '21']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "3">Figure 1 provides some statistics about this corpus.</S><S sid ="20" ssid = "13">For statistics on this test set  refer to Figure 1.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="4" ssid = "2">This revealed interesting clues about the properties of automatic and manual scoring.</S><S sid ="120" ssid = "13">In Figure 4  we displayed the number of system comparisons  for which we concluded statistical significance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'20'", "'134'", "'4'", "'120'"]
'10'
'20'
'134'
'4'
'120'
['10', '20', '134', '4', '120']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "4">To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="161" ssid = "54">Since all long sentence translation are somewhat muddled  even a contrastive evaluation between systems was difficult.</S><S sid ="152" ssid = "45">The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.</S><S sid ="31" ssid = "24">Compared to last year’s shared task  the participants represent more long-term research efforts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'28'", "'161'", "'152'", "'31'"]
'11'
'28'
'161'
'152'
'31'
['11', '28', '161', '152', '31']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "5">To summarize  we provided: The performance of the baseline system is similar to the best submissions in last year’s shared task.</S><S sid ="141" ssid = "34">In-domain Systran scores on this metric are lower than all statistical systems  even the ones that have much worse human scores.</S><S sid ="8" ssid = "1">The evaluation framework for the shared task is similar to the one used in last year’s shared task.</S><S sid ="81" ssid = "20">This is less than the 694 judgements 2004 DARPA/NIST evaluation  or the 532 judgements in the 2005 DARPA/NIST evaluation.</S><S sid ="151" ssid = "44">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages  as demonstrated by the results for English-German and English-French.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'141'", "'8'", "'81'", "'151'"]
'12'
'141'
'8'
'81'
'151'
['12', '141', '8', '81', '151']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "6">We are currently working on a complete open source implementation of a training and decoding system  which should become available over the summer. pus  from which also the in-domain test set is taken.</S><S sid ="33" ssid = "26">While building a machine translation system is a serious undertaking  in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.</S><S sid ="43" ssid = "9">At the very least  we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.</S><S sid ="74" ssid = "13">While we had up to 11 submissions for a translation direction  we did decide against presenting all 11 system outputs to the human judge.</S><S sid ="42" ssid = "8">It was our hope that this competition  which included the manual and automatic evaluation of statistical systems and one rulebased commercial system  will give further insight into the relation between automatic and manual evaluation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'33'", "'43'", "'74'", "'42'"]
'13'
'33'
'43'
'74'
'42'
['13', '33', '43', '74', '42']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "7">There is twice as much language modelling data  since training data for the machine translation system is filtered against sentences of length larger than 40 words.</S><S sid ="165" ssid = "58">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid ="58" ssid = "24">We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set  53 blocks for the out-of-domain test set)  check for each block  if one system has a higher BLEU score than the other  and then use the sign test.</S><S sid ="151" ssid = "44">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages  as demonstrated by the results for English-German and English-French.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'165'", "'58'", "'151'", "'155'"]
'14'
'165'
'58'
'151'
'155'
['14', '165', '58', '151', '155']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "8">Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.</S><S sid ="21" ssid = "14">The out-of-domain test set differs from the Europarl data in various ways.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="142" ssid = "35">Surprisingly  this effect is much less obvious for out-of-domain test data.</S><S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'21'", "'28'", "'142'", "'18'"]
'15'
'21'
'28'
'142'
'18'
['15', '21', '28', '142', '18']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="165" ssid = "58">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid ="14" ssid = "7">There is twice as much language modelling data  since training data for the machine translation system is filtered against sentences of length larger than 40 words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'126'", "'18'", "'165'", "'14'"]
'16'
'126'
'18'
'165'
'14'
['16', '126', '18', '165', '14']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="69" ssid = "8">We settled on contrastive evaluations of 5 system outputs for a single test sentence.</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="19" ssid = "12">We aligned the texts at a sentence level across all four languages  resulting in 1064 sentence per language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'69'", "'162'", "'170'", "'19'"]
'17'
'69'
'162'
'170'
'19'
['17', '69', '162', '170', '19']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="15" ssid = "8">Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'126'", "'16'", "'15'", "'155'"]
'18'
'126'
'16'
'15'
'155'
['18', '126', '16', '15', '155']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/J01-2004_swastika.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="3">In the past few years, however, some improvements have been made over these language models through the use of statistical methods of natural language processing, and the development of innovative, linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
<S sid="215" ssid="119">The first word in the string remaining to be parsed, w1, we will call the look-ahead word.</S>
original cit marker offset is 0
new cit marker offset is 0



['215']
215
['215']
parsed_discourse_facet ['method_citation']
    <S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



['302']
302
['302']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="100" ssid="4">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['method_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S sid ="1" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="7" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="209" ssid = "113">This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).</S><S sid ="132" ssid = "36">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid ="277" ssid = "33">The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989 860 words  39 832 sentences) of the Penn Treebank (Marcus  Santorini  and Marcinkiewicz 1993) served as the training data  Section 24 (34 199 words  1 346 sentences) as the held-out data for parameter estimation  and Section 23 (59 100 words  2 416 sentences) as the test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'7'", "'209'", "'132'", "'277'"]
'1'
'7'
'209'
'132'
'277'
['1', '7', '209', '132', '277']
parsed_discourse_facet ['aim_citation']
<S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="41" ssid = "29">There will also be a brief review of previous work using syntactic information for language modeling  before we introduce our model in Section 4.</S><S sid ="18" ssid = "6">Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture long-distance dependencies for language modeling will be reviewed.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'2'", "'41'", "'18'", "'4'"]
'8'
'2'
'41'
'18'
'4'
['8', '2', '41', '18', '4']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="9" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'390'", "'10'", "'4'"]
'3'
'9'
'390'
'10'
'4'
['3', '9', '390', '10', '4']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="100" ssid = "4">The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'100'", "'2'", "'8'"]
'10'
'4'
'100'
'2'
'8'
['10', '4', '100', '2', '8']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="11" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="379" ssid = "135">While the improvements over the trigram model in these trials are modest  they do indicate that our model is robust enough to provide good information even in the face of noisy input.</S><S sid ="300" ssid = "56">The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'11'", "'379'", "'300'", "'404'"]
'5'
'11'
'379'
'300'
'404'
['5', '11', '379', '300', '404']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="6" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="39" ssid = "27">The top-down guidance that is provided makes this approach quite efficient in practice.</S><S sid ="388" ssid = "1">The empirical results presented above are quite encouraging  and the potential of this kind of approach both for parsing and language modeling seems very promising.</S><S sid ="114" ssid = "18">He was able to reduce both sentence and word error rates on the ATIS corpus using this method.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'6'", "'39'", "'388'", "'114'"]
'12'
'6'
'39'
'388'
'114'
['12', '6', '39', '388', '114']
parsed_discourse_facet ['method_citation']
<S sid ="1" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="7" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="209" ssid = "113">This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).</S><S sid ="132" ssid = "36">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid ="277" ssid = "33">The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989 860 words  39 832 sentences) of the Penn Treebank (Marcus  Santorini  and Marcinkiewicz 1993) served as the training data  Section 24 (34 199 words  1 346 sentences) as the held-out data for parameter estimation  and Section 23 (59 100 words  2 416 sentences) as the test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'7'", "'209'", "'132'", "'277'"]
'1'
'7'
'209'
'132'
'277'
['1', '7', '209', '132', '277']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="41" ssid = "29">There will also be a brief review of previous work using syntactic information for language modeling  before we introduce our model in Section 4.</S><S sid ="18" ssid = "6">Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture long-distance dependencies for language modeling will be reviewed.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'2'", "'41'", "'18'", "'4'"]
'8'
'2'
'41'
'18'
'4'
['8', '2', '41', '18', '4']
parsed_discourse_facet ['results_citation']
<S sid ="3" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="9" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'390'", "'10'", "'4'"]
'3'
'9'
'390'
'10'
'4'
['3', '9', '390', '10', '4']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="100" ssid = "4">The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'100'", "'2'", "'8'"]
'10'
'4'
'100'
'2'
'8'
['10', '4', '100', '2', '8']
parsed_discourse_facet ['method_citation']
<S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="11" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="379" ssid = "135">While the improvements over the trigram model in these trials are modest  they do indicate that our model is robust enough to provide good information even in the face of noisy input.</S><S sid ="300" ssid = "56">The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'11'", "'379'", "'300'", "'404'"]
'5'
'11'
'379'
'300'
'404'
['5', '11', '379', '300', '404']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="6" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="39" ssid = "27">The top-down guidance that is provided makes this approach quite efficient in practice.</S><S sid ="388" ssid = "1">The empirical results presented above are quite encouraging  and the potential of this kind of approach both for parsing and language modeling seems very promising.</S><S sid ="114" ssid = "18">He was able to reduce both sentence and word error rates on the ATIS corpus using this method.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'6'", "'39'", "'388'", "'114'"]
'12'
'6'
'39'
'388'
'114'
['12', '6', '39', '388', '114']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "1">With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.</S><S sid ="14" ssid = "2">Perhaps one reason for this is that  until relatively recently  few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.</S><S sid ="15" ssid = "3">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S><S sid ="383" ssid = "139">It was selected with the goal of high parser accuracy; but in this new domain  parser accuracy is a secondary measure of performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'14'", "'15'", "'404'", "'383'"]
'13'
'14'
'15'
'404'
'383'
['13', '14', '15', '404', '383']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "2">Perhaps one reason for this is that  until relatively recently  few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.</S><S sid ="13" ssid = "1">With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.</S><S sid ="300" ssid = "56">The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.</S><S sid ="15" ssid = "3">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid ="319" ssid = "75">The hope  however  is that the improved parsing model provided by our conditional probability model will cause the distribution over structures to be more peaked  thus enabling us to capture more of the total probability mass  and making this a fairly snug upper bound on the perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'13'", "'300'", "'15'", "'319'"]
'14'
'13'
'300'
'15'
'319'
['14', '13', '300', '15', '319']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "3">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid ="97" ssid = "1">There have been attempts to jump over adjacent words to words farther back in the left context  without the use of dependency links or syntactic structure  for example Saul and Pereira (1997) and Rosenfeld (1996  1997).</S><S sid ="13" ssid = "1">With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.</S><S sid ="38" ssid = "26">This has lead us to a formulation of the conditional probability model in terms of values returned from tree-walking functions that themselves are contextually sensitive.</S><S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'97'", "'13'", "'38'", "'5'"]
'15'
'97'
'13'
'38'
'5'
['15', '97', '13', '38', '5']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "4">While language models built around shallow local dependencies are still the standard in state-of-the-art speech recognition systems  there is reason to hope that better language models can and will be developed by computational linguists for this task.</S><S sid ="397" ssid = "10">A top-down parser  in contrast to a standard bottom-up chart parser  has enough information to predict empty categories only where they are likely to occur.</S><S sid ="139" ssid = "43">This on-line characteristic allows our language model to be interpolated on a word-by-word basis with other models  such as the trigram  yielding further improvements.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'397'", "'139'", "'4'", "'10'"]
'16'
'397'
'139'
'4'
'10'
['16', '397', '139', '4', '10']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "5">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S sid ="132" ssid = "36">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid ="88" ssid = "46">This section will briefly introduce language modeling for statistical speech recognition.'</S><S sid ="40" ssid = "28">The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.</S><S sid ="247" ssid = "3">Perplexity is a standard measure within the speech recognition community for comparing language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'132'", "'88'", "'40'", "'247'"]
'17'
'132'
'88'
'40'
'247'
['17', '132', '88', '40', '247']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/D10-1044_swastika.csv
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
<S sid="71" ssid="8">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c&#955;(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



['71']
71
['71']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
<S sid="71" ssid="8">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c&#955;(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



['71']
71
['71']
parsed_discourse_facet ['result_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="9">An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (Och, 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



['45']
45
['45']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



['75']
75
['75']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
  <S sid="42" ssid="6">The natural baseline approach is to concatenate data from IN and OUT.</S>
original cit marker offset is 0
new cit marker offset is 0



['42']
42
['42']
parsed_discourse_facet ['aim_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/D10-1044.csv
<S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="145" ssid = "2">Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="62" ssid = "26">To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'65'", "'145'", "'64'", "'62'"]
'1'
'65'
'145'
'64'
'62'
['1', '65', '145', '64', '62']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S><S sid ="23" ssid = "20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="153" ssid = "10">Finally  we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.</S><S sid ="68" ssid = "5">First  we learn weights on individual phrase pairs rather than sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'23'", "'20'", "'153'", "'68'"]
'2'
'23'
'20'
'153'
'68'
['2', '23', '20', '153', '68']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">We incorporate instance weighting into a mixture-model framework  and find that it yields consistent improvements over a wide range of baselines.</S><S sid ="150" ssid = "7">In both cases  the instanceweighting approach improved over a wide range of baselines  giving gains of over 2 BLEU points over the best non-adapted baseline  and gains of between 0.6 and 1.8 over an equivalent mixture model (with an identical training procedure but without instance weighting).</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'150'", "'65'", "'38'", "'1'"]
'3'
'150'
'65'
'38'
'1'
['3', '150', '65', '38', '1']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'10'", "'6'", "'24'", "'17'"]
'4'
'10'
'6'
'24'
'17'
['4', '10', '6', '24', '17']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "2">Even when there is training data available in the domain of interest  there is often additional data from other domains that could in principle be used to improve performance.</S><S sid ="138" ssid = "7">However  for multinomial models like our LMs and TMs  there is a one to one correspondence between instances and features  eg the correspondence between a phrase pair (s  t) and its conditional multinomial probability p(s1t).</S><S sid ="9" ssid = "6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="39" ssid = "3">Thus  provided at least this amount of IN data is available—as it is in our setting—adapting these weights is straightforward.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'138'", "'9'", "'6'", "'39'"]
'5'
'138'
'9'
'6'
'39'
['5', '138', '9', '6', '39']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'17'", "'24'", "'18'", "'4'"]
'6'
'17'
'24'
'18'
'4'
['6', '17', '24', '18', '4']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="9" ssid = "6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'24'", "'21'", "'4'", "'9'"]
'7'
'24'
'21'
'4'
'9'
['7', '24', '21', '4', '9']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'10'", "'18'", "'6'", "'20'"]
'8'
'10'
'18'
'6'
'20'
['8', '10', '18', '6', '20']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S><S sid ="144" ssid = "1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S><S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="42" ssid = "6">The natural baseline approach is to concatenate data from IN and OUT.</S><S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'144'", "'1'", "'42'", "'2'"]
'9'
'144'
'1'
'42'
'2'
['9', '144', '1', '42', '2']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="29" ssid = "26">This is a simple and effective alternative to setting weights discriminatively to maximize a metric such as BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'18'", "'6'", "'29'"]
'10'
'4'
'18'
'6'
'29'
['10', '4', '18', '6', '29']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="44" ssid = "8">When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.</S><S sid ="92" ssid = "29">6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'18'", "'6'", "'44'", "'92'"]
'11'
'18'
'6'
'44'
'92'
['11', '18', '6', '44', '92']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "9">For simplicity  we assume that OUT is homogeneous.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="92" ssid = "29">6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.</S><S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'18'", "'11'", "'92'", "'16'"]
'12'
'18'
'11'
'92'
'16'
['12', '18', '11', '92', '16']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "10">The techniques we develop can be extended in a relatively straightforward manner to the more general case when OUT consists of multiple sub-domains.</S><S sid ="139" ssid = "8">As mentioned above  it is not obvious how to apply Daum´e’s approach to multinomials  which do not have a mechanism for combining split features.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="82" ssid = "19">This is not unreasonable given the application to phrase pairs from OUT  but it suggests that an interesting alternative might be to use a plain log-linear weighting function exp(Ei Aifi(s  t))  with outputs in [0  oo].</S><S sid ="32" ssid = "29">This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'139'", "'21'", "'82'", "'32'"]
'13'
'139'
'21'
'82'
'32'
['13', '139', '21', '82', '32']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "11">There is a fairly large body of work on SMT adaptation.</S><S sid ="141" ssid = "10">Moving beyond directly related work  major themes in SMT adaptation include the IR (Hildebrand et al.  2005; L¨u et al.  2007; Zhao et al.  2004) and mixture (Finch and Sumita  2008; Foster and Kuhn  2007; Koehn and Schroeder  2007; L¨u et al.  2007) approaches for LMs and TMs described above  as well as methods for exploiting monolingual in-domain text  typically by translating it automatically and then performing self training (Bertoldi and Federico  2009; Ueffing et al.  2007; Schwenk and Senellart  2009).</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="34" ssid = "31">Section 2 describes our baseline techniques for SMT adaptation  and section 3 describes the instance-weighting approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'141'", "'4'", "'10'", "'34'"]
'14'
'141'
'4'
'10'
'34'
['14', '141', '4', '10', '34']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "12">We introduce several new ideas.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="27" ssid = "24">Finally  we make some improvements to baseline approaches.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="88" ssid = "25">We have not explored this strategy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'67'", "'27'", "'20'", "'88'"]
'15'
'67'
'27'
'20'
'88'
['15', '67', '27', '20', '88']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="42" ssid = "6">The natural baseline approach is to concatenate data from IN and OUT.</S><S sid ="68" ssid = "5">First  we learn weights on individual phrase pairs rather than sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'17'", "'24'", "'42'", "'68'"]
'16'
'17'
'24'
'42'
'68'
['16', '17', '24', '42', '68']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S><S sid ="100" ssid = "4">Figure 1 shows sample sentences from these domains  which are widely divergent.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'6'", "'24'", "'16'", "'100'"]
'17'
'6'
'24'
'16'
'100'
['17', '6', '24', '16', '100']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="44" ssid = "8">When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'11'", "'44'", "'6'", "'10'"]
'18'
'11'
'44'
'6'
'10'
['18', '11', '44', '6', '10']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "16">The idea of distinguishing between general and domain-specific examples is due to Daum´e and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daum´e’s (2007) work that splits each feature into domain-specific and general copies.</S><S sid ="73" ssid = "10">Jiang and Zhai (2007) suggest the following derivation  making use of the true OUT distribution po(s  t): where each fi(s  t) is a feature intended to charac- !0ˆ = argmax pf(s  t) log pθ(s|t) (8) terize the usefulness of (s  t)  weighted by Ai. θ s t pf(s  t)po(s  t) log pθ(s|t) The mixing parameters and feature weights (col- != argmax po (s  t) lectively 0) are optimized simultaneously using dev- θ s t pf(s  t)co(s  t) log pθ(s|t)  set maximum likelihood as before: !�argmax po (s  t) ! θ s t �ˆ = argmax ˜p(s  t) log p(s|t; 0).</S><S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'136'", "'73'", "'140'", "'64'"]
'19'
'136'
'73'
'140'
'64'
['19', '136', '73', '140', '64']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P11-1060_sweta.csv
<S sid="8" ssid="4">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
 <S sid="45" ssid="21">The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["45'"]
45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="27">It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent&#8217;s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
 <S sid="166" ssid="51">Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language&#8212;think grounded al., 2010). compositional semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
 <S sid="8" ssid="4">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
 <S sid="11" ssid="7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["11'"]
11'
['11']
parsed_discourse_facet ['method_citation']
  <S sid="115" ssid="91">After training, given a new utterance x, our system outputs the most likely y, summing out the latent logical form z: argmaxy p&#952;(T)(y  |x, z &#8712; &#732;ZL,&#952;(T)).</S>
original cit marker offset is 0
new cit marker offset is 0



["115'"]
115'
['115']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="70">We now turn to the task of mapping natural language For the example in Figure 4(b), the de- utterances to DCS trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["94'"]
94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="157" ssid="42">Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["157'"]
157'
['157']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["106'"]
106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="52">In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="23">Next, we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S>
original cit marker offset is 0
new cit marker offset is 0



["138'"]
138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="16">The CSP has two types of constraints: (i) x &#8712; w(p) for each node x labeled with predicate p &#8712; P; and (ii) xj = yj0 (the j-th component of x must equal the j'-th component of y) for each edge (x, y) labeled with j0j &#8712; R. A solution to the CSP is an assignment of nodes to values that satisfies all the constraints.</S>
original cit marker offset is 0
new cit marker offset is 0



["40'"]
40'
['40']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["171'"]
171'
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S sid ="1" ssid = "1">Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="2" ssid = "2">In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'141'", "'15'", "'6'", "'2'"]
'1'
'141'
'15'
'6'
'2'
['1', '141', '15', '6', '2']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="117" ssid = "2">In each dataset  each sentence x is annotated with a Prolog logical form  which we use only to evaluate and get an answer y.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="149" ssid = "34">Think of DCS as a higher-level Our system learns lexical associations between programming language tailored to natural language  words and predicates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'3'", "'117'", "'100'", "'149'"]
'2'
'3'
'117'
'100'
'149'
['2', '3', '117', '100', '149']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="48" ssid = "24">In addition  trees enable efficient computation  thereby establishing a new connection between dependency syntax and efficient semantic evaluation.</S><S sid ="149" ssid = "34">Think of DCS as a higher-level Our system learns lexical associations between programming language tailored to natural language  words and predicates.</S><S sid ="2" ssid = "2">In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.</S><S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parent’s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'48'", "'149'", "'2'", "'51'"]
'3'
'48'
'149'
'2'
'51'
['3', '48', '149', '2', '51']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">On two stansemantic parsing benchmarks our system obtains the highest published accuracies  despite requiring no annotated logical forms.</S><S sid ="24" ssid = "20">Our system outperforms all existing systems despite using no annotated logical forms.</S><S sid ="136" ssid = "21">In fact  DCS performs comparably to even the version of SEMRESP trained using logical forms.</S><S sid ="139" ssid = "24">All other systems require logical forms as training data  whereas ours does not.</S><S sid ="135" ssid = "20">SEMRESP requires a lexicon of 1.42 words per non-value predicate  WordNet features  and syntactic parse trees; DCS requires only words for the domain-independent predicates (overall  around 0.5 words per non-value predicate)  POS tags  and very simple indicator features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'24'", "'136'", "'139'", "'135'"]
'4'
'24'
'136'
'139'
'135'
['4', '24', '136', '139', '135']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">What is the total population of the ten largest capitals in the US?</S><S sid ="52" ssid = "28">The denotation of the middle node is {s}  where s is all major cities.</S><S sid ="105" ssid = "81">The feature vector φ(x  z) is defined by sums of five simple indicator feature templates: (F1) a word triggers a predicate (e.g.  [city  city]); (F2) a word is under a relation (e.g.  [that  11]); (F3) a word is under a trace predicate (e.g.  [in  loc]); (F4) two predicates are linked via a relation in the left or right direction (e.g.  [city 11  loc  RIGHT]); and (F5) a predicate has a child relation (e.g.  [city  11]).</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S><S sid ="50" ssid = "26">For example  consider the phrase number of major cities  and suppose that number corresponds to the count predicate.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'52'", "'105'", "'62'", "'50'"]
'5'
'52'
'105'
'62'
'50'
['5', '52', '105', '62', '50']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="48" ssid = "24">In addition  trees enable efficient computation  thereby establishing a new connection between dependency syntax and efficient semantic evaluation.</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'169'", "'58'", "'48'", "'165'"]
'6'
'169'
'58'
'48'
'165'
['6', '169', '58', '48', '165']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Supervised semantic parsers (Zelle and Mooney  1996; Tang and Mooney  2001; Ge and Mooney  2005; Zettlemoyer and Collins  2005; Kate and Mooney  2007; Zettlemoyer and Collins  2007; Wong and Mooney  2007; Kwiatkowski et al.  2010) rely on manual annotation of logical forms  which is expensive.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="166" ssid = "51">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid ="125" ssid = "10">We used the same training-test splits as Zettlemoyer and Collins (2005) (600+280 for GEO and 500+140 for JOBS).</S><S sid ="161" ssid = "46">CCG (Steedman  2000)  in which semantic pars- The integration of natural language with denotaing is driven from the lexicon.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'8'", "'166'", "'125'", "'161'"]
'7'
'8'
'166'
'125'
'161'
['7', '8', '166', '125', '161']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="138" ssid = "23">Next  we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S><S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid ="18" ssid = "14">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'138'", "'9'", "'165'", "'18'"]
'8'
'138'
'9'
'165'
'18'
['8', '138', '9', '165', '18']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="20" ssid = "16">At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'58'", "'165'", "'26'", "'20'"]
'9'
'58'
'165'
'26'
'20'
['9', '58', '165', '26', '20']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="37" ssid = "13">The basic version of DCS restricts R to join and aggregate relations (see Table 1).</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'58'", "'169'", "'37'", "'26'"]
'10'
'58'
'169'
'37'
'26'
['10', '58', '169', '37', '26']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts)  producing an answer y.</S><S sid ="118" ssid = "3">This evaluation is done with respect to a world w. Recall that a world w maps each predicate p ∈ P to a set of tuples w(p).</S><S sid ="31" ssid = "7">Conceptually  a world is a relational database where each predicate is a relation (possibly infinite).</S><S sid ="38" ssid = "14">Let us start by considering a DCS tree z with only join relations.</S><S sid ="29" ssid = "5">Let P be a set of predicates (e.g.  state  count ∈ P)  which are just symbols.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'118'", "'31'", "'38'", "'29'"]
'11'
'118'
'31'
'38'
'29'
['11', '118', '31', '38', '29']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="45" ssid = "21">The logical forms in DCS are called DCS trees  where nodes are labeled with predicates  and edges are labeled with relations.</S><S sid ="32" ssid = "8">Define a special predicate ø with w(ø) = V. We represent functions by a set of inputoutput pairs  e.g.  w(count) = {(S  n) : n = |S|}.</S><S sid ="85" ssid = "61">Extraction allows us to return the set of consistent values of a marked non-root node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'169'", "'45'", "'32'", "'85'"]
'12'
'169'
'45'
'32'
'85'
['12', '169', '45', '32', '85']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x  y)  which is much cheaper to obtain than (x  z) pairs.</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S><S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="103" ssid = "79">To define the features  we technically need to augment each tree z ∈ ZL(x) with alignment information—namely  for each predicate in z  the span in x (if any) that triggered it.</S><S sid ="108" ssid = "84">However  in order to learn  we need to sum over {z ∈ ZL(x) : JzKw = y}  and unfortunately  the additional constraint JzKw = y does not factorize.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'62'", "'12'", "'103'", "'108'"]
'13'
'62'
'12'
'103'
'108'
['13', '62', '12', '103', '108']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.</S><S sid ="43" ssid = "19">Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter  2003).</S><S sid ="107" ssid = "83">Our model is arc-factored  so we can sum over all DCS trees in ZL(x) using dynamic programming.</S><S sid ="47" ssid = "23">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.</S><S sid ="59" ssid = "35">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'43'", "'107'", "'47'", "'59'"]
'14'
'43'
'107'
'47'
'59'
['14', '43', '107', '47', '59']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="142" ssid = "27">This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.</S><S sid ="163" ssid = "48">Feedback from the context; for example  the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x  y)  which means (Schuler  2003) and semantic parsing (Popescu et borders looks right for the first argument and left al.  2003; Clarke et al.  2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'100'", "'141'", "'142'", "'163'"]
'15'
'100'
'141'
'142'
'163'
['15', '100', '141', '142', '163']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Which one should we use?</S><S sid ="144" ssid = "29">Intuitions How is our system learning?</S><S sid ="142" ssid = "27">This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.</S><S sid ="95" ssid = "71">Our first question is: given notation of the DCS tree before execution is an utterance x  what trees z ∈ Z are permissible?</S><S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'144'", "'142'", "'95'", "'15'"]
'16'
'144'
'142'
'95'
'15'
['16', '144', '142', '95', '15']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">The dominant paradigm in compositional semantics is Montague semantics  which constructs lambda calculus forms in a bottom-up manner.</S><S sid ="36" ssid = "12">It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.</S><S sid ="37" ssid = "13">The basic version of DCS restricts R to join and aggregate relations (see Table 1).</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="25" ssid = "1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS)  which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'36'", "'37'", "'58'", "'25'"]
'17'
'36'
'37'
'58'
'25'
['17', '36', '37', '58', '25']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="20" ssid = "16">At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.</S><S sid ="167" ssid = "52">In DCS  we start with lexical triggers  which are 6 Conclusion more basic than CCG lexical entries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'165'", "'26'", "'20'", "'167'"]
'18'
'165'
'26'
'20'
'167'
['18', '165', '26', '20', '167']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "15">However  the logical forms there can become quite complex  and in the context of program induction  this would lead to an unwieldy search space.</S><S sid ="122" ssid = "7">For JOBS  if we use the standard Jobs database  close to half the y’s are empty  which makes it uninteresting.</S><S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parent’s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid ="14" ssid = "10">The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.</S><S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'122'", "'51'", "'14'", "'9'"]
'19'
'122'
'51'
'14'
'9'
['19', '122', '51', '14', '9']
parsed_discourse_facet ['method_citation']



P11-1060
P13-2009
0
method_citation
['method_citation']
parsing: input/ref/Task1/W06-3114_aakansha.csv
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="2">Training and testing is based on the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="2">Training and testing is based on the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'"]
'102'
['102']
parsed_discourse_facet ['method_citation']
<S sid="84" ssid="23">The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'"]
'84'
['84']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="4">To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="19">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="6">Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="3">&#8226; We evaluated translation from English, in addition to into English.</S>
    <S sid="6" ssid="4">English was again paired with German, French, and Spanish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'6'"]
'5'
'6'
['5', '6']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="143" ssid = "36">For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.</S><S sid ="129" ssid = "22">All systems (except for Systran  which was not tuned to Europarl) did considerably worse on outof-domain training data.</S><S sid ="121" ssid = "14">For the automatic scoring method BLEU  we can distinguish three quarters of the systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'136'", "'143'", "'129'", "'121'"]
'1'
'136'
'143'
'129'
'121'
['1', '136', '143', '129', '121']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S sid ="86" ssid = "2">The average fluency judgement per judge ranged from 2.33 to 3.67  the average adequacy judgement ranged from 2.56 to 4.13.</S><S sid ="150" ssid = "43">For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French)  at least as measured by BLEU.</S><S sid ="95" ssid = "11">The judgement of 4 in the first case will go to a vastly better system output than in the second case.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'86'", "'150'", "'95'", "'136'"]
'2'
'86'
'150'
'95'
'136'
['2', '86', '150', '95', '136']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "1">was done by the participants.</S><S sid ="67" ssid = "6">Participants and other volunteers contributed about 180 hours of labor in the manual evaluation.</S><S sid ="29" ssid = "22">About half of the participants of last year’s shared task participated again.</S><S sid ="78" ssid = "17">Judges where excluded from assessing the quality of MT systems that were submitted by their institution.</S><S sid ="168" ssid = "61">Annotators argued for the importance of having correct and even multiple references.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'67'", "'29'", "'78'", "'168'"]
'3'
'67'
'29'
'78'
'168'
['3', '67', '29', '78', '168']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "2">This revealed interesting clues about the properties of automatic and manual scoring.</S><S sid ="146" ssid = "39">This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.</S><S sid ="168" ssid = "61">Annotators argued for the importance of having correct and even multiple references.</S><S sid ="176" ssid = "7">Human judges also pointed out difficulties with the evaluation of long sentences.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'146'", "'168'", "'176'", "'134'"]
'4'
'146'
'168'
'176'
'134'
['4', '146', '168', '176', '134']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "3">• We evaluated translation from English  in addition to into English.</S><S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S><S sid ="152" ssid = "45">The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'6'", "'152'", "'170'", "'162'"]
'5'
'6'
'152'
'170'
'162'
['5', '6', '152', '170', '162']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S><S sid ="5" ssid = "3">• We evaluated translation from English  in addition to into English.</S><S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="79" ssid = "18">Sentences and systems were randomly selected and randomly shuffled for presentation.</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'5'", "'163'", "'79'", "'162'"]
'6'
'5'
'163'
'79'
'162'
['6', '5', '163', '79', '162']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "5">We dropped  however  one of the languages  Finnish  partly to keep the number of tracks manageable  partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation.</S><S sid ="165" ssid = "58">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid ="66" ssid = "5">In this shared task  we were also confronted with this problem  and since we had no funding for paying human judgements  we asked participants in the evaluation to share the burden.</S><S sid ="47" ssid = "13">Because of this  we retokenized and lowercased submitted output with our own tokenizer  which was also used to prepare the training and test data.</S><S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'165'", "'66'", "'47'", "'125'"]
'7'
'165'
'66'
'47'
'125'
['7', '165', '66', '47', '125']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "1">The evaluation framework for the shared task is similar to the one used in last year’s shared task.</S><S sid ="57" ssid = "23">We are therefore applying a different method  which has been used at the 2005 DARPA/NIST evaluation.</S><S sid ="29" ssid = "22">About half of the participants of last year’s shared task participated again.</S><S sid ="31" ssid = "24">Compared to last year’s shared task  the participants represent more long-term research efforts.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'57'", "'29'", "'31'", "'28'"]
'8'
'57'
'29'
'31'
'28'
['8', '57', '29', '31', '28']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "2">Training and testing is based on the Europarl corpus.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="143" ssid = "36">For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.</S><S sid ="21" ssid = "14">The out-of-domain test set differs from the Europarl data in various ways.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'28'", "'170'", "'143'", "'21'"]
'9'
'28'
'170'
'143'
'21'
['9', '28', '170', '143', '21']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "3">Figure 1 provides some statistics about this corpus.</S><S sid ="20" ssid = "13">For statistics on this test set  refer to Figure 1.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="4" ssid = "2">This revealed interesting clues about the properties of automatic and manual scoring.</S><S sid ="120" ssid = "13">In Figure 4  we displayed the number of system comparisons  for which we concluded statistical significance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'20'", "'134'", "'4'", "'120'"]
'10'
'20'
'134'
'4'
'120'
['10', '20', '134', '4', '120']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "4">To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="161" ssid = "54">Since all long sentence translation are somewhat muddled  even a contrastive evaluation between systems was difficult.</S><S sid ="152" ssid = "45">The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.</S><S sid ="31" ssid = "24">Compared to last year’s shared task  the participants represent more long-term research efforts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'28'", "'161'", "'152'", "'31'"]
'11'
'28'
'161'
'152'
'31'
['11', '28', '161', '152', '31']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "5">To summarize  we provided: The performance of the baseline system is similar to the best submissions in last year’s shared task.</S><S sid ="141" ssid = "34">In-domain Systran scores on this metric are lower than all statistical systems  even the ones that have much worse human scores.</S><S sid ="8" ssid = "1">The evaluation framework for the shared task is similar to the one used in last year’s shared task.</S><S sid ="81" ssid = "20">This is less than the 694 judgements 2004 DARPA/NIST evaluation  or the 532 judgements in the 2005 DARPA/NIST evaluation.</S><S sid ="151" ssid = "44">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages  as demonstrated by the results for English-German and English-French.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'141'", "'8'", "'81'", "'151'"]
'12'
'141'
'8'
'81'
'151'
['12', '141', '8', '81', '151']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "6">We are currently working on a complete open source implementation of a training and decoding system  which should become available over the summer. pus  from which also the in-domain test set is taken.</S><S sid ="33" ssid = "26">While building a machine translation system is a serious undertaking  in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.</S><S sid ="43" ssid = "9">At the very least  we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.</S><S sid ="74" ssid = "13">While we had up to 11 submissions for a translation direction  we did decide against presenting all 11 system outputs to the human judge.</S><S sid ="42" ssid = "8">It was our hope that this competition  which included the manual and automatic evaluation of statistical systems and one rulebased commercial system  will give further insight into the relation between automatic and manual evaluation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'33'", "'43'", "'74'", "'42'"]
'13'
'33'
'43'
'74'
'42'
['13', '33', '43', '74', '42']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "7">There is twice as much language modelling data  since training data for the machine translation system is filtered against sentences of length larger than 40 words.</S><S sid ="165" ssid = "58">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid ="58" ssid = "24">We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set  53 blocks for the out-of-domain test set)  check for each block  if one system has a higher BLEU score than the other  and then use the sign test.</S><S sid ="151" ssid = "44">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages  as demonstrated by the results for English-German and English-French.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'165'", "'58'", "'151'", "'155'"]
'14'
'165'
'58'
'151'
'155'
['14', '165', '58', '151', '155']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "8">Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.</S><S sid ="21" ssid = "14">The out-of-domain test set differs from the Europarl data in various ways.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="142" ssid = "35">Surprisingly  this effect is much less obvious for out-of-domain test data.</S><S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'21'", "'28'", "'142'", "'18'"]
'15'
'21'
'28'
'142'
'18'
['15', '21', '28', '142', '18']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="165" ssid = "58">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid ="14" ssid = "7">There is twice as much language modelling data  since training data for the machine translation system is filtered against sentences of length larger than 40 words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'126'", "'18'", "'165'", "'14'"]
'16'
'126'
'18'
'165'
'14'
['16', '126', '18', '165', '14']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="69" ssid = "8">We settled on contrastive evaluations of 5 system outputs for a single test sentence.</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="19" ssid = "12">We aligned the texts at a sentence level across all four languages  resulting in 1064 sentence per language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'69'", "'162'", "'170'", "'19'"]
'17'
'69'
'162'
'170'
'19'
['17', '69', '162', '170', '19']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="15" ssid = "8">Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'126'", "'16'", "'15'", "'155'"]
'18'
'126'
'16'
'15'
'155'
['18', '126', '16', '15', '155']
parsed_discourse_facet ['method_citation']



W06-3114
D07-1091
0
method_citation
['method_citation']
parsing: input/ref/Task1/P11-1061_sweta.csv
 <S sid="9" ssid="5">Unfortunately, the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al., 2010), making its practical usability questionable at best.</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="47" ssid="13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["47'"]
47'
['47']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["10'"]
10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="1">Given the bilingual graph described in the previous section, we can use label propagation to project the English POS labels to the foreign language.</S>
original cit marker offset is 0
new cit marker offset is 0



["70'"]
70'
['70']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="18">This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
 <S sid="83" ssid="14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value &#964;: We describe how we choose &#964; in &#167;6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["83'"]
83'
['83']
parsed_discourse_facet ['method_citation']
<S sid="113" ssid="13">For each language under consideration, Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["113'"]
113'
['113']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
 <S sid="13" ssid="9">(2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available.</S>
original cit marker offset is 0
new cit marker offset is 0



["13'"]
13'
['13']
parsed_discourse_facet ['method_citation']
 <S sid="3" ssid="3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="20">We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Syntactic universals are a well studied concept in linguistics (Carnie, 2002; Newmeyer, 2005), and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["19'"]
19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="16">Figure 2 shows an excerpt of a sentence from the Italian test set and the tags assigned by four different models, as well as the gold tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



["161'"]
161'
['161']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["23'"]
23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["23'"]
23'
['23']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1061.csv
<S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S><S sid ="8" ssid = "4">Unsupervised learning approaches appear to be a natural solution to this problem  as they require only unannotated text for training models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'160'", "'3'", "'13'", "'8'"]
'1'
'160'
'3'
'13'
'8'
['1', '160', '3', '13', '8']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed)  making it applicable to a wide array of resource-poor languages.</S><S sid ="9" ssid = "5">Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.</S><S sid ="126" ssid = "26">Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.</S><S sid ="41" ssid = "7">Because the information flow in our graph is asymmetric (from English to the foreign language)  we use different types of vertices for each language.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'9'", "'126'", "'41'", "'107'"]
'2'
'9'
'126'
'41'
'107'
['2', '9', '126', '41', '107']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="33" ssid = "10">The POS distributions over the foreign trigram types are used as features to learn a better unsupervised POS tagger (§5).</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="60" ssid = "26">To initialize the graph for label propagation we use a supervised English tagger to label the English side of the bitext.7 We then simply count the individual labels of the English tokens and normalize the counts to produce tag distributions over English word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'39'", "'33'", "'1'", "'60'"]
'3'
'39'
'33'
'1'
'60'
['3', '39', '33', '1', '60']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S><S sid ="23" ssid = "19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%)  and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S sid ="143" ssid = "6">Overall  it gives improvements ranging from 1.1% for German to 14.7% for Italian  for an average improvement of 8.3% over the unsupervised feature-HMM model.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S><S sid ="144" ssid = "7">For comparison  the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%  and goes up to 88.7% with a treebank dictionary.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'23'", "'143'", "'135'", "'144'"]
'4'
'23'
'143'
'135'
'144'
['4', '23', '143', '135', '144']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing  resulting in highly accurate systems.</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S><S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S><S sid ="15" ssid = "11">First  we use a novel graph-based framework for projecting syntactic information across language boundaries.</S><S sid ="18" ssid = "14">To make the projection practical  we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'158'", "'4'", "'15'", "'18'"]
'5'
'158'
'4'
'15'
'18'
['5', '158', '4', '15', '18']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Supervised part-of-speech (POS) taggers  for example  approach the level of inter-annotator agreement (Shen et al.  2007  97.3% accuracy for English).</S><S sid ="47" ssid = "13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S><S sid ="9" ssid = "5">Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S><S sid ="20" ssid = "16">Because there might be some controversy about the exact definitions of such universals  this set of coarse-grained POS categories is defined operationally  by collapsing language (or treebank) specific distinctions to a set of categories that exists across all languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'47'", "'9'", "'158'", "'20'"]
'6'
'47'
'9'
'158'
'20'
['6', '47', '9', '158', '20']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S><S sid ="27" ssid = "4">Graph construction does not require any labeled data  but makes use of two similarity functions.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'27'", "'107'", "'56'", "'37'"]
'7'
'27'
'107'
'56'
'37'
['7', '27', '107', '56', '37']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Unsupervised learning approaches appear to be a natural solution to this problem  as they require only unannotated text for training models.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'160'", "'1'", "'13'", "'3'"]
'8'
'160'
'1'
'13'
'3'
['8', '160', '1', '13', '3']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.</S><S sid ="47" ssid = "13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S><S sid ="126" ssid = "26">Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.</S><S sid ="2" ssid = "2">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed)  making it applicable to a wide array of resource-poor languages.</S><S sid ="57" ssid = "23">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'47'", "'126'", "'2'", "'57'"]
'9'
'47'
'126'
'2'
'57'
['9', '47', '126', '2', '57']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'39'", "'16'", "'26'", "'13'"]
'10'
'39'
'16'
'26'
'13'
['10', '39', '16', '26', '13']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This scenario is applicable to a large set of languages and has been considered by a number of authors in the past (Alshawi et al.  2000; Xi and Hwa  2005; Ganchev et al.  2009).</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="94" ssid = "25">We adopted this state-of-the-art model because it makes it easy to experiment with various ways of incorporating our novel constraint feature into the log-linear emission model.</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'19'", "'26'", "'94'", "'13'"]
'11'
'19'
'26'
'94'
'13'
['11', '19', '26', '94', '13']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Naseem et al. (2009) and Snyder et al.</S><S sid ="38" ssid = "4">More recently  Subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model.</S><S sid ="93" ssid = "24">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S><S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="123" ssid = "23">This can be seen as a rough approximation of Yarowsky and Ngai (2001).</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'38'", "'93'", "'37'", "'123'"]
'12'
'38'
'93'
'37'
'123'
['12', '38', '93', '37', '123']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="24" ssid = "1">The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'39'", "'160'", "'1'", "'24'"]
'13'
'39'
'160'
'1'
'24'
['13', '39', '160', '1', '24']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">Our work is closest to that of Yarowsky and Ngai (2001)  but differs in two important ways.</S><S sid ="123" ssid = "23">This can be seen as a rough approximation of Yarowsky and Ngai (2001).</S><S sid ="40" ssid = "6">We extend Subramanya et al.’s intuitions to our bilingual setup.</S><S sid ="88" ssid = "19">In our experiments  we used the same set of features as BergKirkpatrick et al. (2010): an indicator feature based In a traditional Markov model  the emission distribution PΘ(Xi = xi  |Zi = zi) is a set of multinomials.</S><S sid ="85" ssid = "16">We develop our POS induction model based on the feature-based HMM of Berg-Kirkpatrick et al. (2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'123'", "'40'", "'88'", "'85'"]
'14'
'123'
'40'
'88'
'85'
['14', '123', '40', '88', '85']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">First  we use a novel graph-based framework for projecting syntactic information across language boundaries.</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'158'", "'29'", "'56'", "'25'"]
'15'
'158'
'29'
'56'
'25'
['15', '158', '29', '56', '25']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="57" ssid = "23">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid ="70" ssid = "1">Given the bilingual graph described in the previous section  we can use label propagation to project the English POS labels to the foreign language.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'57'", "'70'", "'29'", "'26'"]
'16'
'57'
'70'
'29'
'26'
['16', '57', '70', '29', '26']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">Second  we treat the projected labels as features in an unsupervised model (§5)  rather than using them directly for supervised training.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="71" ssid = "2">We use label propagation in two stages to generate soft labels on all the vertices in the graph.</S><S sid ="126" ssid = "26">Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.</S><S sid ="52" ssid = "18">This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'97'", "'71'", "'126'", "'52'"]
'17'
'97'
'71'
'126'
'52'
['17', '97', '71', '126', '52']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">To make the projection practical  we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="111" ssid = "11">We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns)  VERB (verbs)  ADJ (adjectives)  ADV (adverbs)  PRON (pronouns)  DET (determiners)  ADP (prepositions or postpositions)  NUM (numerals)  CONJ (conjunctions)  PRT (particles)  PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).</S><S sid ="85" ssid = "16">We develop our POS induction model based on the feature-based HMM of Berg-Kirkpatrick et al. (2010).</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'56'", "'111'", "'85'", "'158'"]
'18'
'56'
'111'
'85'
'158'
['18', '56', '111', '85', '158']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/E03-1005_aakansha.csv
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'"]
'105'
['105']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'"]
'145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'"]
'80'
['80']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'"]
'143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid="141" ssid="6">This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'141'"]
'140'
'141'
['140', '141']
parsed_discourse_facet ['result_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="102" ssid="5">In case the shortest derivation is not unique, Bod (2000b) proposes to back off to a frequency ordering of the subtrees.</S>
    <S sid="103" ssid="6">That is, all subtrees of each root label are assigned a rank according to their frequency in the treebank: the most frequent subtree (or subtrees) of each root label gets rank 1, the second most frequent subtree gets rank 2, etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'", "'103'"]
'102'
'103'
['102', '103']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="46" ssid="43">Most previous notions of best parse tree in DOP1 were based on a probabilistic metric, with Bod (2000b) as a notable exception, who used a simplicity metric based on the shortest derivation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'"]
'46'
['46']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="37">For example, Bod (2001) samples a fixed number of subtrees of each depth, which has the effect of assigning roughly equal weight to each node in the training data, and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S>
    <S sid="86" ssid="38">Bod reports state-of-the-art results with this method, and observes no decrease in parse accuracy when larger subtrees are included (using subtrees up to depth 14).</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'", "'86'"]
'85'
'86'
['85', '86']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="115" ssid="18">The only thing that needs to be changed for Simplicity-DOP is that all subtrees should be assigned equal probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S sid ="1" ssid = "1">Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.</S><S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S><S sid ="76" ssid = "28">Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.</S><S sid ="26" ssid = "23">Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation  which in some cases is a reasonable approximation of the most probable parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'108'", "'28'", "'76'", "'26'"]
'1'
'108'
'28'
'76'
'26'
['1', '108', '28', '76', '26']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">This paper proposes an integration of the two models which outperforms each of them separately.</S><S sid ="107" ssid = "10">This suggests that a model which combines these two notions of best parse may boost the accuracy.</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S><S sid ="94" ssid = "46">This paper presents the first published results with this estimator on the WSJ.</S><S sid ="56" ssid = "8">Here we will only sketch this PCFG-reduction  which is heavily based on Goodman (2002).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'107'", "'45'", "'94'", "'56'"]
'2'
'107'
'45'
'94'
'56'
['2', '107', '45', '94', '56']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models  and an average processing time of 3.6 seconds per WSJ sentence.</S><S sid ="38" ssid = "35">Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task  the parsing time was reported to be over 200 seconds per sentence (Bod 2003).</S><S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="92" ssid = "44">Tested on the OVIS corpus  Bonnema et al. 's proposal obtains results that are comparable to Sima'an (1999) -- see Bonnema et al.</S><S sid ="41" ssid = "38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'38'", "'130'", "'92'", "'41'"]
'3'
'38'
'130'
'92'
'41'
['3', '38', '130', '92', '41']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S><S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="6" ssid = "3">To appreciate these innovations  it should be noted that the model was radically different from all other statistical parsing models at the time.</S><S sid ="143" ssid = "8">While SL-DOP and LS-DOP have been compared before in Bod (2002)  especially in the context of musical parsing  this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules  and (2) the use of arbitrarily large fragments rather than restricted ones.</S><S sid ="1" ssid = "1">Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'108'", "'6'", "'143'", "'1'"]
'4'
'108'
'6'
'143'
'1'
['4', '108', '6', '143', '1']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "2">Fragments include  for instance  subtrees of depth 1 (corresponding to context-free rules) as well as entire trees.</S><S sid ="62" ssid = "14">Let a represent the number of subtrees headed by nodes with nonterminal A  that is a =Ej aj.</S><S sid ="23" ssid = "20">Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.</S><S sid ="104" ssid = "7">Next  the rank of each (shortest) derivation is computed as the sum of the ranks of the subtrees involved.</S><S sid ="61" ssid = "13">Let aj represent the number of subtrees headed by the node A@j.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'62'", "'23'", "'104'", "'61'"]
'5'
'62'
'23'
'104'
'61'
['5', '62', '23', '104', '61']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "3">To appreciate these innovations  it should be noted that the model was radically different from all other statistical parsing models at the time.</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="106" ssid = "9">What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.</S><S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S><S sid ="76" ssid = "28">Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'79'", "'106'", "'4'", "'76'"]
'6'
'79'
'106'
'4'
'76'
['6', '79', '106', '4', '76']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="128" ssid = "9">Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.</S><S sid ="23" ssid = "20">Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="14" ssid = "11">This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'128'", "'23'", "'15'", "'14'"]
'7'
'128'
'23'
'15'
'14'
['7', '128', '23', '15', '14']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">in Fujisaki et al. 1989; Black et al.</S><S sid ="10" ssid = "7">Waegner 1992; Pereira and Schabes 1992).</S><S sid ="98" ssid = "1">Most DOP models  such as in Bod (1993)  Goodman (1996)  Bonnema et al. (1997)  Sima'an (2000) and Collins & Duffy (2002)  use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.</S><S sid ="18" ssid = "15">Collins 1999; Charniak 2000; Goodman 1998).</S><S sid ="120" ssid = "1">For our experiments we used the standard division of the WSJ (Marcus et al. 1993)  with sections 2 through 21 for training (approx.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'10'", "'98'", "'18'", "'120'"]
'8'
'10'
'98'
'18'
'120'
['8', '10', '98', '18', '120']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "6">1992  1993; Briscoe and I Thanks to Ivan Sag for this pun.</S><S sid ="134" ssid = "15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.</S><S sid ="29" ssid = "26">Johnson (1998b  2002) showed that DOP1's subtree estimation method is statistically biased and inconsistent.</S><S sid ="35" ssid = "32">Bonnema et al. (1999) observed that another problem with DOP1's subtree-estimation method is that it provides more probability to nodes with more subtrees  and therefore more probability to larger subtrees.</S><S sid ="23" ssid = "20">Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'134'", "'29'", "'35'", "'23'"]
'9'
'134'
'29'
'35'
'23'
['9', '134', '29', '35', '23']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">Waegner 1992; Pereira and Schabes 1992).</S><S sid ="18" ssid = "15">Collins 1999; Charniak 2000; Goodman 1998).</S><S sid ="129" ssid = "10">Collins 1996  Charniak 1997  Collins 1999 and Charniak 2000).</S><S sid ="8" ssid = "5">in Fujisaki et al. 1989; Black et al.</S><S sid ="98" ssid = "1">Most DOP models  such as in Bod (1993)  Goodman (1996)  Bonnema et al. (1997)  Sima'an (2000) and Collins & Duffy (2002)  use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'18'", "'129'", "'8'", "'98'"]
'10'
'18'
'129'
'8'
'98'
['10', '18', '129', '8', '98']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "8">The DOP model  on the other hand  was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus  but to directly use corpus fragments as a grammar.</S><S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="143" ssid = "8">While SL-DOP and LS-DOP have been compared before in Bod (2002)  especially in the context of musical parsing  this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules  and (2) the use of arbitrarily large fragments rather than restricted ones.</S><S sid ="110" ssid = "13">We will refer to the first combination (which selects the simplest among the n likeliest trees) as Simplicity-Likelihood-DOP or SL-DOP  and to the second combination (which selects the likeliest among the n simplest trees) as Likelihood-Simplicity-DOP or LS-DOP.</S><S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'13'", "'143'", "'110'", "'4'"]
'11'
'13'
'143'
'110'
'4'
['11', '13', '143', '110', '4']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "9">This approach has now gained wide usage  as exemplified by the work of Collins (1996  1999)  Charniak (1996  1997)  Johnson (1998)  Chiang (2000)  and many others.</S><S sid ="98" ssid = "1">Most DOP models  such as in Bod (1993)  Goodman (1996)  Bonnema et al. (1997)  Sima'an (2000) and Collins & Duffy (2002)  use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.</S><S sid ="143" ssid = "8">While SL-DOP and LS-DOP have been compared before in Bod (2002)  especially in the context of musical parsing  this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules  and (2) the use of arbitrarily large fragments rather than restricted ones.</S><S sid ="16" ssid = "13">While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words  later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a).</S><S sid ="41" ssid = "38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'98'", "'143'", "'16'", "'41'"]
'12'
'98'
'143'
'16'
'41'
['12', '98', '143', '16', '41']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="54" ssid = "6">A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account.</S><S sid ="20" ssid = "17">Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage  and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).</S><S sid ="110" ssid = "13">We will refer to the first combination (which selects the simplest among the n likeliest trees) as Simplicity-Likelihood-DOP or SL-DOP  and to the second combination (which selects the likeliest among the n simplest trees) as Likelihood-Simplicity-DOP or LS-DOP.</S><S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'54'", "'20'", "'110'", "'4'"]
'13'
'54'
'20'
'110'
'4'
['13', '54', '20', '110', '4']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "11">This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="36" ssid = "33">As an alternative  Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'15'", "'7'", "'36'", "'82'"]
'14'
'15'
'7'
'36'
'82'
['14', '15', '7', '36', '82']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="14" ssid = "11">This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.</S><S sid ="134" ssid = "15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'45'", "'82'", "'14'", "'134'"]
'15'
'45'
'82'
'14'
'134'
['15', '45', '82', '14', '134']
parsed_discourse_facet ['method_citation']



E03-1005
P11-1086
0
method_citation
['aim_citation']
parsing: input/ref/Task1/E03-1005_sweta.csv
<S sid="20" ssid="17">Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).</S>
original cit marker offset is 0
new cit marker offset is 0



["20'"]
20'
['20']
parsed_discourse_facet ['method_citation']
 <S sid="74" ssid="26">Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.</S>
original cit marker offset is 0
new cit marker offset is 0



["74'"]
74'
['74']
parsed_discourse_facet ['method_citation']
<S sid="44" ssid="41">But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["44'"]
44'
['44']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["145'"]
145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S>
original cit marker offset is 0
new cit marker offset is 0



["22'"]
22'
['22']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="14">It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).</S>
original cit marker offset is 0
new cit marker offset is 0



["133'"]
133'
['133']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier &amp; Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="35">Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
 <S sid="100" ssid="3">In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="29">However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
 <S sid="130" ssid="11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["130'"]
130'
['130']
parsed_discourse_facet ['method_citation']
 <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["140'"]
140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="24">Goodman (1996, 1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'"]
27'
['27']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S sid ="1" ssid = "1">Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.</S><S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S><S sid ="76" ssid = "28">Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.</S><S sid ="26" ssid = "23">Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation  which in some cases is a reasonable approximation of the most probable parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'108'", "'28'", "'76'", "'26'"]
'1'
'108'
'28'
'76'
'26'
['1', '108', '28', '76', '26']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">This paper proposes an integration of the two models which outperforms each of them separately.</S><S sid ="107" ssid = "10">This suggests that a model which combines these two notions of best parse may boost the accuracy.</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S><S sid ="94" ssid = "46">This paper presents the first published results with this estimator on the WSJ.</S><S sid ="56" ssid = "8">Here we will only sketch this PCFG-reduction  which is heavily based on Goodman (2002).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'107'", "'45'", "'94'", "'56'"]
'2'
'107'
'45'
'94'
'56'
['2', '107', '45', '94', '56']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models  and an average processing time of 3.6 seconds per WSJ sentence.</S><S sid ="38" ssid = "35">Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task  the parsing time was reported to be over 200 seconds per sentence (Bod 2003).</S><S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="92" ssid = "44">Tested on the OVIS corpus  Bonnema et al. 's proposal obtains results that are comparable to Sima'an (1999) -- see Bonnema et al.</S><S sid ="41" ssid = "38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'38'", "'130'", "'92'", "'41'"]
'3'
'38'
'130'
'92'
'41'
['3', '38', '130', '92', '41']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S><S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="6" ssid = "3">To appreciate these innovations  it should be noted that the model was radically different from all other statistical parsing models at the time.</S><S sid ="143" ssid = "8">While SL-DOP and LS-DOP have been compared before in Bod (2002)  especially in the context of musical parsing  this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules  and (2) the use of arbitrarily large fragments rather than restricted ones.</S><S sid ="1" ssid = "1">Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'108'", "'6'", "'143'", "'1'"]
'4'
'108'
'6'
'143'
'1'
['4', '108', '6', '143', '1']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "2">Fragments include  for instance  subtrees of depth 1 (corresponding to context-free rules) as well as entire trees.</S><S sid ="62" ssid = "14">Let a represent the number of subtrees headed by nodes with nonterminal A  that is a =Ej aj.</S><S sid ="23" ssid = "20">Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.</S><S sid ="104" ssid = "7">Next  the rank of each (shortest) derivation is computed as the sum of the ranks of the subtrees involved.</S><S sid ="61" ssid = "13">Let aj represent the number of subtrees headed by the node A@j.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'62'", "'23'", "'104'", "'61'"]
'5'
'62'
'23'
'104'
'61'
['5', '62', '23', '104', '61']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "3">To appreciate these innovations  it should be noted that the model was radically different from all other statistical parsing models at the time.</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="106" ssid = "9">What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.</S><S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S><S sid ="76" ssid = "28">Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'79'", "'106'", "'4'", "'76'"]
'6'
'79'
'106'
'4'
'76'
['6', '79', '106', '4', '76']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="128" ssid = "9">Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.</S><S sid ="23" ssid = "20">Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="14" ssid = "11">This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'128'", "'23'", "'15'", "'14'"]
'7'
'128'
'23'
'15'
'14'
['7', '128', '23', '15', '14']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">in Fujisaki et al. 1989; Black et al.</S><S sid ="10" ssid = "7">Waegner 1992; Pereira and Schabes 1992).</S><S sid ="98" ssid = "1">Most DOP models  such as in Bod (1993)  Goodman (1996)  Bonnema et al. (1997)  Sima'an (2000) and Collins & Duffy (2002)  use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.</S><S sid ="18" ssid = "15">Collins 1999; Charniak 2000; Goodman 1998).</S><S sid ="120" ssid = "1">For our experiments we used the standard division of the WSJ (Marcus et al. 1993)  with sections 2 through 21 for training (approx.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'10'", "'98'", "'18'", "'120'"]
'8'
'10'
'98'
'18'
'120'
['8', '10', '98', '18', '120']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "6">1992  1993; Briscoe and I Thanks to Ivan Sag for this pun.</S><S sid ="134" ssid = "15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.</S><S sid ="29" ssid = "26">Johnson (1998b  2002) showed that DOP1's subtree estimation method is statistically biased and inconsistent.</S><S sid ="35" ssid = "32">Bonnema et al. (1999) observed that another problem with DOP1's subtree-estimation method is that it provides more probability to nodes with more subtrees  and therefore more probability to larger subtrees.</S><S sid ="23" ssid = "20">Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'134'", "'29'", "'35'", "'23'"]
'9'
'134'
'29'
'35'
'23'
['9', '134', '29', '35', '23']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">Waegner 1992; Pereira and Schabes 1992).</S><S sid ="18" ssid = "15">Collins 1999; Charniak 2000; Goodman 1998).</S><S sid ="129" ssid = "10">Collins 1996  Charniak 1997  Collins 1999 and Charniak 2000).</S><S sid ="8" ssid = "5">in Fujisaki et al. 1989; Black et al.</S><S sid ="98" ssid = "1">Most DOP models  such as in Bod (1993)  Goodman (1996)  Bonnema et al. (1997)  Sima'an (2000) and Collins & Duffy (2002)  use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'18'", "'129'", "'8'", "'98'"]
'10'
'18'
'129'
'8'
'98'
['10', '18', '129', '8', '98']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "8">The DOP model  on the other hand  was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus  but to directly use corpus fragments as a grammar.</S><S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="143" ssid = "8">While SL-DOP and LS-DOP have been compared before in Bod (2002)  especially in the context of musical parsing  this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules  and (2) the use of arbitrarily large fragments rather than restricted ones.</S><S sid ="110" ssid = "13">We will refer to the first combination (which selects the simplest among the n likeliest trees) as Simplicity-Likelihood-DOP or SL-DOP  and to the second combination (which selects the likeliest among the n simplest trees) as Likelihood-Simplicity-DOP or LS-DOP.</S><S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'13'", "'143'", "'110'", "'4'"]
'11'
'13'
'143'
'110'
'4'
['11', '13', '143', '110', '4']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "9">This approach has now gained wide usage  as exemplified by the work of Collins (1996  1999)  Charniak (1996  1997)  Johnson (1998)  Chiang (2000)  and many others.</S><S sid ="98" ssid = "1">Most DOP models  such as in Bod (1993)  Goodman (1996)  Bonnema et al. (1997)  Sima'an (2000) and Collins & Duffy (2002)  use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.</S><S sid ="143" ssid = "8">While SL-DOP and LS-DOP have been compared before in Bod (2002)  especially in the context of musical parsing  this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules  and (2) the use of arbitrarily large fragments rather than restricted ones.</S><S sid ="16" ssid = "13">While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words  later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a).</S><S sid ="41" ssid = "38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'98'", "'143'", "'16'", "'41'"]
'12'
'98'
'143'
'16'
'41'
['12', '98', '143', '16', '41']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="54" ssid = "6">A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account.</S><S sid ="20" ssid = "17">Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage  and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).</S><S sid ="110" ssid = "13">We will refer to the first combination (which selects the simplest among the n likeliest trees) as Simplicity-Likelihood-DOP or SL-DOP  and to the second combination (which selects the likeliest among the n simplest trees) as Likelihood-Simplicity-DOP or LS-DOP.</S><S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'54'", "'20'", "'110'", "'4'"]
'13'
'54'
'20'
'110'
'4'
['13', '54', '20', '110', '4']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "11">This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="36" ssid = "33">As an alternative  Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'15'", "'7'", "'36'", "'82'"]
'14'
'15'
'7'
'36'
'82'
['14', '15', '7', '36', '82']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="14" ssid = "11">This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.</S><S sid ="134" ssid = "15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'45'", "'82'", "'14'", "'134'"]
'15'
'45'
'82'
'14'
'134'
['15', '45', '82', '14', '134']
parsed_discourse_facet ['method_citation']



E03-1005
P11-1086
0
method_citation
['aim_citation']
parsing: input/ref/Task1/A00-2018_sweta.csv
 <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["17'"]
17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
 <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["17'"]
17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="11">Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &amp;quot;correct&amp;quot;, and statistics were collected on the resulting parses.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="10">(It is &amp;quot;soft&amp;quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.)</S>
original cit marker offset is 0
new cit marker offset is 0



["119'"]
119'
['119']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.</S>
original cit marker offset is 0
new cit marker offset is 0



["95'"]
95'
['95']
parsed_discourse_facet ['method_citation']
 <S sid="175" ssid="2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["175'"]
175'
['175']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
original cit marker offset is 0
new cit marker offset is 0



["92'"]
92'
['92']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="17">This is indicated in Figure 2, where the model labeled &amp;quot;Best&amp;quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["126'"]
126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="1">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c, t(c) (t for &amp;quot;tag&amp;quot;), then the lexical head of c, h(c), and then the expansion of c into further constituents e(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["174'"]
174'
['174']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="32">As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &amp;quot;before&amp;quot;), and the label of the grandparent of c (la).</S>
original cit marker offset is 0
new cit marker offset is 0



["63'"]
63'
['63']
parsed_discourse_facet ['method_citation']
<S sid="78" ssid="47">With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.</S>
original cit marker offset is 0
new cit marker offset is 0



["78'"]
78'
['78']
parsed_discourse_facet ['method_citation']
 <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="7">From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.</S>
original cit marker offset is 0
new cit marker offset is 0



["180'"]
180'
['180']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2018.csv
<S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'5'", "'2'", "'6'", "'125'"]
'1'
'5'
'2'
'6'
'125'
['1', '5', '2', '6', '125']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="175" ssid = "2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'1'", "'125'", "'175'"]
'2'
'6'
'1'
'125'
'175'
['2', '6', '1', '125', '175']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.</S><S sid ="182" ssid = "9">As noted above  the main methodological innovation presented here is our &quot;maximumentropy-inspired&quot; model for conditioning and smoothing.</S><S sid ="128" ssid = "19">The model labeled &quot;Old&quot; attempts to recreate the Char97 system using the current program.</S><S sid ="118" ssid = "9">First  it uses a clustering scheme on words to give the system a &quot;soft&quot; clustering of heads and sub-heads.</S><S sid ="116" ssid = "7">This is as opposed to the &quot;Markovgrammar&quot; approach used in the current parser.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'182'", "'128'", "'118'", "'116'"]
'3'
'182'
'128'
'118'
'116'
['3', '182', '128', '118', '116']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid ="139" ssid = "30">In Figure 2 we show that this one factor improves performance by nearly 2%.</S><S sid ="164" ssid = "55">When we do so using our maximum-entropy-inspired conditioning  we get another 0.45% improvement in average precision/recall  as indicated in Figure 2 on the line labeled &quot;MaxEnt-Inspired'.</S><S sid ="17" ssid = "6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S><S sid ="94" ssid = "5">We use the gathered statistics for all observed words  even those with very low counts  though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'139'", "'164'", "'17'", "'94'"]
'4'
'139'
'164'
'17'
'94'
['4', '139', '164', '17', '94']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'2'", "'6'", "'7'"]
'5'
'1'
'2'
'6'
'7'
['5', '1', '2', '6', '7']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="175" ssid = "2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'1'", "'125'", "'175'"]
'2'
'6'
'1'
'125'
'175'
['2', '6', '1', '125', '175']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid ="171" ssid = "62">As already noted  our best model uses a Markov-grammar approach.</S><S sid ="90" ssid = "1">We created a parser based upon the maximumentropy-inspired model of the last section  smoothed using standard deleted interpolation.</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'171'", "'90'", "'1'", "'5'"]
'7'
'171'
'90'
'1'
'5'
['7', '171', '90', '1', '5']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">That is  for all sentences s and all parses 7r  the parser assigns a probability p(s   7r) = p(r)  the equality holding when we restrict consideration to 7r whose yield * This research was supported in part by NSF grant LIS SBR 9720368.</S><S sid ="12" ssid = "1">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).</S><S sid ="10" ssid = "6">That is  the parser implements the function arg maxrp(7r s) = arg maxirp(7r  s) = arg maxrp(w).</S><S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S><S sid ="147" ssid = "38">For example  when we first guess the lexical head we can move from computing p(r I 1 1p  h) to p(r I /  t  /p  h).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'12'", "'10'", "'41'", "'147'"]
'8'
'12'
'10'
'41'
'147'
['8', '12', '10', '41', '147']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.</S><S sid ="185" ssid = "12">We expect that as we experiment with other  more semantic conditioning information  the importance of this aspect of the model will increase.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="171" ssid = "62">As already noted  our best model uses a Markov-grammar approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'185'", "'5'", "'1'", "'171'"]
'9'
'185'
'5'
'1'
'171'
['9', '185', '5', '1', '171']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">That is  the parser implements the function arg maxrp(7r s) = arg maxirp(7r  s) = arg maxrp(w).</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid ="83" ssid = "52">In the simple (content-free) form (Equation 6)  it is clear that Z(H) = 1.</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="56" ssid = "25">In max-entropy models one can simply include features for all three events f1 (a  b  c)  f2 (a  b)  and f3(a  c) and combine them in the model according to Equation 3  or equivalently  Equation 4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'13'", "'83'", "'54'", "'56'"]
'10'
'13'
'83'
'54'
'56'
['10', '13', '83', '54', '56']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">What fundamentally distinguishes probabilistic generative parsers is how they compute p(r)  and it is to that topic we turn next.</S><S sid ="53" ssid = "22">That is  suppose we want to compute a conditional probability p(a b  c)  but we are not sure that we have enough examples of the conditioning event b  c in the training corpus to ensure that the empirically obtained probability P (a I b  c) is accurate.</S><S sid ="82" ssid = "51">The major advantage of using Equation 7 is that one can generally get away without computing the partition function Z(H).</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="188" ssid = "15">Ultimately it is this flexibility that let us try the various conditioning events  to move on to a Markov grammar approach  and to try several Markov grammars of different orders  without significant programming.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'53'", "'82'", "'54'", "'188'"]
'11'
'53'
'82'
'54'
'188'
['11', '53', '82', '54', '188']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "1">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).</S><S sid ="18" ssid = "7">The method that gives the best results  however  uses a Markov grammar — a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6 10 15].</S><S sid ="134" ssid = "25">As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.</S><S sid ="147" ssid = "38">For example  when we first guess the lexical head we can move from computing p(r I 1 1p  h) to p(r I /  t  /p  h).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'18'", "'134'", "'147'", "'33'"]
'12'
'18'
'134'
'147'
'33'
['12', '18', '134', '147', '33']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid ="16" ssid = "5">In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.</S><S sid ="45" ssid = "14">The function Z(H)  called the partition function  is a normalizing constant (for fixed H)  so the probabilities over all a sum to one.</S><S sid ="83" ssid = "52">In the simple (content-free) form (Equation 6)  it is clear that Z(H) = 1.</S><S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'16'", "'45'", "'83'", "'41'"]
'13'
'16'
'45'
'83'
'41'
['13', '16', '45', '83', '41']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "3">Much of the interesting work is determining what goes into H (c).</S><S sid ="73" ssid = "42">We make one more point on the connection of Equation 7 to a maximum entropy formulation.</S><S sid ="60" ssid = "29">But let us look at how it works for a particular case in our parsing scheme.</S><S sid ="159" ssid = "50">Something very much like this is done in [15].</S><S sid ="151" ssid = "42">We have already noted the importance of conditioning on the parent label /p.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'73'", "'60'", "'159'", "'151'"]
'14'
'73'
'60'
'159'
'151'
['14', '73', '60', '159', '151']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "4">Whenever it is clear to which constituent we are referring we omit the (c) in  e.g.  h(c).</S><S sid ="67" ssid = "36">For example  it does not seem to make much sense to condition on  say  hp without first conditioning on ti .</S><S sid ="151" ssid = "42">We have already noted the importance of conditioning on the parent label /p.</S><S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="140" ssid = "31">It may not be obvious why this should make so great a difference  since most words are effectively unambiguous.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'67'", "'151'", "'157'", "'140'"]
'15'
'67'
'151'
'157'
'140'
['15', '67', '151', '157', '140']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "5">In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.</S><S sid ="65" ssid = "34">We can now rewrite this in the form of Equation 5 as follows: Here we have sequentially conditioned on steadily increasing portions of c's history.</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid ="58" ssid = "27">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid ="46" ssid = "15">Now for our purposes it is useful to rewrite this as a sequence of multiplicative functions gi(a  H) for 0 < i < j: Here go(a  H) = 11Z (H) and gi(a  H) = eAi(a H) fi(° 11).</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'65'", "'13'", "'58'", "'46'"]
'16'
'65'
'13'
'58'
'46'
['16', '65', '13', '58', '46']
parsed_discourse_facet ['method_citation']



A00-2018
P05-1065
0
method_citation
['method_citation']
parsing: input/ref/Task1/W11-2123_swastika.csv
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



['52']
52
['52']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="177" ssid="49">However, TRIE partitions storage by n-gram length, so walking the trie reads N disjoint pages.</S>
original cit marker offset is 0
new cit marker offset is 0



['177']
177
['177']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="278" ssid="5">We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns.</S>
original cit marker offset is 0
new cit marker offset is 0



['278']
278
['278']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/W11-2123.csv
<S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid ="7" ssid = "2">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'274'", "'7'", "'259'", "'45'"]
'1'
'274'
'7'
'259'
'45'
['1', '274', '7', '259', '45']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">The structure uses linear probing hash tables and is designed for speed.</S><S sid ="194" ssid = "13">Further  the probing hash table does only one random lookup per query  explaining why it is faster on large data.</S><S sid ="23" ssid = "1">We implement two data structures: PROBING  designed for speed  and TRIE  optimized for memory.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S><S sid ="120" ssid = "24">The compressed variant uses block compression and is rather slow as a result.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'194'", "'23'", "'45'", "'120'"]
'2'
'194'
'23'
'45'
'120'
['2', '194', '23', '45', '120']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S><S sid ="128" ssid = "32">These models generally outperform our memory consumption but are much slower  even when cached.</S><S sid ="161" ssid = "33">In both cases  SRILM walks its trie an additional time to minimize context as mentioned in Section 4.1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'275'", "'276'", "'128'", "'161'"]
'3'
'275'
'276'
'128'
'161'
['3', '275', '276', '128', '161']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Our code is thread-safe  and integrated into the Moses  cdec  and Joshua translation systems.</S><S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="169" ssid = "41">In our case multi-threading is trivial because our data structures are read-only and uncached.</S><S sid ="282" ssid = "3">Adam Pauls provided a pre-release comparison to BerkeleyLM and an initial Java interface.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'279'", "'169'", "'282'", "'266'"]
'4'
'279'
'169'
'282'
'266'
['4', '279', '169', '282', '266']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid ="26" ssid = "4">We use two common techniques  hash tables and sorted arrays  describing each before the model that uses the technique.</S><S sid ="129" ssid = "1">In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'274'", "'26'", "'129'"]
'5'
'1'
'274'
'26'
'129'
['5', '1', '274', '26', '129']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "1">Language models are widely applied in natural language processing  and applications such as machine translation make very frequent queries.</S><S sid ="130" ssid = "2">Applications such as machine translation use language model probability as a feature to assist in choosing between hypotheses.</S><S sid ="184" ssid = "3">Sparse lookup is a key subproblem of language model queries.</S><S sid ="7" ssid = "2">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'130'", "'184'", "'7'", "'259'"]
'6'
'130'
'184'
'7'
'259'
['6', '130', '184', '7', '259']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "2">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'1'", "'274'", "'259'", "'45'"]
'7'
'1'
'274'
'259'
'45'
['7', '1', '274', '259', '45']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "3">Queries take the form p(wn|wn−1 1 ) where wn1 is an n-gram.</S><S sid ="84" ssid = "62">In a model we built with default settings  1.2% of n + 1-grams were missing their ngram suffix.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="72" ssid = "50">We maintain a separate array for each length n containing all n-gram entries sorted in suffix order.</S><S sid ="46" ssid = "24">Unigram lookup is dense so we use an array of probability and backoff values.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'84'", "'88'", "'72'", "'46'"]
'8'
'84'
'88'
'72'
'46'
['8', '84', '88', '72', '46']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "4">Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf   returning where the probability p(wn|wn−1 f ) and backoff penalties b(wn−1 i ) are given by an already-estimated model.</S><S sid ="127" ssid = "31">Minimal perfect hashing is used to find the index at which a quantized probability and possibly backoff are stored.</S><S sid ="136" ssid = "8">We offer a state function s(wn1) = wn� where substring wn� is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.</S><S sid ="78" ssid = "56">Unigram records store probability  backoff  and an index in the bigram table.</S><S sid ="154" ssid = "26">To optimize left-to-right queries  we extend state to store backoff information: where m is the minimal context from Section 4.1 and b is the backoff penalty.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'127'", "'136'", "'78'", "'154'"]
'9'
'127'
'136'
'78'
'154'
['9', '127', '136', '78', '154']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "5">The problem is to store these two values for a large and sparse set of n-grams in a way that makes queries efficient.</S><S sid ="25" ssid = "3">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S><S sid ="24" ssid = "2">The set of n-grams appearing in a model is sparse  and we want to efficiently find their associated probabilities and backoff penalties.</S><S sid ="93" ssid = "71">The cost of storing these averages  in bits  is Because there are comparatively few unigrams  we elected to store them byte-aligned and unquantized  making every query faster.</S><S sid ="85" ssid = "63">This causes a problem for reverse trie implementations  including SRILM itself  because it leaves n+1-grams without an n-gram node pointing to them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'25'", "'24'", "'93'", "'85'"]
'10'
'25'
'24'
'93'
'85'
['10', '25', '24', '93', '85']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "6">Many packages perform language model queries.</S><S sid ="68" ssid = "46">The trie data structure is commonly used for language modeling.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'68'", "'266'", "'259'", "'45'"]
'11'
'68'
'266'
'259'
'45'
['11', '68', '266', '259', '45']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke  2002) is a popular toolkit based on tries used in several decoders.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="97" ssid = "1">SRILM (Stolcke  2002) is widely used within academia.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'13'", "'14'", "'103'", "'97'"]
'12'
'13'
'14'
'103'
'97'
['12', '13', '14', '103', '97']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'103'", "'15'", "'14'", "'262'"]
'13'
'103'
'15'
'14'
'262'
['13', '103', '15', '14', '262']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="97" ssid = "1">SRILM (Stolcke  2002) is widely used within academia.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'15'", "'103'", "'97'", "'266'"]
'14'
'15'
'103'
'97'
'266'
['14', '15', '103', '97', '266']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'14'", "'103'", "'13'", "'262'"]
'15'
'14'
'103'
'13'
'262'
['15', '14', '103', '13', '262']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "11">BerkeleyLM revision 152 (Pauls and Klein  2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.</S><S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="51" ssid = "29">This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'15'", "'51'", "'262'", "'13'"]
'16'
'15'
'51'
'262'
'13'
['16', '15', '51', '262', '13']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "12">Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.</S><S sid ="18" ssid = "13">TPT Germann et al. (2009) describe tries with better locality properties  but did not release code.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="215" ssid = "34">Another option is the closedsource data structures from Sheffield (Guthrie and Hepple  2010).</S><S sid ="114" ssid = "18">Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'18'", "'262'", "'215'", "'114'"]
'17'
'18'
'262'
'215'
'114'
['17', '18', '262', '215', '114']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "13">TPT Germann et al. (2009) describe tries with better locality properties  but did not release code.</S><S sid ="17" ssid = "12">Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.</S><S sid ="232" ssid = "51">Memory usage is likely much lower than ours. fThe original paper (Germann et al.  2009) provided only 2s of query timing and compared with SRI when it exceeded available RAM.</S><S sid ="114" ssid = "18">Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'17'", "'232'", "'114'", "'262'"]
'18'
'17'
'232'
'114'
'262'
['18', '17', '232', '114', '262']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "14">These packages are further described in Section 3.</S><S sid ="129" ssid = "1">In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="5" ssid = "5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S sid ="150" ssid = "22">Section 4.1 explained that state s is stored by applications with partial hypotheses to determine when they can be recombined.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'129'", "'266'", "'5'", "'150'"]
'19'
'129'
'266'
'5'
'150'
['19', '129', '266', '5', '150']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "15">We substantially outperform all of them on query speed and offer lower memory consumption than lossless alternatives.</S><S sid ="128" ssid = "32">These models generally outperform our memory consumption but are much slower  even when cached.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S><S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'128'", "'276'", "'1'", "'275'"]
'20'
'128'
'276'
'1'
'275'
['20', '128', '276', '1', '275']
parsed_discourse_facet ['aim_citation']



W11-2123
W12-3154
0
aim_citation
['method_citation']



W11-2123
W11-2138
0
aim_citation
['aim_citation']



W11-2123
W11-2139
0
aim_citation
['method_citation']
parsing: input/ref/Task1/P08-1043_sweta.csv
<S sid="156" ssid="34">To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["4'"]
4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="2">Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.</S>
original cit marker offset is 0
new cit marker offset is 0



["70'"]
70'
['70']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="3">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="9">Tsarfaty (2006) used a morphological analyzer (Segal, 2000), a PoS tagger (Bar-Haim et al., 2005), and a general purpose parser (Schmid, 2000) in an integrated framework in which morphological and syntactic components interact to share information, leading to improved performance on the joint task.</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="107" ssid="39">Firstly, Hebrew unknown tokens are doubly unknown: each unknown token may correspond to several segmentation possibilities, and each segment in such sequences may be able to admit multiple PoS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["107'"]
107'
['107']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="10">The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["14'"]
14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="12">The current work treats both segmental and super-segmental phenomena, yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg, 2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="53" ssid="11">Both (Tsarfaty, 2006; Cohen and Smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has shown a single generative model which handles both tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["53'"]
53'
['53']
parsed_discourse_facet ['method_citation']
 <S sid="48" ssid="6">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["48'"]
48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="141" ssid="19">This analyzer setting is similar to that of (Cohen and Smith, 2007), and models using it are denoted nohsp, Parser and Grammar We used BitPar (Schmid, 2004), an efficient general purpose parser,10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["141'"]
141'
['141']
parsed_discourse_facet ['method_citation']
<S sid="188" ssid="2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



["188'"]
188'
['188']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["94'"]
94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="12">Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training data similar to (Cohen and Smith, 2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
    <S sid="156" ssid="34">To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth, &#8216;tokens&#8217;) that constitute the unanalyzed surface forms (utterances).</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S sid ="1" ssid = "1">Morphological processes in Semitic languages deliver space-delimited words which introduce multiple  distinct  syntactic units into the structure of the input sentence.</S><S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="39" ssid = "18">Such ambiguities cause discrepancies between token boundaries (indexed as white spaces) and constituent boundaries (imposed by syntactic categories) with respect to a surface form.</S><S sid ="34" ssid = "13">Lexical and Morphological Ambiguity The rich morphological processes for deriving Hebrew stems give rise to a high degree of ambiguity for Hebrew space-delimited tokens.</S><S sid ="24" ssid = "3">Co-occurrences among the particles themselves are subject to further syntactic and lexical constraints relative to the stem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'14'", "'39'", "'34'", "'24'"]
'1'
'14'
'39'
'34'
'24'
['1', '14', '39', '34', '24']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.</S><S sid ="16" ssid = "12">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S><S sid ="94" ssid = "26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S><S sid ="20" ssid = "16">We claim that no particular morphological segmentation is a-priory more likely for surface forms before exploring the compositional nature of syntactic structures  including manifestations of various long-distance dependencies.</S><S sid ="191" ssid = "5">In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'16'", "'94'", "'20'", "'191'"]
'2'
'16'
'94'
'20'
'191'
['2', '16', '94', '20', '191']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S><S sid ="19" ssid = "15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S sid ="187" ssid = "1">Employing a PCFG-based generative framework to make both syntactic and morphological disambiguation decisions is not only theoretically clean and linguistically justified and but also probabilistically apropriate and empirically sound.</S><S sid ="189" ssid = "3">Better grammars are shown here to improve performance on both morphological and syntactic tasks  providing support for the advantage of a joint framework over pipelined or factorized ones.</S><S sid ="21" ssid = "17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'19'", "'187'", "'189'", "'21'"]
'3'
'19'
'187'
'189'
'21'
['3', '19', '187', '189', '21']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Using a treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined  integrated or factorized systems for Hebrew morphological and syntactic processing  yielding an error reduction of 12% over the best published results so far.</S><S sid ="21" ssid = "17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S><S sid ="185" ssid = "23">In contrast  our morphological probabilities are based on a unigram  lexeme-based model  and all other (local and non-local) contextual considerations are delegated to the PCFG.</S><S sid ="188" ssid = "2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S><S sid ="191" ssid = "5">In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'21'", "'185'", "'188'", "'191'"]
'4'
'21'
'185'
'188'
'191'
['4', '21', '185', '188', '191']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  ‘tokens’) that constitute the unanalyzed surface forms (utterances).</S><S sid ="12" ssid = "8">This leads to word- and constituent-boundaries discrepancy  which breaks the assumptions underlying current state-of-the-art statistical parsers.</S><S sid ="94" ssid = "26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S><S sid ="21" ssid = "17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S><S sid ="16" ssid = "12">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'12'", "'94'", "'21'", "'16'"]
'5'
'12'
'94'
'21'
'16'
['5', '12', '94', '21', '16']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">In Semitic languages the situation is very different.</S><S sid ="78" ssid = "10">A similar structure is used in speech recognition.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="99" ssid = "31">This is done using a simple PCFG which is lexemebased.</S><S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'78'", "'92'", "'99'", "'180'"]
'6'
'78'
'92'
'99'
'180'
['6', '78', '92', '99', '180']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).</S><S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="17" ssid = "13">Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task.</S><S sid ="8" ssid = "4">The Hebrew token ‘bcl’1  for example  stands for the complete prepositional phrase 'We adopt here the transliteration of (Sima’an et al.  2001).</S><S sid ="29" ssid = "8">A less canonical representation of segmental morphology is triggered by a morpho-phonological process of omitting the definite article h when occurring after the particles b or l. This process triggers ambiguity as for the definiteness status of Nouns following these particles.We refer to such cases in which the concatenation of elements does not strictly correspond to the original surface form as super-segmental morphology.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'14'", "'17'", "'8'", "'29'"]
'7'
'14'
'17'
'8'
'29'
['7', '14', '17', '8', '29']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">The Hebrew token ‘bcl’1  for example  stands for the complete prepositional phrase 'We adopt here the transliteration of (Sima’an et al.  2001).</S><S sid ="35" ssid = "14">The form fmnh  for example  can be understood as the verb “lubricated”  the possessed noun “her oil”  the adjective “fat” or the verb “got fat”.</S><S sid ="62" ssid = "9">When a token fmnh is to be interpreted as the lexeme sequence f/REL mnh/VB  the analysis introduces two distinct entities  the relativizer f (“that”) and the verb mnh (“counted”)  and not as the complex entity “that counted”.</S><S sid ="7" ssid = "3">In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).</S><S sid ="15" ssid = "11">The aforementioned surface form bcl  for example  may also stand for the lexical item “onion”  a Noun.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'35'", "'62'", "'7'", "'15'"]
'8'
'35'
'62'
'7'
'15'
['8', '35', '62', '7', '15']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">“in the shadow”.</S><S sid ="184" ssid = "22">Oracle results).</S><S sid ="109" ssid = "41">The latter arcs correspond to OOV words in English.</S><S sid ="113" ssid = "45">The remaining arcs are marked OOV.</S><S sid ="78" ssid = "10">A similar structure is used in speech recognition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'184'", "'109'", "'113'", "'78'"]
'9'
'184'
'109'
'113'
'78'
['9', '184', '109', '113', '78']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">This token may further embed into a larger utterance  e.g.  ‘bcl hneim’ (literally “in-the-shadow the-pleasant”  meaning roughly “in the pleasant shadow”) in which the dominated Noun is modified by a proceeding space-delimited adjective.</S><S sid ="15" ssid = "11">The aforementioned surface form bcl  for example  may also stand for the lexical item “onion”  a Noun.</S><S sid ="76" ssid = "8">Furthermore  some of the arcs represent lexemes not present in the input tokens (e.g. h/DT  fl/POS)  however these are parts of valid analyses of the token (cf. super-segmental morphology section 2).</S><S sid ="58" ssid = "5">Such tag sequences are often treated as “complex tags” (e.g.</S><S sid ="108" ssid = "40">Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p —* (s  p)) > 0  while other segments have never been observed as a lexical event before.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'15'", "'76'", "'58'", "'108'"]
'10'
'15'
'76'
'58'
'108'
['10', '15', '76', '58', '108']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">It should be clear from the onset that the particle b (“in”) in ‘bcl’ may then attach higher than the bare noun cl (“shadow”).</S><S sid ="37" ssid = "16">The same form fmnh can be segmented as f-mnh  f (“that”) functioning as a reletivizer with the form mnh.</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="38" ssid = "17">The form mnh itself can be read as at least three different verbs (“counted”  “appointed”  “was appointed”)  a noun (“a portion”)  and a possessed noun (“her kind”).</S><S sid ="26" ssid = "5">The relativizer f(“that”) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'37'", "'57'", "'38'", "'26'"]
'11'
'37'
'57'
'38'
'26'
['11', '37', '57', '38', '26']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">This leads to word- and constituent-boundaries discrepancy  which breaks the assumptions underlying current state-of-the-art statistical parsers.</S><S sid ="5" ssid = "1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  ‘tokens’) that constitute the unanalyzed surface forms (utterances).</S><S sid ="13" ssid = "9">One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).</S><S sid ="2" ssid = "2">These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.</S><S sid ="19" ssid = "15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'5'", "'13'", "'2'", "'19'"]
'12'
'5'
'13'
'2'
'19'
['12', '5', '13', '2', '19']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).</S><S sid ="33" ssid = "12">The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).</S><S sid ="140" ssid = "18">For these models we limit the options provided for OOV words by not considering the entire token as a valid segmentation in case at least some prefix segmentation exists.</S><S sid ="139" ssid = "17">To control for the effect of the HSPELL-based pruning  we also experimented with a morphological analyzer that does not perform this pruning.</S><S sid ="3" ssid = "3">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'33'", "'140'", "'139'", "'3'"]
'13'
'33'
'140'
'139'
'3'
['13', '33', '140', '139', '3']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="7" ssid = "3">In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).</S><S sid ="140" ssid = "18">For these models we limit the options provided for OOV words by not considering the entire token as a valid segmentation in case at least some prefix segmentation exists.</S><S sid ="17" ssid = "13">Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task.</S><S sid ="121" ssid = "53">In such cases we use the non-pruned lattice including all (possibly ungrammatical) segmentation  and let the statistics (including OOV) decide.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'7'", "'140'", "'17'", "'121'"]
'14'
'7'
'140'
'17'
'121'
['14', '7', '140', '17', '121']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">The aforementioned surface form bcl  for example  may also stand for the lexical item “onion”  a Noun.</S><S sid ="32" ssid = "11">The additional morphological material in such cases appears after the stem and realizes the extended meaning.</S><S sid ="28" ssid = "7">The same argument holds for resolving PP attachment of a prefixed preposition or marking conjunction of elements of any kind.</S><S sid ="10" ssid = "6">This token may further embed into a larger utterance  e.g.  ‘bcl hneim’ (literally “in-the-shadow the-pleasant”  meaning roughly “in the pleasant shadow”) in which the dominated Noun is modified by a proceeding space-delimited adjective.</S><S sid ="30" ssid = "9">An additional case of super-segmental morphology is the case of Pronominal Clitics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'32'", "'28'", "'10'", "'30'"]
'15'
'32'
'28'
'10'
'30'
['15', '32', '28', '10', '30']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S><S sid ="94" ssid = "26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S><S sid ="188" ssid = "2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S><S sid ="13" ssid = "9">One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).</S><S sid ="2" ssid = "2">These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'94'", "'188'", "'13'", "'2'"]
'16'
'94'
'188'
'13'
'2'
['16', '94', '188', '13', '2']
parsed_discourse_facet ['method_citation']



P08-1043
P11-2124
0
method_citation
['method_citation']
parsing: input/ref/Task1/P04-1036_sweta.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="82" ssid="11">We also calculate the WSD accuracy that would be obtained on SemCor, when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["82'"]
82'
['82']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="20">We briefly summarise the two measures here; for a more detailed summary see (Patwardhan et al., 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="172" ssid="20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="1">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="165" ssid="13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus, whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S>
original cit marker offset is 0
new cit marker offset is 0



["165'"]
165'
['165']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["171'"]
171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["89'"]
89'
['89']
parsed_discourse_facet ['method_citation']
<S sid="172" ssid="20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="12">Additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["189'"]
189'
['189']
parsed_discourse_facet ['method_citation']
<S sid="87" ssid="16">Again, the automatic ranking outperforms this by a large margin.</S>
original cit marker offset is 0
new cit marker offset is 0



["87'"]
87'
['87']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["89'"]
89'
['89']
parsed_discourse_facet ['method_citation']
 <S sid="137" ssid="14">Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["137'"]
137'
['137']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="19">We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline, but because of space limitations give results for the two which perform the best.</S>
original cit marker offset is 0
new cit marker offset is 0



["63'"]
63'
['63']
parsed_discourse_facet ['method_citation']
<S sid="159" ssid="7">Magnini and Cavagli`a (2000) have identified WordNet word senses with particular domains, and this has proven useful for high precision WSD (Magnini et al., 2001); indeed in section 5 we used these domain labels for evaluation.</S>
original cit marker offset is 0
new cit marker offset is 0



["159'"]
159'
['159']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="48" ssid = "4">To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.</S><S sid ="105" ssid = "3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents  rather than a lexical sample task  where the target words are manually determined and the results will depend on the skew of the words in the sample.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'153'", "'154'", "'48'", "'105'"]
'1'
'153'
'154'
'48'
'105'
['1', '153', '154', '48', '105']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">The problem with using the predominant  or first sense heuristic  aside from the fact that it does not take surrounding context into account  is that it assumes some quantity of handtagged data.</S><S sid ="17" ssid = "10">There are words where the first sense in WordNet is counter-intuitive  because of the size of the corpus  and because where the frequency data does not indicate a first sense  the ordering is arbitrary.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'17'", "'91'", "'154'", "'123'"]
'2'
'17'
'91'
'154'
'123'
['2', '17', '91', '154', '123']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.</S><S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="37" ssid = "30">From inspection  one can see that the ordered neighbours of such a thesaurus relate to the different senses of the target word.</S><S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'15'", "'37'", "'1'", "'170'"]
'3'
'15'
'37'
'1'
'170'
['3', '15', '37', '1', '170']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S sid ="179" ssid = "2">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S><S sid ="171" ssid = "19">In contrast  we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'179'", "'178'", "'45'", "'171'"]
'4'
'179'
'178'
'45'
'171'
['4', '179', '178', '45', '171']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="84" ssid = "13">The random baseline for choosing the predominant sense over all these words ( ) is 32%.</S><S sid ="88" ssid = "17">The first sense in SemCor provides an upperbound for this task of 67%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'100'", "'116'", "'84'", "'88'"]
'5'
'100'
'116'
'84'
'88'
['5', '100', '116', '84', '88']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="121" ssid = "19">For these one would need to obtain more sense-tagged text in order to use this heuristic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'126'", "'178'", "'13'", "'121'"]
'6'
'126'
'178'
'13'
'121'
['6', '126', '178', '13', '121']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S><S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S><S sid ="158" ssid = "6">We have evaluated our method using publically available resources  both for balanced and domain specific text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'184'", "'178'", "'126'", "'158'"]
'7'
'184'
'178'
'126'
'158'
['7', '184', '178', '126', '158']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="172" ssid = "20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet  for example those that use it for lexical acquisition or WSD.</S><S sid ="115" ssid = "13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor  which is very encouraging given that our method only uses raw text  with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'154'", "'14'", "'172'", "'115'"]
'8'
'154'
'14'
'172'
'115'
['8', '154', '14', '172', '115']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "2">This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).</S><S sid ="102" ssid = "31">We test this below on the SENSEVAL-2 English all-words data.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'102'", "'116'", "'100'", "'5'"]
'9'
'102'
'116'
'100'
'5'
['9', '102', '116', '100', '5']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="180" ssid = "3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S><S sid ="109" ssid = "7">We generated a thesaurus entry for all polysemous nouns in WordNet as described in section 2.1 above.</S><S sid ="112" ssid = "10">We compare results using the first sense listed in SemCor  and the first sense according to the SENSEVAL-2 English all-words test data itself.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'180'", "'142'", "'109'", "'112'"]
'10'
'180'
'142'
'109'
'112'
['10', '180', '142', '109', '112']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "4">Senses that have not occurred in SemCor are ordered arbitrarily and after those senses of the word that have occurred.</S><S sid ="120" ssid = "18">There were a similar number of words that were not covered by a predominant sense in SemCor.</S><S sid ="42" ssid = "35">The neighbours for a word in a thesaurus are words themselves  rather than senses.</S><S sid ="189" ssid = "12">Additionally  we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity  and what can be done to combat this problem using relation specific thesauruses.</S><S sid ="117" ssid = "15">The items that were not covered by our method were those with insufficient grammatical relations for the tuples employed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'120'", "'42'", "'189'", "'117'"]
'11'
'120'
'42'
'189'
'117'
['11', '120', '42', '189', '117']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "5">The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor  from those that do not (without HTD).</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'123'", "'153'", "'165'", "'178'"]
'12'
'123'
'153'
'165'
'178'
['12', '123', '153', '165', '178']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="84" ssid = "13">The random baseline for choosing the predominant sense over all these words ( ) is 32%.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'116'", "'147'", "'84'", "'126'"]
'13'
'116'
'147'
'84'
'126'
['13', '116', '147', '84', '126']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="182" ssid = "5">In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.</S><S sid ="21" ssid = "14">We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off (Wilks and Stevenson  1998; Hoste et al.  2001) and for systems that use it in lexical acquisition (McCarthy  1997; Merlo and Leybold  2001; Korhonen  2002) because of the limited size of hand-tagged resources.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'8'", "'154'", "'182'", "'21'"]
'14'
'8'
'154'
'182'
'21'
['14', '8', '154', '182', '21']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="3" ssid = "3">Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'154'", "'3'", "'8'", "'170'"]
'15'
'154'
'3'
'8'
'170'
['15', '154', '3', '8', '170']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">SemCor comprises a relatively small sample of 250 000 words.</S><S sid ="131" ssid = "8">The SPORTS corpus consists of 35317 documents (about 9.1 million words).</S><S sid ="140" ssid = "17">The resulting set consisted of 38 words.</S><S sid ="132" ssid = "9">The FINANCE corpus consists of 117734 documents (about 32.5 million words).</S><S sid ="76" ssid = "5">The jcn measure uses corpus data for the calculation of IC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'131'", "'140'", "'132'", "'76'"]
'16'
'131'
'140'
'132'
'76'
['16', '131', '140', '132', '76']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "10">There are words where the first sense in WordNet is counter-intuitive  because of the size of the corpus  and because where the frequency data does not indicate a first sense  the ordering is arbitrary.</S><S sid ="182" ssid = "5">In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.</S><S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="99" ssid = "28">Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'182'", "'13'", "'99'", "'170'"]
'17'
'182'
'13'
'99'
'170'
['17', '182', '13', '99', '170']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P87-1015_sweta.csv
<S sid="205" ssid="11">Independence of paths at this level reflects context freeness of rewriting and suggests why they can be recognized efficiently.</S>
original cit marker offset is 0
new cit marker offset is 0



["205'"]
205'
['205']
parsed_discourse_facet ['method_citation']
<S sid="229" ssid="35">In addition, the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars, and the hierarchy defined by Weir).</S>
original cit marker offset is 0
new cit marker offset is 0



["229'"]
229'
['229']
parsed_discourse_facet ['method_citation']
<S sid="146" ssid="31">Since every CFL is known to be semilinear (Parikh, 1966), in order to show semilinearity of some language, we need only show the existence of a letter equivalent CFL Our definition of LCFRS's insists that the composition operations are linear and nonerasing.</S>
original cit marker offset is 0
new cit marker offset is 0



["146'"]
146'
['146']
parsed_discourse_facet ['method_citation']
<S sid="201" ssid="7">It is interesting to note, however, that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG, FUG and IG's.</S>
original cit marker offset is 0
new cit marker offset is 0



["201'"]
201'
['201']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
 <S sid="222" ssid="28">However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



["222'"]
222'
['222']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="7">Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.</S>
    <S sid="23" ssid="8">He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["22'", "'23'"]
22'
'23'
['22', '23']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="221" ssid="27">Members of LCFRS whose operations have this property can be translated into the ILFP notation (Rounds, 1985).</S>
original cit marker offset is 0
new cit marker offset is 0



["221'"]
221'
['221']
parsed_discourse_facet ['method_citation']
<S sid="54" ssid="39">An IG can be viewed as a CFG in which each nonterminal is associated with a stack.</S>
original cit marker offset is 0
new cit marker offset is 0



["54'"]
54'
['54']
parsed_discourse_facet ['method_citation']
<S sid="128" ssid="13">As in the case of the derivation trees of CFG's, nodes are labeled by a member of some finite set of symbols (perhaps only implicit in the grammar as in TAG's) used to denote derived structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["128'"]
128'
['128']
parsed_discourse_facet ['method_citation']
<S sid="217" ssid="23">In considering the recognition of these languages, we were forced to be more specific regarding the relationship between the structures derived by these formalisms and the substrings they span.</S>
original cit marker offset is 0
new cit marker offset is 0



[";217'"]
;217'
[';217']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="1">From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="1">From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="214" ssid="20">LCFRS's share several properties possessed by the class of mildly context-sensitive formalisms discussed by Joshi (1983/85).</S>
original cit marker offset is 0
new cit marker offset is 0



["214'"]
214'
['214']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="20">TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish, for cross serial dependencies in Dutch subordinate clauses, and for the nestings of paired English complementizers.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P87-1015.csv
<S sid ="1" ssid = "1">We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.</S><S sid ="9" ssid = "7">We examine both the complexity of the paths of trees in the tree sets  and the kinds of dependencies that the formalisms can impose between paths.</S><S sid ="232" ssid = "38">In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="110" ssid = "16">The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'232'", "'195'", "'110'"]
'1'
'9'
'232'
'195'
'110'
['1', '9', '232', '195', '110']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">In considering the relationship between formalisms  we show that it is useful to abstract away from the details of the formalism  and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation  we describe a class of formalisms which we call Linear Context- Free Rewriting Systems  and show they are recognizable in polynomial time and generate only semilinear languages.</S><S sid ="203" ssid = "9">In order to observe the similarity between these constrained systems  it is crucial to abstract away from the details of the structures and operations used by the system.</S><S sid ="232" ssid = "38">In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.</S><S sid ="11" ssid = "9">By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'203'", "'232'", "'11'", "'116'"]
'2'
'203'
'232'
'11'
'116'
['2', '203', '232', '11', '116']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S><S sid ="48" ssid = "33">There has been recent interest in the application of Indexed Grammars (IG's) to natural languages.</S><S sid ="23" ssid = "8">He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.</S><S sid ="56" ssid = "41">Gazdar (1985) argues that sharing of stacks can be used to give analyses for coordination.</S><S sid ="65" ssid = "50">While the generative power of CG's is greater that of CFG's  it appears to be highly constrained.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'48'", "'23'", "'56'", "'65'"]
'3'
'48'
'23'
'56'
'65'
['3', '48', '23', '56', '65']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "2">Little attention  however  has been paid to the structural descriptions that these formalisms can assign to strings  i.e. their strong generative capacity.</S><S sid ="3" ssid = "1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S><S sid ="66" ssid = "51">Hence  their relationship to formalisms such as HG's and TAG's is of interest.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="48" ssid = "33">There has been recent interest in the application of Indexed Grammars (IG's) to natural languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'3'", "'66'", "'195'", "'48'"]
'4'
'3'
'66'
'195'
'48'
['4', '3', '66', '195', '48']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "3">This aspect of the formalism is both linguistically and computationally important.</S><S sid ="198" ssid = "4">This property reflects an important aspect of the underlying linguistic theory associated with the formalism.</S><S sid ="10" ssid = "8">These two properties of the tree sets are not only linguistically relevant  but also have computational importance.</S><S sid ="123" ssid = "8">A second restriction on the formalisms is that choices during the derivation are independent of the context in the derivation.</S><S sid ="3" ssid = "1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'198'", "'10'", "'123'", "'3'"]
'5'
'198'
'10'
'123'
'3'
['5', '198', '10', '123', '3']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">For example  Gazdar (1985) discusses the applicability of Indexed Grammars (IG's) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar (LFG) and Government and Bindings grammars (GB).</S><S sid ="24" ssid = "9">Head Grammars (HG's)  introduced by Pollard (1984)  is a formalism that manipulates headed strings: i.e.  strings  one of whose symbols is distinguished as the head.</S><S sid ="35" ssid = "20">TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish  for cross serial dependencies in Dutch subordinate clauses  and for the nestings of paired English complementizers.</S><S sid ="64" ssid = "49">Steedman (1986) considers Categorial Grammars in which both the operations of function application and composition may be used  and in which function can specify whether they take their arguments from their right or left.</S><S sid ="22" ssid = "7">Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'24'", "'35'", "'64'", "'22'"]
'6'
'24'
'35'
'64'
'22'
['6', '24', '35', '64', '22']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "5">The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG's and IG's.</S><S sid ="50" ssid = "35">The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.</S><S sid ="16" ssid = "1">From Thatcher's (1973) work  it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S><S sid ="98" ssid = "4">Thatcher (1973) describes a tee pumping lemma for recognizable sets related to the string pumping lemma for regular sets.</S><S sid ="24" ssid = "9">Head Grammars (HG's)  introduced by Pollard (1984)  is a formalism that manipulates headed strings: i.e.  strings  one of whose symbols is distinguished as the head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'50'", "'16'", "'98'", "'24'"]
'7'
'50'
'16'
'98'
'24'
['7', '50', '16', '98', '24']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "6">We consider properties of the tree sets generated by CFG's  Tree Adjoining Grammars (TAG's)  Head Grammars (HG's)  Categorial Grammars (CG's)  and IG's.</S><S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="27" ssid = "12">Thus  the tree sets generated by HG's are similar to those of CFG's  with each node annotated by the operation (concatenation or wrapping) used to combine the headed strings derived by the daughters of Tree Adjoining Grammars  a tree rewriting formalism  was introduced by Joshi  Levy and Takahashi (1975) and Joshi (1983/85).</S><S sid ="94" ssid = "79">The semilinearity of Tree Adjoining Languages (TAL's)  MCTAL's  and Head Languages (HL's) can be proved using this property  with suitable restrictions on the composition operations.</S><S sid ="208" ssid = "14">This family represents an attempt to generalize the properties shared by CFG's  HG's  TAG's  and MCTAG's.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'151'", "'27'", "'94'", "'208'"]
'8'
'151'
'27'
'94'
'208'
['8', '151', '27', '94', '208']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "7">We examine both the complexity of the paths of trees in the tree sets  and the kinds of dependencies that the formalisms can impose between paths.</S><S sid ="1" ssid = "1">We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.</S><S sid ="8" ssid = "6">We consider properties of the tree sets generated by CFG's  Tree Adjoining Grammars (TAG's)  Head Grammars (HG's)  Categorial Grammars (CG's)  and IG's.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="110" ssid = "16">The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'1'", "'8'", "'195'", "'110'"]
'9'
'1'
'8'
'195'
'110'
['9', '1', '8', '195', '110']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "8">These two properties of the tree sets are not only linguistically relevant  but also have computational importance.</S><S sid ="5" ssid = "3">This aspect of the formalism is both linguistically and computationally important.</S><S sid ="198" ssid = "4">This property reflects an important aspect of the underlying linguistic theory associated with the formalism.</S><S sid ="68" ssid = "53">This kind of dependency arises from the use of the composition operation to compose two arbitrarily large categories.</S><S sid ="23" ssid = "8">He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'5'", "'198'", "'68'", "'23'"]
'10'
'5'
'198'
'68'
'23'
['10', '5', '198', '68', '23']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "9">By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.</S><S sid ="2" ssid = "2">In considering the relationship between formalisms  we show that it is useful to abstract away from the details of the formalism  and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation  we describe a class of formalisms which we call Linear Context- Free Rewriting Systems  and show they are recognizable in polynomial time and generate only semilinear languages.</S><S sid ="232" ssid = "38">In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.</S><S sid ="14" ssid = "12">This suggests that by generalizing the notion of context-freeness in CFG's  we can define a class of grammatical formalisms that manipulate more complex structures.</S><S sid ="203" ssid = "9">In order to observe the similarity between these constrained systems  it is crucial to abstract away from the details of the structures and operations used by the system.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'2'", "'232'", "'14'", "'203'"]
'11'
'2'
'232'
'14'
'203'
['11', '2', '232', '14', '203']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "10">We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.</S><S sid ="137" ssid = "22">In considering recognition of LCFRS's  we make further assumption concerning the contribution of each structure to the input string  and how the composition operations combine structures in this respect.</S><S sid ="200" ssid = "6">The importance of this property becomes clear in contrasting theories underlying GPSG (Gazdar  Klein  Pulluna  and Sag  1985)  and GB (as described by Berwick  1984) with those underlying LFG and FUG.</S><S sid ="22" ssid = "7">Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.</S><S sid ="60" ssid = "45">Gazdar (1985) considers a restriction of IG's in which no more than one nonterminal on the right-hand-side of a production can inherit the stack from the left-hand-side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'137'", "'200'", "'22'", "'60'"]
'12'
'137'
'200'
'22'
'60'
['12', '137', '200', '22', '60']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "11">It is striking that from this point of view many formalisms can be grouped together as having identically structured derivation tree sets.</S><S sid ="15" ssid = "13">In this paper  we outline how such family of formalisms can be defined  and show that like CFG's  each member possesses a number of desirable linguistic and computational properties: in particular  the constant growth property and polynomial recognizability.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S><S sid ="202" ssid = "8">As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.</S><S sid ="78" ssid = "63">Each member of a set of trees can be adjoined into distinct nodes of trees in a single elementary tree set  i.e  derivations always involve the adjunction of a derived auxiliary tree set into an elementary tree set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'15'", "'116'", "'202'", "'78'"]
'13'
'15'
'116'
'202'
'78'
['13', '15', '116', '202', '78']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "12">This suggests that by generalizing the notion of context-freeness in CFG's  we can define a class of grammatical formalisms that manipulate more complex structures.</S><S sid ="119" ssid = "4">In defining LCFRS's  we hope to generalize the definition of CFG's to formalisms manipulating any structure  e.g. strings  trees  or graphs.</S><S sid ="53" ssid = "38">We focus on this difference between the tree sets of CFG's and IG's  and formalize the notion of dependence between paths in a tree set in Section 3.</S><S sid ="142" ssid = "27">Although this property is not structural  it depends on the structural property that sentences can be built from a finite set of clauses of bounded structure as noted by Joshi (1983/85).</S><S sid ="159" ssid = "44">Similarly  for all the LCFRS's  discussed in Section 2  we can define the relationship between a structure and the sequence of substrings it spans  and the effect of the composition operations on sequences of substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'119'", "'53'", "'142'", "'159'"]
'14'
'119'
'53'
'142'
'159'
['14', '119', '53', '142', '159']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "13">In this paper  we outline how such family of formalisms can be defined  and show that like CFG's  each member possesses a number of desirable linguistic and computational properties: in particular  the constant growth property and polynomial recognizability.</S><S sid ="202" ssid = "8">As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.</S><S sid ="117" ssid = "2">Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S><S sid ="13" ssid = "11">It is striking that from this point of view many formalisms can be grouped together as having identically structured derivation tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'202'", "'117'", "'116'", "'13'"]
'15'
'202'
'117'
'116'
'13'
['15', '202', '117', '116', '13']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "1">From Thatcher's (1973) work  it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S><S sid ="50" ssid = "35">The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.</S><S sid ="19" ssid = "4">It can be easily shown from Thatcher's result that the path set of every local set is a regular set.</S><S sid ="229" ssid = "35">In addition  the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars  and the hierarchy defined by Weir).</S><S sid ="166" ssid = "51">With the additional assumptions  inspired by Rounds (1985)  we can show that members of this class can be recognized in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'50'", "'19'", "'229'", "'166'"]
'16'
'50'
'19'
'229'
'166'
['16', '50', '19', '229', '166']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/D10-1044_sweta.csv
<S sid="4" ssid="1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S>
original cit marker offset is 0
new cit marker offset is 0



["4'"]
4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="1">We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="4">For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components (word-alignment model, language model, translation model, etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["7'"]
7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="14">Linear weights are difficult to incorporate into the standard MERT procedure because they are &#8220;hidden&#8221; within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn, 2007), we circumvent this problem by choosing weights to optimize corpus loglikelihood, which is roughly speaking the training criterion used by the LM and TM themselves.</S>
original cit marker offset is 0
new cit marker offset is 0



["50'"]
50'
['50']
parsed_discourse_facet ['method_citation']
<S sid="152" ssid="9">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["152'"]
152'
['152']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
 <S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="97" ssid="1">We carried out translation experiments in two different settings.</S>
original cit marker offset is 0
new cit marker offset is 0



["97'"]
97'
['97']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="12">Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="10">Finally, we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="141" ssid="10">Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L&#168;u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L&#168;u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["141'"]
141'
['141']
parsed_discourse_facet ['method_citation']
 <S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="37" ssid="1">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S>
original cit marker offset is 0
new cit marker offset is 0



["37'"]
37'
['37']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D10-1044.csv
<S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="145" ssid = "2">Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="62" ssid = "26">To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'65'", "'145'", "'64'", "'62'"]
'1'
'65'
'145'
'64'
'62'
['1', '65', '145', '64', '62']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S><S sid ="23" ssid = "20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="153" ssid = "10">Finally  we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.</S><S sid ="68" ssid = "5">First  we learn weights on individual phrase pairs rather than sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'23'", "'20'", "'153'", "'68'"]
'2'
'23'
'20'
'153'
'68'
['2', '23', '20', '153', '68']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">We incorporate instance weighting into a mixture-model framework  and find that it yields consistent improvements over a wide range of baselines.</S><S sid ="150" ssid = "7">In both cases  the instanceweighting approach improved over a wide range of baselines  giving gains of over 2 BLEU points over the best non-adapted baseline  and gains of between 0.6 and 1.8 over an equivalent mixture model (with an identical training procedure but without instance weighting).</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'150'", "'65'", "'38'", "'1'"]
'3'
'150'
'65'
'38'
'1'
['3', '150', '65', '38', '1']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'10'", "'6'", "'24'", "'17'"]
'4'
'10'
'6'
'24'
'17'
['4', '10', '6', '24', '17']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "2">Even when there is training data available in the domain of interest  there is often additional data from other domains that could in principle be used to improve performance.</S><S sid ="138" ssid = "7">However  for multinomial models like our LMs and TMs  there is a one to one correspondence between instances and features  eg the correspondence between a phrase pair (s  t) and its conditional multinomial probability p(s1t).</S><S sid ="9" ssid = "6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="39" ssid = "3">Thus  provided at least this amount of IN data is available—as it is in our setting—adapting these weights is straightforward.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'138'", "'9'", "'6'", "'39'"]
'5'
'138'
'9'
'6'
'39'
['5', '138', '9', '6', '39']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'17'", "'24'", "'18'", "'4'"]
'6'
'17'
'24'
'18'
'4'
['6', '17', '24', '18', '4']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="9" ssid = "6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'24'", "'21'", "'4'", "'9'"]
'7'
'24'
'21'
'4'
'9'
['7', '24', '21', '4', '9']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'10'", "'18'", "'6'", "'20'"]
'8'
'10'
'18'
'6'
'20'
['8', '10', '18', '6', '20']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S><S sid ="144" ssid = "1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S><S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="42" ssid = "6">The natural baseline approach is to concatenate data from IN and OUT.</S><S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'144'", "'1'", "'42'", "'2'"]
'9'
'144'
'1'
'42'
'2'
['9', '144', '1', '42', '2']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="29" ssid = "26">This is a simple and effective alternative to setting weights discriminatively to maximize a metric such as BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'18'", "'6'", "'29'"]
'10'
'4'
'18'
'6'
'29'
['10', '4', '18', '6', '29']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="44" ssid = "8">When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.</S><S sid ="92" ssid = "29">6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'18'", "'6'", "'44'", "'92'"]
'11'
'18'
'6'
'44'
'92'
['11', '18', '6', '44', '92']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "9">For simplicity  we assume that OUT is homogeneous.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="92" ssid = "29">6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.</S><S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'18'", "'11'", "'92'", "'16'"]
'12'
'18'
'11'
'92'
'16'
['12', '18', '11', '92', '16']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "10">The techniques we develop can be extended in a relatively straightforward manner to the more general case when OUT consists of multiple sub-domains.</S><S sid ="139" ssid = "8">As mentioned above  it is not obvious how to apply Daum´e’s approach to multinomials  which do not have a mechanism for combining split features.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="82" ssid = "19">This is not unreasonable given the application to phrase pairs from OUT  but it suggests that an interesting alternative might be to use a plain log-linear weighting function exp(Ei Aifi(s  t))  with outputs in [0  oo].</S><S sid ="32" ssid = "29">This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'139'", "'21'", "'82'", "'32'"]
'13'
'139'
'21'
'82'
'32'
['13', '139', '21', '82', '32']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "11">There is a fairly large body of work on SMT adaptation.</S><S sid ="141" ssid = "10">Moving beyond directly related work  major themes in SMT adaptation include the IR (Hildebrand et al.  2005; L¨u et al.  2007; Zhao et al.  2004) and mixture (Finch and Sumita  2008; Foster and Kuhn  2007; Koehn and Schroeder  2007; L¨u et al.  2007) approaches for LMs and TMs described above  as well as methods for exploiting monolingual in-domain text  typically by translating it automatically and then performing self training (Bertoldi and Federico  2009; Ueffing et al.  2007; Schwenk and Senellart  2009).</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="34" ssid = "31">Section 2 describes our baseline techniques for SMT adaptation  and section 3 describes the instance-weighting approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'141'", "'4'", "'10'", "'34'"]
'14'
'141'
'4'
'10'
'34'
['14', '141', '4', '10', '34']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "12">We introduce several new ideas.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="27" ssid = "24">Finally  we make some improvements to baseline approaches.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="88" ssid = "25">We have not explored this strategy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'67'", "'27'", "'20'", "'88'"]
'15'
'67'
'27'
'20'
'88'
['15', '67', '27', '20', '88']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="42" ssid = "6">The natural baseline approach is to concatenate data from IN and OUT.</S><S sid ="68" ssid = "5">First  we learn weights on individual phrase pairs rather than sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'17'", "'24'", "'42'", "'68'"]
'16'
'17'
'24'
'42'
'68'
['16', '17', '24', '42', '68']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S><S sid ="100" ssid = "4">Figure 1 shows sample sentences from these domains  which are widely divergent.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'6'", "'24'", "'16'", "'100'"]
'17'
'6'
'24'
'16'
'100'
['17', '6', '24', '16', '100']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="44" ssid = "8">When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'11'", "'44'", "'6'", "'10'"]
'18'
'11'
'44'
'6'
'10'
['18', '11', '44', '6', '10']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "16">The idea of distinguishing between general and domain-specific examples is due to Daum´e and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daum´e’s (2007) work that splits each feature into domain-specific and general copies.</S><S sid ="73" ssid = "10">Jiang and Zhai (2007) suggest the following derivation  making use of the true OUT distribution po(s  t): where each fi(s  t) is a feature intended to charac- !0ˆ = argmax pf(s  t) log pθ(s|t) (8) terize the usefulness of (s  t)  weighted by Ai. θ s t pf(s  t)po(s  t) log pθ(s|t) The mixing parameters and feature weights (col- != argmax po (s  t) lectively 0) are optimized simultaneously using dev- θ s t pf(s  t)co(s  t) log pθ(s|t)  set maximum likelihood as before: !�argmax po (s  t) ! θ s t �ˆ = argmax ˜p(s  t) log p(s|t; 0).</S><S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'136'", "'73'", "'140'", "'64'"]
'19'
'136'
'73'
'140'
'64'
['19', '136', '73', '140', '64']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P08-1028_swastika.csv
<S sid="21" ssid="17">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="23">Our results show that the multiplicative models are superior and correlate significantly with behavioral data.</S>
original cit marker offset is 0
new cit marker offset is 0



['27']
27
['27']
parsed_discourse_facet ['result_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['189']
189
['189']
parsed_discourse_facet ['aim_citation']
<S sid="53" ssid="1">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.</S>
original cit marker offset is 0
new cit marker offset is 0



['53']
53
['53']
parsed_discourse_facet ['method_citation']
<S sid="76" ssid="24">The models considered so far assume that components do not &#8216;interfere&#8217; with each other, i.e., that It is also possible to re-introduce the dependence on K into the model of vector composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['76']
76
['76']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['57']
57
['57']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['57']
57
['57']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="60" ssid="8">To derive specific models from this general framework requires the identification of appropriate constraints to narrow the space of functions being considered.</S>
original cit marker offset is 0
new cit marker offset is 0



['60']
60
['60']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="73" ssid="21">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware, since semantically important constituents can participate more actively in the composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['73']
73
['73']
parsed_discourse_facet ['result_citation']
<S sid="99" ssid="12">In order to establish an independent measure of sentence similarity, we assembled a set of experimental materials and elicited similarity ratings from human subjects.</S>
original cit marker offset is 0
new cit marker offset is 0



['99']
99
['99']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['189']
189
['189']
parsed_discourse_facet ['aim_citation']
<S sid="191" ssid="3">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S>
original cit marker offset is 0
new cit marker offset is 0



['191']
191
['191']
parsed_discourse_facet ['result_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P08-1028.csv
<S sid ="1" ssid = "1">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid ="189" ssid = "1">In this paper we presented a general framework for vector-based semantic composition.</S><S sid ="43" ssid = "16">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid ="24" ssid = "20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'189'", "'43'", "'24'", "'34'"]
'1'
'189'
'43'
'24'
'34'
['1', '189', '43', '24', '34']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S><S sid ="190" ssid = "2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="83" ssid = "31">Although we have presented multiplicative and additive models separately  there is nothing inherent in our formulation that disallows their combination.</S><S sid ="26" ssid = "22">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'190'", "'25'", "'83'", "'26'"]
'2'
'190'
'25'
'83'
'26'
['2', '190', '25', '83', '26']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="26" ssid = "22">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S><S sid ="52" ssid = "25">Under this framework  we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="24" ssid = "20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'26'", "'52'", "'25'", "'24'"]
'3'
'26'
'52'
'25'
'24'
['3', '26', '52', '25', '24']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S><S sid ="27" ssid = "23">Our results show that the multiplicative models are superior and correlate significantly with behavioral data.</S><S sid ="176" ssid = "10">The multiplicative and combined models yield means closer to the human ratings.</S><S sid ="26" ssid = "22">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S><S sid ="174" ssid = "8">The simple additive model fails to distinguish between High and Low Similarity items.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'27'", "'176'", "'26'", "'174'"]
'4'
'27'
'176'
'26'
'174'
['4', '27', '176', '26', '174']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.</S><S sid ="9" ssid = "5">In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S><S sid ="19" ssid = "15">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid ="8" ssid = "4">Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch¨utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'9'", "'20'", "'19'", "'8'"]
'5'
'9'
'20'
'19'
'8'
['5', '9', '20', '19', '8']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).</S><S sid ="21" ssid = "17">Central in these models is the notion of compositionality — the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S><S sid ="138" ssid = "51">Model Parameters Irrespectively of their form  all composition models discussed here are based on a semantic space for representing the meanings of individual words.</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S><S sid ="29" ssid = "2">While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'21'", "'138'", "'20'", "'29'"]
'6'
'21'
'138'
'20'
'29'
['6', '21', '138', '20', '29']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">A variety of NLP tasks have made good use of vector-based models.</S><S sid ="43" ssid = "16">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid ="200" ssid = "12">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid ="1" ssid = "1">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'43'", "'200'", "'1'", "'42'"]
'7'
'43'
'200'
'1'
'42'
['7', '43', '200', '1', '42']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch¨utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).</S><S sid ="19" ssid = "15">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid ="5" ssid = "1">Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.</S><S sid ="193" ssid = "5">Previous applications of vector addition to document indexing (Deerwester et al.  1990) or essay grading (Landauer et al.  1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'19'", "'5'", "'193'", "'20'"]
'8'
'19'
'5'
'193'
'20'
['8', '19', '5', '193', '20']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).</S><S sid ="5" ssid = "1">Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.</S><S sid ="19" ssid = "15">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid ="148" ssid = "61">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid ="193" ssid = "5">Previous applications of vector addition to document indexing (Deerwester et al.  1990) or essay grading (Landauer et al.  1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'5'", "'19'", "'148'", "'193'"]
'9'
'5'
'19'
'148'
'193'
['9', '5', '19', '148', '193']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S><S sid ="141" ssid = "54">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S><S sid ="163" ssid = "76">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S><S sid ="191" ssid = "3">Despite the popularity of additive models  our experimental results showed the superiority of models utilizing multiplicative combinations  at least for the sentence similarity task attempted here.</S><S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'141'", "'163'", "'191'", "'108'"]
'10'
'141'
'163'
'191'
'108'
['10', '141', '163', '191', '108']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">Despite their widespread use  vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature.</S><S sid ="33" ssid = "6">In contrast  symbolic representations can naturally handle the binding of constituents to their roles  in a systematic manner that avoids both these problems.</S><S sid ="6" ssid = "2">The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).</S><S sid ="29" ssid = "2">While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S><S sid ="95" ssid = "8">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'33'", "'6'", "'29'", "'95'"]
'11'
'33'
'6'
'29'
'95'
['11', '33', '6', '29', '95']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">In fact  the commonest method for combining the vectors is to average them.</S><S sid ="77" ssid = "25">For additive models  a natural way to achieve this is to include further vectors into the summation.</S><S sid ="43" ssid = "16">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'77'", "'43'", "'34'", "'39'"]
'12'
'77'
'43'
'34'
'39'
['12', '77', '43', '34', '39']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">Vector averaging is unfortunately insensitive to word order  and more generally syntactic structure  giving the same representation to any constructions that happen to share the same vocabulary.</S><S sid ="6" ssid = "2">The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).</S><S sid ="46" ssid = "19">However  since it is order independent  it cannot capture meaning differences that are modulated by differences in syntactic structure.</S><S sid ="93" ssid = "6">The idea here is that an appropriate composition model when applied to horse and ran will yield a vector closer to the landmark gallop than dissolve.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'6'", "'46'", "'93'", "'194'"]
'13'
'6'
'46'
'93'
'194'
['13', '6', '46', '93', '194']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">This is illustrated in the example below taken from Landauer et al. (1997).</S><S sid ="175" ssid = "9">We observe a similar pattern for the non compositional baseline model  the weighted additive model and Kintsch (2001).</S><S sid ="179" ssid = "13">The results of our correlation analysis are also given in Table 2.</S><S sid ="55" ssid = "3">A hypothetical semantic space is illustrated in Figure 1.</S><S sid ="88" ssid = "1">We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'175'", "'179'", "'55'", "'88'"]
'14'
'175'
'179'
'55'
'88'
['14', '175', '179', '55', '88']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">Sentences (1-a) and (1-b) contain exactly the same set of words but their meaning is entirely different.</S><S sid ="22" ssid = "18">Here  semantic analysis is guided by syntactic structure  and therefore sentences (1-a) and (1-b) receive distinct representations.</S><S sid ="56" ssid = "4">Here  the space has only five dimensions  and the matrix cells denote the co-occurrence of the target words (horse and run) with the context words animal  stable  and so on.</S><S sid ="79" ssid = "27">When modeling predicate-argument structures  Kintsch (2001) proposes including one or more distributional neighbors  n  of the predicate: Note that considerable latitude is allowed in selecting the appropriate neighbors.</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'22'", "'56'", "'79'", "'20'"]
'15'
'22'
'56'
'79'
'20'
['15', '22', '56', '79', '20']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">(1) a.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="15" ssid = "11">Sentences (1-a) and (1-b) contain exactly the same set of words but their meaning is entirely different.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="22" ssid = "18">Here  semantic analysis is guided by syntactic structure  and therefore sentences (1-a) and (1-b) receive distinct representations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'150'", "'15'", "'34'", "'22'"]
'16'
'150'
'15'
'34'
'22'
['16', '150', '15', '34', '22']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">It was not the sales manager who hit the bottle that day  but the office worker with the serious drinking problem. b.</S><S sid ="18" ssid = "14">That day the office manager  who was drinking  hit the problem sales worker with a bottle  but it was not serious.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S><S sid ="174" ssid = "8">The simple additive model fails to distinguish between High and Low Similarity items.</S><S sid ="122" ssid = "35">The opposite is the case for the reference The face glowed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'18'", "'164'", "'174'", "'122'"]
'17'
'18'
'164'
'174'
'122'
['17', '18', '164', '174', '122']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P11-1060_aakansha.csv
<S sid="11" ssid="7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'"]
'2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="5">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'"]
'112'
['112']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="11">Although a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">However, we still model the logical form (now as a latent variable) to capture the complexities of language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="12">It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="21">The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="88" ssid="64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets, a restrictor A and a nuclear scope B.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'"]
'88'
['88']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S sid ="1" ssid = "1">Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="2" ssid = "2">In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'141'", "'15'", "'6'", "'2'"]
'1'
'141'
'15'
'6'
'2'
['1', '141', '15', '6', '2']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="117" ssid = "2">In each dataset  each sentence x is annotated with a Prolog logical form  which we use only to evaluate and get an answer y.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="149" ssid = "34">Think of DCS as a higher-level Our system learns lexical associations between programming language tailored to natural language  words and predicates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'3'", "'117'", "'100'", "'149'"]
'2'
'3'
'117'
'100'
'149'
['2', '3', '117', '100', '149']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="48" ssid = "24">In addition  trees enable efficient computation  thereby establishing a new connection between dependency syntax and efficient semantic evaluation.</S><S sid ="149" ssid = "34">Think of DCS as a higher-level Our system learns lexical associations between programming language tailored to natural language  words and predicates.</S><S sid ="2" ssid = "2">In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.</S><S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parent’s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'48'", "'149'", "'2'", "'51'"]
'3'
'48'
'149'
'2'
'51'
['3', '48', '149', '2', '51']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">On two stansemantic parsing benchmarks our system obtains the highest published accuracies  despite requiring no annotated logical forms.</S><S sid ="24" ssid = "20">Our system outperforms all existing systems despite using no annotated logical forms.</S><S sid ="136" ssid = "21">In fact  DCS performs comparably to even the version of SEMRESP trained using logical forms.</S><S sid ="139" ssid = "24">All other systems require logical forms as training data  whereas ours does not.</S><S sid ="135" ssid = "20">SEMRESP requires a lexicon of 1.42 words per non-value predicate  WordNet features  and syntactic parse trees; DCS requires only words for the domain-independent predicates (overall  around 0.5 words per non-value predicate)  POS tags  and very simple indicator features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'24'", "'136'", "'139'", "'135'"]
'4'
'24'
'136'
'139'
'135'
['4', '24', '136', '139', '135']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">What is the total population of the ten largest capitals in the US?</S><S sid ="52" ssid = "28">The denotation of the middle node is {s}  where s is all major cities.</S><S sid ="105" ssid = "81">The feature vector φ(x  z) is defined by sums of five simple indicator feature templates: (F1) a word triggers a predicate (e.g.  [city  city]); (F2) a word is under a relation (e.g.  [that  11]); (F3) a word is under a trace predicate (e.g.  [in  loc]); (F4) two predicates are linked via a relation in the left or right direction (e.g.  [city 11  loc  RIGHT]); and (F5) a predicate has a child relation (e.g.  [city  11]).</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S><S sid ="50" ssid = "26">For example  consider the phrase number of major cities  and suppose that number corresponds to the count predicate.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'52'", "'105'", "'62'", "'50'"]
'5'
'52'
'105'
'62'
'50'
['5', '52', '105', '62', '50']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="48" ssid = "24">In addition  trees enable efficient computation  thereby establishing a new connection between dependency syntax and efficient semantic evaluation.</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'169'", "'58'", "'48'", "'165'"]
'6'
'169'
'58'
'48'
'165'
['6', '169', '58', '48', '165']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Supervised semantic parsers (Zelle and Mooney  1996; Tang and Mooney  2001; Ge and Mooney  2005; Zettlemoyer and Collins  2005; Kate and Mooney  2007; Zettlemoyer and Collins  2007; Wong and Mooney  2007; Kwiatkowski et al.  2010) rely on manual annotation of logical forms  which is expensive.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="166" ssid = "51">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid ="125" ssid = "10">We used the same training-test splits as Zettlemoyer and Collins (2005) (600+280 for GEO and 500+140 for JOBS).</S><S sid ="161" ssid = "46">CCG (Steedman  2000)  in which semantic pars- The integration of natural language with denotaing is driven from the lexicon.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'8'", "'166'", "'125'", "'161'"]
'7'
'8'
'166'
'125'
'161'
['7', '8', '166', '125', '161']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="138" ssid = "23">Next  we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S><S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid ="18" ssid = "14">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'138'", "'9'", "'165'", "'18'"]
'8'
'138'
'9'
'165'
'18'
['8', '138', '9', '165', '18']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="20" ssid = "16">At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'58'", "'165'", "'26'", "'20'"]
'9'
'58'
'165'
'26'
'20'
['9', '58', '165', '26', '20']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="37" ssid = "13">The basic version of DCS restricts R to join and aggregate relations (see Table 1).</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'58'", "'169'", "'37'", "'26'"]
'10'
'58'
'169'
'37'
'26'
['10', '58', '169', '37', '26']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts)  producing an answer y.</S><S sid ="118" ssid = "3">This evaluation is done with respect to a world w. Recall that a world w maps each predicate p ∈ P to a set of tuples w(p).</S><S sid ="31" ssid = "7">Conceptually  a world is a relational database where each predicate is a relation (possibly infinite).</S><S sid ="38" ssid = "14">Let us start by considering a DCS tree z with only join relations.</S><S sid ="29" ssid = "5">Let P be a set of predicates (e.g.  state  count ∈ P)  which are just symbols.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'118'", "'31'", "'38'", "'29'"]
'11'
'118'
'31'
'38'
'29'
['11', '118', '31', '38', '29']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="45" ssid = "21">The logical forms in DCS are called DCS trees  where nodes are labeled with predicates  and edges are labeled with relations.</S><S sid ="32" ssid = "8">Define a special predicate ø with w(ø) = V. We represent functions by a set of inputoutput pairs  e.g.  w(count) = {(S  n) : n = |S|}.</S><S sid ="85" ssid = "61">Extraction allows us to return the set of consistent values of a marked non-root node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'169'", "'45'", "'32'", "'85'"]
'12'
'169'
'45'
'32'
'85'
['12', '169', '45', '32', '85']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x  y)  which is much cheaper to obtain than (x  z) pairs.</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S><S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="103" ssid = "79">To define the features  we technically need to augment each tree z ∈ ZL(x) with alignment information—namely  for each predicate in z  the span in x (if any) that triggered it.</S><S sid ="108" ssid = "84">However  in order to learn  we need to sum over {z ∈ ZL(x) : JzKw = y}  and unfortunately  the additional constraint JzKw = y does not factorize.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'62'", "'12'", "'103'", "'108'"]
'13'
'62'
'12'
'103'
'108'
['13', '62', '12', '103', '108']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.</S><S sid ="43" ssid = "19">Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter  2003).</S><S sid ="107" ssid = "83">Our model is arc-factored  so we can sum over all DCS trees in ZL(x) using dynamic programming.</S><S sid ="47" ssid = "23">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.</S><S sid ="59" ssid = "35">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'43'", "'107'", "'47'", "'59'"]
'14'
'43'
'107'
'47'
'59'
['14', '43', '107', '47', '59']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="142" ssid = "27">This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.</S><S sid ="163" ssid = "48">Feedback from the context; for example  the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x  y)  which means (Schuler  2003) and semantic parsing (Popescu et borders looks right for the first argument and left al.  2003; Clarke et al.  2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'100'", "'141'", "'142'", "'163'"]
'15'
'100'
'141'
'142'
'163'
['15', '100', '141', '142', '163']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Which one should we use?</S><S sid ="144" ssid = "29">Intuitions How is our system learning?</S><S sid ="142" ssid = "27">This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.</S><S sid ="95" ssid = "71">Our first question is: given notation of the DCS tree before execution is an utterance x  what trees z ∈ Z are permissible?</S><S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'144'", "'142'", "'95'", "'15'"]
'16'
'144'
'142'
'95'
'15'
['16', '144', '142', '95', '15']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">The dominant paradigm in compositional semantics is Montague semantics  which constructs lambda calculus forms in a bottom-up manner.</S><S sid ="36" ssid = "12">It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.</S><S sid ="37" ssid = "13">The basic version of DCS restricts R to join and aggregate relations (see Table 1).</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="25" ssid = "1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS)  which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'36'", "'37'", "'58'", "'25'"]
'17'
'36'
'37'
'58'
'25'
['17', '36', '37', '58', '25']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="20" ssid = "16">At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.</S><S sid ="167" ssid = "52">In DCS  we start with lexical triggers  which are 6 Conclusion more basic than CCG lexical entries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'165'", "'26'", "'20'", "'167'"]
'18'
'165'
'26'
'20'
'167'
['18', '165', '26', '20', '167']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "15">However  the logical forms there can become quite complex  and in the context of program induction  this would lead to an unwieldy search space.</S><S sid ="122" ssid = "7">For JOBS  if we use the standard Jobs database  close to half the y’s are empty  which makes it uninteresting.</S><S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parent’s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid ="14" ssid = "10">The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.</S><S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'122'", "'51'", "'14'", "'9'"]
'19'
'122'
'51'
'14'
'9'
['19', '122', '51', '14', '9']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W06-3114_swastika.csv
<S sid="47" ssid="13">Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data.</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
    <S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="37">Our suspicion is that BLEU is very sensitive to jargon, to selecting exactly the right words, and not synonyms that human judges may appreciate as equally good.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
<S sid="145" ssid="38">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S>
original cit marker offset is 0
new cit marker offset is 0



['145']
145
['145']
parsed_discourse_facet ['result_citation']
    <S sid="103" ssid="19">Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['103']
103
['103']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="16">Following this method, we repeatedly &#8212; say, 1000 times &#8212; sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.</S>
original cit marker offset is 0
new cit marker offset is 0



['50']
50
['50']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="7">We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.</S>
original cit marker offset is 0
new cit marker offset is 0



['68']
68
['68']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="8">The way judgements are collected, human judges tend to use the scores to rank systems against each other.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['result_citation']
<S sid="145" ssid="38">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S>
original cit marker offset is 0
new cit marker offset is 0



['145']
145
['145']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="143" ssid = "36">For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.</S><S sid ="129" ssid = "22">All systems (except for Systran  which was not tuned to Europarl) did considerably worse on outof-domain training data.</S><S sid ="121" ssid = "14">For the automatic scoring method BLEU  we can distinguish three quarters of the systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'136'", "'143'", "'129'", "'121'"]
'1'
'136'
'143'
'129'
'121'
['1', '136', '143', '129', '121']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S sid ="86" ssid = "2">The average fluency judgement per judge ranged from 2.33 to 3.67  the average adequacy judgement ranged from 2.56 to 4.13.</S><S sid ="150" ssid = "43">For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French)  at least as measured by BLEU.</S><S sid ="95" ssid = "11">The judgement of 4 in the first case will go to a vastly better system output than in the second case.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'86'", "'150'", "'95'", "'136'"]
'2'
'86'
'150'
'95'
'136'
['2', '86', '150', '95', '136']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "1">was done by the participants.</S><S sid ="67" ssid = "6">Participants and other volunteers contributed about 180 hours of labor in the manual evaluation.</S><S sid ="29" ssid = "22">About half of the participants of last year’s shared task participated again.</S><S sid ="78" ssid = "17">Judges where excluded from assessing the quality of MT systems that were submitted by their institution.</S><S sid ="168" ssid = "61">Annotators argued for the importance of having correct and even multiple references.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'67'", "'29'", "'78'", "'168'"]
'3'
'67'
'29'
'78'
'168'
['3', '67', '29', '78', '168']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "2">This revealed interesting clues about the properties of automatic and manual scoring.</S><S sid ="146" ssid = "39">This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.</S><S sid ="168" ssid = "61">Annotators argued for the importance of having correct and even multiple references.</S><S sid ="176" ssid = "7">Human judges also pointed out difficulties with the evaluation of long sentences.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'146'", "'168'", "'176'", "'134'"]
'4'
'146'
'168'
'176'
'134'
['4', '146', '168', '176', '134']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "3">• We evaluated translation from English  in addition to into English.</S><S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S><S sid ="152" ssid = "45">The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'6'", "'152'", "'170'", "'162'"]
'5'
'6'
'152'
'170'
'162'
['5', '6', '152', '170', '162']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S><S sid ="5" ssid = "3">• We evaluated translation from English  in addition to into English.</S><S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="79" ssid = "18">Sentences and systems were randomly selected and randomly shuffled for presentation.</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'5'", "'163'", "'79'", "'162'"]
'6'
'5'
'163'
'79'
'162'
['6', '5', '163', '79', '162']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "5">We dropped  however  one of the languages  Finnish  partly to keep the number of tracks manageable  partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation.</S><S sid ="165" ssid = "58">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid ="66" ssid = "5">In this shared task  we were also confronted with this problem  and since we had no funding for paying human judgements  we asked participants in the evaluation to share the burden.</S><S sid ="47" ssid = "13">Because of this  we retokenized and lowercased submitted output with our own tokenizer  which was also used to prepare the training and test data.</S><S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'165'", "'66'", "'47'", "'125'"]
'7'
'165'
'66'
'47'
'125'
['7', '165', '66', '47', '125']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "1">The evaluation framework for the shared task is similar to the one used in last year’s shared task.</S><S sid ="57" ssid = "23">We are therefore applying a different method  which has been used at the 2005 DARPA/NIST evaluation.</S><S sid ="29" ssid = "22">About half of the participants of last year’s shared task participated again.</S><S sid ="31" ssid = "24">Compared to last year’s shared task  the participants represent more long-term research efforts.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'57'", "'29'", "'31'", "'28'"]
'8'
'57'
'29'
'31'
'28'
['8', '57', '29', '31', '28']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "2">Training and testing is based on the Europarl corpus.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="143" ssid = "36">For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.</S><S sid ="21" ssid = "14">The out-of-domain test set differs from the Europarl data in various ways.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'28'", "'170'", "'143'", "'21'"]
'9'
'28'
'170'
'143'
'21'
['9', '28', '170', '143', '21']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "3">Figure 1 provides some statistics about this corpus.</S><S sid ="20" ssid = "13">For statistics on this test set  refer to Figure 1.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="4" ssid = "2">This revealed interesting clues about the properties of automatic and manual scoring.</S><S sid ="120" ssid = "13">In Figure 4  we displayed the number of system comparisons  for which we concluded statistical significance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'20'", "'134'", "'4'", "'120'"]
'10'
'20'
'134'
'4'
'120'
['10', '20', '134', '4', '120']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "4">To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="161" ssid = "54">Since all long sentence translation are somewhat muddled  even a contrastive evaluation between systems was difficult.</S><S sid ="152" ssid = "45">The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.</S><S sid ="31" ssid = "24">Compared to last year’s shared task  the participants represent more long-term research efforts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'28'", "'161'", "'152'", "'31'"]
'11'
'28'
'161'
'152'
'31'
['11', '28', '161', '152', '31']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "5">To summarize  we provided: The performance of the baseline system is similar to the best submissions in last year’s shared task.</S><S sid ="141" ssid = "34">In-domain Systran scores on this metric are lower than all statistical systems  even the ones that have much worse human scores.</S><S sid ="8" ssid = "1">The evaluation framework for the shared task is similar to the one used in last year’s shared task.</S><S sid ="81" ssid = "20">This is less than the 694 judgements 2004 DARPA/NIST evaluation  or the 532 judgements in the 2005 DARPA/NIST evaluation.</S><S sid ="151" ssid = "44">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages  as demonstrated by the results for English-German and English-French.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'141'", "'8'", "'81'", "'151'"]
'12'
'141'
'8'
'81'
'151'
['12', '141', '8', '81', '151']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "6">We are currently working on a complete open source implementation of a training and decoding system  which should become available over the summer. pus  from which also the in-domain test set is taken.</S><S sid ="33" ssid = "26">While building a machine translation system is a serious undertaking  in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.</S><S sid ="43" ssid = "9">At the very least  we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.</S><S sid ="74" ssid = "13">While we had up to 11 submissions for a translation direction  we did decide against presenting all 11 system outputs to the human judge.</S><S sid ="42" ssid = "8">It was our hope that this competition  which included the manual and automatic evaluation of statistical systems and one rulebased commercial system  will give further insight into the relation between automatic and manual evaluation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'33'", "'43'", "'74'", "'42'"]
'13'
'33'
'43'
'74'
'42'
['13', '33', '43', '74', '42']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "7">There is twice as much language modelling data  since training data for the machine translation system is filtered against sentences of length larger than 40 words.</S><S sid ="165" ssid = "58">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid ="58" ssid = "24">We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set  53 blocks for the out-of-domain test set)  check for each block  if one system has a higher BLEU score than the other  and then use the sign test.</S><S sid ="151" ssid = "44">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages  as demonstrated by the results for English-German and English-French.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'165'", "'58'", "'151'", "'155'"]
'14'
'165'
'58'
'151'
'155'
['14', '165', '58', '151', '155']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "8">Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.</S><S sid ="21" ssid = "14">The out-of-domain test set differs from the Europarl data in various ways.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="142" ssid = "35">Surprisingly  this effect is much less obvious for out-of-domain test data.</S><S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'21'", "'28'", "'142'", "'18'"]
'15'
'21'
'28'
'142'
'18'
['15', '21', '28', '142', '18']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="165" ssid = "58">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid ="14" ssid = "7">There is twice as much language modelling data  since training data for the machine translation system is filtered against sentences of length larger than 40 words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'126'", "'18'", "'165'", "'14'"]
'16'
'126'
'18'
'165'
'14'
['16', '126', '18', '165', '14']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="69" ssid = "8">We settled on contrastive evaluations of 5 system outputs for a single test sentence.</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="19" ssid = "12">We aligned the texts at a sentence level across all four languages  resulting in 1064 sentence per language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'69'", "'162'", "'170'", "'19'"]
'17'
'69'
'162'
'170'
'19'
['17', '69', '162', '170', '19']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="15" ssid = "8">Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'126'", "'16'", "'15'", "'155'"]
'18'
'126'
'16'
'15'
'155'
['18', '126', '16', '15', '155']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W99-0623_sweta.csv
 <S sid="144" ssid="6">Combining multiple highly-accurate independent parsers yields promising results.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
 <S sid="125" ssid="54">The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["125'"]
125'
['125']
parsed_discourse_facet ['method_citation']
<S sid="125" ssid="54">The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["125'"]
125'
['125']
parsed_discourse_facet ['method_citation']
    <S sid="48" ssid="34">&#8226; Similarly, when the na&#239;ve Bayes classifier is configured such that the constituents require estimated probabilities strictly larger than 0.5 to be accepted, there is not enough probability mass remaining on crossing brackets for them to be included in the hypothesis.</S>
original cit marker offset is 0
new cit marker offset is 0



["48'"]
48'
['48']
parsed_discourse_facet ['method_citation']
 <S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="63">As seen by the drop in average individual parser performance baseline, the introduced parser does not perform very well.</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
    <S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="13" ssid="9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993).</S>
original cit marker offset is 0
new cit marker offset is 0



["13'"]
13'
['13']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="37">From this we see that a finer-grained model for parser combination, at least for the features we have examined, will not give us any additional power.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="98" ssid="27">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["98'"]
98'
['98']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="13">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'"]
27'
['27']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="9">For our experiments we also report the mean of precision and recall, which we denote by (P + R)I2 and F-measure.</S>
    <S sid="81" ssid="10">F-measure is the harmonic mean of precision and recall, 2PR/(P + R).</S>
    <S sid="82" ssid="11">It is closer to the smaller value of precision and recall when there is a large skew in their values.</S>
original cit marker offset is 0
new cit marker offset is 0



["80'", "'81'", "'82'"]
80'
'81'
'82'
['80', '81', '82']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="35">In general, the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["49'"]
49'
['49']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="7">Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["11'"]
11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="98" ssid="27">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["98'"]
98'
['98']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0623.csv
<S sid ="1" ssid = "1">Three state-of-the-art statistical parsers are combined to produce more accurate parses  as well as new bounds on achievable Treebank parsing accuracy.</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="129" ssid = "58">In the interest of testing the robustness of these combining techniques  we added a fourth  simple nonlexicalized PCFG parser.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'72'", "'38'", "'129'", "'130'"]
'1'
'72'
'38'
'129'
'130'
['1', '72', '38', '129', '130']
Error in Discourse Facet
<S sid ="2" ssid = "2">Two general approaches are presented and two combination techniques are described for each approach.</S><S sid ="59" ssid = "45">Once again we present both a non-parametric and a parametric technique for this task.</S><S sid ="3" ssid = "3">Both parametric and non-parametric models are explored.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'59'", "'3'", "'85'", "'139'"]
'2'
'59'
'3'
'85'
'139'
['2', '59', '3', '85', '139']
Error in Discourse Facet
<S sid ="3" ssid = "3">Both parametric and non-parametric models are explored.</S><S sid ="59" ssid = "45">Once again we present both a non-parametric and a parametric technique for this task.</S><S sid ="2" ssid = "2">Two general approaches are presented and two combination techniques are described for each approach.</S><S sid ="138" ssid = "67">Surprisingly  the non-parametric switching technique also exhibited robust behaviour in this situation.</S><S sid ="136" ssid = "65">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'59'", "'2'", "'138'", "'136'"]
'3'
'59'
'2'
'138'
'136'
['3', '59', '2', '138', '136']
Error in Discourse Facet
<S sid ="4" ssid = "4">The resulting parsers surpass the best previously published performance results for the Penn Treebank.</S><S sid ="76" ssid = "5">The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="134" ssid = "63">As seen by the drop in average individual parser performance baseline  the introduced parser does not perform very well.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'76'", "'85'", "'130'", "'134'"]
'4'
'76'
'85'
'130'
'134'
['4', '76', '85', '130', '134']
Error in Discourse Facet
<S sid ="5" ssid = "1">The natural language processing community is in the strong position of having many available approaches to solving some of its most fundamental problems.</S><S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid ="26" ssid = "12">This technique has the advantage of requiring no training  but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'6'", "'12'", "'13'", "'26'"]
'5'
'6'
'12'
'13'
'26'
['5', '6', '12', '13', '26']
Error in Discourse Facet
<S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="108" ssid = "37">From this we see that a finer-grained model for parser combination  at least for the features we have examined  will not give us any additional power.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'11'", "'9'", "'12'", "'108'"]
'6'
'11'
'9'
'12'
'108'
['6', '11', '9', '12', '108']
Error in Discourse Facet
<S sid ="7" ssid = "3">Their theoretical finding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent  accurate classifiers.</S><S sid ="30" ssid = "16">This is equivalent to the assumption used in probability estimation for naïve Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.</S><S sid ="116" ssid = "45">The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="41" ssid = "27">IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'30'", "'116'", "'38'", "'41'"]
'7'
'30'
'116'
'38'
'41'
['7', '30', '116', '38', '41']
Error in Discourse Facet
<S sid ="8" ssid = "4">The theory has also been validated empirically.</S><S sid ="36" ssid = "22">The estimation of the probabilities in the model is carried out as shown in Equation 4.</S><S sid ="83" ssid = "12">We performed three experiments to evaluate our techniques.</S><S sid ="121" ssid = "50">Table 3 contains the results for evaluating our systems on the test set (section 22).</S><S sid ="109" ssid = "38">The results in Table 2 were achieved on the development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'36'", "'83'", "'121'", "'109'"]
'8'
'36'
'83'
'121'
'109'
['8', '36', '83', '121', '109']
Error in Discourse Facet
<S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="88" ssid = "17">For example  one parser could be more accurate at predicting noun phrases than the other parsers.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'55'", "'88'", "'21'", "'139'"]
'9'
'55'
'88'
'21'
'139'
['9', '55', '88', '21', '139']
Error in Discourse Facet
<S sid ="10" ssid = "6">In both cases the investigators were able to achieve significant improvements over the previous best tagging results.</S><S sid ="136" ssid = "65">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="141" ssid = "3">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid ="134" ssid = "63">As seen by the drop in average individual parser performance baseline  the introduced parser does not perform very well.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'136'", "'85'", "'141'", "'134'"]
'10'
'136'
'85'
'141'
'134'
['10', '136', '85', '141', '134']
Error in Discourse Facet
<S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'9'", "'6'", "'12'", "'13'"]
'11'
'9'
'6'
'12'
'13'
['11', '9', '6', '12', '13']
Error in Discourse Facet
<S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'13'", "'11'", "'6'", "'72'"]
'12'
'13'
'11'
'6'
'72'
['12', '13', '11', '6', '72']
Error in Discourse Facet
<S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="4" ssid = "4">The resulting parsers surpass the best previously published performance results for the Penn Treebank.</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'12'", "'4'", "'72'", "'130'"]
'13'
'12'
'4'
'72'
'130'
['13', '12', '4', '72', '130']
Error in Discourse Facet
<S sid ="14" ssid = "10">We used these three parsers to explore parser combination techniques.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="126" ssid = "55">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S><S sid ="140" ssid = "2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'139'", "'85'", "'126'", "'140'"]
'14'
'139'
'85'
'126'
'140'
['14', '139', '85', '126', '140']
Error in Discourse Facet
<S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="88" ssid = "17">For example  one parser could be more accurate at predicting noun phrases than the other parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'55'", "'27'", "'85'", "'88'"]
'15'
'55'
'27'
'85'
'88'
['15', '55', '27', '85', '88']
Error in Discourse Facet
<S sid ="16" ssid = "2">We call this approach parse hybridization.</S><S sid ="58" ssid = "44">We call this approach parser switching.</S><S sid ="23" ssid = "9">We call this technique constituent voting.</S><S sid ="96" ssid = "25">We call such a constituent an isolated constituent.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'58'", "'23'", "'96'", "'139'"]
'16'
'58'
'23'
'96'
'139'
['16', '58', '23', '96', '139']
Error in Discourse Facet
<S sid ="17" ssid = "3">The substructures that are unanimously hypothesized by the parsers should be preserved after combination  and the combination technique should not foolishly create substructures for which there is no supporting evidence.</S><S sid ="49" ssid = "35">In general  the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.</S><S sid ="117" ssid = "46">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S><S sid ="125" ssid = "54">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S><S sid ="95" ssid = "24">One side of the decision making process is when we choose to believe a constituent should be in the parse  even though only one parser suggests it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'49'", "'117'", "'125'", "'95'"]
'17'
'49'
'117'
'125'
'95'
['17', '49', '117', '125', '95']
Error in Discourse Facet
IGNORE THIS: key error 1



W99-0623
W05-1518
0
method_citation
parsing: input/ref/Task1/P87-1015_swastika.csv
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="149" ssid="34">We can obtain a letter equivalent CFL defined by a CFG in which the for each rule as above, we have the production A &#8212;* A1 Anup where tk (up) = cp.</S>
original cit marker offset is 0
new cit marker offset is 0



['149']
149
['149']
parsed_discourse_facet ['method_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
    <S sid="2" ssid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['2']
2
['2']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="222" ssid="28">However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



['222']
222
['222']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['???']
???
['???']
parsed_discourse_facet ['aim_citation']
<S sid="207" ssid="13">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['207']
207
['207']
parsed_discourse_facet ['result_citation']
<S sid="207" ssid="13">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['207']
207
['207']
parsed_discourse_facet ['result_citation']
<S sid="19" ssid="4">It can be easily shown from Thatcher's result that the path set of every local set is a regular set.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/P87-1015.csv
<S sid ="1" ssid = "1">We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.</S><S sid ="9" ssid = "7">We examine both the complexity of the paths of trees in the tree sets  and the kinds of dependencies that the formalisms can impose between paths.</S><S sid ="232" ssid = "38">In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="110" ssid = "16">The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'232'", "'195'", "'110'"]
'1'
'9'
'232'
'195'
'110'
['1', '9', '232', '195', '110']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">In considering the relationship between formalisms  we show that it is useful to abstract away from the details of the formalism  and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation  we describe a class of formalisms which we call Linear Context- Free Rewriting Systems  and show they are recognizable in polynomial time and generate only semilinear languages.</S><S sid ="203" ssid = "9">In order to observe the similarity between these constrained systems  it is crucial to abstract away from the details of the structures and operations used by the system.</S><S sid ="232" ssid = "38">In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.</S><S sid ="11" ssid = "9">By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'203'", "'232'", "'11'", "'116'"]
'2'
'203'
'232'
'11'
'116'
['2', '203', '232', '11', '116']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S><S sid ="48" ssid = "33">There has been recent interest in the application of Indexed Grammars (IG's) to natural languages.</S><S sid ="23" ssid = "8">He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.</S><S sid ="56" ssid = "41">Gazdar (1985) argues that sharing of stacks can be used to give analyses for coordination.</S><S sid ="65" ssid = "50">While the generative power of CG's is greater that of CFG's  it appears to be highly constrained.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'48'", "'23'", "'56'", "'65'"]
'3'
'48'
'23'
'56'
'65'
['3', '48', '23', '56', '65']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "2">Little attention  however  has been paid to the structural descriptions that these formalisms can assign to strings  i.e. their strong generative capacity.</S><S sid ="3" ssid = "1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S><S sid ="66" ssid = "51">Hence  their relationship to formalisms such as HG's and TAG's is of interest.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="48" ssid = "33">There has been recent interest in the application of Indexed Grammars (IG's) to natural languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'3'", "'66'", "'195'", "'48'"]
'4'
'3'
'66'
'195'
'48'
['4', '3', '66', '195', '48']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "3">This aspect of the formalism is both linguistically and computationally important.</S><S sid ="198" ssid = "4">This property reflects an important aspect of the underlying linguistic theory associated with the formalism.</S><S sid ="10" ssid = "8">These two properties of the tree sets are not only linguistically relevant  but also have computational importance.</S><S sid ="123" ssid = "8">A second restriction on the formalisms is that choices during the derivation are independent of the context in the derivation.</S><S sid ="3" ssid = "1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'198'", "'10'", "'123'", "'3'"]
'5'
'198'
'10'
'123'
'3'
['5', '198', '10', '123', '3']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">For example  Gazdar (1985) discusses the applicability of Indexed Grammars (IG's) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar (LFG) and Government and Bindings grammars (GB).</S><S sid ="24" ssid = "9">Head Grammars (HG's)  introduced by Pollard (1984)  is a formalism that manipulates headed strings: i.e.  strings  one of whose symbols is distinguished as the head.</S><S sid ="35" ssid = "20">TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish  for cross serial dependencies in Dutch subordinate clauses  and for the nestings of paired English complementizers.</S><S sid ="64" ssid = "49">Steedman (1986) considers Categorial Grammars in which both the operations of function application and composition may be used  and in which function can specify whether they take their arguments from their right or left.</S><S sid ="22" ssid = "7">Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'24'", "'35'", "'64'", "'22'"]
'6'
'24'
'35'
'64'
'22'
['6', '24', '35', '64', '22']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "5">The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG's and IG's.</S><S sid ="50" ssid = "35">The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.</S><S sid ="16" ssid = "1">From Thatcher's (1973) work  it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S><S sid ="98" ssid = "4">Thatcher (1973) describes a tee pumping lemma for recognizable sets related to the string pumping lemma for regular sets.</S><S sid ="24" ssid = "9">Head Grammars (HG's)  introduced by Pollard (1984)  is a formalism that manipulates headed strings: i.e.  strings  one of whose symbols is distinguished as the head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'50'", "'16'", "'98'", "'24'"]
'7'
'50'
'16'
'98'
'24'
['7', '50', '16', '98', '24']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "6">We consider properties of the tree sets generated by CFG's  Tree Adjoining Grammars (TAG's)  Head Grammars (HG's)  Categorial Grammars (CG's)  and IG's.</S><S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="27" ssid = "12">Thus  the tree sets generated by HG's are similar to those of CFG's  with each node annotated by the operation (concatenation or wrapping) used to combine the headed strings derived by the daughters of Tree Adjoining Grammars  a tree rewriting formalism  was introduced by Joshi  Levy and Takahashi (1975) and Joshi (1983/85).</S><S sid ="94" ssid = "79">The semilinearity of Tree Adjoining Languages (TAL's)  MCTAL's  and Head Languages (HL's) can be proved using this property  with suitable restrictions on the composition operations.</S><S sid ="208" ssid = "14">This family represents an attempt to generalize the properties shared by CFG's  HG's  TAG's  and MCTAG's.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'151'", "'27'", "'94'", "'208'"]
'8'
'151'
'27'
'94'
'208'
['8', '151', '27', '94', '208']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "7">We examine both the complexity of the paths of trees in the tree sets  and the kinds of dependencies that the formalisms can impose between paths.</S><S sid ="1" ssid = "1">We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.</S><S sid ="8" ssid = "6">We consider properties of the tree sets generated by CFG's  Tree Adjoining Grammars (TAG's)  Head Grammars (HG's)  Categorial Grammars (CG's)  and IG's.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="110" ssid = "16">The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'1'", "'8'", "'195'", "'110'"]
'9'
'1'
'8'
'195'
'110'
['9', '1', '8', '195', '110']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "8">These two properties of the tree sets are not only linguistically relevant  but also have computational importance.</S><S sid ="5" ssid = "3">This aspect of the formalism is both linguistically and computationally important.</S><S sid ="198" ssid = "4">This property reflects an important aspect of the underlying linguistic theory associated with the formalism.</S><S sid ="68" ssid = "53">This kind of dependency arises from the use of the composition operation to compose two arbitrarily large categories.</S><S sid ="23" ssid = "8">He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'5'", "'198'", "'68'", "'23'"]
'10'
'5'
'198'
'68'
'23'
['10', '5', '198', '68', '23']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "9">By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.</S><S sid ="2" ssid = "2">In considering the relationship between formalisms  we show that it is useful to abstract away from the details of the formalism  and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation  we describe a class of formalisms which we call Linear Context- Free Rewriting Systems  and show they are recognizable in polynomial time and generate only semilinear languages.</S><S sid ="232" ssid = "38">In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.</S><S sid ="14" ssid = "12">This suggests that by generalizing the notion of context-freeness in CFG's  we can define a class of grammatical formalisms that manipulate more complex structures.</S><S sid ="203" ssid = "9">In order to observe the similarity between these constrained systems  it is crucial to abstract away from the details of the structures and operations used by the system.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'2'", "'232'", "'14'", "'203'"]
'11'
'2'
'232'
'14'
'203'
['11', '2', '232', '14', '203']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "10">We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.</S><S sid ="137" ssid = "22">In considering recognition of LCFRS's  we make further assumption concerning the contribution of each structure to the input string  and how the composition operations combine structures in this respect.</S><S sid ="200" ssid = "6">The importance of this property becomes clear in contrasting theories underlying GPSG (Gazdar  Klein  Pulluna  and Sag  1985)  and GB (as described by Berwick  1984) with those underlying LFG and FUG.</S><S sid ="22" ssid = "7">Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.</S><S sid ="60" ssid = "45">Gazdar (1985) considers a restriction of IG's in which no more than one nonterminal on the right-hand-side of a production can inherit the stack from the left-hand-side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'137'", "'200'", "'22'", "'60'"]
'12'
'137'
'200'
'22'
'60'
['12', '137', '200', '22', '60']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "11">It is striking that from this point of view many formalisms can be grouped together as having identically structured derivation tree sets.</S><S sid ="15" ssid = "13">In this paper  we outline how such family of formalisms can be defined  and show that like CFG's  each member possesses a number of desirable linguistic and computational properties: in particular  the constant growth property and polynomial recognizability.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S><S sid ="202" ssid = "8">As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.</S><S sid ="78" ssid = "63">Each member of a set of trees can be adjoined into distinct nodes of trees in a single elementary tree set  i.e  derivations always involve the adjunction of a derived auxiliary tree set into an elementary tree set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'15'", "'116'", "'202'", "'78'"]
'13'
'15'
'116'
'202'
'78'
['13', '15', '116', '202', '78']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "12">This suggests that by generalizing the notion of context-freeness in CFG's  we can define a class of grammatical formalisms that manipulate more complex structures.</S><S sid ="119" ssid = "4">In defining LCFRS's  we hope to generalize the definition of CFG's to formalisms manipulating any structure  e.g. strings  trees  or graphs.</S><S sid ="53" ssid = "38">We focus on this difference between the tree sets of CFG's and IG's  and formalize the notion of dependence between paths in a tree set in Section 3.</S><S sid ="142" ssid = "27">Although this property is not structural  it depends on the structural property that sentences can be built from a finite set of clauses of bounded structure as noted by Joshi (1983/85).</S><S sid ="159" ssid = "44">Similarly  for all the LCFRS's  discussed in Section 2  we can define the relationship between a structure and the sequence of substrings it spans  and the effect of the composition operations on sequences of substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'119'", "'53'", "'142'", "'159'"]
'14'
'119'
'53'
'142'
'159'
['14', '119', '53', '142', '159']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "13">In this paper  we outline how such family of formalisms can be defined  and show that like CFG's  each member possesses a number of desirable linguistic and computational properties: in particular  the constant growth property and polynomial recognizability.</S><S sid ="202" ssid = "8">As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.</S><S sid ="117" ssid = "2">Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S><S sid ="13" ssid = "11">It is striking that from this point of view many formalisms can be grouped together as having identically structured derivation tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'202'", "'117'", "'116'", "'13'"]
'15'
'202'
'117'
'116'
'13'
['15', '202', '117', '116', '13']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "1">From Thatcher's (1973) work  it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S><S sid ="50" ssid = "35">The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.</S><S sid ="19" ssid = "4">It can be easily shown from Thatcher's result that the path set of every local set is a regular set.</S><S sid ="229" ssid = "35">In addition  the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars  and the hierarchy defined by Weir).</S><S sid ="166" ssid = "51">With the additional assumptions  inspired by Rounds (1985)  we can show that members of this class can be recognized in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'50'", "'19'", "'229'", "'166'"]
'16'
'50'
'19'
'229'
'166'
['16', '50', '19', '229', '166']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P04-1036_swastika.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="24">We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.</S>
original cit marker offset is 0
new cit marker offset is 0



['68']
68
['68']
parsed_discourse_facet ['result_citation']
<S sid="83" ssid="12">The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.</S>
original cit marker offset is 0
new cit marker offset is 0



['83']
83
['83']
parsed_discourse_facet ['result_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
<S sid="101" ssid="30">Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.</S>
original cit marker offset is 0
new cit marker offset is 0



['101']
101
['101']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="83" ssid="12">The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.</S>
original cit marker offset is 0
new cit marker offset is 0



['83']
83
['83']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="46" ssid="2">This provides the nearest neighbours to each target word, along with the distributional similarity score between the target word and its neighbour.</S>
original cit marker offset is 0
new cit marker offset is 0



['46']
46
['46']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents, rather than a lexical sample task, where the target words are manually determined and the results will depend on the skew of the words in the sample.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
<S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['method_citation']
<S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="48" ssid = "4">To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.</S><S sid ="105" ssid = "3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents  rather than a lexical sample task  where the target words are manually determined and the results will depend on the skew of the words in the sample.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'153'", "'154'", "'48'", "'105'"]
'1'
'153'
'154'
'48'
'105'
['1', '153', '154', '48', '105']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">The problem with using the predominant  or first sense heuristic  aside from the fact that it does not take surrounding context into account  is that it assumes some quantity of handtagged data.</S><S sid ="17" ssid = "10">There are words where the first sense in WordNet is counter-intuitive  because of the size of the corpus  and because where the frequency data does not indicate a first sense  the ordering is arbitrary.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'17'", "'91'", "'154'", "'123'"]
'2'
'17'
'91'
'154'
'123'
['2', '17', '91', '154', '123']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.</S><S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="37" ssid = "30">From inspection  one can see that the ordered neighbours of such a thesaurus relate to the different senses of the target word.</S><S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'15'", "'37'", "'1'", "'170'"]
'3'
'15'
'37'
'1'
'170'
['3', '15', '37', '1', '170']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S sid ="179" ssid = "2">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S><S sid ="171" ssid = "19">In contrast  we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'179'", "'178'", "'45'", "'171'"]
'4'
'179'
'178'
'45'
'171'
['4', '179', '178', '45', '171']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="84" ssid = "13">The random baseline for choosing the predominant sense over all these words ( ) is 32%.</S><S sid ="88" ssid = "17">The first sense in SemCor provides an upperbound for this task of 67%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'100'", "'116'", "'84'", "'88'"]
'5'
'100'
'116'
'84'
'88'
['5', '100', '116', '84', '88']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="121" ssid = "19">For these one would need to obtain more sense-tagged text in order to use this heuristic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'126'", "'178'", "'13'", "'121'"]
'6'
'126'
'178'
'13'
'121'
['6', '126', '178', '13', '121']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S><S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S><S sid ="158" ssid = "6">We have evaluated our method using publically available resources  both for balanced and domain specific text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'184'", "'178'", "'126'", "'158'"]
'7'
'184'
'178'
'126'
'158'
['7', '184', '178', '126', '158']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="172" ssid = "20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet  for example those that use it for lexical acquisition or WSD.</S><S sid ="115" ssid = "13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor  which is very encouraging given that our method only uses raw text  with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'154'", "'14'", "'172'", "'115'"]
'8'
'154'
'14'
'172'
'115'
['8', '154', '14', '172', '115']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "2">This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).</S><S sid ="102" ssid = "31">We test this below on the SENSEVAL-2 English all-words data.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'102'", "'116'", "'100'", "'5'"]
'9'
'102'
'116'
'100'
'5'
['9', '102', '116', '100', '5']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="180" ssid = "3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S><S sid ="109" ssid = "7">We generated a thesaurus entry for all polysemous nouns in WordNet as described in section 2.1 above.</S><S sid ="112" ssid = "10">We compare results using the first sense listed in SemCor  and the first sense according to the SENSEVAL-2 English all-words test data itself.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'180'", "'142'", "'109'", "'112'"]
'10'
'180'
'142'
'109'
'112'
['10', '180', '142', '109', '112']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "4">Senses that have not occurred in SemCor are ordered arbitrarily and after those senses of the word that have occurred.</S><S sid ="120" ssid = "18">There were a similar number of words that were not covered by a predominant sense in SemCor.</S><S sid ="42" ssid = "35">The neighbours for a word in a thesaurus are words themselves  rather than senses.</S><S sid ="189" ssid = "12">Additionally  we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity  and what can be done to combat this problem using relation specific thesauruses.</S><S sid ="117" ssid = "15">The items that were not covered by our method were those with insufficient grammatical relations for the tuples employed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'120'", "'42'", "'189'", "'117'"]
'11'
'120'
'42'
'189'
'117'
['11', '120', '42', '189', '117']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "5">The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor  from those that do not (without HTD).</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'123'", "'153'", "'165'", "'178'"]
'12'
'123'
'153'
'165'
'178'
['12', '123', '153', '165', '178']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="84" ssid = "13">The random baseline for choosing the predominant sense over all these words ( ) is 32%.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'116'", "'147'", "'84'", "'126'"]
'13'
'116'
'147'
'84'
'126'
['13', '116', '147', '84', '126']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="182" ssid = "5">In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.</S><S sid ="21" ssid = "14">We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off (Wilks and Stevenson  1998; Hoste et al.  2001) and for systems that use it in lexical acquisition (McCarthy  1997; Merlo and Leybold  2001; Korhonen  2002) because of the limited size of hand-tagged resources.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'8'", "'154'", "'182'", "'21'"]
'14'
'8'
'154'
'182'
'21'
['14', '8', '154', '182', '21']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="3" ssid = "3">Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'154'", "'3'", "'8'", "'170'"]
'15'
'154'
'3'
'8'
'170'
['15', '154', '3', '8', '170']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">SemCor comprises a relatively small sample of 250 000 words.</S><S sid ="131" ssid = "8">The SPORTS corpus consists of 35317 documents (about 9.1 million words).</S><S sid ="140" ssid = "17">The resulting set consisted of 38 words.</S><S sid ="132" ssid = "9">The FINANCE corpus consists of 117734 documents (about 32.5 million words).</S><S sid ="76" ssid = "5">The jcn measure uses corpus data for the calculation of IC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'131'", "'140'", "'132'", "'76'"]
'16'
'131'
'140'
'132'
'76'
['16', '131', '140', '132', '76']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "10">There are words where the first sense in WordNet is counter-intuitive  because of the size of the corpus  and because where the frequency data does not indicate a first sense  the ordering is arbitrary.</S><S sid ="182" ssid = "5">In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.</S><S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="99" ssid = "28">Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'182'", "'13'", "'99'", "'170'"]
'17'
'182'
'13'
'99'
'170'
['17', '182', '13', '99', '170']
parsed_discourse_facet ['method_citation']



P04-1036
P06-1012
0
result_citation
['results_citation']
parsing: input/ref/Task1/W06-2932_vardha.csv
    <S sid="19" ssid="1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="5">However, in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="76" ssid="14">For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'"]
'76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="54" ssid="2">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'"]
'54'
['54']
parsed_discourse_facet ['method_citation']
    <S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="58" ssid="6">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'"]
'58'
['58']
parsed_discourse_facet ['method_citation']
    <S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'"]
'22'
['22']
parsed_discourse_facet ['method_citation']
    <S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
    <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'"]
'64'
['64']
parsed_discourse_facet ['method_citation']
    <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'"]
'43'
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S sid ="1" ssid = "1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S><S sid ="9" ssid = "5">Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S><S sid ="23" ssid = "5">That system uses MIRA  an online large-margin learning algorithm  to compute model parameters.</S><S sid ="27" ssid = "9">We augmented this model to incorporate morphological features derived from each token.</S>
original cit marker offset is nan
new cit marker offset is 0



["'1'", "'9'", "'107'", "'23'", "'27'"]
'1'
'9'
'107'
'23'
'27'
['1', '9', '107', '23', '27']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="14" ssid = "10">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S><S sid ="58" ssid = "6">These results show that the discriminative spanning tree parsing framework (McDonald et al.  2005b; McDonald and Pereira  2006) is easily adapted across all these languages.</S><S sid ="25" ssid = "7">For instance  the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'20'", "'14'", "'58'", "'25'"]
'2'
'20'
'14'
'58'
'25'
['2', '20', '14', '58', '25']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S><S sid ="19" ssid = "1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S><S sid ="109" ssid = "6">The current system simply includes all morphological bi-gram features.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="51" ssid = "20">Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'19'", "'109'", "'10'", "'51'"]
'3'
'19'
'109'
'10'
'51'
['3', '19', '109', '10', '51']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.</S><S sid ="53" ssid = "1">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).</S><S sid ="44" ssid = "13">We use the MIRA online learner to set the weights (Crammer and Singer  2003; McDonald et al.  2005a) since we found it trained quickly and provide good performance.</S><S sid ="13" ssid = "9">We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajiˇc et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B¨ohmov´a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dˇzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).</S><S sid ="56" ssid = "4">Results on the test set are given in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'53'", "'44'", "'13'", "'56'"]
'4'
'53'
'44'
'13'
'56'
['4', '53', '44', '13', '56']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="24" ssid = "6">Its power lies in the ability to define a rich set of features over parsing decisions  as well as surface level features relative to these decisions.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'10'", "'88'", "'24'", "'107'"]
'5'
'10'
'88'
'24'
'107'
['5', '10', '88', '24', '107']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.</S><S sid ="14" ssid = "10">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'14'", "'7'", "'36'", "'8'"]
'6'
'14'
'7'
'36'
'8'
['6', '14', '7', '36', '8']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="110" ssid = "7">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="26" ssid = "8">These features are highly important to overall accuracy since they eliminate unlikely scenarios such as a preposition modifying a noun not directly to its left  or a noun modifying a verb with another verb occurring between them.</S><S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'110'", "'11'", "'26'", "'8'"]
'7'
'110'
'11'
'26'
'8'
['7', '110', '11', '26', '8']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="30" ssid = "12">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="6" ssid = "2">With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'7'", "'30'", "'11'", "'6'"]
'8'
'7'
'30'
'11'
'6'
['8', '7', '30', '11', '6']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="5" ssid = "1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S><S sid ="25" ssid = "7">For instance  the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'20'", "'88'", "'5'", "'25'"]
'9'
'20'
'88'
'5'
'25'
['9', '20', '88', '5', '25']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="5" ssid = "1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S><S sid ="24" ssid = "6">Its power lies in the ability to define a rich set of features over parsing decisions  as well as surface level features relative to these decisions.</S><S sid ="34" ssid = "3">However  the parser is fundamentally limited by the scope of local factorizations that make inference tractable.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'5'", "'24'", "'34'", "'107'"]
'10'
'5'
'24'
'34'
'107'
['10', '5', '24', '34', '107']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="110" ssid = "7">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S><S sid ="45" ssid = "14">Furthermore  it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira  2006).</S><S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'7'", "'110'", "'45'", "'8'"]
'11'
'7'
'110'
'45'
'8'
['11', '7', '110', '45', '8']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S sid ="95" ssid = "17">Note the difference in error between the unlabeled parser and the edge labeler: the former makes mistakes on edges into prepositions  conjunctions and verbs  and the latter makes mistakes on edges into nouns (subject/objects).</S><S sid ="110" ssid = "7">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S><S sid ="69" ssid = "7">However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).</S><S sid ="106" ssid = "3">First  we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'95'", "'110'", "'69'", "'106'"]
'12'
'95'
'110'
'69'
'106'
['12', '95', '110', '69', '106']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajiˇc et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B¨ohmov´a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dˇzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).</S><S sid ="54" ssid = "2">Based on performance from a held-out section of the training data  we used non-projective parsing algorithms for Czech  Danish  Dutch  German  Japanese  Portuguese and Slovene  and projective parsing algorithms for Arabic  Bulgarian  Chinese  Spanish  Swedish and Turkish.</S><S sid ="4" ssid = "4">We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.</S><S sid ="53" ssid = "1">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).</S><S sid ="14" ssid = "10">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'54'", "'4'", "'53'", "'14'"]
'13'
'54'
'4'
'53'
'14'
['13', '54', '4', '53', '14']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W06-2932_sweta.csv
 <S sid="5" ssid="1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="5">However, in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["36'"]
36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="9">In fact, for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="76" ssid="14">For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["76'"]
76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="45" ssid="14">Furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["45'"]
45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="3">First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["106'"]
106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="58" ssid="6">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["58'"]
58'
['58']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
 <S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="2">Ideally one would like to make all parsing and labeling decisions jointly so that the shared knowledge of both decisions will help resolve any ambiguities.</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
 <S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["43'"]
43'
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S sid ="1" ssid = "1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S><S sid ="9" ssid = "5">Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S><S sid ="23" ssid = "5">That system uses MIRA  an online large-margin learning algorithm  to compute model parameters.</S><S sid ="27" ssid = "9">We augmented this model to incorporate morphological features derived from each token.</S>
original cit marker offset is nan
new cit marker offset is 0



["'1'", "'9'", "'107'", "'23'", "'27'"]
'1'
'9'
'107'
'23'
'27'
['1', '9', '107', '23', '27']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="14" ssid = "10">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S><S sid ="58" ssid = "6">These results show that the discriminative spanning tree parsing framework (McDonald et al.  2005b; McDonald and Pereira  2006) is easily adapted across all these languages.</S><S sid ="25" ssid = "7">For instance  the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'20'", "'14'", "'58'", "'25'"]
'2'
'20'
'14'
'58'
'25'
['2', '20', '14', '58', '25']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S><S sid ="19" ssid = "1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S><S sid ="109" ssid = "6">The current system simply includes all morphological bi-gram features.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="51" ssid = "20">Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'19'", "'109'", "'10'", "'51'"]
'3'
'19'
'109'
'10'
'51'
['3', '19', '109', '10', '51']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.</S><S sid ="53" ssid = "1">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).</S><S sid ="44" ssid = "13">We use the MIRA online learner to set the weights (Crammer and Singer  2003; McDonald et al.  2005a) since we found it trained quickly and provide good performance.</S><S sid ="13" ssid = "9">We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajiˇc et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B¨ohmov´a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dˇzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).</S><S sid ="56" ssid = "4">Results on the test set are given in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'53'", "'44'", "'13'", "'56'"]
'4'
'53'
'44'
'13'
'56'
['4', '53', '44', '13', '56']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="24" ssid = "6">Its power lies in the ability to define a rich set of features over parsing decisions  as well as surface level features relative to these decisions.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'10'", "'88'", "'24'", "'107'"]
'5'
'10'
'88'
'24'
'107'
['5', '10', '88', '24', '107']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.</S><S sid ="14" ssid = "10">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'14'", "'7'", "'36'", "'8'"]
'6'
'14'
'7'
'36'
'8'
['6', '14', '7', '36', '8']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="110" ssid = "7">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="26" ssid = "8">These features are highly important to overall accuracy since they eliminate unlikely scenarios such as a preposition modifying a noun not directly to its left  or a noun modifying a verb with another verb occurring between them.</S><S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'110'", "'11'", "'26'", "'8'"]
'7'
'110'
'11'
'26'
'8'
['7', '110', '11', '26', '8']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="30" ssid = "12">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="6" ssid = "2">With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'7'", "'30'", "'11'", "'6'"]
'8'
'7'
'30'
'11'
'6'
['8', '7', '30', '11', '6']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="5" ssid = "1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S><S sid ="25" ssid = "7">For instance  the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'20'", "'88'", "'5'", "'25'"]
'9'
'20'
'88'
'5'
'25'
['9', '20', '88', '5', '25']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="5" ssid = "1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S><S sid ="24" ssid = "6">Its power lies in the ability to define a rich set of features over parsing decisions  as well as surface level features relative to these decisions.</S><S sid ="34" ssid = "3">However  the parser is fundamentally limited by the scope of local factorizations that make inference tractable.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'5'", "'24'", "'34'", "'107'"]
'10'
'5'
'24'
'34'
'107'
['10', '5', '24', '34', '107']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="110" ssid = "7">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S><S sid ="45" ssid = "14">Furthermore  it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira  2006).</S><S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'7'", "'110'", "'45'", "'8'"]
'11'
'7'
'110'
'45'
'8'
['11', '7', '110', '45', '8']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S sid ="95" ssid = "17">Note the difference in error between the unlabeled parser and the edge labeler: the former makes mistakes on edges into prepositions  conjunctions and verbs  and the latter makes mistakes on edges into nouns (subject/objects).</S><S sid ="110" ssid = "7">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S><S sid ="69" ssid = "7">However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).</S><S sid ="106" ssid = "3">First  we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'95'", "'110'", "'69'", "'106'"]
'12'
'95'
'110'
'69'
'106'
['12', '95', '110', '69', '106']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajiˇc et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B¨ohmov´a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dˇzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).</S><S sid ="54" ssid = "2">Based on performance from a held-out section of the training data  we used non-projective parsing algorithms for Czech  Danish  Dutch  German  Japanese  Portuguese and Slovene  and projective parsing algorithms for Arabic  Bulgarian  Chinese  Spanish  Swedish and Turkish.</S><S sid ="4" ssid = "4">We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.</S><S sid ="53" ssid = "1">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).</S><S sid ="14" ssid = "10">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'54'", "'4'", "'53'", "'14'"]
'13'
'54'
'4'
'53'
'14'
['13', '54', '4', '53', '14']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
parsing: input/ref/Task1/P04-1036_aakansha.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="34">In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
    <S sid="181" ssid="4">This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'181'"]
'180'
'181'
['180', '181']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="4">To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'"]
'48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="169" ssid="17">This method obtains precision of 61% and recall 51%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'"]
'169'
['169']
parsed_discourse_facet ['result_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="115" ssid="13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
    <S sid="181" ssid="4">This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'181'"]
'180'
'181'
['180', '181']
parsed_discourse_facet ['result_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'"]
'180'
['180']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="34">In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="48" ssid = "4">To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.</S><S sid ="105" ssid = "3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents  rather than a lexical sample task  where the target words are manually determined and the results will depend on the skew of the words in the sample.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'153'", "'154'", "'48'", "'105'"]
'1'
'153'
'154'
'48'
'105'
['1', '153', '154', '48', '105']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">The problem with using the predominant  or first sense heuristic  aside from the fact that it does not take surrounding context into account  is that it assumes some quantity of handtagged data.</S><S sid ="17" ssid = "10">There are words where the first sense in WordNet is counter-intuitive  because of the size of the corpus  and because where the frequency data does not indicate a first sense  the ordering is arbitrary.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'17'", "'91'", "'154'", "'123'"]
'2'
'17'
'91'
'154'
'123'
['2', '17', '91', '154', '123']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.</S><S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="37" ssid = "30">From inspection  one can see that the ordered neighbours of such a thesaurus relate to the different senses of the target word.</S><S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'15'", "'37'", "'1'", "'170'"]
'3'
'15'
'37'
'1'
'170'
['3', '15', '37', '1', '170']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S sid ="179" ssid = "2">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S><S sid ="171" ssid = "19">In contrast  we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'179'", "'178'", "'45'", "'171'"]
'4'
'179'
'178'
'45'
'171'
['4', '179', '178', '45', '171']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="84" ssid = "13">The random baseline for choosing the predominant sense over all these words ( ) is 32%.</S><S sid ="88" ssid = "17">The first sense in SemCor provides an upperbound for this task of 67%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'100'", "'116'", "'84'", "'88'"]
'5'
'100'
'116'
'84'
'88'
['5', '100', '116', '84', '88']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="121" ssid = "19">For these one would need to obtain more sense-tagged text in order to use this heuristic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'126'", "'178'", "'13'", "'121'"]
'6'
'126'
'178'
'13'
'121'
['6', '126', '178', '13', '121']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S><S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S><S sid ="158" ssid = "6">We have evaluated our method using publically available resources  both for balanced and domain specific text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'184'", "'178'", "'126'", "'158'"]
'7'
'184'
'178'
'126'
'158'
['7', '184', '178', '126', '158']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="172" ssid = "20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet  for example those that use it for lexical acquisition or WSD.</S><S sid ="115" ssid = "13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor  which is very encouraging given that our method only uses raw text  with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'154'", "'14'", "'172'", "'115'"]
'8'
'154'
'14'
'172'
'115'
['8', '154', '14', '172', '115']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "2">This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).</S><S sid ="102" ssid = "31">We test this below on the SENSEVAL-2 English all-words data.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'102'", "'116'", "'100'", "'5'"]
'9'
'102'
'116'
'100'
'5'
['9', '102', '116', '100', '5']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="180" ssid = "3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S><S sid ="109" ssid = "7">We generated a thesaurus entry for all polysemous nouns in WordNet as described in section 2.1 above.</S><S sid ="112" ssid = "10">We compare results using the first sense listed in SemCor  and the first sense according to the SENSEVAL-2 English all-words test data itself.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'180'", "'142'", "'109'", "'112'"]
'10'
'180'
'142'
'109'
'112'
['10', '180', '142', '109', '112']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "4">Senses that have not occurred in SemCor are ordered arbitrarily and after those senses of the word that have occurred.</S><S sid ="120" ssid = "18">There were a similar number of words that were not covered by a predominant sense in SemCor.</S><S sid ="42" ssid = "35">The neighbours for a word in a thesaurus are words themselves  rather than senses.</S><S sid ="189" ssid = "12">Additionally  we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity  and what can be done to combat this problem using relation specific thesauruses.</S><S sid ="117" ssid = "15">The items that were not covered by our method were those with insufficient grammatical relations for the tuples employed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'120'", "'42'", "'189'", "'117'"]
'11'
'120'
'42'
'189'
'117'
['11', '120', '42', '189', '117']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "5">The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor  from those that do not (without HTD).</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'123'", "'153'", "'165'", "'178'"]
'12'
'123'
'153'
'165'
'178'
['12', '123', '153', '165', '178']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="84" ssid = "13">The random baseline for choosing the predominant sense over all these words ( ) is 32%.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'116'", "'147'", "'84'", "'126'"]
'13'
'116'
'147'
'84'
'126'
['13', '116', '147', '84', '126']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="182" ssid = "5">In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.</S><S sid ="21" ssid = "14">We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off (Wilks and Stevenson  1998; Hoste et al.  2001) and for systems that use it in lexical acquisition (McCarthy  1997; Merlo and Leybold  2001; Korhonen  2002) because of the limited size of hand-tagged resources.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'8'", "'154'", "'182'", "'21'"]
'14'
'8'
'154'
'182'
'21'
['14', '8', '154', '182', '21']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="3" ssid = "3">Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'154'", "'3'", "'8'", "'170'"]
'15'
'154'
'3'
'8'
'170'
['15', '154', '3', '8', '170']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">SemCor comprises a relatively small sample of 250 000 words.</S><S sid ="131" ssid = "8">The SPORTS corpus consists of 35317 documents (about 9.1 million words).</S><S sid ="140" ssid = "17">The resulting set consisted of 38 words.</S><S sid ="132" ssid = "9">The FINANCE corpus consists of 117734 documents (about 32.5 million words).</S><S sid ="76" ssid = "5">The jcn measure uses corpus data for the calculation of IC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'131'", "'140'", "'132'", "'76'"]
'16'
'131'
'140'
'132'
'76'
['16', '131', '140', '132', '76']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "10">There are words where the first sense in WordNet is counter-intuitive  because of the size of the corpus  and because where the frequency data does not indicate a first sense  the ordering is arbitrary.</S><S sid ="182" ssid = "5">In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.</S><S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="99" ssid = "28">Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'182'", "'13'", "'99'", "'170'"]
'17'
'182'
'13'
'99'
'170'
['17', '182', '13', '99', '170']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P04-1036_vardha.csv
    <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
    <S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="66" ssid="22">It uses the glosses of semantically related (according to WordNet) senses too. jcn (Jiang and Conrath, 1997) This score uses corpus data to populate classes (synsets) in the WordNet hierarchy with frequency counts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'66'"]
'66'
['66']
parsed_discourse_facet ['method_citation']
 <S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="101" ssid="30">Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'"]
'101'
['101']
parsed_discourse_facet ['method_citation']
 <S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
 <S sid="115" ssid="13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
  <S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'"]
'89'
['89']
parsed_discourse_facet ['method_citation']
 <S sid="137" ssid="14">Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'"]
'137'
['137']
parsed_discourse_facet ['method_citation']
75 ssid="4">We generated a thesaurus entry for all polysemous nouns which occurred in SemCor with a frequency 2, and in the BNC with a frequency 10 in the grammatical relations listed in section 2.1 above.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'"]
'75'
['75']
Error in Reference Offset
 <S sid="155" ssid="3">A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
  <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
 <S sid="155" ssid="3">A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="24">We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'"]
'68'
['68']
parsed_discourse_facet ['method_citation']
    <S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'"]
'13'
['13']
parsed_discourse_facet ['method_citation']
  <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="48" ssid = "4">To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.</S><S sid ="105" ssid = "3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents  rather than a lexical sample task  where the target words are manually determined and the results will depend on the skew of the words in the sample.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'153'", "'154'", "'48'", "'105'"]
'1'
'153'
'154'
'48'
'105'
['1', '153', '154', '48', '105']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">The problem with using the predominant  or first sense heuristic  aside from the fact that it does not take surrounding context into account  is that it assumes some quantity of handtagged data.</S><S sid ="17" ssid = "10">There are words where the first sense in WordNet is counter-intuitive  because of the size of the corpus  and because where the frequency data does not indicate a first sense  the ordering is arbitrary.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'17'", "'91'", "'154'", "'123'"]
'2'
'17'
'91'
'154'
'123'
['2', '17', '91', '154', '123']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.</S><S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="37" ssid = "30">From inspection  one can see that the ordered neighbours of such a thesaurus relate to the different senses of the target word.</S><S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'15'", "'37'", "'1'", "'170'"]
'3'
'15'
'37'
'1'
'170'
['3', '15', '37', '1', '170']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S sid ="179" ssid = "2">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S><S sid ="171" ssid = "19">In contrast  we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'179'", "'178'", "'45'", "'171'"]
'4'
'179'
'178'
'45'
'171'
['4', '179', '178', '45', '171']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="84" ssid = "13">The random baseline for choosing the predominant sense over all these words ( ) is 32%.</S><S sid ="88" ssid = "17">The first sense in SemCor provides an upperbound for this task of 67%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'100'", "'116'", "'84'", "'88'"]
'5'
'100'
'116'
'84'
'88'
['5', '100', '116', '84', '88']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="121" ssid = "19">For these one would need to obtain more sense-tagged text in order to use this heuristic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'126'", "'178'", "'13'", "'121'"]
'6'
'126'
'178'
'13'
'121'
['6', '126', '178', '13', '121']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S><S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S><S sid ="158" ssid = "6">We have evaluated our method using publically available resources  both for balanced and domain specific text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'184'", "'178'", "'126'", "'158'"]
'7'
'184'
'178'
'126'
'158'
['7', '184', '178', '126', '158']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="172" ssid = "20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet  for example those that use it for lexical acquisition or WSD.</S><S sid ="115" ssid = "13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor  which is very encouraging given that our method only uses raw text  with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'154'", "'14'", "'172'", "'115'"]
'8'
'154'
'14'
'172'
'115'
['8', '154', '14', '172', '115']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "2">This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).</S><S sid ="102" ssid = "31">We test this below on the SENSEVAL-2 English all-words data.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'102'", "'116'", "'100'", "'5'"]
'9'
'102'
'116'
'100'
'5'
['9', '102', '116', '100', '5']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="180" ssid = "3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S><S sid ="109" ssid = "7">We generated a thesaurus entry for all polysemous nouns in WordNet as described in section 2.1 above.</S><S sid ="112" ssid = "10">We compare results using the first sense listed in SemCor  and the first sense according to the SENSEVAL-2 English all-words test data itself.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'180'", "'142'", "'109'", "'112'"]
'10'
'180'
'142'
'109'
'112'
['10', '180', '142', '109', '112']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "4">Senses that have not occurred in SemCor are ordered arbitrarily and after those senses of the word that have occurred.</S><S sid ="120" ssid = "18">There were a similar number of words that were not covered by a predominant sense in SemCor.</S><S sid ="42" ssid = "35">The neighbours for a word in a thesaurus are words themselves  rather than senses.</S><S sid ="189" ssid = "12">Additionally  we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity  and what can be done to combat this problem using relation specific thesauruses.</S><S sid ="117" ssid = "15">The items that were not covered by our method were those with insufficient grammatical relations for the tuples employed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'120'", "'42'", "'189'", "'117'"]
'11'
'120'
'42'
'189'
'117'
['11', '120', '42', '189', '117']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "5">The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor  from those that do not (without HTD).</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'123'", "'153'", "'165'", "'178'"]
'12'
'123'
'153'
'165'
'178'
['12', '123', '153', '165', '178']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="84" ssid = "13">The random baseline for choosing the predominant sense over all these words ( ) is 32%.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'116'", "'147'", "'84'", "'126'"]
'13'
'116'
'147'
'84'
'126'
['13', '116', '147', '84', '126']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="182" ssid = "5">In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.</S><S sid ="21" ssid = "14">We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off (Wilks and Stevenson  1998; Hoste et al.  2001) and for systems that use it in lexical acquisition (McCarthy  1997; Merlo and Leybold  2001; Korhonen  2002) because of the limited size of hand-tagged resources.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'8'", "'154'", "'182'", "'21'"]
'14'
'8'
'154'
'182'
'21'
['14', '8', '154', '182', '21']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="3" ssid = "3">Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'154'", "'3'", "'8'", "'170'"]
'15'
'154'
'3'
'8'
'170'
['15', '154', '3', '8', '170']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">SemCor comprises a relatively small sample of 250 000 words.</S><S sid ="131" ssid = "8">The SPORTS corpus consists of 35317 documents (about 9.1 million words).</S><S sid ="140" ssid = "17">The resulting set consisted of 38 words.</S><S sid ="132" ssid = "9">The FINANCE corpus consists of 117734 documents (about 32.5 million words).</S><S sid ="76" ssid = "5">The jcn measure uses corpus data for the calculation of IC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'131'", "'140'", "'132'", "'76'"]
'16'
'131'
'140'
'132'
'76'
['16', '131', '140', '132', '76']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "10">There are words where the first sense in WordNet is counter-intuitive  because of the size of the corpus  and because where the frequency data does not indicate a first sense  the ordering is arbitrary.</S><S sid ="182" ssid = "5">In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.</S><S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="99" ssid = "28">Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'182'", "'13'", "'99'", "'170'"]
'17'
'182'
'13'
'99'
'170'
['17', '182', '13', '99', '170']
parsed_discourse_facet ['method_citation']



P04-1036
P06-1012
0
method_citation
['results_citation']
parsing: input/ref/Task1/W99-0623_swastika.csv
<S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['aim_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['result_citation']
<S sid="91" ssid="20">Features and context were initially introduced into the models, but they refused to offer any gains in performance.</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



['139']
139
['139']
parsed_discourse_facet ['result_citation']
<S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['method_citation']
<S sid="103" ssid="32">The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.</S>
original cit marker offset is 0
new cit marker offset is 0



['103']
103
['103']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



['139']
139
['139']
parsed_discourse_facet ['result_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
<S sid="140" ssid="2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
<S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['result_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/W99-0623.csv
<S sid ="1" ssid = "1">Three state-of-the-art statistical parsers are combined to produce more accurate parses  as well as new bounds on achievable Treebank parsing accuracy.</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="129" ssid = "58">In the interest of testing the robustness of these combining techniques  we added a fourth  simple nonlexicalized PCFG parser.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'72'", "'38'", "'129'", "'130'"]
'1'
'72'
'38'
'129'
'130'
['1', '72', '38', '129', '130']
Error in Discourse Facet
<S sid ="2" ssid = "2">Two general approaches are presented and two combination techniques are described for each approach.</S><S sid ="59" ssid = "45">Once again we present both a non-parametric and a parametric technique for this task.</S><S sid ="3" ssid = "3">Both parametric and non-parametric models are explored.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'59'", "'3'", "'85'", "'139'"]
'2'
'59'
'3'
'85'
'139'
['2', '59', '3', '85', '139']
Error in Discourse Facet
<S sid ="3" ssid = "3">Both parametric and non-parametric models are explored.</S><S sid ="59" ssid = "45">Once again we present both a non-parametric and a parametric technique for this task.</S><S sid ="2" ssid = "2">Two general approaches are presented and two combination techniques are described for each approach.</S><S sid ="138" ssid = "67">Surprisingly  the non-parametric switching technique also exhibited robust behaviour in this situation.</S><S sid ="136" ssid = "65">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'59'", "'2'", "'138'", "'136'"]
'3'
'59'
'2'
'138'
'136'
['3', '59', '2', '138', '136']
Error in Discourse Facet
<S sid ="4" ssid = "4">The resulting parsers surpass the best previously published performance results for the Penn Treebank.</S><S sid ="76" ssid = "5">The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="134" ssid = "63">As seen by the drop in average individual parser performance baseline  the introduced parser does not perform very well.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'76'", "'85'", "'130'", "'134'"]
'4'
'76'
'85'
'130'
'134'
['4', '76', '85', '130', '134']
Error in Discourse Facet
<S sid ="5" ssid = "1">The natural language processing community is in the strong position of having many available approaches to solving some of its most fundamental problems.</S><S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid ="26" ssid = "12">This technique has the advantage of requiring no training  but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'6'", "'12'", "'13'", "'26'"]
'5'
'6'
'12'
'13'
'26'
['5', '6', '12', '13', '26']
Error in Discourse Facet
<S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="108" ssid = "37">From this we see that a finer-grained model for parser combination  at least for the features we have examined  will not give us any additional power.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'11'", "'9'", "'12'", "'108'"]
'6'
'11'
'9'
'12'
'108'
['6', '11', '9', '12', '108']
Error in Discourse Facet
<S sid ="7" ssid = "3">Their theoretical finding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent  accurate classifiers.</S><S sid ="30" ssid = "16">This is equivalent to the assumption used in probability estimation for naïve Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.</S><S sid ="116" ssid = "45">The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="41" ssid = "27">IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'30'", "'116'", "'38'", "'41'"]
'7'
'30'
'116'
'38'
'41'
['7', '30', '116', '38', '41']
Error in Discourse Facet
<S sid ="8" ssid = "4">The theory has also been validated empirically.</S><S sid ="36" ssid = "22">The estimation of the probabilities in the model is carried out as shown in Equation 4.</S><S sid ="83" ssid = "12">We performed three experiments to evaluate our techniques.</S><S sid ="121" ssid = "50">Table 3 contains the results for evaluating our systems on the test set (section 22).</S><S sid ="109" ssid = "38">The results in Table 2 were achieved on the development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'36'", "'83'", "'121'", "'109'"]
'8'
'36'
'83'
'121'
'109'
['8', '36', '83', '121', '109']
Error in Discourse Facet
<S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="88" ssid = "17">For example  one parser could be more accurate at predicting noun phrases than the other parsers.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'55'", "'88'", "'21'", "'139'"]
'9'
'55'
'88'
'21'
'139'
['9', '55', '88', '21', '139']
Error in Discourse Facet
<S sid ="10" ssid = "6">In both cases the investigators were able to achieve significant improvements over the previous best tagging results.</S><S sid ="136" ssid = "65">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="141" ssid = "3">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid ="134" ssid = "63">As seen by the drop in average individual parser performance baseline  the introduced parser does not perform very well.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'136'", "'85'", "'141'", "'134'"]
'10'
'136'
'85'
'141'
'134'
['10', '136', '85', '141', '134']
Error in Discourse Facet
<S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'9'", "'6'", "'12'", "'13'"]
'11'
'9'
'6'
'12'
'13'
['11', '9', '6', '12', '13']
Error in Discourse Facet
<S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'13'", "'11'", "'6'", "'72'"]
'12'
'13'
'11'
'6'
'72'
['12', '13', '11', '6', '72']
Error in Discourse Facet
<S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="4" ssid = "4">The resulting parsers surpass the best previously published performance results for the Penn Treebank.</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'12'", "'4'", "'72'", "'130'"]
'13'
'12'
'4'
'72'
'130'
['13', '12', '4', '72', '130']
Error in Discourse Facet
<S sid ="14" ssid = "10">We used these three parsers to explore parser combination techniques.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="126" ssid = "55">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S><S sid ="140" ssid = "2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'139'", "'85'", "'126'", "'140'"]
'14'
'139'
'85'
'126'
'140'
['14', '139', '85', '126', '140']
Error in Discourse Facet
<S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="88" ssid = "17">For example  one parser could be more accurate at predicting noun phrases than the other parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'55'", "'27'", "'85'", "'88'"]
'15'
'55'
'27'
'85'
'88'
['15', '55', '27', '85', '88']
Error in Discourse Facet
<S sid ="16" ssid = "2">We call this approach parse hybridization.</S><S sid ="58" ssid = "44">We call this approach parser switching.</S><S sid ="23" ssid = "9">We call this technique constituent voting.</S><S sid ="96" ssid = "25">We call such a constituent an isolated constituent.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'58'", "'23'", "'96'", "'139'"]
'16'
'58'
'23'
'96'
'139'
['16', '58', '23', '96', '139']
Error in Discourse Facet
<S sid ="17" ssid = "3">The substructures that are unanimously hypothesized by the parsers should be preserved after combination  and the combination technique should not foolishly create substructures for which there is no supporting evidence.</S><S sid ="49" ssid = "35">In general  the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.</S><S sid ="117" ssid = "46">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S><S sid ="125" ssid = "54">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S><S sid ="95" ssid = "24">One side of the decision making process is when we choose to believe a constituent should be in the parse  even though only one parser suggests it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'49'", "'117'", "'125'", "'95'"]
'17'
'49'
'117'
'125'
'95'
['17', '49', '117', '125', '95']
Error in Discourse Facet
parsing: input/ref/Task1/W06-2932_swastika.csv
<S sid="86" ssid="8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S>
original cit marker offset is 
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="54" ssid="2">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



['54']
54
['54']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



['12']
12
['12']
parsed_discourse_facet ['method_citation']
    <S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



['41']
41
['41']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="6">Its power lies in the ability to define a rich set of features over parsing decisions, as well as surface level features relative to these decisions.</S>
original cit marker offset is 0
new cit marker offset is 0



['24']
24
['24']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



['12']
12
['12']
parsed_discourse_facet ['method_citation']
<S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



['43']
43
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S sid ="1" ssid = "1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S><S sid ="9" ssid = "5">Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S><S sid ="23" ssid = "5">That system uses MIRA  an online large-margin learning algorithm  to compute model parameters.</S><S sid ="27" ssid = "9">We augmented this model to incorporate morphological features derived from each token.</S>
original cit marker offset is nan
new cit marker offset is 0



["'1'", "'9'", "'107'", "'23'", "'27'"]
'1'
'9'
'107'
'23'
'27'
['1', '9', '107', '23', '27']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="14" ssid = "10">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S><S sid ="58" ssid = "6">These results show that the discriminative spanning tree parsing framework (McDonald et al.  2005b; McDonald and Pereira  2006) is easily adapted across all these languages.</S><S sid ="25" ssid = "7">For instance  the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'20'", "'14'", "'58'", "'25'"]
'2'
'20'
'14'
'58'
'25'
['2', '20', '14', '58', '25']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S><S sid ="19" ssid = "1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S><S sid ="109" ssid = "6">The current system simply includes all morphological bi-gram features.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="51" ssid = "20">Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'19'", "'109'", "'10'", "'51'"]
'3'
'19'
'109'
'10'
'51'
['3', '19', '109', '10', '51']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.</S><S sid ="53" ssid = "1">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).</S><S sid ="44" ssid = "13">We use the MIRA online learner to set the weights (Crammer and Singer  2003; McDonald et al.  2005a) since we found it trained quickly and provide good performance.</S><S sid ="13" ssid = "9">We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajiˇc et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B¨ohmov´a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dˇzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).</S><S sid ="56" ssid = "4">Results on the test set are given in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'53'", "'44'", "'13'", "'56'"]
'4'
'53'
'44'
'13'
'56'
['4', '53', '44', '13', '56']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="24" ssid = "6">Its power lies in the ability to define a rich set of features over parsing decisions  as well as surface level features relative to these decisions.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'10'", "'88'", "'24'", "'107'"]
'5'
'10'
'88'
'24'
'107'
['5', '10', '88', '24', '107']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.</S><S sid ="14" ssid = "10">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'14'", "'7'", "'36'", "'8'"]
'6'
'14'
'7'
'36'
'8'
['6', '14', '7', '36', '8']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="110" ssid = "7">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="26" ssid = "8">These features are highly important to overall accuracy since they eliminate unlikely scenarios such as a preposition modifying a noun not directly to its left  or a noun modifying a verb with another verb occurring between them.</S><S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'110'", "'11'", "'26'", "'8'"]
'7'
'110'
'11'
'26'
'8'
['7', '110', '11', '26', '8']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="30" ssid = "12">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="6" ssid = "2">With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'7'", "'30'", "'11'", "'6'"]
'8'
'7'
'30'
'11'
'6'
['8', '7', '30', '11', '6']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="5" ssid = "1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S><S sid ="25" ssid = "7">For instance  the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'20'", "'88'", "'5'", "'25'"]
'9'
'20'
'88'
'5'
'25'
['9', '20', '88', '5', '25']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="5" ssid = "1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S><S sid ="24" ssid = "6">Its power lies in the ability to define a rich set of features over parsing decisions  as well as surface level features relative to these decisions.</S><S sid ="34" ssid = "3">However  the parser is fundamentally limited by the scope of local factorizations that make inference tractable.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'5'", "'24'", "'34'", "'107'"]
'10'
'5'
'24'
'34'
'107'
['10', '5', '24', '34', '107']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="110" ssid = "7">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S><S sid ="45" ssid = "14">Furthermore  it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira  2006).</S><S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'7'", "'110'", "'45'", "'8'"]
'11'
'7'
'110'
'45'
'8'
['11', '7', '110', '45', '8']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S sid ="95" ssid = "17">Note the difference in error between the unlabeled parser and the edge labeler: the former makes mistakes on edges into prepositions  conjunctions and verbs  and the latter makes mistakes on edges into nouns (subject/objects).</S><S sid ="110" ssid = "7">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S><S sid ="69" ssid = "7">However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).</S><S sid ="106" ssid = "3">First  we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'95'", "'110'", "'69'", "'106'"]
'12'
'95'
'110'
'69'
'106'
['12', '95', '110', '69', '106']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajiˇc et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B¨ohmov´a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dˇzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).</S><S sid ="54" ssid = "2">Based on performance from a held-out section of the training data  we used non-projective parsing algorithms for Czech  Danish  Dutch  German  Japanese  Portuguese and Slovene  and projective parsing algorithms for Arabic  Bulgarian  Chinese  Spanish  Swedish and Turkish.</S><S sid ="4" ssid = "4">We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.</S><S sid ="53" ssid = "1">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).</S><S sid ="14" ssid = "10">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'54'", "'4'", "'53'", "'14'"]
'13'
'54'
'4'
'53'
'14'
['13', '54', '4', '53', '14']
parsed_discourse_facet ['method_citation']



W06-2932
D07-1015
0
method_citation
['method_citation']
parsing: input/ref/Task1/J01-2004_sweta.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["372'"]
372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="28">The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.</S>
    <S sid="41" ssid="29">There will also be a brief review of previous work using syntactic information for language modeling, before we introduce our model in Section 4.</S>
    <S sid="42" ssid="30">Three parse trees: (a) a complete parse tree; (b) a complete parse tree with an explicit stop symbol; and (c) a partial parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["40'", "'41'", "'42'"]
40'
'41'
'42'
['40', '41', '42']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="13">A parser that is not left to right, but which has rooted derivations, e.g., a headfirst parser, will be able to calculate generative joint probabilities for entire strings; however, it will not be able to calculate probabilities for each word conditioned on previously generated words, unless each derivation generates the words in the string in exactly the same order.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="364" ssid="120">We follow Chelba (2000) in dealing with this problem: for parsing purposes, we use the Penn Treebank tokenization; for interpolation with the provided trigram model, and for evaluation, the lattice tokenization is used.</S>
original cit marker offset is 0
new cit marker offset is 0



["364'"]
364'
['364']
parsed_discourse_facet ['method_citation']
<S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



["302'"]
302'
['302']
parsed_discourse_facet ['method_citation']
 <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="231" ssid="135">Since we do not know the POS for the word, we must sum the LAP for all POS For a PCFG G, a stack S = Ao An$ (which we will write AN and a look-ahead terminal item wi, we define the look-ahead probability as follows: We recursively estimate this with two empirically observed conditional probabilities for every nonterminal A,: 13(A, w,a) and P(A, c).</S>
original cit marker offset is 0
new cit marker offset is 0



["231'"]
231'
['231']
parsed_discourse_facet ['method_citation']
<S sid="297" ssid="53">The differences between a k-best and a beam-search parser (not to mention the use of dynamic programming) make a running time difference unsurprising.</S>
original cit marker offset is 0
new cit marker offset is 0



["297'"]
297'
['297']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="37">Statistically based heuristic best-first or beam-search strategies (Caraballo and Charniak 1998; Charniak, Goldwater, and Johnson 1998; Goodman 1997) have yielded an enormous improvement in the quality and speed of parsers, even without any guarantee that the parse returned is, in fact, that with the maximum likelihood for the probability model.</S>
original cit marker offset is 0
new cit marker offset is 0



["133'"]
133'
['133']
parsed_discourse_facet ['method_citation']
<S sid="291" ssid="47">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["291'"]
291'
['291']
parsed_discourse_facet ['method_citation']
<S sid="355" ssid="111">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.</S>
original cit marker offset is 0
new cit marker offset is 0



["355'"]
355'
['355']
parsed_discourse_facet ['method_citation']
<S sid="59" ssid="17">A PCFG is a CFG with a probability assigned to each rule; specifically, each righthand side has a probability given the left-hand side of the rule.</S>
original cit marker offset is 0
new cit marker offset is 0



["59'"]
59'
['59']
parsed_discourse_facet ['method_citation']
<S sid="100" ssid="4">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="12">Another approach that uses syntactic structure for language modeling has been to use a shift-reduce parser to &amp;quot;surface&amp;quot; c-commanding phrasal headwords or part-of-speech (POS) tags from arbitrarily far back in the prefix string, for use in a trigram-like model.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S sid ="1" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="7" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="209" ssid = "113">This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).</S><S sid ="132" ssid = "36">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid ="277" ssid = "33">The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989 860 words  39 832 sentences) of the Penn Treebank (Marcus  Santorini  and Marcinkiewicz 1993) served as the training data  Section 24 (34 199 words  1 346 sentences) as the held-out data for parameter estimation  and Section 23 (59 100 words  2 416 sentences) as the test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'7'", "'209'", "'132'", "'277'"]
'1'
'7'
'209'
'132'
'277'
['1', '7', '209', '132', '277']
parsed_discourse_facet ['aim_citation']
<S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="41" ssid = "29">There will also be a brief review of previous work using syntactic information for language modeling  before we introduce our model in Section 4.</S><S sid ="18" ssid = "6">Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture long-distance dependencies for language modeling will be reviewed.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'2'", "'41'", "'18'", "'4'"]
'8'
'2'
'41'
'18'
'4'
['8', '2', '41', '18', '4']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="9" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'390'", "'10'", "'4'"]
'3'
'9'
'390'
'10'
'4'
['3', '9', '390', '10', '4']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="100" ssid = "4">The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'100'", "'2'", "'8'"]
'10'
'4'
'100'
'2'
'8'
['10', '4', '100', '2', '8']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="11" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="379" ssid = "135">While the improvements over the trigram model in these trials are modest  they do indicate that our model is robust enough to provide good information even in the face of noisy input.</S><S sid ="300" ssid = "56">The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'11'", "'379'", "'300'", "'404'"]
'5'
'11'
'379'
'300'
'404'
['5', '11', '379', '300', '404']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="6" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="39" ssid = "27">The top-down guidance that is provided makes this approach quite efficient in practice.</S><S sid ="388" ssid = "1">The empirical results presented above are quite encouraging  and the potential of this kind of approach both for parsing and language modeling seems very promising.</S><S sid ="114" ssid = "18">He was able to reduce both sentence and word error rates on the ATIS corpus using this method.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'6'", "'39'", "'388'", "'114'"]
'12'
'6'
'39'
'388'
'114'
['12', '6', '39', '388', '114']
parsed_discourse_facet ['method_citation']
<S sid ="1" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="7" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="209" ssid = "113">This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).</S><S sid ="132" ssid = "36">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid ="277" ssid = "33">The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989 860 words  39 832 sentences) of the Penn Treebank (Marcus  Santorini  and Marcinkiewicz 1993) served as the training data  Section 24 (34 199 words  1 346 sentences) as the held-out data for parameter estimation  and Section 23 (59 100 words  2 416 sentences) as the test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'7'", "'209'", "'132'", "'277'"]
'1'
'7'
'209'
'132'
'277'
['1', '7', '209', '132', '277']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="41" ssid = "29">There will also be a brief review of previous work using syntactic information for language modeling  before we introduce our model in Section 4.</S><S sid ="18" ssid = "6">Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture long-distance dependencies for language modeling will be reviewed.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'2'", "'41'", "'18'", "'4'"]
'8'
'2'
'41'
'18'
'4'
['8', '2', '41', '18', '4']
parsed_discourse_facet ['results_citation']
<S sid ="3" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="9" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'390'", "'10'", "'4'"]
'3'
'9'
'390'
'10'
'4'
['3', '9', '390', '10', '4']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="100" ssid = "4">The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'100'", "'2'", "'8'"]
'10'
'4'
'100'
'2'
'8'
['10', '4', '100', '2', '8']
parsed_discourse_facet ['method_citation']
<S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="11" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="379" ssid = "135">While the improvements over the trigram model in these trials are modest  they do indicate that our model is robust enough to provide good information even in the face of noisy input.</S><S sid ="300" ssid = "56">The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'11'", "'379'", "'300'", "'404'"]
'5'
'11'
'379'
'300'
'404'
['5', '11', '379', '300', '404']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="6" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="39" ssid = "27">The top-down guidance that is provided makes this approach quite efficient in practice.</S><S sid ="388" ssid = "1">The empirical results presented above are quite encouraging  and the potential of this kind of approach both for parsing and language modeling seems very promising.</S><S sid ="114" ssid = "18">He was able to reduce both sentence and word error rates on the ATIS corpus using this method.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'6'", "'39'", "'388'", "'114'"]
'12'
'6'
'39'
'388'
'114'
['12', '6', '39', '388', '114']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "1">With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.</S><S sid ="14" ssid = "2">Perhaps one reason for this is that  until relatively recently  few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.</S><S sid ="15" ssid = "3">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S><S sid ="383" ssid = "139">It was selected with the goal of high parser accuracy; but in this new domain  parser accuracy is a secondary measure of performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'14'", "'15'", "'404'", "'383'"]
'13'
'14'
'15'
'404'
'383'
['13', '14', '15', '404', '383']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "2">Perhaps one reason for this is that  until relatively recently  few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.</S><S sid ="13" ssid = "1">With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.</S><S sid ="300" ssid = "56">The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.</S><S sid ="15" ssid = "3">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid ="319" ssid = "75">The hope  however  is that the improved parsing model provided by our conditional probability model will cause the distribution over structures to be more peaked  thus enabling us to capture more of the total probability mass  and making this a fairly snug upper bound on the perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'13'", "'300'", "'15'", "'319'"]
'14'
'13'
'300'
'15'
'319'
['14', '13', '300', '15', '319']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "3">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid ="97" ssid = "1">There have been attempts to jump over adjacent words to words farther back in the left context  without the use of dependency links or syntactic structure  for example Saul and Pereira (1997) and Rosenfeld (1996  1997).</S><S sid ="13" ssid = "1">With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.</S><S sid ="38" ssid = "26">This has lead us to a formulation of the conditional probability model in terms of values returned from tree-walking functions that themselves are contextually sensitive.</S><S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'97'", "'13'", "'38'", "'5'"]
'15'
'97'
'13'
'38'
'5'
['15', '97', '13', '38', '5']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "4">While language models built around shallow local dependencies are still the standard in state-of-the-art speech recognition systems  there is reason to hope that better language models can and will be developed by computational linguists for this task.</S><S sid ="397" ssid = "10">A top-down parser  in contrast to a standard bottom-up chart parser  has enough information to predict empty categories only where they are likely to occur.</S><S sid ="139" ssid = "43">This on-line characteristic allows our language model to be interpolated on a word-by-word basis with other models  such as the trigram  yielding further improvements.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'397'", "'139'", "'4'", "'10'"]
'16'
'397'
'139'
'4'
'10'
['16', '397', '139', '4', '10']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "5">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S sid ="132" ssid = "36">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid ="88" ssid = "46">This section will briefly introduce language modeling for statistical speech recognition.'</S><S sid ="40" ssid = "28">The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.</S><S sid ="247" ssid = "3">Perplexity is a standard measure within the speech recognition community for comparing language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'132'", "'88'", "'40'", "'247'"]
'17'
'132'
'88'
'40'
'247'
['17', '132', '88', '40', '247']
parsed_discourse_facet ['method_citation']



J01-2004
P08-1013
0
method_citation
['method_citation']
parsing: input/ref/Task1/P05-1013_vardha.csv
 <S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
  <S sid="9" ssid="5">This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
  <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
    <S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
 <S sid="62" ssid="1">In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al., 2004) and English (Nivre and Scholz, 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="23" ssid="19">By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
    <S sid="96" ssid="7">With respect to exact match, the improvement is even more noticeable, which shows quite clearly that even if non-projective dependencies are rare on the token level, they are nevertheless important for getting the global syntactic structure correct.</S>
original cit marker offset is 0
new cit marker offset is 0



["'96'"]
'96'
['96']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
  <S sid="40" ssid="11">Even this may be nondeterministic, in case the graph contains several non-projective arcs whose lifts interact, but we use the following algorithm to construct a minimal projective transformation D0 = (W, A0) of a (nonprojective) dependency graph D = (W, A): The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right).</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
  <S sid="7" ssid="3">From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="14" ssid="10">While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
 <S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'"]
'49'
['49']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P05-1013.csv
<S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="13" ssid = "9">The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="16" ssid = "12">Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.</S><S sid ="80" ssid = "7">As shown in Table 3  the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'13'", "'7'", "'16'", "'80'"]
'1'
'13'
'7'
'16'
'80'
['1', '13', '7', '16', '80']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="44" ssid = "15">Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="30" ssid = "1">We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.</S><S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'44'", "'36'", "'30'", "'8'"]
'2'
'44'
'36'
'30'
'8'
['2', '44', '36', '30', '8']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy.</S><S sid ="28" ssid = "24">First  in section 4  we evaluate the graph transformation techniques in themselves  with data from the Prague Dependency Treebank and the Danish Dependency Treebank.</S><S sid ="29" ssid = "25">In section 5  we then evaluate the entire parsing system by training and evaluating on data from the Prague Dependency Treebank.</S><S sid ="62" ssid = "1">In the experiments below  we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs 3 previously tested on Swedish (Nivre et al.  2004) and English (Nivre and Scholz  2004).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'28'", "'29'", "'62'", "'75'"]
'3'
'28'
'29'
'62'
'75'
['3', '28', '29', '62', '75']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'104'", "'106'", "'78'", "'8'"]
'4'
'104'
'106'
'78'
'8'
['4', '104', '106', '78', '8']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order  where discontinuous syntactic constructions are more common than in languages like English (Mel’ˇcuk  1988; Covington  1990).</S><S sid ="94" ssid = "5">The first thing to note is that projectivizing helps in itself  even if no encoding is used  as seen from the fact that the projective baseline outperforms the non-projective training condition by more than half a percentage point on attachment score  although the gain is much smaller with respect to exact match.</S><S sid ="99" ssid = "10">This may seem surprising  given the experiments reported in section 4  but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift  where the encoding of path information is often redundant.</S><S sid ="9" ssid = "5">This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).</S><S sid ="47" ssid = "18">In practice  we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'94'", "'99'", "'9'", "'47'"]
'5'
'94'
'99'
'9'
'47'
['5', '94', '99', '9', '47']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.</S><S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="44" ssid = "15">Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.</S><S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'2'", "'36'", "'44'", "'89'"]
'6'
'2'
'36'
'44'
'89'
['6', '2', '36', '44', '89']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="16" ssid = "12">Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.</S><S sid ="19" ssid = "15">Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).</S><S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="47" ssid = "18">In practice  we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'16'", "'19'", "'1'", "'47'"]
'7'
'16'
'19'
'1'
'47'
['7', '16', '19', '1', '47']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S><S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="30" ssid = "1">We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'2'", "'104'", "'4'", "'30'"]
'8'
'2'
'104'
'4'
'30'
['8', '2', '104', '4', '30']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).</S><S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="17" ssid = "13">There exist a few robust broad-coverage parsers that produce non-projective dependency structures  notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English  Foth et al. (2004) for German  and Holan (2004) for Czech.</S><S sid ="105" ssid = "16">Although the best published results for the Collins parser is 80% UAS (Collins  1999)  this parser reaches 82% when trained on the entire training data set  and an adapted version of Charniak’s parser (Charniak  2000) performs at 84% (Jan Hajiˇc  pers. comm.).</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'10'", "'17'", "'105'", "'24'"]
'9'
'10'
'17'
'105'
'24'
['9', '10', '17', '105', '24']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="42" ssid = "13">Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="12" ssid = "8">Prague Dependency Treebank (Hajiˇc et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'42'", "'36'", "'4'", "'12'"]
'10'
'42'
'36'
'4'
'12'
['10', '42', '36', '4', '12']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="6" ssid = "2">However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'6'", "'78'", "'10'", "'36'"]
'11'
'6'
'78'
'10'
'36'
['11', '6', '78', '10', '36']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Prague Dependency Treebank (Hajiˇc et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S><S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="42" ssid = "13">Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'10'", "'36'", "'42'", "'78'"]
'12'
'10'
'36'
'42'
'78'
['12', '10', '36', '42', '78']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.</S><S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'2'", "'95'", "'1'", "'104'"]
'13'
'2'
'95'
'1'
'104'
['13', '2', '95', '1', '104']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 15–25%  the total proportion of non-projective arcs is normally only 1–2%.</S><S sid ="80" ssid = "7">As shown in Table 3  the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S><S sid ="100" ssid = "11">It is likely that the more complex cases  where path information could make a difference  are beyond the reach of the parser in most cases.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'80'", "'106'", "'81'", "'100'"]
'14'
'80'
'106'
'81'
'100'
['14', '80', '106', '81', '100']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">As long as the main evaluation metric is dependency accuracy per word  with state-of-the-art accuracy mostly below 90%  the penalty for not handling non-projective constructions is almost negligible.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="101" ssid = "12">However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.</S><S sid ="110" ssid = "2">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="105" ssid = "16">Although the best published results for the Collins parser is 80% UAS (Collins  1999)  this parser reaches 82% when trained on the entire training data set  and an adapted version of Charniak’s parser (Charniak  2000) performs at 84% (Jan Hajiˇc  pers. comm.).</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'106'", "'101'", "'110'", "'105'"]
'15'
'106'
'101'
'110'
'105'
['15', '106', '101', '110', '105']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="110" ssid = "2">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="19" ssid = "15">Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'7'", "'1'", "'110'", "'19'"]
'16'
'7'
'1'
'110'
'19'
['16', '7', '1', '110', '19']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'104'", "'106'", "'78'", "'8'"]
'4'
'104'
'106'
'78'
'8'
['4', '104', '106', '78', '8']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">In addition  there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington  1990; Kahane et al.  1998; Duchier and Debusmann  2001; Holan et al.  2001; Hellwig  2003).</S><S sid ="17" ssid = "13">There exist a few robust broad-coverage parsers that produce non-projective dependency structures  notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English  Foth et al. (2004) for German  and Holan (2004) for Czech.</S><S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S><S sid ="100" ssid = "11">It is likely that the more complex cases  where path information could make a difference  are beyond the reach of the parser in most cases.</S><S sid ="109" ssid = "1">We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'17'", "'20'", "'100'", "'109'"]
'18'
'17'
'20'
'100'
'109'
['18', '17', '20', '100', '109']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "15">Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="13" ssid = "9">The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.</S><S sid ="107" ssid = "18">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'7'", "'95'", "'13'", "'107'"]
'19'
'7'
'95'
'13'
'107'
['19', '7', '95', '13', '107']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S><S sid ="109" ssid = "1">We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'109'", "'24'", "'60'", "'95'"]
'20'
'109'
'24'
'60'
'95'
['20', '109', '24', '60', '95']
parsed_discourse_facet ['aim_citation']



P05-1013
P12-3029
0
method_citation
['aim_citation']
parsing: input/ref/Task1/W99-0613_aakansha.csv
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="30">Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'"]
'137'
['137']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="12">2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="4">The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="12">But we will show that the use of unlabeled data can drastically reduce the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
    <S sid="237" ssid="4">The numbers falling into the location, person, organization categories were 186, 289 and 402 respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'", "'237'"]
'236'
'237'
['236', '237']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
    <S sid="10" ssid="4">The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'10'"]
'9'
'10'
['9', '10']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S sid="139" ssid="6">This section describes AdaBoost, which is the basis for the CoBoost algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'39'"]
'137'
'39'
['137', '39']
parsed_discourse_facet ['method_citation']
<S sid="26" ssid="20">We present two algorithms.</S>
    <S sid="27" ssid="21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'27'"]
'26'
'27'
['26', '27']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="12">But we will show that the use of unlabeled data can drastically reduce the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="2">Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S>
    <S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'9'"]
'8'
'9'
['8', '9']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="1" ssid = "1">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="9" ssid = "3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="136" ssid = "3">We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'250'", "'136'", "'163'"]
'1'
'9'
'250'
'136'
'163'
['1', '9', '250', '136', '163']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.</S><S sid ="17" ssid = "11">At first glance  the problem seems quite complex: a large number of rules is needed to cover the domain  suggesting that a large number of labeled examples is required to train an accurate classifier.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="105" ssid = "38">Thus the method makes the fairly strong assumption that the features can be partitioned into two types such that each type alone is sufficient for classification.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'17'", "'250'", "'105'", "'248'"]
'2'
'17'
'250'
'105'
'248'
['2', '17', '250', '105', '248']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid ="92" ssid = "25">Thus an explicit assumption about the redundancy of the features — that either the spelling or context alone should be sufficient to build a classifier — has been built into the algorithm.</S><S sid ="121" ssid = "54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).</S><S sid ="58" ssid = "12">Having found (spelling  context) pairs in the parsed data  a number of features are extracted.</S><S sid ="232" ssid = "11">For example  the independence assumptions mean that the model fails to capture the dependence between specific and more general features (for example the fact that the feature full.-string=New_York is always seen with the features contains (New) and The baseline method tags all entities as the most frequent class type (organization). contains (York) and is never seen with a feature such as contains (Group) ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'92'", "'121'", "'58'", "'232'"]
'3'
'92'
'121'
'58'
'232'
['3', '92', '121', '58', '232']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "20">We present two algorithms.</S><S sid ="4" ssid = "4">We present two algorithms.</S><S sid ="68" ssid = "1">The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S><S sid ="88" ssid = "21">We can now compare this algorithm to that of (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'4'", "'68'", "'163'", "'88'"]
'26'
'4'
'68'
'163'
'88'
['26', '4', '68', '163', '88']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid ="6" ssid = "6">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="68" ssid = "1">The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'31'", "'6'", "'97'", "'68'"]
'5'
'31'
'6'
'97'
'68'
['5', '31', '6', '97', '68']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "6">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid ="5" ssid = "5">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="30" ssid = "24">(Blum and Mitchell 98) offer a promising formulation of redundancy  also prove some results about how the use of unlabeled examples can help classification  and suggest an objective function when training with unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'31'", "'5'", "'204'", "'30'"]
'6'
'31'
'5'
'204'
'30'
['6', '31', '5', '204', '30']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision  in the form of labeled training examples.</S><S sid ="2" ssid = "2">A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="3" ssid = "3">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid ="130" ssid = "63">So the success of the algorithm may well be due to its success in maximizing the number of unlabeled examples on which the two decision lists agree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'2'", "'250'", "'3'", "'130'"]
'7'
'2'
'250'
'3'
'130'
['7', '2', '250', '3', '130']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "2">Recent results (e.g.  (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S><S sid ="27" ssid = "21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S><S sid ="120" ssid = "53">(Blum and Mitchell 98) go on to give PAC results for learning in the cotraining case.</S><S sid ="40" ssid = "34">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid ="88" ssid = "21">We can now compare this algorithm to that of (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'27'", "'120'", "'40'", "'88'"]
'8'
'27'
'120'
'40'
'88'
['8', '27', '120', '40', '88']
parsed_discourse_facet ['results_citation']
<S sid ="1" ssid = "1">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="9" ssid = "3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="136" ssid = "3">We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'250'", "'136'", "'163'"]
'1'
'9'
'250'
'136'
'163'
['1', '9', '250', '136', '163']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "4">The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.</S><S sid ="147" ssid = "14">The weak learner for two-class problems computes a weak hypothesis h from the input space into the reals (h : 2x -4 R)  where the sign4 of h(x) is interpreted as the predicted label and the magnitude I h(x)I is the confidence in the prediction: large numbers for I h(x)I indicate high confidence in the prediction  and numbers close to zero indicate low confidence.</S><S sid ="35" ssid = "29">AdaBoost finds a weighted combination of simple (weak) classifiers  where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.</S><S sid ="224" ssid = "3">The model was parameterized such that the joint probability of a (label  feature-set) pair P(yi  xi) is written as The model assumes that (y  x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).</S><S sid ="74" ssid = "7">Output of the learning algorithm: a function h:Xxy [0  1] where h(x  y) is an estimate of the conditional probability p(y1x) of seeing label y given that feature x is present.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'147'", "'35'", "'224'", "'74'"]
'10'
'147'
'35'
'224'
'74'
['10', '147', '35', '224', '74']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "5">For example  a good classifier would identify Mrs. Frank as a person  Steptoe & Johnson as a company  and Honduras as a location.</S><S sid ="40" ssid = "34">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid ="23" ssid = "17">For example  in ..  says Mr. Cooper  a vice president of.. both a spelling feature (that the string contains Mr.) and a contextual feature (that president modifies the string) are strong indications that Mr. Cooper is of type Person.</S><S sid ="48" ssid = "2">For example  take ...  says Maury Cooper  a vice president at S.&P.</S><S sid ="55" ssid = "9">In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'40'", "'23'", "'48'", "'55'"]
'11'
'40'
'23'
'48'
'55'
['11', '40', '23', '48', '55']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "6">The approach uses both spelling and contextual rules.</S><S sid ="58" ssid = "12">Having found (spelling  context) pairs in the parsed data  a number of features are extracted.</S><S sid ="22" ssid = "16">In many cases  inspection of either the spelling or context alone is sufficient to classify an example.</S><S sid ="165" ssid = "32">In the namedentity problem each example is a (spelling context) pair.</S><S sid ="55" ssid = "9">In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'58'", "'22'", "'165'", "'55'"]
'12'
'58'
'22'
'165'
'55'
['12', '58', '22', '165', '55']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "7">A spelling rule might be a simple look-up for the string (e.g.  a rule that Honduras is a location) or a rule that looks at words within a string (e.g.  a rule that any string containing Mr. is a person).</S><S sid ="14" ssid = "8">A contextual rule considers words surrounding the string in the sentence in which it appears (e.g.  a rule that any proper name modified by an appositive whose head is president is a person).</S><S sid ="61" ssid = "15">The following features were used: full-string=x The full string (e.g.  for Maury Cooper  full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word  this feature applies for any words that the string contains (e.g.  Maury Cooper contributes two such features  contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g.  IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods  and contains at least one period.</S><S sid ="22" ssid = "16">In many cases  inspection of either the spelling or context alone is sufficient to classify an example.</S><S sid ="165" ssid = "32">In the namedentity problem each example is a (spelling context) pair.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'14'", "'61'", "'22'", "'165'"]
'13'
'14'
'61'
'22'
'165'
['13', '14', '61', '22', '165']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P08-1043_swastika.csv
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['result_citation']
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="37">3An English sentence with ambiguous PoS assignment can be trivially represented as a lattice similar to our own, where every pair of consecutive nodes correspond to a word, and every possible PoS assignment for this word is a connecting arc.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="163" ssid="1">The accuracy results for segmentation, tagging and parsing using our different models and our standard data split are summarized in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



['163']
163
['163']
parsed_discourse_facet ['result_citation']
    <S sid="100" ssid="32">This means that the rules in our grammar are of two kinds: (a) syntactic rules relating nonterminals to a sequence of non-terminals and/or PoS tags, and (b) lexical rules relating PoS tags to lattice arcs (lexemes).</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['method_citation']
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['result_citation']
<S sid="188" ssid="2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



['188']
188
['188']
parsed_discourse_facet ['result_citation']
<S sid="86" ssid="18">A morphological analyzer M : W&#8212;* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S>
original cit marker offset is 0
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['result_citation']
<S sid="97" ssid="29">Thus our proposed model is a proper model assigning probability mass to all (7r, L) pairs, where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S>
original cit marker offset is 0
new cit marker offset is 0



['97']
97
['97']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S sid ="1" ssid = "1">Morphological processes in Semitic languages deliver space-delimited words which introduce multiple  distinct  syntactic units into the structure of the input sentence.</S><S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="39" ssid = "18">Such ambiguities cause discrepancies between token boundaries (indexed as white spaces) and constituent boundaries (imposed by syntactic categories) with respect to a surface form.</S><S sid ="34" ssid = "13">Lexical and Morphological Ambiguity The rich morphological processes for deriving Hebrew stems give rise to a high degree of ambiguity for Hebrew space-delimited tokens.</S><S sid ="24" ssid = "3">Co-occurrences among the particles themselves are subject to further syntactic and lexical constraints relative to the stem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'14'", "'39'", "'34'", "'24'"]
'1'
'14'
'39'
'34'
'24'
['1', '14', '39', '34', '24']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.</S><S sid ="16" ssid = "12">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S><S sid ="94" ssid = "26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S><S sid ="20" ssid = "16">We claim that no particular morphological segmentation is a-priory more likely for surface forms before exploring the compositional nature of syntactic structures  including manifestations of various long-distance dependencies.</S><S sid ="191" ssid = "5">In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'16'", "'94'", "'20'", "'191'"]
'2'
'16'
'94'
'20'
'191'
['2', '16', '94', '20', '191']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S><S sid ="19" ssid = "15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S sid ="187" ssid = "1">Employing a PCFG-based generative framework to make both syntactic and morphological disambiguation decisions is not only theoretically clean and linguistically justified and but also probabilistically apropriate and empirically sound.</S><S sid ="189" ssid = "3">Better grammars are shown here to improve performance on both morphological and syntactic tasks  providing support for the advantage of a joint framework over pipelined or factorized ones.</S><S sid ="21" ssid = "17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'19'", "'187'", "'189'", "'21'"]
'3'
'19'
'187'
'189'
'21'
['3', '19', '187', '189', '21']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Using a treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined  integrated or factorized systems for Hebrew morphological and syntactic processing  yielding an error reduction of 12% over the best published results so far.</S><S sid ="21" ssid = "17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S><S sid ="185" ssid = "23">In contrast  our morphological probabilities are based on a unigram  lexeme-based model  and all other (local and non-local) contextual considerations are delegated to the PCFG.</S><S sid ="188" ssid = "2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S><S sid ="191" ssid = "5">In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'21'", "'185'", "'188'", "'191'"]
'4'
'21'
'185'
'188'
'191'
['4', '21', '185', '188', '191']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  ‘tokens’) that constitute the unanalyzed surface forms (utterances).</S><S sid ="12" ssid = "8">This leads to word- and constituent-boundaries discrepancy  which breaks the assumptions underlying current state-of-the-art statistical parsers.</S><S sid ="94" ssid = "26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S><S sid ="21" ssid = "17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S><S sid ="16" ssid = "12">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'12'", "'94'", "'21'", "'16'"]
'5'
'12'
'94'
'21'
'16'
['5', '12', '94', '21', '16']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">In Semitic languages the situation is very different.</S><S sid ="78" ssid = "10">A similar structure is used in speech recognition.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="99" ssid = "31">This is done using a simple PCFG which is lexemebased.</S><S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'78'", "'92'", "'99'", "'180'"]
'6'
'78'
'92'
'99'
'180'
['6', '78', '92', '99', '180']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).</S><S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="17" ssid = "13">Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task.</S><S sid ="8" ssid = "4">The Hebrew token ‘bcl’1  for example  stands for the complete prepositional phrase 'We adopt here the transliteration of (Sima’an et al.  2001).</S><S sid ="29" ssid = "8">A less canonical representation of segmental morphology is triggered by a morpho-phonological process of omitting the definite article h when occurring after the particles b or l. This process triggers ambiguity as for the definiteness status of Nouns following these particles.We refer to such cases in which the concatenation of elements does not strictly correspond to the original surface form as super-segmental morphology.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'14'", "'17'", "'8'", "'29'"]
'7'
'14'
'17'
'8'
'29'
['7', '14', '17', '8', '29']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">The Hebrew token ‘bcl’1  for example  stands for the complete prepositional phrase 'We adopt here the transliteration of (Sima’an et al.  2001).</S><S sid ="35" ssid = "14">The form fmnh  for example  can be understood as the verb “lubricated”  the possessed noun “her oil”  the adjective “fat” or the verb “got fat”.</S><S sid ="62" ssid = "9">When a token fmnh is to be interpreted as the lexeme sequence f/REL mnh/VB  the analysis introduces two distinct entities  the relativizer f (“that”) and the verb mnh (“counted”)  and not as the complex entity “that counted”.</S><S sid ="7" ssid = "3">In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).</S><S sid ="15" ssid = "11">The aforementioned surface form bcl  for example  may also stand for the lexical item “onion”  a Noun.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'35'", "'62'", "'7'", "'15'"]
'8'
'35'
'62'
'7'
'15'
['8', '35', '62', '7', '15']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">“in the shadow”.</S><S sid ="184" ssid = "22">Oracle results).</S><S sid ="109" ssid = "41">The latter arcs correspond to OOV words in English.</S><S sid ="113" ssid = "45">The remaining arcs are marked OOV.</S><S sid ="78" ssid = "10">A similar structure is used in speech recognition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'184'", "'109'", "'113'", "'78'"]
'9'
'184'
'109'
'113'
'78'
['9', '184', '109', '113', '78']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">This token may further embed into a larger utterance  e.g.  ‘bcl hneim’ (literally “in-the-shadow the-pleasant”  meaning roughly “in the pleasant shadow”) in which the dominated Noun is modified by a proceeding space-delimited adjective.</S><S sid ="15" ssid = "11">The aforementioned surface form bcl  for example  may also stand for the lexical item “onion”  a Noun.</S><S sid ="76" ssid = "8">Furthermore  some of the arcs represent lexemes not present in the input tokens (e.g. h/DT  fl/POS)  however these are parts of valid analyses of the token (cf. super-segmental morphology section 2).</S><S sid ="58" ssid = "5">Such tag sequences are often treated as “complex tags” (e.g.</S><S sid ="108" ssid = "40">Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p —* (s  p)) > 0  while other segments have never been observed as a lexical event before.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'15'", "'76'", "'58'", "'108'"]
'10'
'15'
'76'
'58'
'108'
['10', '15', '76', '58', '108']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">It should be clear from the onset that the particle b (“in”) in ‘bcl’ may then attach higher than the bare noun cl (“shadow”).</S><S sid ="37" ssid = "16">The same form fmnh can be segmented as f-mnh  f (“that”) functioning as a reletivizer with the form mnh.</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="38" ssid = "17">The form mnh itself can be read as at least three different verbs (“counted”  “appointed”  “was appointed”)  a noun (“a portion”)  and a possessed noun (“her kind”).</S><S sid ="26" ssid = "5">The relativizer f(“that”) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'37'", "'57'", "'38'", "'26'"]
'11'
'37'
'57'
'38'
'26'
['11', '37', '57', '38', '26']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">This leads to word- and constituent-boundaries discrepancy  which breaks the assumptions underlying current state-of-the-art statistical parsers.</S><S sid ="5" ssid = "1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  ‘tokens’) that constitute the unanalyzed surface forms (utterances).</S><S sid ="13" ssid = "9">One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).</S><S sid ="2" ssid = "2">These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.</S><S sid ="19" ssid = "15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'5'", "'13'", "'2'", "'19'"]
'12'
'5'
'13'
'2'
'19'
['12', '5', '13', '2', '19']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).</S><S sid ="33" ssid = "12">The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).</S><S sid ="140" ssid = "18">For these models we limit the options provided for OOV words by not considering the entire token as a valid segmentation in case at least some prefix segmentation exists.</S><S sid ="139" ssid = "17">To control for the effect of the HSPELL-based pruning  we also experimented with a morphological analyzer that does not perform this pruning.</S><S sid ="3" ssid = "3">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'33'", "'140'", "'139'", "'3'"]
'13'
'33'
'140'
'139'
'3'
['13', '33', '140', '139', '3']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="7" ssid = "3">In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).</S><S sid ="140" ssid = "18">For these models we limit the options provided for OOV words by not considering the entire token as a valid segmentation in case at least some prefix segmentation exists.</S><S sid ="17" ssid = "13">Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task.</S><S sid ="121" ssid = "53">In such cases we use the non-pruned lattice including all (possibly ungrammatical) segmentation  and let the statistics (including OOV) decide.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'7'", "'140'", "'17'", "'121'"]
'14'
'7'
'140'
'17'
'121'
['14', '7', '140', '17', '121']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">The aforementioned surface form bcl  for example  may also stand for the lexical item “onion”  a Noun.</S><S sid ="32" ssid = "11">The additional morphological material in such cases appears after the stem and realizes the extended meaning.</S><S sid ="28" ssid = "7">The same argument holds for resolving PP attachment of a prefixed preposition or marking conjunction of elements of any kind.</S><S sid ="10" ssid = "6">This token may further embed into a larger utterance  e.g.  ‘bcl hneim’ (literally “in-the-shadow the-pleasant”  meaning roughly “in the pleasant shadow”) in which the dominated Noun is modified by a proceeding space-delimited adjective.</S><S sid ="30" ssid = "9">An additional case of super-segmental morphology is the case of Pronominal Clitics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'32'", "'28'", "'10'", "'30'"]
'15'
'32'
'28'
'10'
'30'
['15', '32', '28', '10', '30']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S><S sid ="94" ssid = "26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S><S sid ="188" ssid = "2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S><S sid ="13" ssid = "9">One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).</S><S sid ="2" ssid = "2">These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'94'", "'188'", "'13'", "'2'"]
'16'
'94'
'188'
'13'
'2'
['16', '94', '188', '13', '2']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W11-2123_vardha.csv
    <S sid="12" ssid="7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
 <S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
 <S sid="131" ssid="3">Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N &#8722; 1 preceding words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'"]
'131'
['131']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="16">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="16">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="12">Compared with SRILM, IRSTLM adds several features: lower memory consumption, a binary file format with memory mapping, caching to increase speed, and quantization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'"]
'108'
['108']
parsed_discourse_facet ['method_citation']
    <S sid="129" ssid="1">In addition to the optimizations specific to each datastructure described in Section 2, we implement several general optimizations for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'129'"]
'129'
['129']
parsed_discourse_facet ['method_citation']
    <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
    <S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
    <S sid="263" ssid="5">Quantization can be improved by jointly encoding probability and backoff.</S>
original cit marker offset is 0
new cit marker offset is 0



["'263'"]
'263'
['263']
parsed_discourse_facet ['method_citation']
    <S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
    <S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="17">If the context wnf will never extend to the right (i.e. wnf v is not present in the model for all words v) then no subsequent query will match the full context.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'"]
'145'
['145']
parsed_discourse_facet ['method_citation']
 <S sid="182" ssid="1">This section measures performance on shared tasks in order of increasing complexity: sparse lookups, evaluating perplexity of a large file, and translation with Moses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'182'"]
'182'
['182']
parsed_discourse_facet ['method_citation']
    <S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
    <S sid="12" ssid="7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
    <S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="12">We have modified Moses (Koehn et al., 2007) to keep our state with hypotheses; to conserve memory, phrases do not keep state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
 <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
 <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W11-2123.csv
<S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid ="7" ssid = "2">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'274'", "'7'", "'259'", "'45'"]
'1'
'274'
'7'
'259'
'45'
['1', '274', '7', '259', '45']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">The structure uses linear probing hash tables and is designed for speed.</S><S sid ="194" ssid = "13">Further  the probing hash table does only one random lookup per query  explaining why it is faster on large data.</S><S sid ="23" ssid = "1">We implement two data structures: PROBING  designed for speed  and TRIE  optimized for memory.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S><S sid ="120" ssid = "24">The compressed variant uses block compression and is rather slow as a result.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'194'", "'23'", "'45'", "'120'"]
'2'
'194'
'23'
'45'
'120'
['2', '194', '23', '45', '120']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S><S sid ="128" ssid = "32">These models generally outperform our memory consumption but are much slower  even when cached.</S><S sid ="161" ssid = "33">In both cases  SRILM walks its trie an additional time to minimize context as mentioned in Section 4.1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'275'", "'276'", "'128'", "'161'"]
'3'
'275'
'276'
'128'
'161'
['3', '275', '276', '128', '161']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Our code is thread-safe  and integrated into the Moses  cdec  and Joshua translation systems.</S><S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="169" ssid = "41">In our case multi-threading is trivial because our data structures are read-only and uncached.</S><S sid ="282" ssid = "3">Adam Pauls provided a pre-release comparison to BerkeleyLM and an initial Java interface.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'279'", "'169'", "'282'", "'266'"]
'4'
'279'
'169'
'282'
'266'
['4', '279', '169', '282', '266']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid ="26" ssid = "4">We use two common techniques  hash tables and sorted arrays  describing each before the model that uses the technique.</S><S sid ="129" ssid = "1">In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'274'", "'26'", "'129'"]
'5'
'1'
'274'
'26'
'129'
['5', '1', '274', '26', '129']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "1">Language models are widely applied in natural language processing  and applications such as machine translation make very frequent queries.</S><S sid ="130" ssid = "2">Applications such as machine translation use language model probability as a feature to assist in choosing between hypotheses.</S><S sid ="184" ssid = "3">Sparse lookup is a key subproblem of language model queries.</S><S sid ="7" ssid = "2">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'130'", "'184'", "'7'", "'259'"]
'6'
'130'
'184'
'7'
'259'
['6', '130', '184', '7', '259']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "2">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'1'", "'274'", "'259'", "'45'"]
'7'
'1'
'274'
'259'
'45'
['7', '1', '274', '259', '45']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "3">Queries take the form p(wn|wn−1 1 ) where wn1 is an n-gram.</S><S sid ="84" ssid = "62">In a model we built with default settings  1.2% of n + 1-grams were missing their ngram suffix.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="72" ssid = "50">We maintain a separate array for each length n containing all n-gram entries sorted in suffix order.</S><S sid ="46" ssid = "24">Unigram lookup is dense so we use an array of probability and backoff values.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'84'", "'88'", "'72'", "'46'"]
'8'
'84'
'88'
'72'
'46'
['8', '84', '88', '72', '46']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "4">Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf   returning where the probability p(wn|wn−1 f ) and backoff penalties b(wn−1 i ) are given by an already-estimated model.</S><S sid ="127" ssid = "31">Minimal perfect hashing is used to find the index at which a quantized probability and possibly backoff are stored.</S><S sid ="136" ssid = "8">We offer a state function s(wn1) = wn� where substring wn� is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.</S><S sid ="78" ssid = "56">Unigram records store probability  backoff  and an index in the bigram table.</S><S sid ="154" ssid = "26">To optimize left-to-right queries  we extend state to store backoff information: where m is the minimal context from Section 4.1 and b is the backoff penalty.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'127'", "'136'", "'78'", "'154'"]
'9'
'127'
'136'
'78'
'154'
['9', '127', '136', '78', '154']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "5">The problem is to store these two values for a large and sparse set of n-grams in a way that makes queries efficient.</S><S sid ="25" ssid = "3">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S><S sid ="24" ssid = "2">The set of n-grams appearing in a model is sparse  and we want to efficiently find their associated probabilities and backoff penalties.</S><S sid ="93" ssid = "71">The cost of storing these averages  in bits  is Because there are comparatively few unigrams  we elected to store them byte-aligned and unquantized  making every query faster.</S><S sid ="85" ssid = "63">This causes a problem for reverse trie implementations  including SRILM itself  because it leaves n+1-grams without an n-gram node pointing to them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'25'", "'24'", "'93'", "'85'"]
'10'
'25'
'24'
'93'
'85'
['10', '25', '24', '93', '85']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "6">Many packages perform language model queries.</S><S sid ="68" ssid = "46">The trie data structure is commonly used for language modeling.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'68'", "'266'", "'259'", "'45'"]
'11'
'68'
'266'
'259'
'45'
['11', '68', '266', '259', '45']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke  2002) is a popular toolkit based on tries used in several decoders.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="97" ssid = "1">SRILM (Stolcke  2002) is widely used within academia.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'13'", "'14'", "'103'", "'97'"]
'12'
'13'
'14'
'103'
'97'
['12', '13', '14', '103', '97']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'103'", "'15'", "'14'", "'262'"]
'13'
'103'
'15'
'14'
'262'
['13', '103', '15', '14', '262']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="97" ssid = "1">SRILM (Stolcke  2002) is widely used within academia.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'15'", "'103'", "'97'", "'266'"]
'14'
'15'
'103'
'97'
'266'
['14', '15', '103', '97', '266']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'14'", "'103'", "'13'", "'262'"]
'15'
'14'
'103'
'13'
'262'
['15', '14', '103', '13', '262']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "11">BerkeleyLM revision 152 (Pauls and Klein  2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.</S><S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="51" ssid = "29">This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'15'", "'51'", "'262'", "'13'"]
'16'
'15'
'51'
'262'
'13'
['16', '15', '51', '262', '13']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "12">Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.</S><S sid ="18" ssid = "13">TPT Germann et al. (2009) describe tries with better locality properties  but did not release code.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="215" ssid = "34">Another option is the closedsource data structures from Sheffield (Guthrie and Hepple  2010).</S><S sid ="114" ssid = "18">Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'18'", "'262'", "'215'", "'114'"]
'17'
'18'
'262'
'215'
'114'
['17', '18', '262', '215', '114']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "13">TPT Germann et al. (2009) describe tries with better locality properties  but did not release code.</S><S sid ="17" ssid = "12">Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.</S><S sid ="232" ssid = "51">Memory usage is likely much lower than ours. fThe original paper (Germann et al.  2009) provided only 2s of query timing and compared with SRI when it exceeded available RAM.</S><S sid ="114" ssid = "18">Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'17'", "'232'", "'114'", "'262'"]
'18'
'17'
'232'
'114'
'262'
['18', '17', '232', '114', '262']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "14">These packages are further described in Section 3.</S><S sid ="129" ssid = "1">In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="5" ssid = "5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S sid ="150" ssid = "22">Section 4.1 explained that state s is stored by applications with partial hypotheses to determine when they can be recombined.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'129'", "'266'", "'5'", "'150'"]
'19'
'129'
'266'
'5'
'150'
['19', '129', '266', '5', '150']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "15">We substantially outperform all of them on query speed and offer lower memory consumption than lossless alternatives.</S><S sid ="128" ssid = "32">These models generally outperform our memory consumption but are much slower  even when cached.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S><S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'128'", "'276'", "'1'", "'275'"]
'20'
'128'
'276'
'1'
'275'
['20', '128', '276', '1', '275']
parsed_discourse_facet ['aim_citation']





input/ref/Task1/W99-0613_swastika.csv
input/res/Task1/W99-0613.csv
parsing: input/ref/Task1/W99-0613_swastika.csv
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
<S sid="159" ssid="26">To prevent this we &amp;quot;smooth&amp;quot; the confidence by adding a small value, e, to both W+ and W_, giving at = Plugging the value of at from Equ.</S>
original cit marker offset is 0
new cit marker offset is 0



['159']
159
['159']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S>
original cit marker offset is 0
new cit marker offset is 0



['137']
137
['137']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['method_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
 <S sid="250" ssid="1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S>
original cit marker offset is 0
new cit marker offset is 0



['250']
250
['250']
parsed_discourse_facet ['result_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
    <S sid="36" ssid="30">Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.</S>
original cit marker offset is 0
new cit marker offset is 0



['36']
36
['36']
parsed_discourse_facet ['result_citation']
<S sid="29" ssid="23">Unfortunately, Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective function.</S>
original cit marker offset is 0
new cit marker offset is 0



['29']
29
['29']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
    <S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['method_citation']
    <S sid="95" ssid="28">(Specifically, the limit n starts at 5 and increases by 5 at each iteration.)</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['method_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="1" ssid = "1">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="9" ssid = "3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="136" ssid = "3">We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'250'", "'136'", "'163'"]
'1'
'9'
'250'
'136'
'163'
['1', '9', '250', '136', '163']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.</S><S sid ="17" ssid = "11">At first glance  the problem seems quite complex: a large number of rules is needed to cover the domain  suggesting that a large number of labeled examples is required to train an accurate classifier.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="105" ssid = "38">Thus the method makes the fairly strong assumption that the features can be partitioned into two types such that each type alone is sufficient for classification.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'17'", "'250'", "'105'", "'248'"]
'2'
'17'
'250'
'105'
'248'
['2', '17', '250', '105', '248']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid ="92" ssid = "25">Thus an explicit assumption about the redundancy of the features — that either the spelling or context alone should be sufficient to build a classifier — has been built into the algorithm.</S><S sid ="121" ssid = "54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).</S><S sid ="58" ssid = "12">Having found (spelling  context) pairs in the parsed data  a number of features are extracted.</S><S sid ="232" ssid = "11">For example  the independence assumptions mean that the model fails to capture the dependence between specific and more general features (for example the fact that the feature full.-string=New_York is always seen with the features contains (New) and The baseline method tags all entities as the most frequent class type (organization). contains (York) and is never seen with a feature such as contains (Group) ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'92'", "'121'", "'58'", "'232'"]
'3'
'92'
'121'
'58'
'232'
['3', '92', '121', '58', '232']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "20">We present two algorithms.</S><S sid ="4" ssid = "4">We present two algorithms.</S><S sid ="68" ssid = "1">The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S><S sid ="88" ssid = "21">We can now compare this algorithm to that of (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'4'", "'68'", "'163'", "'88'"]
'26'
'4'
'68'
'163'
'88'
['26', '4', '68', '163', '88']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid ="6" ssid = "6">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="68" ssid = "1">The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'31'", "'6'", "'97'", "'68'"]
'5'
'31'
'6'
'97'
'68'
['5', '31', '6', '97', '68']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "6">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid ="5" ssid = "5">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="30" ssid = "24">(Blum and Mitchell 98) offer a promising formulation of redundancy  also prove some results about how the use of unlabeled examples can help classification  and suggest an objective function when training with unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'31'", "'5'", "'204'", "'30'"]
'6'
'31'
'5'
'204'
'30'
['6', '31', '5', '204', '30']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision  in the form of labeled training examples.</S><S sid ="2" ssid = "2">A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="3" ssid = "3">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid ="130" ssid = "63">So the success of the algorithm may well be due to its success in maximizing the number of unlabeled examples on which the two decision lists agree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'2'", "'250'", "'3'", "'130'"]
'7'
'2'
'250'
'3'
'130'
['7', '2', '250', '3', '130']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "2">Recent results (e.g.  (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S><S sid ="27" ssid = "21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S><S sid ="120" ssid = "53">(Blum and Mitchell 98) go on to give PAC results for learning in the cotraining case.</S><S sid ="40" ssid = "34">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid ="88" ssid = "21">We can now compare this algorithm to that of (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'27'", "'120'", "'40'", "'88'"]
'8'
'27'
'120'
'40'
'88'
['8', '27', '120', '40', '88']
parsed_discourse_facet ['results_citation']
<S sid ="1" ssid = "1">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="9" ssid = "3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="136" ssid = "3">We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'250'", "'136'", "'163'"]
'1'
'9'
'250'
'136'
'163'
['1', '9', '250', '136', '163']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "4">The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.</S><S sid ="147" ssid = "14">The weak learner for two-class problems computes a weak hypothesis h from the input space into the reals (h : 2x -4 R)  where the sign4 of h(x) is interpreted as the predicted label and the magnitude I h(x)I is the confidence in the prediction: large numbers for I h(x)I indicate high confidence in the prediction  and numbers close to zero indicate low confidence.</S><S sid ="35" ssid = "29">AdaBoost finds a weighted combination of simple (weak) classifiers  where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.</S><S sid ="224" ssid = "3">The model was parameterized such that the joint probability of a (label  feature-set) pair P(yi  xi) is written as The model assumes that (y  x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).</S><S sid ="74" ssid = "7">Output of the learning algorithm: a function h:Xxy [0  1] where h(x  y) is an estimate of the conditional probability p(y1x) of seeing label y given that feature x is present.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'147'", "'35'", "'224'", "'74'"]
'10'
'147'
'35'
'224'
'74'
['10', '147', '35', '224', '74']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "5">For example  a good classifier would identify Mrs. Frank as a person  Steptoe & Johnson as a company  and Honduras as a location.</S><S sid ="40" ssid = "34">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid ="23" ssid = "17">For example  in ..  says Mr. Cooper  a vice president of.. both a spelling feature (that the string contains Mr.) and a contextual feature (that president modifies the string) are strong indications that Mr. Cooper is of type Person.</S><S sid ="48" ssid = "2">For example  take ...  says Maury Cooper  a vice president at S.&P.</S><S sid ="55" ssid = "9">In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'40'", "'23'", "'48'", "'55'"]
'11'
'40'
'23'
'48'
'55'
['11', '40', '23', '48', '55']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "6">The approach uses both spelling and contextual rules.</S><S sid ="58" ssid = "12">Having found (spelling  context) pairs in the parsed data  a number of features are extracted.</S><S sid ="22" ssid = "16">In many cases  inspection of either the spelling or context alone is sufficient to classify an example.</S><S sid ="165" ssid = "32">In the namedentity problem each example is a (spelling context) pair.</S><S sid ="55" ssid = "9">In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'58'", "'22'", "'165'", "'55'"]
'12'
'58'
'22'
'165'
'55'
['12', '58', '22', '165', '55']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "7">A spelling rule might be a simple look-up for the string (e.g.  a rule that Honduras is a location) or a rule that looks at words within a string (e.g.  a rule that any string containing Mr. is a person).</S><S sid ="14" ssid = "8">A contextual rule considers words surrounding the string in the sentence in which it appears (e.g.  a rule that any proper name modified by an appositive whose head is president is a person).</S><S sid ="61" ssid = "15">The following features were used: full-string=x The full string (e.g.  for Maury Cooper  full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word  this feature applies for any words that the string contains (e.g.  Maury Cooper contributes two such features  contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g.  IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods  and contains at least one period.</S><S sid ="22" ssid = "16">In many cases  inspection of either the spelling or context alone is sufficient to classify an example.</S><S sid ="165" ssid = "32">In the namedentity problem each example is a (spelling context) pair.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'14'", "'61'", "'22'", "'165'"]
'13'
'14'
'61'
'22'
'165'
['13', '14', '61', '22', '165']
parsed_discourse_facet ['method_citation']
['The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.']
['They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).', 'The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.', u'Thus an explicit assumption about the redundancy of the features \u2014 that either the spelling or context alone should be sufficient to build a classifier \u2014 has been built into the algorithm.', 'Having found (spelling  context) pairs in the parsed data  a number of features are extracted.', 'For example  the independence assumptions mean that the model fails to capture the dependence between specific and more general features (for example the fact that the feature full.-string=New_York is always seen with the features contains (New) and The baseline method tags all entities as the most frequent class type (organization). contains (York) and is never seen with a feature such as contains (Group) ).']
['system', 'ROUGE-S*', 'Average_R:', '0.00144', '(95%-conf.int.', '0.00144', '-', '0.00144)']
['system', 'ROUGE-S*', 'Average_P:', '0.11111', '(95%-conf.int.', '0.11111', '-', '0.11111)']
['system', 'ROUGE-S*', 'Average_F:', '0.00285', '(95%-conf.int.', '0.00285', '-', '0.00285)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:36', 'F:4']
["Unfortunately, Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective function."]
['The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.', 'Output of the learning algorithm: a function h:Xxy [0  1] where h(x  y) is an estimate of the conditional probability p(y1x) of seeing label y given that feature x is present.', 'The model was parameterized such that the joint probability of a (label  feature-set) pair P(yi  xi) is written as The model assumes that (y  x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).', 'AdaBoost finds a weighted combination of simple (weak) classifiers  where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.', 'The weak learner for two-class problems computes a weak hypothesis h from the input space into the reals (h : 2x -4 R)  where the sign4 of h(x) is interpreted as the predicted label and the magnitude I h(x)I is the confidence in the prediction: large numbers for I h(x)I indicate high confidence in the prediction  and numbers close to zero indicate low confidence.']
['system', 'ROUGE-S*', 'Average_R:', '0.00053', '(95%-conf.int.', '0.00053', '-', '0.00053)']
['system', 'ROUGE-S*', 'Average_P:', '0.02500', '(95%-conf.int.', '0.02500', '-', '0.02500)']
['system', 'ROUGE-S*', 'Average_F:', '0.00104', '(95%-conf.int.', '0.00104', '-', '0.00104)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5671', 'P:120', 'F:3']
['Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.']
['The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).', "Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).", 'The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', 'The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).']
['system', 'ROUGE-S*', 'Average_R:', '0.00065', '(95%-conf.int.', '0.00065', '-', '0.00065)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00125', '(95%-conf.int.', '0.00125', '-', '0.00125)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:55', 'F:1']
['Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.']
['The following features were used: full-string=x The full string (e.g.  for Maury Cooper  full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word  this feature applies for any words that the string contains (e.g.  Maury Cooper contributes two such features  contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g.  IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods  and contains at least one period.', 'A spelling rule might be a simple look-up for the string (e.g.  a rule that Honduras is a location) or a rule that looks at words within a string (e.g.  a rule that any string containing Mr. is a person).', 'A contextual rule considers words surrounding the string in the sentence in which it appears (e.g.  a rule that any proper name modified by an appositive whose head is president is a person).', 'In many cases  inspection of either the spelling or context alone is sufficient to classify an example.', 'In the namedentity problem each example is a (spelling context) pair.']
['system', 'ROUGE-S*', 'Average_R:', '0.00172', '(95%-conf.int.', '0.00172', '-', '0.00172)']
['system', 'ROUGE-S*', 'Average_P:', '0.10909', '(95%-conf.int.', '0.10909', '-', '0.10909)']
['system', 'ROUGE-S*', 'Average_F:', '0.00339', '(95%-conf.int.', '0.00339', '-', '0.00339)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:55', 'F:6']
['Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.']
['So the success of the algorithm may well be due to its success in maximizing the number of unlabeled examples on which the two decision lists agree.', 'Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.', 'A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.', 'The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.', 'Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision  in the form of labeled training examples.']
['system', 'ROUGE-S*', 'Average_R:', '0.00032', '(95%-conf.int.', '0.00032', '-', '0.00032)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00064', '(95%-conf.int.', '0.00064', '-', '0.00064)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:55', 'F:1']
['This paper discusses the use of unlabeled examples for the problem of named entity classification.']
['(Blum and Mitchell 98) go on to give PAC results for learning in the cotraining case.', 'Recent results (e.g.  (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.', 'We can now compare this algorithm to that of (Yarowsky 95).', 'The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).', '(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:28', 'F:0']
['To prevent this we &quot;smooth&quot; the confidence by adding a small value, e, to both W+ and W_, giving at = Plugging the value of at from Equ.']
['Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.', 'A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.', 'With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.', 'At first glance  the problem seems quite complex: a large number of rules is needed to cover the domain  suggesting that a large number of labeled examples is required to train an accurate classifier.', 'Thus the method makes the fairly strong assumption that the features can be partitioned into two types such that each type alone is sufficient for classification.']
['system', 'ROUGE-S*', 'Average_R:', '0.00034', '(95%-conf.int.', '0.00034', '-', '0.00034)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00067', '(95%-conf.int.', '0.00067', '-', '0.00067)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:45', 'F:1']
['(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.']
['In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.', 'In the namedentity problem each example is a (spelling context) pair.', 'The approach uses both spelling and contextual rules.', 'Having found (spelling  context) pairs in the parsed data  a number of features are extracted.', 'In many cases  inspection of either the spelling or context alone is sufficient to classify an example.']
['system', 'ROUGE-S*', 'Average_R:', '0.00178', '(95%-conf.int.', '0.00178', '-', '0.00178)']
['system', 'ROUGE-S*', 'Average_P:', '0.00159', '(95%-conf.int.', '0.00159', '-', '0.00159)']
['system', 'ROUGE-S*', 'Average_F:', '0.00168', '(95%-conf.int.', '0.00168', '-', '0.00168)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:630', 'F:1']
['Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.']
["Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).", '(Blum and Mitchell 98) offer a promising formulation of redundancy  also prove some results about how the use of unlabeled examples can help classification  and suggest an objective function when training with unlabeled examples.', 'The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).', 'In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.', 'The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).']
['system', 'ROUGE-S*', 'Average_R:', '0.00307', '(95%-conf.int.', '0.00307', '-', '0.00307)']
['system', 'ROUGE-S*', 'Average_P:', '0.10909', '(95%-conf.int.', '0.10909', '-', '0.10909)']
['system', 'ROUGE-S*', 'Average_F:', '0.00598', '(95%-conf.int.', '0.00598', '-', '0.00598)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:55', 'F:6']
0.0460511105994 0.00109444443228 0.00194444442284





input/ref/Task1/E03-1005_swastika.csv
input/res/Task1/E03-1005.csv
parsing: input/ref/Task1/E03-1005_swastika.csv
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
    <S sid="41" ssid="38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



['41']
41
['41']
parsed_discourse_facet ['aim_citation']
<S sid="80" ssid="32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S>
original cit marker offset is 0
new cit marker offset is 0



['80']
80
['80']
parsed_discourse_facet ['result_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



['143']
143
['143']
parsed_discourse_facet ['method_citation']
<S sid="146" ssid="11">This paper also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic depen dencies (as e.g. in Collins 1999 and Charniak 2000).</S>
original cit marker offset is 0
new cit marker offset is 0



['146']
146
['146']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
<S sid="100" ssid="3">In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['result_citation']
    <S sid="30" ssid="27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S>
original cit marker offset is 0
new cit marker offset is 0



['30']
30
['30']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
    <S sid="30" ssid="27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S>
original cit marker offset is 0
new cit marker offset is 0



['30']
30
['30']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S sid ="1" ssid = "1">Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.</S><S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S><S sid ="76" ssid = "28">Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.</S><S sid ="26" ssid = "23">Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation  which in some cases is a reasonable approximation of the most probable parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'108'", "'28'", "'76'", "'26'"]
'1'
'108'
'28'
'76'
'26'
['1', '108', '28', '76', '26']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">This paper proposes an integration of the two models which outperforms each of them separately.</S><S sid ="107" ssid = "10">This suggests that a model which combines these two notions of best parse may boost the accuracy.</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S><S sid ="94" ssid = "46">This paper presents the first published results with this estimator on the WSJ.</S><S sid ="56" ssid = "8">Here we will only sketch this PCFG-reduction  which is heavily based on Goodman (2002).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'107'", "'45'", "'94'", "'56'"]
'2'
'107'
'45'
'94'
'56'
['2', '107', '45', '94', '56']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models  and an average processing time of 3.6 seconds per WSJ sentence.</S><S sid ="38" ssid = "35">Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task  the parsing time was reported to be over 200 seconds per sentence (Bod 2003).</S><S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="92" ssid = "44">Tested on the OVIS corpus  Bonnema et al. 's proposal obtains results that are comparable to Sima'an (1999) -- see Bonnema et al.</S><S sid ="41" ssid = "38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'38'", "'130'", "'92'", "'41'"]
'3'
'38'
'130'
'92'
'41'
['3', '38', '130', '92', '41']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S><S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="6" ssid = "3">To appreciate these innovations  it should be noted that the model was radically different from all other statistical parsing models at the time.</S><S sid ="143" ssid = "8">While SL-DOP and LS-DOP have been compared before in Bod (2002)  especially in the context of musical parsing  this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules  and (2) the use of arbitrarily large fragments rather than restricted ones.</S><S sid ="1" ssid = "1">Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'108'", "'6'", "'143'", "'1'"]
'4'
'108'
'6'
'143'
'1'
['4', '108', '6', '143', '1']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "2">Fragments include  for instance  subtrees of depth 1 (corresponding to context-free rules) as well as entire trees.</S><S sid ="62" ssid = "14">Let a represent the number of subtrees headed by nodes with nonterminal A  that is a =Ej aj.</S><S sid ="23" ssid = "20">Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.</S><S sid ="104" ssid = "7">Next  the rank of each (shortest) derivation is computed as the sum of the ranks of the subtrees involved.</S><S sid ="61" ssid = "13">Let aj represent the number of subtrees headed by the node A@j.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'62'", "'23'", "'104'", "'61'"]
'5'
'62'
'23'
'104'
'61'
['5', '62', '23', '104', '61']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "3">To appreciate these innovations  it should be noted that the model was radically different from all other statistical parsing models at the time.</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="106" ssid = "9">What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.</S><S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S><S sid ="76" ssid = "28">Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'79'", "'106'", "'4'", "'76'"]
'6'
'79'
'106'
'4'
'76'
['6', '79', '106', '4', '76']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="128" ssid = "9">Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.</S><S sid ="23" ssid = "20">Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="14" ssid = "11">This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'128'", "'23'", "'15'", "'14'"]
'7'
'128'
'23'
'15'
'14'
['7', '128', '23', '15', '14']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">in Fujisaki et al. 1989; Black et al.</S><S sid ="10" ssid = "7">Waegner 1992; Pereira and Schabes 1992).</S><S sid ="98" ssid = "1">Most DOP models  such as in Bod (1993)  Goodman (1996)  Bonnema et al. (1997)  Sima'an (2000) and Collins & Duffy (2002)  use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.</S><S sid ="18" ssid = "15">Collins 1999; Charniak 2000; Goodman 1998).</S><S sid ="120" ssid = "1">For our experiments we used the standard division of the WSJ (Marcus et al. 1993)  with sections 2 through 21 for training (approx.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'10'", "'98'", "'18'", "'120'"]
'8'
'10'
'98'
'18'
'120'
['8', '10', '98', '18', '120']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "6">1992  1993; Briscoe and I Thanks to Ivan Sag for this pun.</S><S sid ="134" ssid = "15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.</S><S sid ="29" ssid = "26">Johnson (1998b  2002) showed that DOP1's subtree estimation method is statistically biased and inconsistent.</S><S sid ="35" ssid = "32">Bonnema et al. (1999) observed that another problem with DOP1's subtree-estimation method is that it provides more probability to nodes with more subtrees  and therefore more probability to larger subtrees.</S><S sid ="23" ssid = "20">Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'134'", "'29'", "'35'", "'23'"]
'9'
'134'
'29'
'35'
'23'
['9', '134', '29', '35', '23']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">Waegner 1992; Pereira and Schabes 1992).</S><S sid ="18" ssid = "15">Collins 1999; Charniak 2000; Goodman 1998).</S><S sid ="129" ssid = "10">Collins 1996  Charniak 1997  Collins 1999 and Charniak 2000).</S><S sid ="8" ssid = "5">in Fujisaki et al. 1989; Black et al.</S><S sid ="98" ssid = "1">Most DOP models  such as in Bod (1993)  Goodman (1996)  Bonnema et al. (1997)  Sima'an (2000) and Collins & Duffy (2002)  use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'18'", "'129'", "'8'", "'98'"]
'10'
'18'
'129'
'8'
'98'
['10', '18', '129', '8', '98']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "8">The DOP model  on the other hand  was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus  but to directly use corpus fragments as a grammar.</S><S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="143" ssid = "8">While SL-DOP and LS-DOP have been compared before in Bod (2002)  especially in the context of musical parsing  this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules  and (2) the use of arbitrarily large fragments rather than restricted ones.</S><S sid ="110" ssid = "13">We will refer to the first combination (which selects the simplest among the n likeliest trees) as Simplicity-Likelihood-DOP or SL-DOP  and to the second combination (which selects the likeliest among the n simplest trees) as Likelihood-Simplicity-DOP or LS-DOP.</S><S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'13'", "'143'", "'110'", "'4'"]
'11'
'13'
'143'
'110'
'4'
['11', '13', '143', '110', '4']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "9">This approach has now gained wide usage  as exemplified by the work of Collins (1996  1999)  Charniak (1996  1997)  Johnson (1998)  Chiang (2000)  and many others.</S><S sid ="98" ssid = "1">Most DOP models  such as in Bod (1993)  Goodman (1996)  Bonnema et al. (1997)  Sima'an (2000) and Collins & Duffy (2002)  use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.</S><S sid ="143" ssid = "8">While SL-DOP and LS-DOP have been compared before in Bod (2002)  especially in the context of musical parsing  this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules  and (2) the use of arbitrarily large fragments rather than restricted ones.</S><S sid ="16" ssid = "13">While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words  later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a).</S><S sid ="41" ssid = "38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'98'", "'143'", "'16'", "'41'"]
'12'
'98'
'143'
'16'
'41'
['12', '98', '143', '16', '41']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="54" ssid = "6">A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account.</S><S sid ="20" ssid = "17">Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage  and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).</S><S sid ="110" ssid = "13">We will refer to the first combination (which selects the simplest among the n likeliest trees) as Simplicity-Likelihood-DOP or SL-DOP  and to the second combination (which selects the likeliest among the n simplest trees) as Likelihood-Simplicity-DOP or LS-DOP.</S><S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'54'", "'20'", "'110'", "'4'"]
'13'
'54'
'20'
'110'
'4'
['13', '54', '20', '110', '4']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "11">This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="36" ssid = "33">As an alternative  Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'15'", "'7'", "'36'", "'82'"]
'14'
'15'
'7'
'36'
'82'
['14', '15', '7', '36', '82']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="14" ssid = "11">This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.</S><S sid ="134" ssid = "15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'45'", "'82'", "'14'", "'134'"]
'15'
'45'
'82'
'14'
'134'
['15', '45', '82', '14', '134']
parsed_discourse_facet ['method_citation']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
["Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.", 'What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.', 'The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.', "While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.", 'To appreciate these innovations  it should be noted that the model was radically different from all other statistical parsing models at the time.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:78', 'F:0']
['The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.']
['Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.', "Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.", 'The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.', "While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.", "Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation  which in some cases is a reasonable approximation of the most probable parse."]
['system', 'ROUGE-S*', 'Average_R:', '0.00631', '(95%-conf.int.', '0.00631', '-', '0.00631)']
['system', 'ROUGE-S*', 'Average_P:', '0.07333', '(95%-conf.int.', '0.07333', '-', '0.07333)']
['system', 'ROUGE-S*', 'Average_F:', '0.01162', '(95%-conf.int.', '0.01162', '-', '0.01162)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:300', 'F:22']
['Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.']
['A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account.', 'The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.', 'Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage  and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).', 'We will refer to the first combination (which selects the simplest among the n likeliest trees) as Simplicity-Likelihood-DOP or SL-DOP  and to the second combination (which selects the likeliest among the n simplest trees) as Likelihood-Simplicity-DOP or LS-DOP.', 'The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.']
['system', 'ROUGE-S*', 'Average_R:', '0.00070', '(95%-conf.int.', '0.00070', '-', '0.00070)']
['system', 'ROUGE-S*', 'Average_P:', '0.02564', '(95%-conf.int.', '0.02564', '-', '0.02564)']
['system', 'ROUGE-S*', 'Average_F:', '0.00137', '(95%-conf.int.', '0.00137', '-', '0.00137)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:78', 'F:2']
['While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.']
['Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.', 'The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.', 'The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.', 'While SL-DOP and LS-DOP have been compared before in Bod (2002)  especially in the context of musical parsing  this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules  and (2) the use of arbitrarily large fragments rather than restricted ones.', 'To appreciate these innovations  it should be noted that the model was radically different from all other statistical parsing models at the time.']
['system', 'ROUGE-S*', 'Average_R:', '0.08963', '(95%-conf.int.', '0.08963', '-', '0.08963)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.16452', '(95%-conf.int.', '0.16452', '-', '0.16452)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3916', 'P:351', 'F:351']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.', 'However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.', 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.', 'As an alternative  Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:78', 'F:0']
['Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.']
['Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.', 'In the second part of this paper  we extend our experiments with a new notion of the best parse tree.', 'However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.', 'This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00036', '(95%-conf.int.', '0.00036', '-', '0.00036)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00070', '(95%-conf.int.', '0.00070', '-', '0.00070)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:78', 'F:1']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['Waegner 1992; Pereira and Schabes 1992).', "Most DOP models  such as in Bod (1993)  Goodman (1996)  Bonnema et al. (1997)  Sima'an (2000) and Collins & Duffy (2002)  use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.", 'in Fujisaki et al. 1989; Black et al.', 'Collins 1996  Charniak 1997  Collins 1999 and Charniak 2000).', 'Collins 1999; Charniak 2000; Goodman 1998).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:78', 'F:0']
['DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).']
["While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).", 'Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models  and an average processing time of 3.6 seconds per WSJ sentence.', "Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task  the parsing time was reported to be over 200 seconds per sentence (Bod 2003).", "This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.", "Tested on the OVIS corpus  Bonnema et al. 's proposal obtains results that are comparable to Sima'an (1999) -- see Bonnema et al."]
['system', 'ROUGE-S*', 'Average_R:', '0.00140', '(95%-conf.int.', '0.00140', '-', '0.00140)']
['system', 'ROUGE-S*', 'Average_P:', '0.13333', '(95%-conf.int.', '0.13333', '-', '0.13333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00278', '(95%-conf.int.', '0.00278', '-', '0.00278)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4278', 'P:45', 'F:6']
['The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.']
['1992  1993; Briscoe and I Thanks to Ivan Sag for this pun.', 'Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.', "Johnson (1998b  2002) showed that DOP1's subtree estimation method is statistically biased and inconsistent.", "Bonnema et al. (1999) observed that another problem with DOP1's subtree-estimation method is that it provides more probability to nodes with more subtrees  and therefore more probability to larger subtrees.", 'This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00169', '(95%-conf.int.', '0.00169', '-', '0.00169)']
['system', 'ROUGE-S*', 'Average_P:', '0.01000', '(95%-conf.int.', '0.01000', '-', '0.01000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00290', '(95%-conf.int.', '0.00290', '-', '0.00290)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:300', 'F:3']
0.139457776228 0.0111211109875 0.0204322219952





input/ref/Task1/A97-1014_swastika.csv
input/res/Task1/A97-1014.csv
parsing: input/ref/Task1/A97-1014_swastika.csv
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



['151']
151
['151']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
<S sid="47" ssid="37">Argument structure can be represented in terms of unordered trees (with crossing branches).</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



['167']
167
['167']
parsed_discourse_facet ['method_citation']
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



['160']
160
['160']
parsed_discourse_facet ['method_citation']
<S sid="127" ssid="8">As the need for certain functionalities becomes obvious with growing annotation experience, we have decided to implement the tool in two stages.</S>
original cit marker offset is 0
new cit marker offset is 0



['127']
127
['127']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



['39']
39
['39']
parsed_discourse_facet ['method_citation']
<S sid="72" ssid="17">In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.</S>
original cit marker offset is 0
new cit marker offset is 0



['72']
72
['72']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/A97-1014.csv
<S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'162'", "'11'", "'5'", "'4'"]
'1'
'162'
'11'
'5'
'4'
['1', '162', '11', '5', '4']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Since the requirements for such a formalism differ from those posited for configurational languages  several features have been added  influencing the architecture of the scheme.</S><S sid ="3" ssid = "3">The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.</S><S sid ="31" ssid = "21">Due to the substantial differences between existing models of constituent structure  the question arises of how the theory independencf requirement  can be satisfied.</S><S sid ="13" ssid = "3">The data-drivenness of this approach presents a clear advantage over the traditional  idealised notion of competence grammar.</S><S sid ="165" ssid = "7">In addition the approach provides empirical material for psycholinguistic investigation  since preferences for the choice of certain syntactic constructions  linea.rizations  and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'3'", "'31'", "'13'", "'165'"]
'2'
'3'
'31'
'13'
'165'
['2', '3', '31', '13', '165']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.</S><S sid ="31" ssid = "21">Due to the substantial differences between existing models of constituent structure  the question arises of how the theory independencf requirement  can be satisfied.</S><S sid ="113" ssid = "26">Since a precise structural description of non-constituent coordination would require a. rich inventory of incomplete phrase types  we have agreed on a sort of unde.rspe.cified representations: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links.</S><S sid ="2" ssid = "2">Since the requirements for such a formalism differ from those posited for configurational languages  several features have been added  influencing the architecture of the scheme.</S><S sid ="41" ssid = "31">Apart from this rather technical problem  two further arguments speak against phrase structure as the structural pivot of the annotation scheme: Finally  the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies  which has proved tedious.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'31'", "'113'", "'2'", "'41'"]
'3'
'31'
'113'
'2'
'41'
['3', '31', '113', '2', '41']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'1'", "'162'", "'74'", "'11'"]
'4'
'1'
'162'
'74'
'11'
['4', '1', '162', '74', '11']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'162'", "'1'", "'6'", "'74'"]
'5'
'162'
'1'
'6'
'74'
['5', '162', '1', '6', '74']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="9" ssid = "6">Section 4 deals with the treatment of selected phenomena.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="160" ssid = "2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'9'", "'5'", "'1'", "'160'"]
'6'
'9'
'5'
'1'
'160'
['6', '9', '5', '1', '160']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">On the basis of these considerations  we formulate several additional requirements.</S><S sid ="37" ssid = "27">In light of these facts  serious difficulties can be expected arising from the structural component of the existing formalisms.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="104" ssid = "17">The head of the phrase can be determined in a similar way according to theory-specific assumptions.</S><S sid ="127" ssid = "8">As the need for certain functionalities becomes obvious with growing annotation experience  we have decided to implement the tool in two stages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'37'", "'5'", "'104'", "'127'"]
'7'
'37'
'5'
'104'
'127'
['7', '37', '5', '104', '127']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">A formalism complying with these requirements is described in section 3.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="9" ssid = "6">Section 4 deals with the treatment of selected phenomena.</S><S sid ="10" ssid = "7">For a description of the annotation tool see section 5.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'6'", "'9'", "'10'", "'5'"]
'8'
'6'
'9'
'10'
'5'
['8', '6', '9', '10', '5']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "6">Section 4 deals with the treatment of selected phenomena.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="8" ssid = "5">A formalism complying with these requirements is described in section 3.</S><S sid ="128" ssid = "9">In the first phase  the main functionality for building and displaying unordered trees is supplied.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'6'", "'5'", "'8'", "'128'"]
'9'
'6'
'5'
'8'
'128'
['9', '6', '5', '8', '128']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">For a description of the annotation tool see section 5.</S><S sid ="128" ssid = "9">In the first phase  the main functionality for building and displaying unordered trees is supplied.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="160" ssid = "2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'128'", "'1'", "'160'", "'6'"]
'10'
'128'
'1'
'160'
'6'
['10', '128', '1', '160', '6']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="12" ssid = "2">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="170" ssid = "12">Our annotation tool supplies efficient manipulation and immediate visualization of argument structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'12'", "'162'", "'1'", "'170'"]
'11'
'12'
'162'
'1'
'170'
['11', '12', '162', '1', '170']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "2">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="143" ssid = "24">Sentences annotated in previous steps are used as training material for further processing.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'11'", "'143'", "'162'", "'1'"]
'12'
'11'
'143'
'162'
'1'
['12', '11', '143', '162', '1']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "3">The data-drivenness of this approach presents a clear advantage over the traditional  idealised notion of competence grammar.</S><S sid ="54" ssid = "44">Such a word order independent representation has the advantage of all structural information being encoded in a single data structure.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="43" ssid = "33">An alternative solution is to make argument structure the main structural component of the formalism.</S><S sid ="44" ssid = "34">This assumption underlies a growing number of recent syntactic theories which give up the context-free constituent backbone  cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'54'", "'162'", "'43'", "'44'"]
'13'
'54'
'162'
'43'
'44'
['13', '54', '162', '43', '44']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "4">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S><S sid ="12" ssid = "2">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="166" ssid = "8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'12'", "'162'", "'4'", "'166'"]
'14'
'12'
'162'
'4'
'166'
['14', '12', '162', '4', '166']
parsed_discourse_facet ['method_citation']
['In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.']
['Section 4 deals with the treatment of selected phenomena.', 'We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.', 'These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.', 'In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.', 'In section 2  we examine the appropriateness of existing annotation schemes.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:91', 'F:0']
['We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.']
['In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.', 'In light of these facts  serious difficulties can be expected arising from the structural component of the existing formalisms.', 'The head of the phrase can be determined in a similar way according to theory-specific assumptions.', 'On the basis of these considerations  we formulate several additional requirements.', 'As the need for certain functionalities becomes obvious with growing annotation experience  we have decided to implement the tool in two stages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00159', '(95%-conf.int.', '0.00159', '-', '0.00159)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00292', '(95%-conf.int.', '0.00292', '-', '0.00292)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:55', 'F:1']
['These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.']
['Section 4 deals with the treatment of selected phenomena.', 'A formalism complying with these requirements is described in section 3.', 'In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.', 'In the first phase  the main functionality for building and displaying unordered trees is supplied.', 'In section 2  we examine the appropriateness of existing annotation schemes.']
['system', 'ROUGE-S*', 'Average_R:', '0.00189', '(95%-conf.int.', '0.00189', '-', '0.00189)']
['system', 'ROUGE-S*', 'Average_P:', '0.04762', '(95%-conf.int.', '0.04762', '-', '0.04762)']
['system', 'ROUGE-S*', 'Average_F:', '0.00364', '(95%-conf.int.', '0.00364', '-', '0.00364)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:528', 'P:21', 'F:1']
['In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.']
['Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.', 'We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.', 'Sentences annotated in previous steps are used as training material for further processing.', 'We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.']
['system', 'ROUGE-S*', 'Average_R:', '0.00070', '(95%-conf.int.', '0.00070', '-', '0.00070)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00136', '(95%-conf.int.', '0.00136', '-', '0.00136)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:36', 'F:1']
["The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction."]
['Such a word order independent representation has the advantage of all structural information being encoded in a single data structure.', 'The data-drivenness of this approach presents a clear advantage over the traditional  idealised notion of competence grammar.', 'An alternative solution is to make argument structure the main structural component of the formalism.', 'We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.', 'This assumption underlies a growing number of recent syntactic theories which give up the context-free constituent backbone  cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:66', 'F:0']
["The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction."]
['Syntactically annotated corpora of German have been missing until now.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.', 'Corpora annotated with syntactic structures are commonly referred to as trctbank.5.', 'We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.', "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction."]
['system', 'ROUGE-S*', 'Average_R:', '0.06105', '(95%-conf.int.', '0.06105', '-', '0.06105)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.11508', '(95%-conf.int.', '0.11508', '-', '0.11508)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:66', 'F:66']
['For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).']
[u'The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- \xe2\u20ac\xa2lar representational strata.', 'Since the requirements for such a formalism differ from those posited for configurational languages  several features have been added  influencing the architecture of the scheme.', 'Due to the substantial differences between existing models of constituent structure  the question arises of how the theory independencf requirement  can be satisfied.', 'Since a precise structural description of non-constituent coordination would require a. rich inventory of incomplete phrase types  we have agreed on a sort of unde.rspe.cified representations: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links.', 'Apart from this rather technical problem  two further arguments speak against phrase structure as the structural pivot of the annotation scheme: Finally  the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies  which has proved tedious.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4278', 'P:55', 'F:0']
["Consider the German sentence (1) daran wird ihn Anna erkennen, &di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes."]
['Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.', 'We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.', 'We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.', 'Our annotation tool supplies efficient manipulation and immediate visualization of argument structures.']
['system', 'ROUGE-S*', 'Average_R:', '0.00067', '(95%-conf.int.', '0.00067', '-', '0.00067)']
['system', 'ROUGE-S*', 'Average_P:', '0.00246', '(95%-conf.int.', '0.00246', '-', '0.00246)']
['system', 'ROUGE-S*', 'Average_F:', '0.00106', '(95%-conf.int.', '0.00106', '-', '0.00106)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:406', 'F:1']
['As the need for certain functionalities becomes obvious with growing annotation experience, we have decided to implement the tool in two stages.']
['In the first phase  the main functionality for building and displaying unordered trees is supplied.', 'For a description of the annotation tool see section 5.', 'We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.', 'These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.', 'In section 2  we examine the appropriateness of existing annotation schemes.']
['system', 'ROUGE-S*', 'Average_R:', '0.00405', '(95%-conf.int.', '0.00405', '-', '0.00405)']
['system', 'ROUGE-S*', 'Average_P:', '0.08333', '(95%-conf.int.', '0.08333', '-', '0.08333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00772', '(95%-conf.int.', '0.00772', '-', '0.00772)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:36', 'F:3']
['We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.']
['In addition the approach provides empirical material for psycholinguistic investigation  since preferences for the choice of certain syntactic constructions  linea.rizations  and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.', u'The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- \xe2\u20ac\xa2lar representational strata.', 'Since the requirements for such a formalism differ from those posited for configurational languages  several features have been added  influencing the architecture of the scheme.', 'The data-drivenness of this approach presents a clear advantage over the traditional  idealised notion of competence grammar.', 'Due to the substantial differences between existing models of constituent structure  the question arises of how the theory independencf requirement  can be satisfied.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:55', 'F:0']
['Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.']
['We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.', 'Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.', 'During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.', "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.", 'We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.']
['system', 'ROUGE-S*', 'Average_R:', '0.00190', '(95%-conf.int.', '0.00190', '-', '0.00190)']
['system', 'ROUGE-S*', 'Average_P:', '0.05714', '(95%-conf.int.', '0.05714', '-', '0.05714)']
['system', 'ROUGE-S*', 'Average_F:', '0.00368', '(95%-conf.int.', '0.00368', '-', '0.00368)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:105', 'F:6']
0.112409998978 0.00653181812244 0.0123145453426





input/ref/Task1/A00-2030_sweta.csv
input/res/Task1/A00-2030.csv
parsing: input/ref/Task1/A00-2030_sweta.csv
<S sid="18" ssid="1">Almost all approaches to information extraction &#8212; even at the sentence level &#8212; are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="100" ssid="5">Given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a While our focus throughout the project was on TE and TR, we became curious about how well the model did at part-of-speech tagging, syntactic parsing, and at name finding.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="15">Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
 <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="1">Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.<
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="43" ssid = "3">Thus  we did not consider simply adding semantic labels to the existing Penn TREEBANK  which is drawn from a single source — the Wall Street Journal — and is impoverished in articles about rocket launches.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'10'", "'5'", "'43'", "'104'"]
'1'
'10'
'5'
'43'
'104'
['1', '10', '5', '43', '104']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S><S sid ="26" ssid = "9">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993)  and more recently  had begun using a generative statistical model for name finding (Bikel et al.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'28'", "'96'", "'26'"]
'2'
'6'
'28'
'96'
'26'
['2', '6', '28', '96', '26']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "1">Since 1995  a few statistical parsing algorithms (Magerman  1995; Collins  1996 and 1997; Charniak  1997; Rathnaparki  1997) demonstrated a breakthrough in parsing accuracy  as measured against the University of Pennsylvania TREEBANK as a gold standard.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'1'", "'104'", "'5'", "'96'"]
'3'
'1'
'104'
'5'
'96'
['3', '1', '104', '5', '96']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "2">Yet  relatively few have embedded one of these algorithms in a task.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="108" ssid = "5">This simple semantic annotation was the only source of task knowledge used to configure the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'105'", "'23'", "'28'", "'108'"]
'4'
'105'
'23'
'28'
'108'
['4', '105', '23', '28', '108']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'106'", "'1'", "'28'", "'10'"]
'5'
'106'
'1'
'28'
'10'
['5', '106', '1', '28', '10']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S><S sid ="2" ssid = "2">In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'104'", "'2'", "'51'", "'1'"]
'6'
'104'
'2'
'51'
'1'
['6', '104', '2', '51', '1']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "5">The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.</S><S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="50" ssid = "10">Figure 4 shows an example of the semantic annotation  which was the only type of manual annotation we performed.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'11'", "'28'", "'50'", "'107'"]
'7'
'11'
'28'
'50'
'107'
['7', '11', '28', '50', '107']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "6">Several technical challenges confronted us and were solved: TREEBANK on Wall Street Journal adequately train the algorithm for New York Times newswire  which includes dozens of newspapers?</S><S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'10'", "'5'", "'51'", "'1'"]
'8'
'10'
'5'
'51'
'1'
['8', '10', '5', '51', '1']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "7">Manually creating sourcespecific training data for syntax was not required.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="108" ssid = "5">This simple semantic annotation was the only source of task knowledge used to configure the model.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S><S sid ="41" ssid = "1">To train our integrated model  we required a large corpus of augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'106'", "'108'", "'107'", "'41'"]
'9'
'106'
'108'
'107'
'41'
['9', '106', '108', '107', '41']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="43" ssid = "3">Thus  we did not consider simply adding semantic labels to the existing Penn TREEBANK  which is drawn from a single source — the Wall Street Journal — and is impoverished in articles about rocket launches.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'1'", "'43'", "'51'", "'5'"]
'10'
'1'
'43'
'51'
'5'
['10', '1', '43', '51', '5']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S><S sid ="7" ssid = "5">The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'7'", "'101'", "'5'", "'96'"]
'11'
'7'
'101'
'5'
'96'
['11', '7', '101', '5', '96']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "2">The Template Element (TE) task identifies organizations  persons  locations  and some artifacts (rocket and airplane-related artifacts).</S><S sid ="98" ssid = "3">In both Template Entity (TE) and Template Relation (TR)  our system finished in second place among all entrants.</S><S sid ="16" ssid = "6">For the following example  the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S><S sid ="78" ssid = "19">If we generalize the tree components (constituent labels  words  tags  etc.) and treat them all as simply elements  e  and treat all the conditioning factors as the history  h  we can write:</S><S sid ="34" ssid = "2">In these trees  the standard TREEBANK structures are augmented to convey semantic information  that is  entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'98'", "'16'", "'78'", "'34'"]
'12'
'98'
'16'
'78'
'34'
['12', '98', '16', '78', '34']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="14" ssid = "4">For each person  one must find all of the person's names within the document  his/her type (civilian or military)  and any significant descriptions (e.g.  titles).</S><S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="58" ssid = "4">Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree  semantic pointer labels are attached to all of the intermediate nodes.</S><S sid ="100" ssid = "5">Given multiple constituents that cover identical spans in the chart  only those constituents with probabilities within a While our focus throughout the project was on TE and TR  we became curious about how well the model did at part-of-speech tagging  syntactic parsing  and at name finding.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'14'", "'15'", "'58'", "'100'"]
'13'
'14'
'15'
'58'
'100'
['13', '14', '15', '58', '100']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "4">For each person  one must find all of the person's names within the document  his/her type (civilian or military)  and any significant descriptions (e.g.  titles).</S><S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S><S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'13'", "'74'", "'15'", "'36'"]
'14'
'13'
'74'
'15'
'36'
['14', '13', '74', '15', '36']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="78" ssid = "19">If we generalize the tree components (constituent labels  words  tags  etc.) and treat them all as simply elements  e  and treat all the conditioning factors as the history  h  we can write:</S><S sid ="91" ssid = "10">The probability of generating a constituent of the specified category  starting at the topmost node.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="55" ssid = "1">syntactic modifier of the other  the inserted node serves to indicate the relation as well as the argument.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'78'", "'91'", "'82'", "'55'"]
'15'
'78'
'91'
'82'
'55'
['15', '78', '91', '82', '55']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "6">For the following example  the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S><S sid ="98" ssid = "3">In both Template Entity (TE) and Template Relation (TR)  our system finished in second place among all entrants.</S><S sid ="12" ssid = "2">The Template Element (TE) task identifies organizations  persons  locations  and some artifacts (rocket and airplane-related artifacts).</S><S sid ="94" ssid = "13">Given a new sentence  the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'74'", "'98'", "'12'", "'94'"]
'16'
'74'
'98'
'12'
'94'
['16', '74', '98', '12', '94']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "7">For the following example  the template relation in Figure 2 was to be generated: &quot;Donald M. Goldstein  a historian at the University of Pittsburgh who helped write...&quot;</S><S sid ="50" ssid = "10">Figure 4 shows an example of the semantic annotation  which was the only type of manual annotation we performed.</S><S sid ="35" ssid = "3">An example of an augmented parse tree is shown in Figure 3.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'50'", "'35'", "'82'", "'56'"]
'17'
'50'
'35'
'82'
'56'
['17', '50', '35', '82', '56']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "1">Almost all approaches to information extraction — even at the sentence level — are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.</S><S sid ="19" ssid = "2">Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S><S sid ="29" ssid = "12">Thus  each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="84" ssid = "3">Although mathematically the model predicts tree elements in a top-down fashion  we search the space bottom-up using a chartbased search.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'19'", "'29'", "'5'", "'84'"]
'18'
'19'
'29'
'5'
'84'
['18', '19', '29', '5', '84']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "2">Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S><S sid ="29" ssid = "12">Thus  each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.</S><S sid ="24" ssid = "7">For this reason  we focused on designing an integrated model in which tagging  namefinding  parsing  and semantic interpretation decisions all have the opportunity to mutually influence each other.</S><S sid ="90" ssid = "9">Thus  the scores used in pruning can be considered as the product of: 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'96'", "'29'", "'24'", "'90'"]
'19'
'96'
'29'
'24'
'90'
['19', '96', '29', '24', '90']
parsed_discourse_facet ['method_citation']
['We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.']
['The probability of generating a constituent of the specified category  starting at the topmost node.', 'Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.', 'syntactic modifier of the other  the inserted node serves to indicate the relation as well as the argument.', 'For each location  one must also give its type (city  province  county  body of water  etc.).', 'If we generalize the tree components (constituent labels  words  tags  etc.) and treat them all as simply elements  e  and treat all the conditioning factors as the history  h  we can write:']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:78', 'F:0']
['In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.']
['Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.', 'An example of an augmented parse tree is shown in Figure 3.', 'For example  in the phrase &quot;Lt. Cmdr.', 'Figure 4 shows an example of the semantic annotation  which was the only type of manual annotation we performed.', 'For the following example  the template relation in Figure 2 was to be generated: &quot;Donald M. Goldstein  a historian at the University of Pittsburgh who helped write...&quot;']
['system', 'ROUGE-S*', 'Average_R:', '0.00122', '(95%-conf.int.', '0.00122', '-', '0.00122)']
['system', 'ROUGE-S*', 'Average_P:', '0.00526', '(95%-conf.int.', '0.00526', '-', '0.00526)']
['system', 'ROUGE-S*', 'Average_F:', '0.00198', '(95%-conf.int.', '0.00198', '-', '0.00198)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:190', 'F:1']
['We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction']
['Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.', u'Almost all approaches to information extraction \u2014 even at the sentence level \u2014 are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.', 'Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.', 'Thus  each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.', 'Although mathematically the model predicts tree elements in a top-down fashion  we search the space bottom-up using a chartbased search.']
['system', 'ROUGE-S*', 'Average_R:', '0.00121', '(95%-conf.int.', '0.00121', '-', '0.00121)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00234', '(95%-conf.int.', '0.00234', '-', '0.00234)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:78', 'F:3']
['Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.']
['Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.', 'We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.', 'Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.', 'Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.', 'Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.']
['system', 'ROUGE-S*', 'Average_R:', '0.04262', '(95%-conf.int.', '0.04262', '-', '0.04262)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.08176', '(95%-conf.int.', '0.08176', '-', '0.08176)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:78', 'F:78']
['The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.']
['Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.', 'In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.', 'We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.', 'In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00028', '(95%-conf.int.', '0.00028', '-', '0.00028)']
['system', 'ROUGE-S*', 'Average_P:', '0.00654', '(95%-conf.int.', '0.00654', '-', '0.00654)']
['system', 'ROUGE-S*', 'Average_F:', '0.00054', '(95%-conf.int.', '0.00054', '-', '0.00054)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3570', 'P:153', 'F:1']
['We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.']
['We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).', 'Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.', 'Figure 4 shows an example of the semantic annotation  which was the only type of manual annotation we performed.', 'The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.', 'The semantic training corpus was produced by students according to a simple set of guidelines.']
['system', 'ROUGE-S*', 'Average_R:', '0.00082', '(95%-conf.int.', '0.00082', '-', '0.00082)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00153', '(95%-conf.int.', '0.00153', '-', '0.00153)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:78', 'F:1']
['In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.']
['Manually creating sourcespecific training data for syntax was not required.', 'This simple semantic annotation was the only source of task knowledge used to configure the model.', 'We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.', 'The semantic training corpus was produced by students according to a simple set of guidelines.', 'To train our integrated model  we required a large corpus of augmented parse trees.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:190', 'F:0']
['The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.']
['Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.', 'Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.', 'Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.', 'To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.', u'Thus  we did not consider simply adding semantic labels to the existing Penn TREEBANK  which is drawn from a single source \u2014 the Wall Street Journal \u2014 and is impoverished in articles about rocket launches.']
['system', 'ROUGE-S*', 'Average_R:', '0.00143', '(95%-conf.int.', '0.00143', '-', '0.00143)']
['system', 'ROUGE-S*', 'Average_P:', '0.03268', '(95%-conf.int.', '0.03268', '-', '0.03268)']
['system', 'ROUGE-S*', 'Average_F:', '0.00275', '(95%-conf.int.', '0.00275', '-', '0.00275)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:153', 'F:5']
['In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.']
['Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.', 'For this reason  we focused on designing an integrated model in which tagging  namefinding  parsing  and semantic interpretation decisions all have the opportunity to mutually influence each other.', 'Thus  each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.', 'Thus  the scores used in pruning can be considered as the product of: 1.', 'Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.']
['system', 'ROUGE-S*', 'Average_R:', '0.00140', '(95%-conf.int.', '0.00140', '-', '0.00140)']
['system', 'ROUGE-S*', 'Average_P:', '0.01579', '(95%-conf.int.', '0.01579', '-', '0.01579)']
['system', 'ROUGE-S*', 'Average_F:', '0.00257', '(95%-conf.int.', '0.00257', '-', '0.00257)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:190', 'F:3']
['In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.']
['In both Template Entity (TE) and Template Relation (TR)  our system finished in second place among all entrants.', 'In these trees  the standard TREEBANK structures are augmented to convey semantic information  that is  entities and relations.', 'The Template Element (TE) task identifies organizations  persons  locations  and some artifacts (rocket and airplane-related artifacts).', 'If we generalize the tree components (constituent labels  words  tags  etc.) and treat them all as simply elements  e  and treat all the conditioning factors as the history  h  we can write:', 'For the following example  the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.']
['system', 'ROUGE-S*', 'Average_R:', '0.02098', '(95%-conf.int.', '0.02098', '-', '0.02098)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.04110', '(95%-conf.int.', '0.04110', '-', '0.04110)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:45', 'F:45']
['We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.']
['In both Template Entity (TE) and Template Relation (TR)  our system finished in second place among all entrants.', 'The Template Element (TE) task identifies organizations  persons  locations  and some artifacts (rocket and airplane-related artifacts).', u'The categories for head constituents  cl\u201e are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c\u201e _1  and the head word of their parent  wp.', 'Given a new sentence  the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.', 'For the following example  the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3570', 'P:78', 'F:0']
['In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.']
['For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.', u'The categories for head constituents  cl\u201e are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c\u201e _1  and the head word of their parent  wp.', 'For each location  one must also give its type (city  province  county  body of water  etc.).', "For each person  one must find all of the person's names within the document  his/her type (civilian or military)  and any significant descriptions (e.g.  titles).", 'The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.']
['system', 'ROUGE-S*', 'Average_R:', '0.00035', '(95%-conf.int.', '0.00035', '-', '0.00035)']
['system', 'ROUGE-S*', 'Average_P:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00067', '(95%-conf.int.', '0.00067', '-', '0.00067)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:120', 'F:1']
['Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.']
['Given multiple constituents that cover identical spans in the chart  only those constituents with probabilities within a While our focus throughout the project was on TE and TR  we became curious about how well the model did at part-of-speech tagging  syntactic parsing  and at name finding.', 'For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.', 'For each location  one must also give its type (city  province  county  body of water  etc.).', "For each person  one must find all of the person's names within the document  his/her type (civilian or military)  and any significant descriptions (e.g.  titles).", 'Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree  semantic pointer labels are attached to all of the intermediate nodes.']
['system', 'ROUGE-S*', 'Average_R:', '0.00050', '(95%-conf.int.', '0.00050', '-', '0.00050)']
['system', 'ROUGE-S*', 'Average_P:', '0.00395', '(95%-conf.int.', '0.00395', '-', '0.00395)']
['system', 'ROUGE-S*', 'Average_F:', '0.00088', '(95%-conf.int.', '0.00088', '-', '0.00088)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:253', 'F:1']
0.163371537205 0.00544692303502 0.0104707691502





input/ref/Task1/P08-1028_aakansha.csv
input/res/Task1/P08-1028.csv
parsing: input/ref/Task1/P08-1028_aakansha.csv
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="185" ssid="19">The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.</S>
    <S sid="186" ssid="20">The combined model is best overall with &#961; = 0.19.</S>
    <S sid="187" ssid="21">However, the difference between the two models is not statistically significant.</S>
original cit marker offset is 0
new cit marker offset is 0



["'185'", "'186'"]
'185'
'186'
['185', '186']
parsed_discourse_facet ['result_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="21">The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'"]
'48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="53" ssid="1">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.</S><S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'53'", "'57'"]
'53'
'57'
['53', '57']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="16">Although the composition model in (5) is commonly used in the literature, from a linguistic perspective, the model in (6) is more appealing.</S>
    <S sid="69" ssid="17">Simply adding the vectors u and v lumps their contents together rather than allowing the content of one vector to pick out the relevant content of the other.</S>
    <S sid="70" ssid="18">Instead, it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v, and vice versa.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'69'", "'70'"]
'68'
'69'
'70'
['68', '69', '70']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
    <S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'", "'25'"]
'24'
'25'
['24', '25']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="12">Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'"]
'64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="176" ssid="10">The multiplicative and combined models yield means closer to the human ratings.</S>
    <S sid="177" ssid="11">The difference between High and Low similarity values estimated by these models are statistically significant (p &lt; 0.01 using the Wilcoxon rank sum test).</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'", "'177'"]
'176'
'177'
['176', '177']
parsed_discourse_facet ['method_citation']
<S sid="191" ssid="3">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'191'"]
'191'
['191']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1028.csv
<S sid ="1" ssid = "1">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid ="189" ssid = "1">In this paper we presented a general framework for vector-based semantic composition.</S><S sid ="43" ssid = "16">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid ="24" ssid = "20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'189'", "'43'", "'24'", "'34'"]
'1'
'189'
'43'
'24'
'34'
['1', '189', '43', '24', '34']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S><S sid ="190" ssid = "2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="83" ssid = "31">Although we have presented multiplicative and additive models separately  there is nothing inherent in our formulation that disallows their combination.</S><S sid ="26" ssid = "22">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'190'", "'25'", "'83'", "'26'"]
'2'
'190'
'25'
'83'
'26'
['2', '190', '25', '83', '26']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="26" ssid = "22">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S><S sid ="52" ssid = "25">Under this framework  we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="24" ssid = "20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'26'", "'52'", "'25'", "'24'"]
'3'
'26'
'52'
'25'
'24'
['3', '26', '52', '25', '24']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S><S sid ="27" ssid = "23">Our results show that the multiplicative models are superior and correlate significantly with behavioral data.</S><S sid ="176" ssid = "10">The multiplicative and combined models yield means closer to the human ratings.</S><S sid ="26" ssid = "22">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S><S sid ="174" ssid = "8">The simple additive model fails to distinguish between High and Low Similarity items.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'27'", "'176'", "'26'", "'174'"]
'4'
'27'
'176'
'26'
'174'
['4', '27', '176', '26', '174']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.</S><S sid ="9" ssid = "5">In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S><S sid ="19" ssid = "15">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid ="8" ssid = "4">Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch¨utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'9'", "'20'", "'19'", "'8'"]
'5'
'9'
'20'
'19'
'8'
['5', '9', '20', '19', '8']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).</S><S sid ="21" ssid = "17">Central in these models is the notion of compositionality — the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S><S sid ="138" ssid = "51">Model Parameters Irrespectively of their form  all composition models discussed here are based on a semantic space for representing the meanings of individual words.</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S><S sid ="29" ssid = "2">While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'21'", "'138'", "'20'", "'29'"]
'6'
'21'
'138'
'20'
'29'
['6', '21', '138', '20', '29']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">A variety of NLP tasks have made good use of vector-based models.</S><S sid ="43" ssid = "16">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid ="200" ssid = "12">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid ="1" ssid = "1">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'43'", "'200'", "'1'", "'42'"]
'7'
'43'
'200'
'1'
'42'
['7', '43', '200', '1', '42']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch¨utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).</S><S sid ="19" ssid = "15">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid ="5" ssid = "1">Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.</S><S sid ="193" ssid = "5">Previous applications of vector addition to document indexing (Deerwester et al.  1990) or essay grading (Landauer et al.  1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'19'", "'5'", "'193'", "'20'"]
'8'
'19'
'5'
'193'
'20'
['8', '19', '5', '193', '20']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).</S><S sid ="5" ssid = "1">Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.</S><S sid ="19" ssid = "15">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid ="148" ssid = "61">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid ="193" ssid = "5">Previous applications of vector addition to document indexing (Deerwester et al.  1990) or essay grading (Landauer et al.  1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'5'", "'19'", "'148'", "'193'"]
'9'
'5'
'19'
'148'
'193'
['9', '5', '19', '148', '193']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S><S sid ="141" ssid = "54">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S><S sid ="163" ssid = "76">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S><S sid ="191" ssid = "3">Despite the popularity of additive models  our experimental results showed the superiority of models utilizing multiplicative combinations  at least for the sentence similarity task attempted here.</S><S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'141'", "'163'", "'191'", "'108'"]
'10'
'141'
'163'
'191'
'108'
['10', '141', '163', '191', '108']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">Despite their widespread use  vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature.</S><S sid ="33" ssid = "6">In contrast  symbolic representations can naturally handle the binding of constituents to their roles  in a systematic manner that avoids both these problems.</S><S sid ="6" ssid = "2">The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).</S><S sid ="29" ssid = "2">While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S><S sid ="95" ssid = "8">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'33'", "'6'", "'29'", "'95'"]
'11'
'33'
'6'
'29'
'95'
['11', '33', '6', '29', '95']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">In fact  the commonest method for combining the vectors is to average them.</S><S sid ="77" ssid = "25">For additive models  a natural way to achieve this is to include further vectors into the summation.</S><S sid ="43" ssid = "16">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'77'", "'43'", "'34'", "'39'"]
'12'
'77'
'43'
'34'
'39'
['12', '77', '43', '34', '39']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">Vector averaging is unfortunately insensitive to word order  and more generally syntactic structure  giving the same representation to any constructions that happen to share the same vocabulary.</S><S sid ="6" ssid = "2">The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).</S><S sid ="46" ssid = "19">However  since it is order independent  it cannot capture meaning differences that are modulated by differences in syntactic structure.</S><S sid ="93" ssid = "6">The idea here is that an appropriate composition model when applied to horse and ran will yield a vector closer to the landmark gallop than dissolve.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'6'", "'46'", "'93'", "'194'"]
'13'
'6'
'46'
'93'
'194'
['13', '6', '46', '93', '194']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">This is illustrated in the example below taken from Landauer et al. (1997).</S><S sid ="175" ssid = "9">We observe a similar pattern for the non compositional baseline model  the weighted additive model and Kintsch (2001).</S><S sid ="179" ssid = "13">The results of our correlation analysis are also given in Table 2.</S><S sid ="55" ssid = "3">A hypothetical semantic space is illustrated in Figure 1.</S><S sid ="88" ssid = "1">We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'175'", "'179'", "'55'", "'88'"]
'14'
'175'
'179'
'55'
'88'
['14', '175', '179', '55', '88']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">Sentences (1-a) and (1-b) contain exactly the same set of words but their meaning is entirely different.</S><S sid ="22" ssid = "18">Here  semantic analysis is guided by syntactic structure  and therefore sentences (1-a) and (1-b) receive distinct representations.</S><S sid ="56" ssid = "4">Here  the space has only five dimensions  and the matrix cells denote the co-occurrence of the target words (horse and run) with the context words animal  stable  and so on.</S><S sid ="79" ssid = "27">When modeling predicate-argument structures  Kintsch (2001) proposes including one or more distributional neighbors  n  of the predicate: Note that considerable latitude is allowed in selecting the appropriate neighbors.</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'22'", "'56'", "'79'", "'20'"]
'15'
'22'
'56'
'79'
'20'
['15', '22', '56', '79', '20']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">(1) a.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="15" ssid = "11">Sentences (1-a) and (1-b) contain exactly the same set of words but their meaning is entirely different.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="22" ssid = "18">Here  semantic analysis is guided by syntactic structure  and therefore sentences (1-a) and (1-b) receive distinct representations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'150'", "'15'", "'34'", "'22'"]
'16'
'150'
'15'
'34'
'22'
['16', '150', '15', '34', '22']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">It was not the sales manager who hit the bottle that day  but the office worker with the serious drinking problem. b.</S><S sid ="18" ssid = "14">That day the office manager  who was drinking  hit the problem sales worker with a bottle  but it was not serious.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S><S sid ="174" ssid = "8">The simple additive model fails to distinguish between High and Low Similarity items.</S><S sid ="122" ssid = "35">The opposite is the case for the reference The face glowed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'18'", "'164'", "'174'", "'122'"]
'17'
'18'
'164'
'174'
'122'
['17', '18', '164', '174', '122']
parsed_discourse_facet ['method_citation']
['Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
['This paper proposes a framework for representing the meaning of phrases and sentences in vector space.', 'In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.', 'In this paper we presented a general framework for vector-based semantic composition.', 'Vector addition is by far the most common method for representing the meaning of linguistic sequences.', 'Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.']
['system', 'ROUGE-S*', 'Average_R:', '0.02215', '(95%-conf.int.', '0.02215', '-', '0.02215)']
['system', 'ROUGE-S*', 'Average_P:', '0.19048', '(95%-conf.int.', '0.19048', '-', '0.19048)']
['system', 'ROUGE-S*', 'Average_F:', '0.03968', '(95%-conf.int.', '0.03968', '-', '0.03968)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:105', 'F:20']
['The combined model is best overall with &#961; = 0.19.', 'However, the difference between the two models is not statistically significant.', 'The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.']
['The simple additive model fails to distinguish between High and Low Similarity items.', 'The multiplicative and combined models yield means closer to the human ratings.', 'Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.', 'Our results show that the multiplicative models are superior and correlate significantly with behavioral data.', 'Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.']
['system', 'ROUGE-S*', 'Average_R:', '0.02204', '(95%-conf.int.', '0.02204', '-', '0.02204)']
['system', 'ROUGE-S*', 'Average_P:', '0.17647', '(95%-conf.int.', '0.17647', '-', '0.17647)']
['system', 'ROUGE-S*', 'Average_F:', '0.03919', '(95%-conf.int.', '0.03919', '-', '0.03919)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:153', 'F:27']
['Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
['In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).', 'Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.', 'Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.', u'Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch\xc2\xa8utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).', 'While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).']
['system', 'ROUGE-S*', 'Average_R:', '0.00035', '(95%-conf.int.', '0.00035', '-', '0.00035)']
['system', 'ROUGE-S*', 'Average_P:', '0.02857', '(95%-conf.int.', '0.02857', '-', '0.02857)']
['system', 'ROUGE-S*', 'Average_F:', '0.00070', '(95%-conf.int.', '0.00070', '-', '0.00070)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:8515', 'P:105', 'F:3']
['Although the composition model in (5) is commonly used in the literature, from a linguistic perspective, the model in (6) is more appealing.', 'Simply adding the vectors u and v lumps their contents together rather than allowing the content of one vector to pick out the relevant content of the other.', 'Instead, it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v, and vice versa.']
['In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).', 'While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).', 'Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.', 'Previous applications of vector addition to document indexing (Deerwester et al.  1990) or essay grading (Landauer et al.  1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.', 'In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).']
['system', 'ROUGE-S*', 'Average_R:', '0.00083', '(95%-conf.int.', '0.00083', '-', '0.00083)']
['system', 'ROUGE-S*', 'Average_P:', '0.01478', '(95%-conf.int.', '0.01478', '-', '0.01478)']
['system', 'ROUGE-S*', 'Average_F:', '0.00157', '(95%-conf.int.', '0.00157', '-', '0.00157)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:7260', 'P:406', 'F:6']
['In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.']
[u'A more scrupulous evaluation requires directly correlating all the individual participants\u2019 similarity judgments with those of the models.6 We used Spearman\u2019s p for our correlation analyses.', 'That day the office manager  who was drinking  hit the problem sales worker with a bottle  but it was not serious.', 'The opposite is the case for the reference The face glowed.', 'The simple additive model fails to distinguish between High and Low Similarity items.', 'It was not the sales manager who hit the bottle that day  but the office worker with the serious drinking problem. b.']
['system', 'ROUGE-S*', 'Average_R:', '0.00097', '(95%-conf.int.', '0.00097', '-', '0.00097)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00185', '(95%-conf.int.', '0.00185', '-', '0.00185)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:45', 'F:1']
['In this paper we presented a general framework for vector-based semantic composition.', 'We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['Despite their widespread use  vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature.', 'In contrast  symbolic representations can naturally handle the binding of constituents to their roles  in a systematic manner that avoids both these problems.', 'Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.', 'While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.', 'The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).']
['system', 'ROUGE-S*', 'Average_R:', '0.00581', '(95%-conf.int.', '0.00581', '-', '0.00581)']
['system', 'ROUGE-S*', 'Average_P:', '0.12500', '(95%-conf.int.', '0.12500', '-', '0.12500)']
['system', 'ROUGE-S*', 'Average_F:', '0.01110', '(95%-conf.int.', '0.01110', '-', '0.01110)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:136', 'F:17']
['The difference between High and Low similarity values estimated by these models are statistically significant (p &lt; 0.01 using the Wilcoxon rank sum test).', 'The multiplicative and combined models yield means closer to the human ratings.']
['A hypothetical semantic space is illustrated in Figure 1.', 'We observe a similar pattern for the non compositional baseline model  the weighted additive model and Kintsch (2001).', 'We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).', 'This is illustrated in the example below taken from Landauer et al. (1997).', 'The results of our correlation analysis are also given in Table 2.']
['system', 'ROUGE-S*', 'Average_R:', '0.00427', '(95%-conf.int.', '0.00427', '-', '0.00427)']
['system', 'ROUGE-S*', 'Average_P:', '0.01087', '(95%-conf.int.', '0.01087', '-', '0.01087)']
['system', 'ROUGE-S*', 'Average_F:', '0.00613', '(95%-conf.int.', '0.00613', '-', '0.00613)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:276', 'F:3']
['The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.']
['This paper proposes a framework for representing the meaning of phrases and sentences in vector space.', 'The applications of the framework discussed here are many and varied both for cognitive science and NLP.', 'This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.', 'Vector addition is by far the most common method for representing the meaning of linguistic sequences.', 'A variety of NLP tasks have made good use of vector-based models.']
['system', 'ROUGE-S*', 'Average_R:', '0.00116', '(95%-conf.int.', '0.00116', '-', '0.00116)']
['system', 'ROUGE-S*', 'Average_P:', '0.04762', '(95%-conf.int.', '0.04762', '-', '0.04762)']
['system', 'ROUGE-S*', 'Average_F:', '0.00227', '(95%-conf.int.', '0.00227', '-', '0.00227)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:21', 'F:1']
0.0770012490375 0.00719749991003 0.0128112498399





input/ref/Task1/P05-1013_swastika.csv
input/res/Task1/P05-1013.csv
parsing: input/ref/Task1/P05-1013_swastika.csv
<S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



['49']
49
['49']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="6">In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
<S sid="38" ssid="9">Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="6">In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
Error in Reference Offset
S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
Error in Reference Offset
    <S sid="86" ssid="13">As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
 <S sid="51" ssid="22">In the second scheme, Head+Path, we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p&#8595;.</S>
original cit marker offset is 0
new cit marker offset is 0



['51']
51
['51']
parsed_discourse_facet ['method_citation']
<S sid="99" ssid="10">This may seem surprising, given the experiments reported in section 4, but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift, where the encoding of path information is often redundant.</S>
original cit marker offset is 0
new cit marker offset is 0



['99']
99
['99']
parsed_discourse_facet ['result_citation']
<S sid="7" ssid="3">From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



['2']
2
['2']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/P05-1013.csv
<S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="13" ssid = "9">The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="16" ssid = "12">Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.</S><S sid ="80" ssid = "7">As shown in Table 3  the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'13'", "'7'", "'16'", "'80'"]
'1'
'13'
'7'
'16'
'80'
['1', '13', '7', '16', '80']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="44" ssid = "15">Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="30" ssid = "1">We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.</S><S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'44'", "'36'", "'30'", "'8'"]
'2'
'44'
'36'
'30'
'8'
['2', '44', '36', '30', '8']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy.</S><S sid ="28" ssid = "24">First  in section 4  we evaluate the graph transformation techniques in themselves  with data from the Prague Dependency Treebank and the Danish Dependency Treebank.</S><S sid ="29" ssid = "25">In section 5  we then evaluate the entire parsing system by training and evaluating on data from the Prague Dependency Treebank.</S><S sid ="62" ssid = "1">In the experiments below  we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs 3 previously tested on Swedish (Nivre et al.  2004) and English (Nivre and Scholz  2004).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'28'", "'29'", "'62'", "'75'"]
'3'
'28'
'29'
'62'
'75'
['3', '28', '29', '62', '75']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'104'", "'106'", "'78'", "'8'"]
'4'
'104'
'106'
'78'
'8'
['4', '104', '106', '78', '8']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order  where discontinuous syntactic constructions are more common than in languages like English (Mel’ˇcuk  1988; Covington  1990).</S><S sid ="94" ssid = "5">The first thing to note is that projectivizing helps in itself  even if no encoding is used  as seen from the fact that the projective baseline outperforms the non-projective training condition by more than half a percentage point on attachment score  although the gain is much smaller with respect to exact match.</S><S sid ="99" ssid = "10">This may seem surprising  given the experiments reported in section 4  but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift  where the encoding of path information is often redundant.</S><S sid ="9" ssid = "5">This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).</S><S sid ="47" ssid = "18">In practice  we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'94'", "'99'", "'9'", "'47'"]
'5'
'94'
'99'
'9'
'47'
['5', '94', '99', '9', '47']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.</S><S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="44" ssid = "15">Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.</S><S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'2'", "'36'", "'44'", "'89'"]
'6'
'2'
'36'
'44'
'89'
['6', '2', '36', '44', '89']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="16" ssid = "12">Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.</S><S sid ="19" ssid = "15">Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).</S><S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="47" ssid = "18">In practice  we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'16'", "'19'", "'1'", "'47'"]
'7'
'16'
'19'
'1'
'47'
['7', '16', '19', '1', '47']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S><S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="30" ssid = "1">We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'2'", "'104'", "'4'", "'30'"]
'8'
'2'
'104'
'4'
'30'
['8', '2', '104', '4', '30']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).</S><S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="17" ssid = "13">There exist a few robust broad-coverage parsers that produce non-projective dependency structures  notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English  Foth et al. (2004) for German  and Holan (2004) for Czech.</S><S sid ="105" ssid = "16">Although the best published results for the Collins parser is 80% UAS (Collins  1999)  this parser reaches 82% when trained on the entire training data set  and an adapted version of Charniak’s parser (Charniak  2000) performs at 84% (Jan Hajiˇc  pers. comm.).</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'10'", "'17'", "'105'", "'24'"]
'9'
'10'
'17'
'105'
'24'
['9', '10', '17', '105', '24']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="42" ssid = "13">Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="12" ssid = "8">Prague Dependency Treebank (Hajiˇc et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'42'", "'36'", "'4'", "'12'"]
'10'
'42'
'36'
'4'
'12'
['10', '42', '36', '4', '12']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="6" ssid = "2">However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'6'", "'78'", "'10'", "'36'"]
'11'
'6'
'78'
'10'
'36'
['11', '6', '78', '10', '36']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Prague Dependency Treebank (Hajiˇc et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S><S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="42" ssid = "13">Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'10'", "'36'", "'42'", "'78'"]
'12'
'10'
'36'
'42'
'78'
['12', '10', '36', '42', '78']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.</S><S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'2'", "'95'", "'1'", "'104'"]
'13'
'2'
'95'
'1'
'104'
['13', '2', '95', '1', '104']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 15–25%  the total proportion of non-projective arcs is normally only 1–2%.</S><S sid ="80" ssid = "7">As shown in Table 3  the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S><S sid ="100" ssid = "11">It is likely that the more complex cases  where path information could make a difference  are beyond the reach of the parser in most cases.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'80'", "'106'", "'81'", "'100'"]
'14'
'80'
'106'
'81'
'100'
['14', '80', '106', '81', '100']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">As long as the main evaluation metric is dependency accuracy per word  with state-of-the-art accuracy mostly below 90%  the penalty for not handling non-projective constructions is almost negligible.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="101" ssid = "12">However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.</S><S sid ="110" ssid = "2">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="105" ssid = "16">Although the best published results for the Collins parser is 80% UAS (Collins  1999)  this parser reaches 82% when trained on the entire training data set  and an adapted version of Charniak’s parser (Charniak  2000) performs at 84% (Jan Hajiˇc  pers. comm.).</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'106'", "'101'", "'110'", "'105'"]
'15'
'106'
'101'
'110'
'105'
['15', '106', '101', '110', '105']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="110" ssid = "2">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="19" ssid = "15">Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'7'", "'1'", "'110'", "'19'"]
'16'
'7'
'1'
'110'
'19'
['16', '7', '1', '110', '19']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'104'", "'106'", "'78'", "'8'"]
'4'
'104'
'106'
'78'
'8'
['4', '104', '106', '78', '8']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">In addition  there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington  1990; Kahane et al.  1998; Duchier and Debusmann  2001; Holan et al.  2001; Hellwig  2003).</S><S sid ="17" ssid = "13">There exist a few robust broad-coverage parsers that produce non-projective dependency structures  notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English  Foth et al. (2004) for German  and Holan (2004) for Czech.</S><S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S><S sid ="100" ssid = "11">It is likely that the more complex cases  where path information could make a difference  are beyond the reach of the parser in most cases.</S><S sid ="109" ssid = "1">We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'17'", "'20'", "'100'", "'109'"]
'18'
'17'
'20'
'100'
'109'
['18', '17', '20', '100', '109']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "15">Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="13" ssid = "9">The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.</S><S sid ="107" ssid = "18">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'7'", "'95'", "'13'", "'107'"]
'19'
'7'
'95'
'13'
'107'
['19', '7', '95', '13', '107']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S><S sid ="109" ssid = "1">We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'109'", "'24'", "'60'", "'95'"]
'20'
'109'
'24'
'60'
'95'
['20', '109', '24', '60', '95']
parsed_discourse_facet ['aim_citation']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).', 'It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).', 'We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).', u'There exist a few robust broad-coverage parsers that produce non-projective dependency structures  notably Tapanainen and J\xa8arvinen (1997) and Wang and Harper (2004) for English  Foth et al. (2004) for German  and Holan (2004) for Czech.', u'Although the best published results for the Collins parser is 80% UAS (Collins  1999)  this parser reaches 82% when trained on the entire training data set  and an adapted version of Charniak\u2019s parser (Charniak  2000) performs at 84% (Jan Haji\u02c7c  pers. comm.).']
['system', 'ROUGE-S*', 'Average_R:', '0.00330', '(95%-conf.int.', '0.00330', '-', '0.00330)']
['system', 'ROUGE-S*', 'Average_P:', '0.17143', '(95%-conf.int.', '0.17143', '-', '0.17143)']
['system', 'ROUGE-S*', 'Average_F:', '0.00647', '(95%-conf.int.', '0.00647', '-', '0.00647)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5460', 'P:105', 'F:18']
['In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.']
['Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.', 'However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).', 'This leads to the best reported performance for robust non-projective parsing of Czech.', 'The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.', 'The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.']
['system', 'ROUGE-S*', 'Average_R:', '0.00135', '(95%-conf.int.', '0.00135', '-', '0.00135)']
['system', 'ROUGE-S*', 'Average_P:', '0.04444', '(95%-conf.int.', '0.04444', '-', '0.04444)']
['system', 'ROUGE-S*', 'Average_F:', '0.00261', '(95%-conf.int.', '0.00261', '-', '0.00261)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:45', 'F:2']
['In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.']
['Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.', 'We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.', 'This leads to the best reported performance for robust non-projective parsing of Czech.', 'The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.', 'We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00075', '(95%-conf.int.', '0.00075', '-', '0.00075)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00146', '(95%-conf.int.', '0.00146', '-', '0.00146)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:45', 'F:1']
['From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.']
['In addition  there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington  1990; Kahane et al.  1998; Duchier and Debusmann  2001; Holan et al.  2001; Hellwig  2003).', 'It is likely that the more complex cases  where path information could make a difference  are beyond the reach of the parser in most cases.', 'We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.', 'In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.', u'There exist a few robust broad-coverage parsers that produce non-projective dependency structures  notably Tapanainen and J\xc2\xa8arvinen (1997) and Wang and Harper (2004) for English  Foth et al. (2004) for German  and Holan (2004) for Czech.']
['system', 'ROUGE-S*', 'Average_R:', '0.00301', '(95%-conf.int.', '0.00301', '-', '0.00301)']
['system', 'ROUGE-S*', 'Average_P:', '0.07353', '(95%-conf.int.', '0.07353', '-', '0.07353)']
['system', 'ROUGE-S*', 'Average_F:', '0.00579', '(95%-conf.int.', '0.00579', '-', '0.00579)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3321', 'P:136', 'F:10']
no Reference Text in gold P05-1013
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.']
['We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).', 'In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.', 'We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.', 'In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.', 'The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.']
['system', 'ROUGE-S*', 'Average_R:', '0.01781', '(95%-conf.int.', '0.01781', '-', '0.01781)']
['system', 'ROUGE-S*', 'Average_P:', '0.40952', '(95%-conf.int.', '0.40952', '-', '0.40952)']
['system', 'ROUGE-S*', 'Average_F:', '0.03413', '(95%-conf.int.', '0.03413', '-', '0.03413)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:105', 'F:43']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['This may seem surprising  given the experiments reported in section 4  but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift  where the encoding of path information is often redundant.', 'This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).', u'It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order  where discontinuous syntactic constructions are more common than in languages like English (Mel\u2019\u02c7cuk  1988; Covington  1990).', 'The first thing to note is that projectivizing helps in itself  even if no encoding is used  as seen from the fact that the projective baseline outperforms the non-projective training condition by more than half a percentage point on attachment score  although the gain is much smaller with respect to exact match.', 'In practice  we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00225', '(95%-conf.int.', '0.00225', '-', '0.00225)']
['system', 'ROUGE-S*', 'Average_P:', '0.13333', '(95%-conf.int.', '0.13333', '-', '0.13333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00443', '(95%-conf.int.', '0.00443', '-', '0.00443)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6216', 'P:105', 'F:14']
['The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\xcb\u2021c  1998).', 'Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy.', 'First  in section 4  we evaluate the graph transformation techniques in themselves  with data from the Prague Dependency Treebank and the Danish Dependency Treebank.', 'In section 5  we then evaluate the entire parsing system by training and evaluating on data from the Prague Dependency Treebank.', 'In the experiments below  we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs 3 previously tested on Swedish (Nivre et al.  2004) and English (Nivre and Scholz  2004).']
['system', 'ROUGE-S*', 'Average_R:', '0.00057', '(95%-conf.int.', '0.00057', '-', '0.00057)']
['system', 'ROUGE-S*', 'Average_P:', '0.01471', '(95%-conf.int.', '0.01471', '-', '0.01471)']
['system', 'ROUGE-S*', 'Average_F:', '0.00110', '(95%-conf.int.', '0.00110', '-', '0.00110)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:136', 'F:2']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.', 'Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.', 'We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', 'We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.01876', '(95%-conf.int.', '0.01876', '-', '0.01876)']
['system', 'ROUGE-S*', 'Average_P:', '0.41905', '(95%-conf.int.', '0.41905', '-', '0.41905)']
['system', 'ROUGE-S*', 'Average_F:', '0.03590', '(95%-conf.int.', '0.03590', '-', '0.03590)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:105', 'F:44']
['In the second scheme, Head+Path, we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p&#8595;.']
['In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.', 'Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).', 'The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.', 'From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.', 'Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5151', 'P:120', 'F:0']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).', 'In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.', 'In practice  we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.', 'From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.', 'Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.']
['system', 'ROUGE-S*', 'Average_R:', '0.00614', '(95%-conf.int.', '0.00614', '-', '0.00614)']
['system', 'ROUGE-S*', 'Average_P:', '0.26667', '(95%-conf.int.', '0.26667', '-', '0.26667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01200', '(95%-conf.int.', '0.01200', '-', '0.01200)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4560', 'P:105', 'F:28']
['The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.']
['In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.', 'The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.', 'As shown in Table 3  the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.', 'From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.', 'Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.']
['system', 'ROUGE-S*', 'Average_R:', '0.00922', '(95%-conf.int.', '0.00922', '-', '0.00922)']
['system', 'ROUGE-S*', 'Average_P:', '0.02834', '(95%-conf.int.', '0.02834', '-', '0.02834)']
['system', 'ROUGE-S*', 'Average_F:', '0.01391', '(95%-conf.int.', '0.01391', '-', '0.01391)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:741', 'F:21']
['This may seem surprising, given the experiments reported in section 4, but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift, where the encoding of path information is often redundant.']
['Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.', 'However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).', 'This leads to the best reported performance for robust non-projective parsing of Czech.', 'The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.', 'The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.']
['system', 'ROUGE-S*', 'Average_R:', '0.00404', '(95%-conf.int.', '0.00404', '-', '0.00404)']
['system', 'ROUGE-S*', 'Average_P:', '0.03922', '(95%-conf.int.', '0.03922', '-', '0.03922)']
['system', 'ROUGE-S*', 'Average_F:', '0.00733', '(95%-conf.int.', '0.00733', '-', '0.00733)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:153', 'F:6']
['Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.']
['Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.', 'We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.', 'The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', 'However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.']
['system', 'ROUGE-S*', 'Average_R:', '0.00663', '(95%-conf.int.', '0.00663', '-', '0.00663)']
['system', 'ROUGE-S*', 'Average_P:', '0.35556', '(95%-conf.int.', '0.35556', '-', '0.35556)']
['system', 'ROUGE-S*', 'Average_F:', '0.01301', '(95%-conf.int.', '0.01301', '-', '0.01301)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:45', 'F:16']
no Reference Text in gold P05-1013
0.152155383445 0.00567923072554 0.0106261537644





input/ref/Task1/A00-2018_vardha.csv
input/res/Task1/A00-2018.csv
parsing: input/ref/Task1/A00-2018_vardha.csv
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'91'"]
'91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="8">In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'"]
'39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="162" ssid="53">Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'"]
'162'
['162']
parsed_discourse_facet ['method_citation']
 <S sid="87" ssid="56">In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17].</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'"]
'87'
['87']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'91'"]
'91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="176" ssid="3">That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'"]
'176'
['176']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['method_citation']
 <S sid="101" ssid="12">In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning).</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'"]
'101'
['101']
parsed_discourse_facet ['method_citation']
<S sid="155" ssid="46">For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
  <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="162" ssid="53">Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'"]
'162'
['162']
parsed_discourse_facet ['method_citation']
 <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
 <S sid="79" ssid="48">We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
    <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
    <S sid="175" ssid="2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["'175'"]
'175'
['175']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2018.csv
<S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'5'", "'2'", "'6'", "'125'"]
'1'
'5'
'2'
'6'
'125'
['1', '5', '2', '6', '125']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="175" ssid = "2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'1'", "'125'", "'175'"]
'2'
'6'
'1'
'125'
'175'
['2', '6', '1', '125', '175']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.</S><S sid ="182" ssid = "9">As noted above  the main methodological innovation presented here is our &quot;maximumentropy-inspired&quot; model for conditioning and smoothing.</S><S sid ="128" ssid = "19">The model labeled &quot;Old&quot; attempts to recreate the Char97 system using the current program.</S><S sid ="118" ssid = "9">First  it uses a clustering scheme on words to give the system a &quot;soft&quot; clustering of heads and sub-heads.</S><S sid ="116" ssid = "7">This is as opposed to the &quot;Markovgrammar&quot; approach used in the current parser.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'182'", "'128'", "'118'", "'116'"]
'3'
'182'
'128'
'118'
'116'
['3', '182', '128', '118', '116']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid ="139" ssid = "30">In Figure 2 we show that this one factor improves performance by nearly 2%.</S><S sid ="164" ssid = "55">When we do so using our maximum-entropy-inspired conditioning  we get another 0.45% improvement in average precision/recall  as indicated in Figure 2 on the line labeled &quot;MaxEnt-Inspired'.</S><S sid ="17" ssid = "6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S><S sid ="94" ssid = "5">We use the gathered statistics for all observed words  even those with very low counts  though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'139'", "'164'", "'17'", "'94'"]
'4'
'139'
'164'
'17'
'94'
['4', '139', '164', '17', '94']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'2'", "'6'", "'7'"]
'5'
'1'
'2'
'6'
'7'
['5', '1', '2', '6', '7']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="175" ssid = "2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'1'", "'125'", "'175'"]
'2'
'6'
'1'
'125'
'175'
['2', '6', '1', '125', '175']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid ="171" ssid = "62">As already noted  our best model uses a Markov-grammar approach.</S><S sid ="90" ssid = "1">We created a parser based upon the maximumentropy-inspired model of the last section  smoothed using standard deleted interpolation.</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'171'", "'90'", "'1'", "'5'"]
'7'
'171'
'90'
'1'
'5'
['7', '171', '90', '1', '5']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">That is  for all sentences s and all parses 7r  the parser assigns a probability p(s   7r) = p(r)  the equality holding when we restrict consideration to 7r whose yield * This research was supported in part by NSF grant LIS SBR 9720368.</S><S sid ="12" ssid = "1">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).</S><S sid ="10" ssid = "6">That is  the parser implements the function arg maxrp(7r s) = arg maxirp(7r  s) = arg maxrp(w).</S><S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S><S sid ="147" ssid = "38">For example  when we first guess the lexical head we can move from computing p(r I 1 1p  h) to p(r I /  t  /p  h).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'12'", "'10'", "'41'", "'147'"]
'8'
'12'
'10'
'41'
'147'
['8', '12', '10', '41', '147']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.</S><S sid ="185" ssid = "12">We expect that as we experiment with other  more semantic conditioning information  the importance of this aspect of the model will increase.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="171" ssid = "62">As already noted  our best model uses a Markov-grammar approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'185'", "'5'", "'1'", "'171'"]
'9'
'185'
'5'
'1'
'171'
['9', '185', '5', '1', '171']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">That is  the parser implements the function arg maxrp(7r s) = arg maxirp(7r  s) = arg maxrp(w).</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid ="83" ssid = "52">In the simple (content-free) form (Equation 6)  it is clear that Z(H) = 1.</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="56" ssid = "25">In max-entropy models one can simply include features for all three events f1 (a  b  c)  f2 (a  b)  and f3(a  c) and combine them in the model according to Equation 3  or equivalently  Equation 4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'13'", "'83'", "'54'", "'56'"]
'10'
'13'
'83'
'54'
'56'
['10', '13', '83', '54', '56']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">What fundamentally distinguishes probabilistic generative parsers is how they compute p(r)  and it is to that topic we turn next.</S><S sid ="53" ssid = "22">That is  suppose we want to compute a conditional probability p(a b  c)  but we are not sure that we have enough examples of the conditioning event b  c in the training corpus to ensure that the empirically obtained probability P (a I b  c) is accurate.</S><S sid ="82" ssid = "51">The major advantage of using Equation 7 is that one can generally get away without computing the partition function Z(H).</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="188" ssid = "15">Ultimately it is this flexibility that let us try the various conditioning events  to move on to a Markov grammar approach  and to try several Markov grammars of different orders  without significant programming.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'53'", "'82'", "'54'", "'188'"]
'11'
'53'
'82'
'54'
'188'
['11', '53', '82', '54', '188']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "1">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).</S><S sid ="18" ssid = "7">The method that gives the best results  however  uses a Markov grammar — a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6 10 15].</S><S sid ="134" ssid = "25">As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.</S><S sid ="147" ssid = "38">For example  when we first guess the lexical head we can move from computing p(r I 1 1p  h) to p(r I /  t  /p  h).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'18'", "'134'", "'147'", "'33'"]
'12'
'18'
'134'
'147'
'33'
['12', '18', '134', '147', '33']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid ="16" ssid = "5">In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.</S><S sid ="45" ssid = "14">The function Z(H)  called the partition function  is a normalizing constant (for fixed H)  so the probabilities over all a sum to one.</S><S sid ="83" ssid = "52">In the simple (content-free) form (Equation 6)  it is clear that Z(H) = 1.</S><S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'16'", "'45'", "'83'", "'41'"]
'13'
'16'
'45'
'83'
'41'
['13', '16', '45', '83', '41']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "3">Much of the interesting work is determining what goes into H (c).</S><S sid ="73" ssid = "42">We make one more point on the connection of Equation 7 to a maximum entropy formulation.</S><S sid ="60" ssid = "29">But let us look at how it works for a particular case in our parsing scheme.</S><S sid ="159" ssid = "50">Something very much like this is done in [15].</S><S sid ="151" ssid = "42">We have already noted the importance of conditioning on the parent label /p.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'73'", "'60'", "'159'", "'151'"]
'14'
'73'
'60'
'159'
'151'
['14', '73', '60', '159', '151']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "4">Whenever it is clear to which constituent we are referring we omit the (c) in  e.g.  h(c).</S><S sid ="67" ssid = "36">For example  it does not seem to make much sense to condition on  say  hp without first conditioning on ti .</S><S sid ="151" ssid = "42">We have already noted the importance of conditioning on the parent label /p.</S><S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="140" ssid = "31">It may not be obvious why this should make so great a difference  since most words are effectively unambiguous.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'67'", "'151'", "'157'", "'140'"]
'15'
'67'
'151'
'157'
'140'
['15', '67', '151', '157', '140']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "5">In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.</S><S sid ="65" ssid = "34">We can now rewrite this in the form of Equation 5 as follows: Here we have sequentially conditioned on steadily increasing portions of c's history.</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid ="58" ssid = "27">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid ="46" ssid = "15">Now for our purposes it is useful to rewrite this as a sequence of multiplicative functions gi(a  H) for 0 < i < j: Here go(a  H) = 11Z (H) and gi(a  H) = eAi(a H) fi(° 11).</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'65'", "'13'", "'58'", "'46'"]
'16'
'65'
'13'
'58'
'46'
['16', '65', '13', '58', '46']
parsed_discourse_facet ['method_citation']
['We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.']
['We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.', 'We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.50594', '(95%-conf.int.', '0.50594', '-', '0.50594)']
['system', 'ROUGE-S*', 'Average_P:', '0.76533', '(95%-conf.int.', '0.76533', '-', '0.76533)']
['system', 'ROUGE-S*', 'Average_F:', '0.60917', '(95%-conf.int.', '0.60917', '-', '0.60917)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:946', 'F:724']
['In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.']
["When we do so using our maximum-entropy-inspired conditioning  we get another 0.45% improvement in average precision/recall  as indicated in Figure 2 on the line labeled &quot;MaxEnt-Inspired'.", 'In Figure 2 we show that this one factor improves performance by nearly 2%.', "We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.", 'In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.', 'We use the gathered statistics for all observed words  even those with very low counts  though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:55', 'F:0']
["In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17]."]
['We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.', 'We created a parser based upon the maximumentropy-inspired model of the last section  smoothed using standard deleted interpolation.', 'We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length ', 'As already noted  our best model uses a Markov-grammar approach.', 'Following [5 10]  our parser is based upon a probabilistic generative model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00067', '(95%-conf.int.', '0.00067', '-', '0.00067)']
['system', 'ROUGE-S*', 'Average_P:', '0.03636', '(95%-conf.int.', '0.03636', '-', '0.03636)']
['system', 'ROUGE-S*', 'Average_F:', '0.00131', '(95%-conf.int.', '0.00131', '-', '0.00131)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:55', 'F:2']
['In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.']
['We have already noted the importance of conditioning on the parent label /p.', 'Whenever it is clear to which constituent we are referring we omit the (c) in  e.g.  h(c).', 'It may not be obvious why this should make so great a difference  since most words are effectively unambiguous.', 'Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.', 'For example  it does not seem to make much sense to condition on  say  hp without first conditioning on ti .']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:45', 'F:0']
['Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.']
['We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.', 'This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].', 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].', 'For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.', 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3741', 'P:28', 'F:0']
['As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.']
['As noted above  the main methodological innovation presented here is our &quot;maximumentropy-inspired&quot; model for conditioning and smoothing.', 'The model labeled &quot;Old&quot; attempts to recreate the Char97 system using the current program.', 'The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.', 'This is as opposed to the &quot;Markovgrammar&quot; approach used in the current parser.', 'First  it uses a clustering scheme on words to give the system a &quot;soft&quot; clustering of heads and sub-heads.']
['system', 'ROUGE-S*', 'Average_R:', '0.00376', '(95%-conf.int.', '0.00376', '-', '0.00376)']
['system', 'ROUGE-S*', 'Average_P:', '0.02597', '(95%-conf.int.', '0.02597', '-', '0.02597)']
['system', 'ROUGE-S*', 'Average_F:', '0.00657', '(95%-conf.int.', '0.00657', '-', '0.00657)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:231', 'F:6']
['That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.']
['The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.', 'We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length ', 'We expect that as we experiment with other  more semantic conditioning information  the importance of this aspect of the model will increase.']
['system', 'ROUGE-S*', 'Average_R:', '0.00116', '(95%-conf.int.', '0.00116', '-', '0.00116)']
['system', 'ROUGE-S*', 'Average_P:', '0.00476', '(95%-conf.int.', '0.00476', '-', '0.00476)']
['system', 'ROUGE-S*', 'Average_F:', '0.00187', '(95%-conf.int.', '0.00187', '-', '0.00187)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:210', 'F:1']
['We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.']
['We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.', 'This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].', 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].', 'For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.', 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].']
['system', 'ROUGE-S*', 'Average_R:', '0.16653', '(95%-conf.int.', '0.16653', '-', '0.16653)']
['system', 'ROUGE-S*', 'Average_P:', '0.65856', '(95%-conf.int.', '0.65856', '-', '0.65856)']
['system', 'ROUGE-S*', 'Average_F:', '0.26584', '(95%-conf.int.', '0.26584', '-', '0.26584)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3741', 'P:946', 'F:623']
['This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].']
['Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.', 'Now for our purposes it is useful to rewrite this as a sequence of multiplicative functions gi(a  H) for 0 ', u'Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c \u2014 information outside c that our probability model deems important in determining the probability in question.', "We can now rewrite this in the form of Equation 5 as follows: Here we have sequentially conditioned on steadily increasing portions of c's history.", 'In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:78', 'F:0']
['Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.']
['For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).', u'The method that gives the best results  however  uses a Markov grammar \xe2\u20ac\u201d a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6 10 15].', 'For example  when we first guess the lexical head we can move from computing p(r I 1 1p  h) to p(r I /  t  /p  h).', 'The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).', 'As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:28', 'F:0']
['We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.']
['But let us look at how it works for a particular case in our parsing scheme.', 'Something very much like this is done in [15].', 'We have already noted the importance of conditioning on the parent label /p.', 'We make one more point on the connection of Equation 7 to a maximum entropy formulation.', 'Much of the interesting work is determining what goes into H (c).']
['system', 'ROUGE-S*', 'Average_R:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:210', 'F:2']
0.136409089669 0.062507272159 0.0812981810791





input/ref/Task1/P11-1060_swastika.csv
input/res/Task1/P11-1060.csv
parsing: input/ref/Task1/P11-1060_swastika.csv
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['aim_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['aim_citation']
    <S sid="148" ssid="33">This bootstrapping behavior occurs naturally: The &#8220;easy&#8221; examples are processed first, where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
    <S sid="13" ssid="9">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['result_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="47" ssid="23">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
S sid="44" ssid="20">The recurrence is as follows: At each node, we compute the set of tuples v consistent with the predicate at that node (v &#8712; w(p)), and S(x)}, where a set of pairs S is treated as a set-valued function S(x) = {y : (x, y) &#8712; S} with domain S1 = {x : (x, y) &#8712; S}.</S>
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
Error in Reference Offset
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



['106']
106
['106']
parsed_discourse_facet ['aim_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
    <S sid="172" ssid="57">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms, we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus, and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S>
original cit marker offset is 0
new cit marker offset is 0



['172']
172
['172']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



['171']
171
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S sid ="1" ssid = "1">Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="2" ssid = "2">In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'141'", "'15'", "'6'", "'2'"]
'1'
'141'
'15'
'6'
'2'
['1', '141', '15', '6', '2']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="117" ssid = "2">In each dataset  each sentence x is annotated with a Prolog logical form  which we use only to evaluate and get an answer y.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="149" ssid = "34">Think of DCS as a higher-level Our system learns lexical associations between programming language tailored to natural language  words and predicates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'3'", "'117'", "'100'", "'149'"]
'2'
'3'
'117'
'100'
'149'
['2', '3', '117', '100', '149']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="48" ssid = "24">In addition  trees enable efficient computation  thereby establishing a new connection between dependency syntax and efficient semantic evaluation.</S><S sid ="149" ssid = "34">Think of DCS as a higher-level Our system learns lexical associations between programming language tailored to natural language  words and predicates.</S><S sid ="2" ssid = "2">In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.</S><S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parent’s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'48'", "'149'", "'2'", "'51'"]
'3'
'48'
'149'
'2'
'51'
['3', '48', '149', '2', '51']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">On two stansemantic parsing benchmarks our system obtains the highest published accuracies  despite requiring no annotated logical forms.</S><S sid ="24" ssid = "20">Our system outperforms all existing systems despite using no annotated logical forms.</S><S sid ="136" ssid = "21">In fact  DCS performs comparably to even the version of SEMRESP trained using logical forms.</S><S sid ="139" ssid = "24">All other systems require logical forms as training data  whereas ours does not.</S><S sid ="135" ssid = "20">SEMRESP requires a lexicon of 1.42 words per non-value predicate  WordNet features  and syntactic parse trees; DCS requires only words for the domain-independent predicates (overall  around 0.5 words per non-value predicate)  POS tags  and very simple indicator features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'24'", "'136'", "'139'", "'135'"]
'4'
'24'
'136'
'139'
'135'
['4', '24', '136', '139', '135']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">What is the total population of the ten largest capitals in the US?</S><S sid ="52" ssid = "28">The denotation of the middle node is {s}  where s is all major cities.</S><S sid ="105" ssid = "81">The feature vector φ(x  z) is defined by sums of five simple indicator feature templates: (F1) a word triggers a predicate (e.g.  [city  city]); (F2) a word is under a relation (e.g.  [that  11]); (F3) a word is under a trace predicate (e.g.  [in  loc]); (F4) two predicates are linked via a relation in the left or right direction (e.g.  [city 11  loc  RIGHT]); and (F5) a predicate has a child relation (e.g.  [city  11]).</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S><S sid ="50" ssid = "26">For example  consider the phrase number of major cities  and suppose that number corresponds to the count predicate.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'52'", "'105'", "'62'", "'50'"]
'5'
'52'
'105'
'62'
'50'
['5', '52', '105', '62', '50']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="48" ssid = "24">In addition  trees enable efficient computation  thereby establishing a new connection between dependency syntax and efficient semantic evaluation.</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'169'", "'58'", "'48'", "'165'"]
'6'
'169'
'58'
'48'
'165'
['6', '169', '58', '48', '165']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Supervised semantic parsers (Zelle and Mooney  1996; Tang and Mooney  2001; Ge and Mooney  2005; Zettlemoyer and Collins  2005; Kate and Mooney  2007; Zettlemoyer and Collins  2007; Wong and Mooney  2007; Kwiatkowski et al.  2010) rely on manual annotation of logical forms  which is expensive.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="166" ssid = "51">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid ="125" ssid = "10">We used the same training-test splits as Zettlemoyer and Collins (2005) (600+280 for GEO and 500+140 for JOBS).</S><S sid ="161" ssid = "46">CCG (Steedman  2000)  in which semantic pars- The integration of natural language with denotaing is driven from the lexicon.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'8'", "'166'", "'125'", "'161'"]
'7'
'8'
'166'
'125'
'161'
['7', '8', '166', '125', '161']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="138" ssid = "23">Next  we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S><S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid ="18" ssid = "14">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'138'", "'9'", "'165'", "'18'"]
'8'
'138'
'9'
'165'
'18'
['8', '138', '9', '165', '18']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="20" ssid = "16">At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'58'", "'165'", "'26'", "'20'"]
'9'
'58'
'165'
'26'
'20'
['9', '58', '165', '26', '20']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="37" ssid = "13">The basic version of DCS restricts R to join and aggregate relations (see Table 1).</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'58'", "'169'", "'37'", "'26'"]
'10'
'58'
'169'
'37'
'26'
['10', '58', '169', '37', '26']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts)  producing an answer y.</S><S sid ="118" ssid = "3">This evaluation is done with respect to a world w. Recall that a world w maps each predicate p ∈ P to a set of tuples w(p).</S><S sid ="31" ssid = "7">Conceptually  a world is a relational database where each predicate is a relation (possibly infinite).</S><S sid ="38" ssid = "14">Let us start by considering a DCS tree z with only join relations.</S><S sid ="29" ssid = "5">Let P be a set of predicates (e.g.  state  count ∈ P)  which are just symbols.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'118'", "'31'", "'38'", "'29'"]
'11'
'118'
'31'
'38'
'29'
['11', '118', '31', '38', '29']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="45" ssid = "21">The logical forms in DCS are called DCS trees  where nodes are labeled with predicates  and edges are labeled with relations.</S><S sid ="32" ssid = "8">Define a special predicate ø with w(ø) = V. We represent functions by a set of inputoutput pairs  e.g.  w(count) = {(S  n) : n = |S|}.</S><S sid ="85" ssid = "61">Extraction allows us to return the set of consistent values of a marked non-root node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'169'", "'45'", "'32'", "'85'"]
'12'
'169'
'45'
'32'
'85'
['12', '169', '45', '32', '85']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x  y)  which is much cheaper to obtain than (x  z) pairs.</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S><S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="103" ssid = "79">To define the features  we technically need to augment each tree z ∈ ZL(x) with alignment information—namely  for each predicate in z  the span in x (if any) that triggered it.</S><S sid ="108" ssid = "84">However  in order to learn  we need to sum over {z ∈ ZL(x) : JzKw = y}  and unfortunately  the additional constraint JzKw = y does not factorize.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'62'", "'12'", "'103'", "'108'"]
'13'
'62'
'12'
'103'
'108'
['13', '62', '12', '103', '108']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.</S><S sid ="43" ssid = "19">Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter  2003).</S><S sid ="107" ssid = "83">Our model is arc-factored  so we can sum over all DCS trees in ZL(x) using dynamic programming.</S><S sid ="47" ssid = "23">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.</S><S sid ="59" ssid = "35">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'43'", "'107'", "'47'", "'59'"]
'14'
'43'
'107'
'47'
'59'
['14', '43', '107', '47', '59']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="142" ssid = "27">This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.</S><S sid ="163" ssid = "48">Feedback from the context; for example  the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x  y)  which means (Schuler  2003) and semantic parsing (Popescu et borders looks right for the first argument and left al.  2003; Clarke et al.  2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'100'", "'141'", "'142'", "'163'"]
'15'
'100'
'141'
'142'
'163'
['15', '100', '141', '142', '163']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Which one should we use?</S><S sid ="144" ssid = "29">Intuitions How is our system learning?</S><S sid ="142" ssid = "27">This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.</S><S sid ="95" ssid = "71">Our first question is: given notation of the DCS tree before execution is an utterance x  what trees z ∈ Z are permissible?</S><S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'144'", "'142'", "'95'", "'15'"]
'16'
'144'
'142'
'95'
'15'
['16', '144', '142', '95', '15']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">The dominant paradigm in compositional semantics is Montague semantics  which constructs lambda calculus forms in a bottom-up manner.</S><S sid ="36" ssid = "12">It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.</S><S sid ="37" ssid = "13">The basic version of DCS restricts R to join and aggregate relations (see Table 1).</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="25" ssid = "1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS)  which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'36'", "'37'", "'58'", "'25'"]
'17'
'36'
'37'
'58'
'25'
['17', '36', '37', '58', '25']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="20" ssid = "16">At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.</S><S sid ="167" ssid = "52">In DCS  we start with lexical triggers  which are 6 Conclusion more basic than CCG lexical entries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'165'", "'26'", "'20'", "'167'"]
'18'
'165'
'26'
'20'
'167'
['18', '165', '26', '20', '167']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "15">However  the logical forms there can become quite complex  and in the context of program induction  this would lead to an unwieldy search space.</S><S sid ="122" ssid = "7">For JOBS  if we use the standard Jobs database  close to half the y’s are empty  which makes it uninteresting.</S><S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parent’s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid ="14" ssid = "10">The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.</S><S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'122'", "'51'", "'14'", "'9'"]
'19'
'122'
'51'
'14'
'9'
['19', '122', '51', '14', '9']
parsed_discourse_facet ['method_citation']
['Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.']
['This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.', 'Our model is arc-factored  so we can sum over all DCS trees in ZL(x) using dynamic programming.', 'The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).', 'The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.', 'Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter  2003).']
['system', 'ROUGE-S*', 'Average_R:', '0.00483', '(95%-conf.int.', '0.00483', '-', '0.00483)']
['system', 'ROUGE-S*', 'Average_P:', '0.04000', '(95%-conf.int.', '0.04000', '-', '0.04000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00862', '(95%-conf.int.', '0.00862', '-', '0.00862)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:300', 'F:12']
['The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.', u'The feature vector \xcf\u2020(x  z) is defined by sums of five simple indicator feature templates: (F1) a word triggers a predicate (e.g.  [city  city]); (F2) a word is under a relation (e.g.  [that  11]); (F3) a word is under a trace predicate (e.g.  [in  loc]); (F4) two predicates are linked via a relation in the left or right direction (e.g.  [city 11  loc  RIGHT]); and (F5) a predicate has a child relation (e.g.  [city  11]).', 'What is the total population of the ten largest capitals in the US?', 'For example  consider the phrase number of major cities  and suppose that number corresponds to the count predicate.', 'The denotation of the middle node is {s}  where s is all major cities.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:105', 'F:0']
['Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.']
['However  we still model the logical form (now as a latent variable) to capture the complexities of language.', 'We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.', 'The basic version of DCS restricts R to join and aggregate relations (see Table 1).', 'We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.', 'The combination rules are encoded in the tems  despite using no annotated logical forms.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:300', 'F:0']
['This bootstrapping behavior occurs naturally: The &#8220;easy&#8221; examples are processed first, where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.']
['As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.', 'On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.', 'Next  we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).', 'CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).', 'These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:210', 'F:0']
['This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.']
['However  the logical forms there can become quite complex  and in the context of program induction  this would lead to an unwieldy search space.', 'As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.', u'For JOBS  if we use the standard Jobs database  close to half the y\u2019s are empty  which makes it uninteresting.', u'It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parent\u2019s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.', 'The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.']
['system', 'ROUGE-S*', 'Average_R:', '0.00070', '(95%-conf.int.', '0.00070', '-', '0.00070)']
['system', 'ROUGE-S*', 'Average_P:', '0.01053', '(95%-conf.int.', '0.01053', '-', '0.01053)']
['system', 'ROUGE-S*', 'Average_F:', '0.00132', '(95%-conf.int.', '0.00132', '-', '0.00132)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:190', 'F:2']
['The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).', 'We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.', 'In DCS  we start with lexical triggers  which are 6 Conclusion more basic than CCG lexical entries.', 'At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.', 'These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.']
['system', 'ROUGE-S*', 'Average_R:', '0.00432', '(95%-conf.int.', '0.00432', '-', '0.00432)']
['system', 'ROUGE-S*', 'Average_P:', '0.11429', '(95%-conf.int.', '0.11429', '-', '0.11429)']
['system', 'ROUGE-S*', 'Average_F:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:105', 'F:12']
['We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs.']
['As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.', 'These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.', 'We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.', 'At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.', 'We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.']
['system', 'ROUGE-S*', 'Average_R:', '0.00195', '(95%-conf.int.', '0.00195', '-', '0.00195)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.00381', '(95%-conf.int.', '0.00381', '-', '0.00381)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:66', 'F:6']
['The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.', u'Our first question is: given notation of the DCS tree before execution is an utterance x  what trees z \xe2\u02c6\u02c6 Z are permissible?', 'Intuitions How is our system learning?', 'This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.', 'Which one should we use?']
['system', 'ROUGE-S*', 'Average_R:', '0.00568', '(95%-conf.int.', '0.00568', '-', '0.00568)']
['system', 'ROUGE-S*', 'Average_P:', '0.02857', '(95%-conf.int.', '0.02857', '-', '0.02857)']
['system', 'ROUGE-S*', 'Average_F:', '0.00948', '(95%-conf.int.', '0.00948', '-', '0.00948)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:528', 'P:105', 'F:3']
['Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.']
['Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.', 'In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.', 'Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).', 'Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.', 'Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.']
['system', 'ROUGE-S*', 'Average_R:', '0.00256', '(95%-conf.int.', '0.00256', '-', '0.00256)']
['system', 'ROUGE-S*', 'Average_P:', '0.02000', '(95%-conf.int.', '0.02000', '-', '0.02000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00454', '(95%-conf.int.', '0.00454', '-', '0.00454)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:300', 'F:6']
['Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms, we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus, and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.']
['We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS)  which captures the core idea of using trees to represent formal semantics.', 'The basic version of DCS restricts R to join and aggregate relations (see Table 1).', 'It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.', 'The dominant paradigm in compositional semantics is Montague semantics  which constructs lambda calculus forms in a bottom-up manner.', 'We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.']
['system', 'ROUGE-S*', 'Average_R:', '0.00768', '(95%-conf.int.', '0.00768', '-', '0.00768)']
['system', 'ROUGE-S*', 'Average_P:', '0.02381', '(95%-conf.int.', '0.02381', '-', '0.02381)']
['system', 'ROUGE-S*', 'Average_F:', '0.01161', '(95%-conf.int.', '0.01161', '-', '0.01161)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:630', 'F:15']
['We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.']
['We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.', 'These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.', 'The combination rules are encoded in the tems  despite using no annotated logical forms.', 'In addition  trees enable efficient computation  thereby establishing a new connection between dependency syntax and efficient semantic evaluation.', 'Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).']
['system', 'ROUGE-S*', 'Average_R:', '0.01010', '(95%-conf.int.', '0.01010', '-', '0.01010)']
['system', 'ROUGE-S*', 'Average_P:', '0.13450', '(95%-conf.int.', '0.13450', '-', '0.13450)']
['system', 'ROUGE-S*', 'Average_F:', '0.01878', '(95%-conf.int.', '0.01878', '-', '0.01878)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:171', 'F:23']
0.0420554541631 0.00343818178693 0.0060445453996





input/ref/Task1/P08-1028_sweta.csv
input/res/Task1/P08-1028.csv
parsing: input/ref/Task1/P08-1028_sweta.csv
<S sid="65" ssid="13">So, if we assume that only the ith components of u and v contribute to the ith component of p, that these components are not dependent on i, and that the function is symmetric with regard to the interchange of u and v, we obtain a simpler instantiation of an additive model: Analogously, under the same assumptions, we obtain the following simpler multiplicative model: only the ith components of u and v contribute to the ith component of p. Another class of models can be derived by relaxing this constraint.</S>
original cit marker offset is 0
new cit marker offset is 0



["65'"]
65'
['65']
parsed_discourse_facet ['method_citation']
 <S sid="75" ssid="23">An extreme form of this differential in the contribution of constituents is where one of the vectors, say u, contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic, however it can serve as a simple baseline against which to compare more sophisticated models.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["42'"]
42'
['42']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="17">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="195" ssid="7">The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately.</S>
original cit marker offset is 0
new cit marker offset is 0



["195'"]
195'
['195']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["57'"]
57'
['57']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["190'"]
190'
['190']
parsed_discourse_facet ['method_citation']
 <S sid="29" ssid="2">While neural networks can readily represent single distinct objects, in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>
original cit marker offset is 0
new cit marker offset is 0



["29'"]
29'
['29']
parsed_discourse_facet ['method_citation']
    <S sid="38" ssid="11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="12">Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="163" ssid="76">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["163'"]
163'
['163']
parsed_discourse_facet ['method_citation']
<S sid="185" ssid="19">The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.</S>
original cit marker offset is 0
new cit marker offset is 0



["185'"]
185'
['185']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["190'"]
190'
['190']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1028.csv
<S sid ="1" ssid = "1">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid ="189" ssid = "1">In this paper we presented a general framework for vector-based semantic composition.</S><S sid ="43" ssid = "16">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid ="24" ssid = "20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'189'", "'43'", "'24'", "'34'"]
'1'
'189'
'43'
'24'
'34'
['1', '189', '43', '24', '34']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S><S sid ="190" ssid = "2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="83" ssid = "31">Although we have presented multiplicative and additive models separately  there is nothing inherent in our formulation that disallows their combination.</S><S sid ="26" ssid = "22">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'190'", "'25'", "'83'", "'26'"]
'2'
'190'
'25'
'83'
'26'
['2', '190', '25', '83', '26']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="26" ssid = "22">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S><S sid ="52" ssid = "25">Under this framework  we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="24" ssid = "20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'26'", "'52'", "'25'", "'24'"]
'3'
'26'
'52'
'25'
'24'
['3', '26', '52', '25', '24']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S><S sid ="27" ssid = "23">Our results show that the multiplicative models are superior and correlate significantly with behavioral data.</S><S sid ="176" ssid = "10">The multiplicative and combined models yield means closer to the human ratings.</S><S sid ="26" ssid = "22">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S><S sid ="174" ssid = "8">The simple additive model fails to distinguish between High and Low Similarity items.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'27'", "'176'", "'26'", "'174'"]
'4'
'27'
'176'
'26'
'174'
['4', '27', '176', '26', '174']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.</S><S sid ="9" ssid = "5">In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S><S sid ="19" ssid = "15">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid ="8" ssid = "4">Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch¨utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'9'", "'20'", "'19'", "'8'"]
'5'
'9'
'20'
'19'
'8'
['5', '9', '20', '19', '8']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).</S><S sid ="21" ssid = "17">Central in these models is the notion of compositionality — the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S><S sid ="138" ssid = "51">Model Parameters Irrespectively of their form  all composition models discussed here are based on a semantic space for representing the meanings of individual words.</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S><S sid ="29" ssid = "2">While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'21'", "'138'", "'20'", "'29'"]
'6'
'21'
'138'
'20'
'29'
['6', '21', '138', '20', '29']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">A variety of NLP tasks have made good use of vector-based models.</S><S sid ="43" ssid = "16">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid ="200" ssid = "12">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid ="1" ssid = "1">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'43'", "'200'", "'1'", "'42'"]
'7'
'43'
'200'
'1'
'42'
['7', '43', '200', '1', '42']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch¨utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).</S><S sid ="19" ssid = "15">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid ="5" ssid = "1">Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.</S><S sid ="193" ssid = "5">Previous applications of vector addition to document indexing (Deerwester et al.  1990) or essay grading (Landauer et al.  1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'19'", "'5'", "'193'", "'20'"]
'8'
'19'
'5'
'193'
'20'
['8', '19', '5', '193', '20']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).</S><S sid ="5" ssid = "1">Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.</S><S sid ="19" ssid = "15">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid ="148" ssid = "61">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid ="193" ssid = "5">Previous applications of vector addition to document indexing (Deerwester et al.  1990) or essay grading (Landauer et al.  1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'5'", "'19'", "'148'", "'193'"]
'9'
'5'
'19'
'148'
'193'
['9', '5', '19', '148', '193']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S><S sid ="141" ssid = "54">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S><S sid ="163" ssid = "76">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S><S sid ="191" ssid = "3">Despite the popularity of additive models  our experimental results showed the superiority of models utilizing multiplicative combinations  at least for the sentence similarity task attempted here.</S><S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'141'", "'163'", "'191'", "'108'"]
'10'
'141'
'163'
'191'
'108'
['10', '141', '163', '191', '108']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">Despite their widespread use  vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature.</S><S sid ="33" ssid = "6">In contrast  symbolic representations can naturally handle the binding of constituents to their roles  in a systematic manner that avoids both these problems.</S><S sid ="6" ssid = "2">The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).</S><S sid ="29" ssid = "2">While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S><S sid ="95" ssid = "8">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'33'", "'6'", "'29'", "'95'"]
'11'
'33'
'6'
'29'
'95'
['11', '33', '6', '29', '95']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">In fact  the commonest method for combining the vectors is to average them.</S><S sid ="77" ssid = "25">For additive models  a natural way to achieve this is to include further vectors into the summation.</S><S sid ="43" ssid = "16">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'77'", "'43'", "'34'", "'39'"]
'12'
'77'
'43'
'34'
'39'
['12', '77', '43', '34', '39']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">Vector averaging is unfortunately insensitive to word order  and more generally syntactic structure  giving the same representation to any constructions that happen to share the same vocabulary.</S><S sid ="6" ssid = "2">The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).</S><S sid ="46" ssid = "19">However  since it is order independent  it cannot capture meaning differences that are modulated by differences in syntactic structure.</S><S sid ="93" ssid = "6">The idea here is that an appropriate composition model when applied to horse and ran will yield a vector closer to the landmark gallop than dissolve.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'6'", "'46'", "'93'", "'194'"]
'13'
'6'
'46'
'93'
'194'
['13', '6', '46', '93', '194']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">This is illustrated in the example below taken from Landauer et al. (1997).</S><S sid ="175" ssid = "9">We observe a similar pattern for the non compositional baseline model  the weighted additive model and Kintsch (2001).</S><S sid ="179" ssid = "13">The results of our correlation analysis are also given in Table 2.</S><S sid ="55" ssid = "3">A hypothetical semantic space is illustrated in Figure 1.</S><S sid ="88" ssid = "1">We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'175'", "'179'", "'55'", "'88'"]
'14'
'175'
'179'
'55'
'88'
['14', '175', '179', '55', '88']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">Sentences (1-a) and (1-b) contain exactly the same set of words but their meaning is entirely different.</S><S sid ="22" ssid = "18">Here  semantic analysis is guided by syntactic structure  and therefore sentences (1-a) and (1-b) receive distinct representations.</S><S sid ="56" ssid = "4">Here  the space has only five dimensions  and the matrix cells denote the co-occurrence of the target words (horse and run) with the context words animal  stable  and so on.</S><S sid ="79" ssid = "27">When modeling predicate-argument structures  Kintsch (2001) proposes including one or more distributional neighbors  n  of the predicate: Note that considerable latitude is allowed in selecting the appropriate neighbors.</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'22'", "'56'", "'79'", "'20'"]
'15'
'22'
'56'
'79'
'20'
['15', '22', '56', '79', '20']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">(1) a.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="15" ssid = "11">Sentences (1-a) and (1-b) contain exactly the same set of words but their meaning is entirely different.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="22" ssid = "18">Here  semantic analysis is guided by syntactic structure  and therefore sentences (1-a) and (1-b) receive distinct representations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'150'", "'15'", "'34'", "'22'"]
'16'
'150'
'15'
'34'
'22'
['16', '150', '15', '34', '22']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">It was not the sales manager who hit the bottle that day  but the office worker with the serious drinking problem. b.</S><S sid ="18" ssid = "14">That day the office manager  who was drinking  hit the problem sales worker with a bottle  but it was not serious.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S><S sid ="174" ssid = "8">The simple additive model fails to distinguish between High and Low Similarity items.</S><S sid ="122" ssid = "35">The opposite is the case for the reference The face glowed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'18'", "'164'", "'174'", "'122'"]
'17'
'18'
'164'
'174'
'122'
['17', '18', '164', '174', '122']
parsed_discourse_facet ['method_citation']
['An extreme form of this differential in the contribution of constituents is where one of the vectors, say u, contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic, however it can serve as a simple baseline against which to compare more sophisticated models.']
['This paper proposes a framework for representing the meaning of phrases and sentences in vector space.', 'In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.', 'In this paper we presented a general framework for vector-based semantic composition.', 'Vector addition is by far the most common method for representing the meaning of linguistic sequences.', 'Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.']
['system', 'ROUGE-S*', 'Average_R:', '0.00111', '(95%-conf.int.', '0.00111', '-', '0.00111)']
['system', 'ROUGE-S*', 'Average_P:', '0.00585', '(95%-conf.int.', '0.00585', '-', '0.00585)']
['system', 'ROUGE-S*', 'Average_F:', '0.00186', '(95%-conf.int.', '0.00186', '-', '0.00186)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:171', 'F:1']
['The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately.']
['The simple additive model fails to distinguish between High and Low Similarity items.', 'The multiplicative and combined models yield means closer to the human ratings.', 'Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.', 'Our results show that the multiplicative models are superior and correlate significantly with behavioral data.', 'Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.']
['system', 'ROUGE-S*', 'Average_R:', '0.00490', '(95%-conf.int.', '0.00490', '-', '0.00490)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.00921', '(95%-conf.int.', '0.00921', '-', '0.00921)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:78', 'F:6']
['We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).']
['A hypothetical semantic space is illustrated in Figure 1.', 'We observe a similar pattern for the non compositional baseline model  the weighted additive model and Kintsch (2001).', 'We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).', 'This is illustrated in the example below taken from Landauer et al. (1997).', 'The results of our correlation analysis are also given in Table 2.']
['system', 'ROUGE-S*', 'Average_R:', '0.00996', '(95%-conf.int.', '0.00996', '-', '0.00996)']
['system', 'ROUGE-S*', 'Average_P:', '0.12727', '(95%-conf.int.', '0.12727', '-', '0.12727)']
['system', 'ROUGE-S*', 'Average_F:', '0.01847', '(95%-conf.int.', '0.01847', '-', '0.01847)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:55', 'F:7']
['We present a general framework for vector-based composition which allows us to consider different classes of models.']
['In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).', 'Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.', 'Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.', u'Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch\xc2\xa8utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).', 'While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).']
['system', 'ROUGE-S*', 'Average_R:', '0.00035', '(95%-conf.int.', '0.00035', '-', '0.00035)']
['system', 'ROUGE-S*', 'Average_P:', '0.10714', '(95%-conf.int.', '0.10714', '-', '0.10714)']
['system', 'ROUGE-S*', 'Average_F:', '0.00070', '(95%-conf.int.', '0.00070', '-', '0.00070)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:8515', 'P:28', 'F:3']
['We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).', 'While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).', 'Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.', 'Previous applications of vector addition to document indexing (Deerwester et al.  1990) or essay grading (Landauer et al.  1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.', 'In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).']
['system', 'ROUGE-S*', 'Average_R:', '0.00083', '(95%-conf.int.', '0.00083', '-', '0.00083)']
['system', 'ROUGE-S*', 'Average_P:', '0.16667', '(95%-conf.int.', '0.16667', '-', '0.16667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00164', '(95%-conf.int.', '0.00164', '-', '0.00164)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:7260', 'P:36', 'F:6']
['We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
[u'A more scrupulous evaluation requires directly correlating all the individual participants\u2019 similarity judgments with those of the models.6 We used Spearman\u2019s p for our correlation analyses.', 'That day the office manager  who was drinking  hit the problem sales worker with a bottle  but it was not serious.', 'The opposite is the case for the reference The face glowed.', 'The simple additive model fails to distinguish between High and Low Similarity items.', 'It was not the sales manager who hit the bottle that day  but the office worker with the serious drinking problem. b.']
['system', 'ROUGE-S*', 'Average_R:', '0.00097', '(95%-conf.int.', '0.00097', '-', '0.00097)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00187', '(95%-conf.int.', '0.00187', '-', '0.00187)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:36', 'F:1']
['The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.']
['Despite their widespread use  vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature.', 'In contrast  symbolic representations can naturally handle the binding of constituents to their roles  in a systematic manner that avoids both these problems.', 'Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.', 'While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.', 'The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:55', 'F:0']
['This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.']
['We present a general framework for vector-based composition which allows us to consider different classes of models.', 'Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.', 'Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.', 'We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.', 'Although we have presented multiplicative and additive models separately  there is nothing inherent in our formulation that disallows their combination.']
['system', 'ROUGE-S*', 'Average_R:', '0.00089', '(95%-conf.int.', '0.00089', '-', '0.00089)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00171', '(95%-conf.int.', '0.00171', '-', '0.00171)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:45', 'F:1']
['Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.']
['This paper proposes a framework for representing the meaning of phrases and sentences in vector space.', 'The applications of the framework discussed here are many and varied both for cognitive science and NLP.', 'This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.', 'Vector addition is by far the most common method for representing the meaning of linguistic sequences.', 'A variety of NLP tasks have made good use of vector-based models.']
['system', 'ROUGE-S*', 'Average_R:', '0.00348', '(95%-conf.int.', '0.00348', '-', '0.00348)']
['system', 'ROUGE-S*', 'Average_P:', '0.02206', '(95%-conf.int.', '0.02206', '-', '0.02206)']
['system', 'ROUGE-S*', 'Average_F:', '0.00602', '(95%-conf.int.', '0.00602', '-', '0.00602)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:136', 'F:3']
0.0617677770915 0.00249888886112 0.00460888883768





input/ref/Task1/D09-1092_vardha.csv
input/res/Task1/D09-1092.csv
parsing: input/ref/Task1/D09-1092_vardha.csv
<S sid="17" ssid="13">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
 <S sid="20" ssid="16">We also explore how the characteristics of different languages affect topic model performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
    <S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
   <S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'"]
'55'
['55']
parsed_discourse_facet ['method_citation']
 sid="9" ssid="5">In this paper, we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
Error in Reference Offset
<S sid="118" ssid="67">We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
 <S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'"]
'55'
['55']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="51">In contrast, PLTM assigns a significant number of tokens to almost all 800 topics, in very similar proportions in both languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'"]
'102'
['102']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="4">This is unlike LDA, in which each document is assumed to have its own document-specific distribution over topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'"]
'38'
['38']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="105">We use both Jensen-Shannon divergence and cosine distance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'"]
'156'
['156']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="2">Each tuple is a set of documents that are loosely equivalent to each other, but written in different languages, e.g., corresponding Wikipedia articles in French, English and German.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
  <S sid="170" ssid="4">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
    <S sid="29" ssid="5">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
  <S sid="170" ssid="4">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="168" ssid="2">However, the growth of the web, and in particular Wikipedia, has made comparable text corpora &#8211; documents that are topically similar but are not direct translations of one another &#8211; considerably more abundant than true parallel corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'168'"]
'168'
['168']
parsed_discourse_facet ['method_citation']
    <S sid="29" ssid="5">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
 <S sid="110" ssid="59">Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'"]
'110'
['110']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="105" ssid = "54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S><S sid ="194" ssid = "3">We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'25'", "'105'", "'194'", "'126'"]
'1'
'25'
'105'
'194'
'126'
['1', '25', '105', '194', '126']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Meanwhile  massive collections of interlinked documents in dozens of languages  such as Wikipedia  are now widely available  calling for tools that can characterize content in many languages.</S><S sid ="16" ssid = "12">However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.</S><S sid ="168" ssid = "2">However  the growth of the web  and in particular Wikipedia  has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid ="196" ssid = "5">When applied to comparable document collections such as Wikipedia  PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S><S sid ="105" ssid = "54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'16'", "'168'", "'196'", "'105'"]
'2'
'16'
'168'
'196'
'105'
['2', '16', '168', '196', '105']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="192" ssid = "1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="20" ssid = "16">We also explore how the characteristics of different languages affect topic model performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'18'", "'31'", "'20'"]
'3'
'192'
'18'
'31'
'20'
['3', '192', '18', '31', '20']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="22" ssid = "18">We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages  and to detect differences in topic emphasis between languages.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="10" ssid = "6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'22'", "'19'", "'10'", "'126'"]
'4'
'22'
'19'
'10'
'126'
['4', '22', '19', '10', '126']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Statistical topic models have emerged as an increasingly useful analysis tool for large text collections.</S><S sid ="15" ssid = "11">Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="6" ssid = "2">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'15'", "'18'", "'3'", "'6'"]
'5'
'15'
'18'
'3'
'6'
['5', '15', '18', '3', '6']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S><S sid ="26" ssid = "2">Tam  Lane and Schultz (Tam et al.  2007) also show improvements in machine translation using bilingual topic models.</S><S sid ="5" ssid = "1">Statistical topic models have emerged as an increasingly useful analysis tool for large text collections.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="29" ssid = "5">A recent extended abstract  developed concurrently by Ni et al. (Ni et al.  2009)  discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'26'", "'5'", "'18'", "'29'"]
'6'
'26'
'5'
'18'
'29'
['6', '26', '5', '18', '29']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Much of this work  however  has occurred in monolingual contexts.</S><S sid ="15" ssid = "11">Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'15'", "'31'", "'18'", "'161'"]
'7'
'15'
'31'
'18'
'161'
['7', '15', '31', '18', '161']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">In an increasingly connected world  the ability to access documents in many languages has become both a strategic asset and a personally enriching experience.</S><S sid ="16" ssid = "12">However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.</S><S sid ="168" ssid = "2">However  the growth of the web  and in particular Wikipedia  has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="108" ssid = "57">If we wish to perform topic-based bibliometric analysis  it is vital to be able to track the same topics across all languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'16'", "'168'", "'23'", "'108'"]
'8'
'16'
'168'
'23'
'108'
['8', '16', '168', '23', '108']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">In this paper  we present the polylingual topic model (PLTM).</S><S sid ="192" ssid = "1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="169" ssid = "3">In this section  we explore two questions relating to comparable text corpora and polylingual topic modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'192'", "'18'", "'3'", "'169'"]
'9'
'192'
'18'
'3'
'169'
['9', '192', '18', '3', '169']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="22" ssid = "18">We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages  and to detect differences in topic emphasis between languages.</S><S sid ="111" ssid = "60">In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts – i.e.  we put each of these documents in a single-document tuple.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'19'", "'22'", "'111'"]
'10'
'4'
'19'
'22'
'111'
['10', '4', '19', '22', '111']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">There are many potential applications for polylingual topic models.</S><S sid ="169" ssid = "3">In this section  we explore two questions relating to comparable text corpora and polylingual topic modeling.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'169'", "'3'", "'18'", "'31'"]
'11'
'169'
'3'
'18'
'31'
['11', '169', '3', '18', '31']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Although research literature is typically written in English  bibliographic databases often contain substantial quantities of work in other languages.</S><S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S><S sid ="174" ssid = "8">These versions of Wikipedia were selected to provide a diverse range of language families  geographic areas  and quantities of text.</S><S sid ="7" ssid = "3">Much of this work  however  has occurred in monolingual contexts.</S><S sid ="14" ssid = "10">Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'161'", "'174'", "'7'", "'14'"]
'12'
'161'
'174'
'7'
'14'
['12', '161', '174', '7', '14']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">To perform topic-based bibliometric analysis on these collections  it is necessary to have topic models that are aligned across languages.</S><S sid ="108" ssid = "57">If we wish to perform topic-based bibliometric analysis  it is vital to be able to track the same topics across all languages.</S><S sid ="196" ssid = "5">When applied to comparable document collections such as Wikipedia  PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S><S sid ="193" ssid = "2">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'108'", "'196'", "'126'", "'193'"]
'13'
'108'
'196'
'126'
'193'
['13', '108', '196', '126', '193']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.</S><S sid ="20" ssid = "16">We also explore how the characteristics of different languages affect topic model performance.</S><S sid ="170" ssid = "4">First  we explore whether comparable document tuples support the alignment of fine-grained topics  as demonstrated earlier using parallel documents.</S><S sid ="53" ssid = "2">In this case  we can be confident that the topic distribution is genuinely shared across all languages.</S><S sid ="68" ssid = "17">The third topic demonstrates differences in inflectional variation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'20'", "'170'", "'53'", "'68'"]
'14'
'20'
'170'
'53'
'68'
['14', '20', '170', '53', '68']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.</S><S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'52'", "'31'", "'3'", "'18'"]
'15'
'52'
'31'
'3'
'18'
['15', '52', '31', '3', '18']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.</S><S sid ="168" ssid = "2">However  the growth of the web  and in particular Wikipedia  has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid ="2" ssid = "2">Meanwhile  massive collections of interlinked documents in dozens of languages  such as Wikipedia  are now widely available  calling for tools that can characterize content in many languages.</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="21" ssid = "17">The second corpus  Wikipedia articles in twelve languages  contains sets of documents that are not translations of one another  but are very likely to be about similar concepts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'168'", "'2'", "'54'", "'21'"]
'16'
'168'
'2'
'54'
'21'
['16', '168', '2', '54', '21']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S><S sid ="194" ssid = "3">We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.</S><S sid ="105" ssid = "54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S><S sid ="13" ssid = "9">To perform topic-based bibliometric analysis on these collections  it is necessary to have topic models that are aligned across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'126'", "'194'", "'105'", "'13'"]
'17'
'126'
'194'
'105'
'13'
['17', '126', '194', '105', '13']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="169" ssid = "3">In this section  we explore two questions relating to comparable text corpora and polylingual topic modeling.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="9" ssid = "5">In this paper  we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'169'", "'3'", "'31'", "'9'"]
'18'
'169'
'3'
'31'
'9'
['18', '169', '3', '31', '9']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'148'", "'131'", "'4'", "'126'"]
'19'
'148'
'131'
'4'
'126'
['19', '148', '131', '4', '126']
parsed_discourse_facet ['method_citation']
['We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).']
['We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.', 'We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).', 'These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.', u'We explore the model\xe2\u20ac\u2122s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.', 'We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages  and to detect differences in topic emphasis between languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.05115', '(95%-conf.int.', '0.05115', '-', '0.05115)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.09732', '(95%-conf.int.', '0.09732', '-', '0.09732)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:120', 'F:120']
['We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.']
['In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'We also explore how the characteristics of different languages affect topic model performance.', 'They also provide little analysis of the differences between polylingual and single-language topic models.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:78', 'F:0']
['Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.']
['We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', 'These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.', u'We explore the model\u2019s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.', 'To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.']
['system', 'ROUGE-S*', 'Average_R:', '0.00133', '(95%-conf.int.', '0.00133', '-', '0.00133)']
['system', 'ROUGE-S*', 'Average_P:', '0.02614', '(95%-conf.int.', '0.02614', '-', '0.02614)']
['system', 'ROUGE-S*', 'Average_F:', '0.00253', '(95%-conf.int.', '0.00253', '-', '0.00253)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:153', 'F:4']
['We also explore how the characteristics of different languages affect topic model performance.']
['When applied to comparable document collections such as Wikipedia  PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.', 'Meanwhile  massive collections of interlinked documents in dozens of languages  such as Wikipedia  are now widely available  calling for tools that can characterize content in many languages.', u'However  the growth of the web  and in particular Wikipedia  has made comparable text corpora \u2013 documents that are topically similar but are not direct translations of one another \u2013 considerably more abundant than true parallel corpora.', 'An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.', u'However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts\u2014documents that are topically similar but are not direct translations of one another\u2014considerably more abundant than ever before.']
['system', 'ROUGE-S*', 'Average_R:', '0.00093', '(95%-conf.int.', '0.00093', '-', '0.00093)']
['system', 'ROUGE-S*', 'Average_P:', '0.14286', '(95%-conf.int.', '0.14286', '-', '0.14286)']
['system', 'ROUGE-S*', 'Average_F:', '0.00184', '(95%-conf.int.', '0.00184', '-', '0.00184)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3240', 'P:21', 'F:3']
['We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.']
['In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.', 'They also provide little analysis of the differences between polylingual and single-language topic models.', 'Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.', 'As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.', 'Much of this work  however  has occurred in monolingual contexts.']
['system', 'ROUGE-S*', 'Average_R:', '0.00106', '(95%-conf.int.', '0.00106', '-', '0.00106)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00193', '(95%-conf.int.', '0.00193', '-', '0.00193)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:91', 'F:1']
['The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.']
['Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).', 'Statistical topic models have emerged as an increasingly useful analysis tool for large text collections.', 'In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.']
['system', 'ROUGE-S*', 'Average_R:', '0.00065', '(95%-conf.int.', '0.00065', '-', '0.00065)']
['system', 'ROUGE-S*', 'Average_P:', '0.01053', '(95%-conf.int.', '0.01053', '-', '0.01053)']
['system', 'ROUGE-S*', 'Average_F:', '0.00122', '(95%-conf.int.', '0.00122', '-', '0.00122)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:190', 'F:2']
['First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.']
['The third topic demonstrates differences in inflectional variation.', 'We also explore how the characteristics of different languages affect topic model performance.', 'Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.', 'In this case  we can be confident that the topic distribution is genuinely shared across all languages.', 'First  we explore whether comparable document tuples support the alignment of fine-grained topics  as demonstrated earlier using parallel documents.']
['system', 'ROUGE-S*', 'Average_R:', '0.09619', '(95%-conf.int.', '0.09619', '-', '0.09619)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.17551', '(95%-conf.int.', '0.17551', '-', '0.17551)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:91', 'F:91']
['A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.']
['In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.', 'They also provide little analysis of the differences between polylingual and single-language topic models.', 'Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'Our first set of experiments focuses on document tuples that are known to consist of direct translations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00082', '(95%-conf.int.', '0.00082', '-', '0.00082)']
['system', 'ROUGE-S*', 'Average_P:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00149', '(95%-conf.int.', '0.00149', '-', '0.00149)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:120', 'F:1']
['We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.']
['Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.', 'Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).', 'We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.', 'These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.', 'An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00870', '(95%-conf.int.', '0.00870', '-', '0.00870)']
['system', 'ROUGE-S*', 'Average_P:', '0.38182', '(95%-conf.int.', '0.38182', '-', '0.38182)']
['system', 'ROUGE-S*', 'Average_F:', '0.01700', '(95%-conf.int.', '0.01700', '-', '0.01700)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:55', 'F:21']
['However, the growth of the web, and in particular Wikipedia, has made comparable text corpora &#8211; documents that are topically similar but are not direct translations of one another &#8211; considerably more abundant than true parallel corpora.']
['We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.', 'To perform topic-based bibliometric analysis on these collections  it is necessary to have topic models that are aligned across languages.', 'These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.', 'We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.', 'An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00512', '(95%-conf.int.', '0.00512', '-', '0.00512)']
['system', 'ROUGE-S*', 'Average_P:', '0.05848', '(95%-conf.int.', '0.05848', '-', '0.05848)']
['system', 'ROUGE-S*', 'Average_F:', '0.00942', '(95%-conf.int.', '0.00942', '-', '0.00942)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:171', 'F:10']
0.263914997361 0.0165949998341 0.0308259996917





input/ref/Task1/P08-1102_sweta.csv
input/res/Task1/P08-1102.csv
parsing: input/ref/Task1/P08-1102_sweta.csv
<S sid="21" ssid="17">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="24">A subsequence of boundary-POS labelling result indicates a word with POS t only if the boundary tag sequence composed of its boundary part conforms to s or bm*e style, and all POS tags in its POS part equal to t. For example, a tag sequence b NN m NN e NN represents a threecharacter word with POS tag NN.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="43" ssid="15">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["43'"]
43'
['43']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["92'"]
92'
['92']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="5">To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&amp;T).</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="5">In following subsections, we describe the feature templates and the perceptron training algorithm.</S>
    <S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["33'", "'34'"]
33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="4">By maintaining a stack of size N at each position i of the sequence, we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S>
original cit marker offset is 0
new cit marker offset is 0



["79'"]
79'
['79']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="7">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["96'"]
96'
['96']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="16" ssid="12">Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="7">To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="130" ssid = "1">We proposed a cascaded linear model for Chinese Joint S&T.</S><S sid ="76" ssid = "1">Sequence segmentation and labelling problem can be solved through a viterbi style decoding procedure.</S><S sid ="24" ssid = "20">It is a better idea to perform segmentation and POS tagging jointly in a uniform framework.</S><S sid ="4" ssid = "4">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'130'", "'76'", "'24'", "'4'"]
'1'
'130'
'76'
'24'
'4'
['1', '130', '76', '24', '4']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="64" ssid = "15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'16'", "'133'", "'64'", "'134'"]
'2'
'16'
'133'
'64'
'134'
['2', '16', '133', '64', '134']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid ="21" ssid = "17">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S sid ="106" ssid = "17">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.</S><S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="4" ssid = "4">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'21'", "'106'", "'1'", "'4'"]
'3'
'21'
'106'
'1'
'4'
['3', '21', '106', '1', '4']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S><S sid ="127" ssid = "38">Finally  the word count penalty gives improvement to the cascaded model  0.13 points on segmentation and 0.16 points on Joint S&T.</S><S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="123" ssid = "34">Without it  the F-measure on segmentation and Joint S&T both suffer a decrement of 0.2 points.</S><S sid ="118" ssid = "29">We find that the cascaded model achieves a F-measure increment of about 0.5 points on segmentation and about 0.9 points on Joint S&T  over the perceptron-only model POS+.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'127'", "'1'", "'123'", "'118'"]
'4'
'127'
'1'
'123'
'118'
['4', '127', '1', '123', '118']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages.</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="51" ssid = "2">Additional features most widely used are related to word or POS ngrams.</S><S sid ="114" ssid = "25">We used SRI Language Modelling Toolkit (Stolcke and Andreas  2002) to train a 3gram word LM with modified Kneser-Ney smoothing (Chen and Goodman  1998)  and a 4-gram POS LM with Witten-Bell smoothing  and we trained a word-POS co-occurrence model simply by MLE without smoothing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'31'", "'1'", "'51'", "'114'"]
'5'
'31'
'1'
'51'
'114'
['5', '31', '1', '51', '114']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Several models were introduced for these problems  for example  the Hidden Markov Model (HMM) (Rabiner  1989)  Maximum Entropy Model (ME) (Ratnaparkhi and Adwait  1996)  and Conditional Random Fields (CRFs) (Lafferty et al.  2001).</S><S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="15" ssid = "11">To cope with this problem  we propose a cascaded linear model inspired by the log-linear model (Och and Ney  2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'8'", "'134'", "'15'", "'45'"]
'6'
'8'
'134'
'15'
'45'
['6', '8', '134', '15', '45']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S><S sid ="100" ssid = "11">Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS  CityU and MSR).</S><S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'8'", "'100'", "'14'", "'31'"]
'7'
'8'
'100'
'14'
'31'
['7', '8', '100', '14', '31']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="29" ssid = "1">The perceptron algorithm introduced into NLP by Collins (2002)  is a simple but effective discriminative training method.</S><S sid ="115" ssid = "26">To obtain their corresponding weights  we adapted the minimum-error-rate training algorithm (Och  2003) to train the outside-layer model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'45'", "'49'", "'29'", "'115'"]
'8'
'45'
'49'
'29'
'115'
['8', '45', '49', '29', '115']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid ="23" ssid = "19">We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag.</S><S sid ="11" ssid = "7">Compared to performing segmentation and POS tagging one at a time  Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low  2004).</S><S sid ="80" ssid = "5">At each position i  we enumerate all possible word-POS pairs by assigning each POS to each possible word formed from the character subsequence spanning length l = L. min(i  K) (K is assigned 20 in all our experiments) and ending at position i  then we derive all candidate results by attaching each word-POS pair p (of length l) to the tail of each candidate result at the prior position of p (position i−l)  and select for position i a N-best list of candidate results from all these candidates.</S><S sid ="81" ssid = "6">When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p  we calculate the scores of the word LM  the POS LM  the labelling probability and the generating probability  Algorithm 2 Decoding algorithm. as well as the score of the perceptron model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'23'", "'11'", "'80'", "'81'"]
'9'
'23'
'11'
'80'
'81'
['9', '23', '11', '80', '81']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen  2003)  Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low  2004).</S><S sid ="25" ssid = "21">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S><S sid ="23" ssid = "19">We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="106" ssid = "17">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'25'", "'23'", "'7'", "'106'"]
'10'
'25'
'23'
'7'
'106'
['10', '25', '23', '7', '106']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">Compared to performing segmentation and POS tagging one at a time  Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low  2004).</S><S sid ="9" ssid = "5">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid ="3" ssid = "3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid ="26" ssid = "22">In order to perform POS tagging at the same time  we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004).</S><S sid ="106" ssid = "17">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'9'", "'3'", "'26'", "'106'"]
'11'
'9'
'3'
'26'
'106'
['11', '9', '3', '26', '106']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Besides the usual character-based features  additional features dependent on POS’s or words can also be employed to improve the performance.</S><S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid ="54" ssid = "5">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S><S sid ="51" ssid = "2">Additional features most widely used are related to word or POS ngrams.</S><S sid ="82" ssid = "7">In addition  we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'14'", "'54'", "'51'", "'82'"]
'12'
'14'
'54'
'51'
'82'
['12', '14', '54', '51', '82']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">However  as such features are generated dynamically during the decoding procedure  two limitation arise: on the one hand  the amount of parameters increases rapidly  which is apt to overfit on training corpus; on the other hand  exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.</S><S sid ="52" ssid = "3">However  such features are generated dynamically during the decoding procedure so that the feature space enlarges much more rapidly.</S><S sid ="55" ssid = "6">In addition  even though these higher grams were managed to be used  there still remains another problem: as the current predication relies on the results of prior ones  the decoding procedure has to resort to approximate inference by maintaining a list of N-best candidates at each predication position  which evokes a potential risk to depress the training.</S><S sid ="131" ssid = "2">Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.</S><S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'52'", "'55'", "'131'", "'14'"]
'13'
'52'
'55'
'131'
'14'
['13', '52', '55', '131', '14']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid ="12" ssid = "8">Besides the usual character-based features  additional features dependent on POS’s or words can also be employed to improve the performance.</S><S sid ="131" ssid = "2">Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.</S><S sid ="132" ssid = "3">This is a substitute method to use both local and non-local features  and it would be especially useful when the training corpus is very large.</S><S sid ="54" ssid = "5">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'12'", "'131'", "'132'", "'54'"]
'14'
'12'
'131'
'132'
'54'
['14', '12', '131', '132', '54']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">To cope with this problem  we propose a cascaded linear model inspired by the log-linear model (Och and Ney  2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.</S><S sid ="56" ssid = "7">To alleviate the drawbacks  we propose a cascaded linear model.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'56'", "'49'", "'45'", "'8'"]
'15'
'56'
'49'
'45'
'8'
['15', '56', '49', '45', '8']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="65" ssid = "16">As shown in Figure 1  the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.</S><S sid ="57" ssid = "8">It has a two-layer architecture  with a perceptron as the core and another linear model as the outside-layer.</S><S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="78" ssid = "3">Given a Chinese character sequence C1:n  the decoding procedure can proceed in a left-right fashion with a dynamic programming approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'65'", "'57'", "'2'", "'78'"]
'16'
'65'
'57'
'2'
'78'
['16', '65', '57', '2', '78']
parsed_discourse_facet ['method_citation']
Length 0 input/ref/Task1/P08-1102_sweta.csv
0.0 0.0 0.0





input/ref/Task1/A97-1014_vardha.csv
input/res/Task1/A97-1014.csv
parsing: input/ref/Task1/A97-1014_vardha.csv
<S sid="72" ssid="17">In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
 <S sid="144" ssid="25">We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="4">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
  <S sid="36" ssid="26">As for free word order languages, the following features may cause problems: sition between the two poles.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
 <S sid="24" ssid="14">The typical treebank architecture is as follows: Structures: A context-free backbone is augmented with trace-filler representations of non-local dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
 <S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'"]
'151'
['151']
parsed_discourse_facet ['method_citation']
 <S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
 <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["'167'"]
'167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'"]
'39'
['39']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'"]
'151'
['151']
parsed_discourse_facet ['method_citation']
<S sid="71" ssid="16">However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'71'"]
'71'
['71']
parsed_discourse_facet ['method_citation']
 <S sid="144" ssid="25">We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A97-1014.csv
<S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'162'", "'11'", "'5'", "'4'"]
'1'
'162'
'11'
'5'
'4'
['1', '162', '11', '5', '4']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Since the requirements for such a formalism differ from those posited for configurational languages  several features have been added  influencing the architecture of the scheme.</S><S sid ="3" ssid = "3">The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.</S><S sid ="31" ssid = "21">Due to the substantial differences between existing models of constituent structure  the question arises of how the theory independencf requirement  can be satisfied.</S><S sid ="13" ssid = "3">The data-drivenness of this approach presents a clear advantage over the traditional  idealised notion of competence grammar.</S><S sid ="165" ssid = "7">In addition the approach provides empirical material for psycholinguistic investigation  since preferences for the choice of certain syntactic constructions  linea.rizations  and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'3'", "'31'", "'13'", "'165'"]
'2'
'3'
'31'
'13'
'165'
['2', '3', '31', '13', '165']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.</S><S sid ="31" ssid = "21">Due to the substantial differences between existing models of constituent structure  the question arises of how the theory independencf requirement  can be satisfied.</S><S sid ="113" ssid = "26">Since a precise structural description of non-constituent coordination would require a. rich inventory of incomplete phrase types  we have agreed on a sort of unde.rspe.cified representations: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links.</S><S sid ="2" ssid = "2">Since the requirements for such a formalism differ from those posited for configurational languages  several features have been added  influencing the architecture of the scheme.</S><S sid ="41" ssid = "31">Apart from this rather technical problem  two further arguments speak against phrase structure as the structural pivot of the annotation scheme: Finally  the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies  which has proved tedious.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'31'", "'113'", "'2'", "'41'"]
'3'
'31'
'113'
'2'
'41'
['3', '31', '113', '2', '41']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'1'", "'162'", "'74'", "'11'"]
'4'
'1'
'162'
'74'
'11'
['4', '1', '162', '74', '11']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'162'", "'1'", "'6'", "'74'"]
'5'
'162'
'1'
'6'
'74'
['5', '162', '1', '6', '74']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="9" ssid = "6">Section 4 deals with the treatment of selected phenomena.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="160" ssid = "2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'9'", "'5'", "'1'", "'160'"]
'6'
'9'
'5'
'1'
'160'
['6', '9', '5', '1', '160']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">On the basis of these considerations  we formulate several additional requirements.</S><S sid ="37" ssid = "27">In light of these facts  serious difficulties can be expected arising from the structural component of the existing formalisms.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="104" ssid = "17">The head of the phrase can be determined in a similar way according to theory-specific assumptions.</S><S sid ="127" ssid = "8">As the need for certain functionalities becomes obvious with growing annotation experience  we have decided to implement the tool in two stages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'37'", "'5'", "'104'", "'127'"]
'7'
'37'
'5'
'104'
'127'
['7', '37', '5', '104', '127']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">A formalism complying with these requirements is described in section 3.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="9" ssid = "6">Section 4 deals with the treatment of selected phenomena.</S><S sid ="10" ssid = "7">For a description of the annotation tool see section 5.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'6'", "'9'", "'10'", "'5'"]
'8'
'6'
'9'
'10'
'5'
['8', '6', '9', '10', '5']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "6">Section 4 deals with the treatment of selected phenomena.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="8" ssid = "5">A formalism complying with these requirements is described in section 3.</S><S sid ="128" ssid = "9">In the first phase  the main functionality for building and displaying unordered trees is supplied.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'6'", "'5'", "'8'", "'128'"]
'9'
'6'
'5'
'8'
'128'
['9', '6', '5', '8', '128']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">For a description of the annotation tool see section 5.</S><S sid ="128" ssid = "9">In the first phase  the main functionality for building and displaying unordered trees is supplied.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="160" ssid = "2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'128'", "'1'", "'160'", "'6'"]
'10'
'128'
'1'
'160'
'6'
['10', '128', '1', '160', '6']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="12" ssid = "2">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="170" ssid = "12">Our annotation tool supplies efficient manipulation and immediate visualization of argument structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'12'", "'162'", "'1'", "'170'"]
'11'
'12'
'162'
'1'
'170'
['11', '12', '162', '1', '170']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "2">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="143" ssid = "24">Sentences annotated in previous steps are used as training material for further processing.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'11'", "'143'", "'162'", "'1'"]
'12'
'11'
'143'
'162'
'1'
['12', '11', '143', '162', '1']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "3">The data-drivenness of this approach presents a clear advantage over the traditional  idealised notion of competence grammar.</S><S sid ="54" ssid = "44">Such a word order independent representation has the advantage of all structural information being encoded in a single data structure.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="43" ssid = "33">An alternative solution is to make argument structure the main structural component of the formalism.</S><S sid ="44" ssid = "34">This assumption underlies a growing number of recent syntactic theories which give up the context-free constituent backbone  cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'54'", "'162'", "'43'", "'44'"]
'13'
'54'
'162'
'43'
'44'
['13', '54', '162', '43', '44']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "4">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S><S sid ="12" ssid = "2">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="166" ssid = "8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'12'", "'162'", "'4'", "'166'"]
'14'
'12'
'162'
'4'
'166'
['14', '12', '162', '4', '166']
parsed_discourse_facet ['method_citation']
['These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.']
['Section 4 deals with the treatment of selected phenomena.', 'We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.', 'These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.', 'In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.', 'In section 2  we examine the appropriateness of existing annotation schemes.']
['system', 'ROUGE-S*', 'Average_R:', '0.03153', '(95%-conf.int.', '0.03153', '-', '0.03153)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.06114', '(95%-conf.int.', '0.06114', '-', '0.06114)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:21', 'F:21']
['For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).']
['In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.', 'In light of these facts  serious difficulties can be expected arising from the structural component of the existing formalisms.', 'The head of the phrase can be determined in a similar way according to theory-specific assumptions.', 'On the basis of these considerations  we formulate several additional requirements.', 'As the need for certain functionalities becomes obvious with growing annotation experience  we have decided to implement the tool in two stages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:55', 'F:0']
['These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.']
['Section 4 deals with the treatment of selected phenomena.', 'A formalism complying with these requirements is described in section 3.', 'In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.', 'In the first phase  the main functionality for building and displaying unordered trees is supplied.', 'In section 2  we examine the appropriateness of existing annotation schemes.']
['system', 'ROUGE-S*', 'Average_R:', '0.00189', '(95%-conf.int.', '0.00189', '-', '0.00189)']
['system', 'ROUGE-S*', 'Average_P:', '0.04762', '(95%-conf.int.', '0.04762', '-', '0.04762)']
['system', 'ROUGE-S*', 'Average_F:', '0.00364', '(95%-conf.int.', '0.00364', '-', '0.00364)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:528', 'P:21', 'F:1']
['For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).']
['Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.', 'We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.', 'Sentences annotated in previous steps are used as training material for further processing.', 'We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.']
['system', 'ROUGE-S*', 'Average_R:', '0.00210', '(95%-conf.int.', '0.00210', '-', '0.00210)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00404', '(95%-conf.int.', '0.00404', '-', '0.00404)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:55', 'F:3']
['However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.']
['Such a word order independent representation has the advantage of all structural information being encoded in a single data structure.', 'The data-drivenness of this approach presents a clear advantage over the traditional  idealised notion of competence grammar.', 'An alternative solution is to make argument structure the main structural component of the formalism.', 'We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.', 'This assumption underlies a growing number of recent syntactic theories which give up the context-free constituent backbone  cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00060', '(95%-conf.int.', '0.00060', '-', '0.00060)']
['system', 'ROUGE-S*', 'Average_P:', '0.03571', '(95%-conf.int.', '0.03571', '-', '0.03571)']
['system', 'ROUGE-S*', 'Average_F:', '0.00119', '(95%-conf.int.', '0.00119', '-', '0.00119)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:28', 'F:1']
['These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.']
['Syntactically annotated corpora of German have been missing until now.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.', 'Corpora annotated with syntactic structures are commonly referred to as trctbank.5.', 'We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.', "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction."]
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:21', 'F:0']
['Corpora annotated with syntactic structures are commonly referred to as trctbank.5.']
[u'The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- \xe2\u20ac\xa2lar representational strata.', 'Since the requirements for such a formalism differ from those posited for configurational languages  several features have been added  influencing the architecture of the scheme.', 'Due to the substantial differences between existing models of constituent structure  the question arises of how the theory independencf requirement  can be satisfied.', 'Since a precise structural description of non-constituent coordination would require a. rich inventory of incomplete phrase types  we have agreed on a sort of unde.rspe.cified representations: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links.', 'Apart from this rather technical problem  two further arguments speak against phrase structure as the structural pivot of the annotation scheme: Finally  the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies  which has proved tedious.']
['system', 'ROUGE-S*', 'Average_R:', '0.00023', '(95%-conf.int.', '0.00023', '-', '0.00023)']
['system', 'ROUGE-S*', 'Average_P:', '0.03571', '(95%-conf.int.', '0.03571', '-', '0.03571)']
['system', 'ROUGE-S*', 'Average_F:', '0.00046', '(95%-conf.int.', '0.00046', '-', '0.00046)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4278', 'P:28', 'F:1']
["Consider the German sentence (1) daran wird ihn Anna erkennen, &di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes."]
['Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.', 'We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.', 'We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.', 'Our annotation tool supplies efficient manipulation and immediate visualization of argument structures.']
['system', 'ROUGE-S*', 'Average_R:', '0.00067', '(95%-conf.int.', '0.00067', '-', '0.00067)']
['system', 'ROUGE-S*', 'Average_P:', '0.00246', '(95%-conf.int.', '0.00246', '-', '0.00246)']
['system', 'ROUGE-S*', 'Average_F:', '0.00106', '(95%-conf.int.', '0.00106', '-', '0.00106)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:406', 'F:1']
['In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.']
['In the first phase  the main functionality for building and displaying unordered trees is supplied.', 'For a description of the annotation tool see section 5.', 'We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.', 'These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.', 'In section 2  we examine the appropriateness of existing annotation schemes.']
['system', 'ROUGE-S*', 'Average_R:', '0.00270', '(95%-conf.int.', '0.00270', '-', '0.00270)']
['system', 'ROUGE-S*', 'Average_P:', '0.02198', '(95%-conf.int.', '0.02198', '-', '0.02198)']
['system', 'ROUGE-S*', 'Average_F:', '0.00481', '(95%-conf.int.', '0.00481', '-', '0.00481)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:91', 'F:2']
['We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.']
['In addition the approach provides empirical material for psycholinguistic investigation  since preferences for the choice of certain syntactic constructions  linea.rizations  and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.', u'The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- \xe2\u20ac\xa2lar representational strata.', 'Since the requirements for such a formalism differ from those posited for configurational languages  several features have been added  influencing the architecture of the scheme.', 'The data-drivenness of this approach presents a clear advantage over the traditional  idealised notion of competence grammar.', 'Due to the substantial differences between existing models of constituent structure  the question arises of how the theory independencf requirement  can be satisfied.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:21', 'F:0']
['Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.']
['We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.', 'Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.', 'During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.', "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.", 'We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.']
['system', 'ROUGE-S*', 'Average_R:', '0.00190', '(95%-conf.int.', '0.00190', '-', '0.00190)']
['system', 'ROUGE-S*', 'Average_P:', '0.05714', '(95%-conf.int.', '0.05714', '-', '0.05714)']
['system', 'ROUGE-S*', 'Average_F:', '0.00368', '(95%-conf.int.', '0.00368', '-', '0.00368)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:105', 'F:6']
0.114106362599 0.00378363632924 0.00727454538841





input/ref/Task1/P08-1102_aakansha.csv
input/res/Task1/P08-1102.csv
parsing: input/ref/Task1/P08-1102_aakansha.csv
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="14">As predications generated from such templates depend on the current character, we name these templates lexical-target.</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="121" ssid="32">Among other features, the 4-gram POS LM plays the most important role, removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'121'"]
'121'
['121']
parsed_discourse_facet ['method_citation']
<S sid="37" ssid="9">C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'"]
'37'
['37']
parsed_discourse_facet ['method_citation']
<S sid="73" ssid="24">For instance, if the word w appears N times in training corpus and is labelled as POS t for n times, the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'73'"]
'73'
['73']
parsed_discourse_facet ['method_citation']
<S sid="46" ssid="18">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) &#8712; X &#215; Y to a feature vector 4)(x, y) &#8712; Rd, and a parameter vector &#945;&#65533; &#8712; Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'"]
'46'
['46']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="130" ssid = "1">We proposed a cascaded linear model for Chinese Joint S&T.</S><S sid ="76" ssid = "1">Sequence segmentation and labelling problem can be solved through a viterbi style decoding procedure.</S><S sid ="24" ssid = "20">It is a better idea to perform segmentation and POS tagging jointly in a uniform framework.</S><S sid ="4" ssid = "4">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'130'", "'76'", "'24'", "'4'"]
'1'
'130'
'76'
'24'
'4'
['1', '130', '76', '24', '4']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="64" ssid = "15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'16'", "'133'", "'64'", "'134'"]
'2'
'16'
'133'
'64'
'134'
['2', '16', '133', '64', '134']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid ="21" ssid = "17">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S sid ="106" ssid = "17">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.</S><S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="4" ssid = "4">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'21'", "'106'", "'1'", "'4'"]
'3'
'21'
'106'
'1'
'4'
['3', '21', '106', '1', '4']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S><S sid ="127" ssid = "38">Finally  the word count penalty gives improvement to the cascaded model  0.13 points on segmentation and 0.16 points on Joint S&T.</S><S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="123" ssid = "34">Without it  the F-measure on segmentation and Joint S&T both suffer a decrement of 0.2 points.</S><S sid ="118" ssid = "29">We find that the cascaded model achieves a F-measure increment of about 0.5 points on segmentation and about 0.9 points on Joint S&T  over the perceptron-only model POS+.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'127'", "'1'", "'123'", "'118'"]
'4'
'127'
'1'
'123'
'118'
['4', '127', '1', '123', '118']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages.</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="51" ssid = "2">Additional features most widely used are related to word or POS ngrams.</S><S sid ="114" ssid = "25">We used SRI Language Modelling Toolkit (Stolcke and Andreas  2002) to train a 3gram word LM with modified Kneser-Ney smoothing (Chen and Goodman  1998)  and a 4-gram POS LM with Witten-Bell smoothing  and we trained a word-POS co-occurrence model simply by MLE without smoothing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'31'", "'1'", "'51'", "'114'"]
'5'
'31'
'1'
'51'
'114'
['5', '31', '1', '51', '114']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Several models were introduced for these problems  for example  the Hidden Markov Model (HMM) (Rabiner  1989)  Maximum Entropy Model (ME) (Ratnaparkhi and Adwait  1996)  and Conditional Random Fields (CRFs) (Lafferty et al.  2001).</S><S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="15" ssid = "11">To cope with this problem  we propose a cascaded linear model inspired by the log-linear model (Och and Ney  2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'8'", "'134'", "'15'", "'45'"]
'6'
'8'
'134'
'15'
'45'
['6', '8', '134', '15', '45']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S><S sid ="100" ssid = "11">Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS  CityU and MSR).</S><S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'8'", "'100'", "'14'", "'31'"]
'7'
'8'
'100'
'14'
'31'
['7', '8', '100', '14', '31']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="29" ssid = "1">The perceptron algorithm introduced into NLP by Collins (2002)  is a simple but effective discriminative training method.</S><S sid ="115" ssid = "26">To obtain their corresponding weights  we adapted the minimum-error-rate training algorithm (Och  2003) to train the outside-layer model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'45'", "'49'", "'29'", "'115'"]
'8'
'45'
'49'
'29'
'115'
['8', '45', '49', '29', '115']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid ="23" ssid = "19">We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag.</S><S sid ="11" ssid = "7">Compared to performing segmentation and POS tagging one at a time  Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low  2004).</S><S sid ="80" ssid = "5">At each position i  we enumerate all possible word-POS pairs by assigning each POS to each possible word formed from the character subsequence spanning length l = L. min(i  K) (K is assigned 20 in all our experiments) and ending at position i  then we derive all candidate results by attaching each word-POS pair p (of length l) to the tail of each candidate result at the prior position of p (position i−l)  and select for position i a N-best list of candidate results from all these candidates.</S><S sid ="81" ssid = "6">When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p  we calculate the scores of the word LM  the POS LM  the labelling probability and the generating probability  Algorithm 2 Decoding algorithm. as well as the score of the perceptron model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'23'", "'11'", "'80'", "'81'"]
'9'
'23'
'11'
'80'
'81'
['9', '23', '11', '80', '81']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen  2003)  Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low  2004).</S><S sid ="25" ssid = "21">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S><S sid ="23" ssid = "19">We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="106" ssid = "17">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'25'", "'23'", "'7'", "'106'"]
'10'
'25'
'23'
'7'
'106'
['10', '25', '23', '7', '106']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">Compared to performing segmentation and POS tagging one at a time  Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low  2004).</S><S sid ="9" ssid = "5">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid ="3" ssid = "3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid ="26" ssid = "22">In order to perform POS tagging at the same time  we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004).</S><S sid ="106" ssid = "17">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'9'", "'3'", "'26'", "'106'"]
'11'
'9'
'3'
'26'
'106'
['11', '9', '3', '26', '106']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Besides the usual character-based features  additional features dependent on POS’s or words can also be employed to improve the performance.</S><S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid ="54" ssid = "5">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S><S sid ="51" ssid = "2">Additional features most widely used are related to word or POS ngrams.</S><S sid ="82" ssid = "7">In addition  we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'14'", "'54'", "'51'", "'82'"]
'12'
'14'
'54'
'51'
'82'
['12', '14', '54', '51', '82']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">However  as such features are generated dynamically during the decoding procedure  two limitation arise: on the one hand  the amount of parameters increases rapidly  which is apt to overfit on training corpus; on the other hand  exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.</S><S sid ="52" ssid = "3">However  such features are generated dynamically during the decoding procedure so that the feature space enlarges much more rapidly.</S><S sid ="55" ssid = "6">In addition  even though these higher grams were managed to be used  there still remains another problem: as the current predication relies on the results of prior ones  the decoding procedure has to resort to approximate inference by maintaining a list of N-best candidates at each predication position  which evokes a potential risk to depress the training.</S><S sid ="131" ssid = "2">Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.</S><S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'52'", "'55'", "'131'", "'14'"]
'13'
'52'
'55'
'131'
'14'
['13', '52', '55', '131', '14']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid ="12" ssid = "8">Besides the usual character-based features  additional features dependent on POS’s or words can also be employed to improve the performance.</S><S sid ="131" ssid = "2">Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.</S><S sid ="132" ssid = "3">This is a substitute method to use both local and non-local features  and it would be especially useful when the training corpus is very large.</S><S sid ="54" ssid = "5">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'12'", "'131'", "'132'", "'54'"]
'14'
'12'
'131'
'132'
'54'
['14', '12', '131', '132', '54']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">To cope with this problem  we propose a cascaded linear model inspired by the log-linear model (Och and Ney  2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.</S><S sid ="56" ssid = "7">To alleviate the drawbacks  we propose a cascaded linear model.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'56'", "'49'", "'45'", "'8'"]
'15'
'56'
'49'
'45'
'8'
['15', '56', '49', '45', '8']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="65" ssid = "16">As shown in Figure 1  the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.</S><S sid ="57" ssid = "8">It has a two-layer architecture  with a perceptron as the core and another linear model as the outside-layer.</S><S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="78" ssid = "3">Given a Chinese character sequence C1:n  the decoding procedure can proceed in a left-right fashion with a dynamic programming approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'65'", "'57'", "'2'", "'78'"]
'16'
'65'
'57'
'2'
'78'
['16', '65', '57', '2', '78']
parsed_discourse_facet ['method_citation']
Length 0 input/ref/Task1/P08-1102_aakansha.csv
0.0 0.0 0.0





input/ref/Task1/A97-1014_sweta.csv
input/res/Task1/A97-1014.csv
parsing: input/ref/Task1/A97-1014_sweta.csv
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["168'"]
168'
['168']
parsed_discourse_facet ['method_citation']
<S sid="165" ssid="7">In addition the approach provides empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linea.rizations, and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.</S>
original cit marker offset is 0
new cit marker offset is 0



["165'"]
165'
['165']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="26">This amount of data suffices as training material to reliably assign the grammatical functions if the user determines the elements of a phrase and its type (step 1 of the list above).</S>
original cit marker offset is 0
new cit marker offset is 0



["145'"]
145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.<
original cit marker offset is 0
new cit marker offset is 0



["15'"]
15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="31">Apart from this rather technical problem, two further arguments speak against phrase structure as the structural pivot of the annotation scheme: Finally, the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies, which has proved tedious.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
<S sid="47" ssid="37">Argument structure can be represented in terms of unordered trees (with crossing branches).</S>
original cit marker offset is 0
new cit marker offset is 0



["47'"]
47'
['47']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
 <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["160'"]
160'
['160']
parsed_discourse_facet ['method_citation']
<S sid="166" ssid="8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
<S sid="166" ssid="8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="24">Sentences annotated in previous steps are used as training material for further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
  <S sid="71" ssid="16">However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["71'"]
71'
['71']
parsed_discourse_facet ['method_citation']
  <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A97-1014.csv
<S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'162'", "'11'", "'5'", "'4'"]
'1'
'162'
'11'
'5'
'4'
['1', '162', '11', '5', '4']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Since the requirements for such a formalism differ from those posited for configurational languages  several features have been added  influencing the architecture of the scheme.</S><S sid ="3" ssid = "3">The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.</S><S sid ="31" ssid = "21">Due to the substantial differences between existing models of constituent structure  the question arises of how the theory independencf requirement  can be satisfied.</S><S sid ="13" ssid = "3">The data-drivenness of this approach presents a clear advantage over the traditional  idealised notion of competence grammar.</S><S sid ="165" ssid = "7">In addition the approach provides empirical material for psycholinguistic investigation  since preferences for the choice of certain syntactic constructions  linea.rizations  and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'3'", "'31'", "'13'", "'165'"]
'2'
'3'
'31'
'13'
'165'
['2', '3', '31', '13', '165']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.</S><S sid ="31" ssid = "21">Due to the substantial differences between existing models of constituent structure  the question arises of how the theory independencf requirement  can be satisfied.</S><S sid ="113" ssid = "26">Since a precise structural description of non-constituent coordination would require a. rich inventory of incomplete phrase types  we have agreed on a sort of unde.rspe.cified representations: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links.</S><S sid ="2" ssid = "2">Since the requirements for such a formalism differ from those posited for configurational languages  several features have been added  influencing the architecture of the scheme.</S><S sid ="41" ssid = "31">Apart from this rather technical problem  two further arguments speak against phrase structure as the structural pivot of the annotation scheme: Finally  the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies  which has proved tedious.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'31'", "'113'", "'2'", "'41'"]
'3'
'31'
'113'
'2'
'41'
['3', '31', '113', '2', '41']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'1'", "'162'", "'74'", "'11'"]
'4'
'1'
'162'
'74'
'11'
['4', '1', '162', '74', '11']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'162'", "'1'", "'6'", "'74'"]
'5'
'162'
'1'
'6'
'74'
['5', '162', '1', '6', '74']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="9" ssid = "6">Section 4 deals with the treatment of selected phenomena.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="160" ssid = "2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'9'", "'5'", "'1'", "'160'"]
'6'
'9'
'5'
'1'
'160'
['6', '9', '5', '1', '160']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">On the basis of these considerations  we formulate several additional requirements.</S><S sid ="37" ssid = "27">In light of these facts  serious difficulties can be expected arising from the structural component of the existing formalisms.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="104" ssid = "17">The head of the phrase can be determined in a similar way according to theory-specific assumptions.</S><S sid ="127" ssid = "8">As the need for certain functionalities becomes obvious with growing annotation experience  we have decided to implement the tool in two stages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'37'", "'5'", "'104'", "'127'"]
'7'
'37'
'5'
'104'
'127'
['7', '37', '5', '104', '127']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">A formalism complying with these requirements is described in section 3.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="9" ssid = "6">Section 4 deals with the treatment of selected phenomena.</S><S sid ="10" ssid = "7">For a description of the annotation tool see section 5.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'6'", "'9'", "'10'", "'5'"]
'8'
'6'
'9'
'10'
'5'
['8', '6', '9', '10', '5']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "6">Section 4 deals with the treatment of selected phenomena.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="8" ssid = "5">A formalism complying with these requirements is described in section 3.</S><S sid ="128" ssid = "9">In the first phase  the main functionality for building and displaying unordered trees is supplied.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'6'", "'5'", "'8'", "'128'"]
'9'
'6'
'5'
'8'
'128'
['9', '6', '5', '8', '128']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">For a description of the annotation tool see section 5.</S><S sid ="128" ssid = "9">In the first phase  the main functionality for building and displaying unordered trees is supplied.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="160" ssid = "2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S><S sid ="6" ssid = "3">In section 2  we examine the appropriateness of existing annotation schemes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'128'", "'1'", "'160'", "'6'"]
'10'
'128'
'1'
'160'
'6'
['10', '128', '1', '160', '6']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="12" ssid = "2">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S><S sid ="170" ssid = "12">Our annotation tool supplies efficient manipulation and immediate visualization of argument structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'12'", "'162'", "'1'", "'170'"]
'11'
'12'
'162'
'1'
'170'
['11', '12', '162', '1', '170']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "2">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="143" ssid = "24">Sentences annotated in previous steps are used as training material for further processing.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="1" ssid = "1">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'11'", "'143'", "'162'", "'1'"]
'12'
'11'
'143'
'162'
'1'
['12', '11', '143', '162', '1']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "3">The data-drivenness of this approach presents a clear advantage over the traditional  idealised notion of competence grammar.</S><S sid ="54" ssid = "44">Such a word order independent representation has the advantage of all structural information being encoded in a single data structure.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="43" ssid = "33">An alternative solution is to make argument structure the main structural component of the formalism.</S><S sid ="44" ssid = "34">This assumption underlies a growing number of recent syntactic theories which give up the context-free constituent backbone  cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'54'", "'162'", "'43'", "'44'"]
'13'
'54'
'162'
'43'
'44'
['13', '54', '162', '43', '44']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "4">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S><S sid ="12" ssid = "2">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="166" ssid = "8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'12'", "'162'", "'4'", "'166'"]
'14'
'12'
'162'
'4'
'166'
['14', '12', '162', '4', '166']
parsed_discourse_facet ['method_citation']
['In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.']
['Section 4 deals with the treatment of selected phenomena.', 'We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.', 'These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.', 'In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.', 'In section 2  we examine the appropriateness of existing annotation schemes.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:91', 'F:0']
['In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.']
['In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.', 'In light of these facts  serious difficulties can be expected arising from the structural component of the existing formalisms.', 'The head of the phrase can be determined in a similar way according to theory-specific assumptions.', 'On the basis of these considerations  we formulate several additional requirements.', 'As the need for certain functionalities becomes obvious with growing annotation experience  we have decided to implement the tool in two stages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:91', 'F:0']
['These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.']
['Section 4 deals with the treatment of selected phenomena.', 'A formalism complying with these requirements is described in section 3.', 'In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.', 'In the first phase  the main functionality for building and displaying unordered trees is supplied.', 'In section 2  we examine the appropriateness of existing annotation schemes.']
['system', 'ROUGE-S*', 'Average_R:', '0.00189', '(95%-conf.int.', '0.00189', '-', '0.00189)']
['system', 'ROUGE-S*', 'Average_P:', '0.04762', '(95%-conf.int.', '0.04762', '-', '0.04762)']
['system', 'ROUGE-S*', 'Average_F:', '0.00364', '(95%-conf.int.', '0.00364', '-', '0.00364)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:528', 'P:21', 'F:1']
['Sentences annotated in previous steps are used as training material for further processing.']
['Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.', 'We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.', 'Sentences annotated in previous steps are used as training material for further processing.', 'We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.']
['system', 'ROUGE-S*', 'Average_R:', '0.01468', '(95%-conf.int.', '0.01468', '-', '0.01468)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.02893', '(95%-conf.int.', '0.02893', '-', '0.02893)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:21', 'F:21']
['However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.']
['Such a word order independent representation has the advantage of all structural information being encoded in a single data structure.', 'The data-drivenness of this approach presents a clear advantage over the traditional  idealised notion of competence grammar.', 'An alternative solution is to make argument structure the main structural component of the formalism.', 'We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.', 'This assumption underlies a growing number of recent syntactic theories which give up the context-free constituent backbone  cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00060', '(95%-conf.int.', '0.00060', '-', '0.00060)']
['system', 'ROUGE-S*', 'Average_P:', '0.03571', '(95%-conf.int.', '0.03571', '-', '0.03571)']
['system', 'ROUGE-S*', 'Average_F:', '0.00119', '(95%-conf.int.', '0.00119', '-', '0.00119)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:28', 'F:1']
['For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).']
['Syntactically annotated corpora of German have been missing until now.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.', 'Corpora annotated with syntactic structures are commonly referred to as trctbank.5.', 'We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.', "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction."]
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:55', 'F:0']
['This amount of data suffices as training material to reliably assign the grammatical functions if the user determines the elements of a phrase and its type (step 1 of the list above).']
[u'The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- \xe2\u20ac\xa2lar representational strata.', 'Since the requirements for such a formalism differ from those posited for configurational languages  several features have been added  influencing the architecture of the scheme.', 'Due to the substantial differences between existing models of constituent structure  the question arises of how the theory independencf requirement  can be satisfied.', 'Since a precise structural description of non-constituent coordination would require a. rich inventory of incomplete phrase types  we have agreed on a sort of unde.rspe.cified representations: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links.', 'Apart from this rather technical problem  two further arguments speak against phrase structure as the structural pivot of the annotation scheme: Finally  the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies  which has proved tedious.']
['system', 'ROUGE-S*', 'Average_R:', '0.00070', '(95%-conf.int.', '0.00070', '-', '0.00070)']
['system', 'ROUGE-S*', 'Average_P:', '0.02206', '(95%-conf.int.', '0.02206', '-', '0.02206)']
['system', 'ROUGE-S*', 'Average_F:', '0.00136', '(95%-conf.int.', '0.00136', '-', '0.00136)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4278', 'P:136', 'F:3']
['Syntactically annotated corpora of German have been missing until now.']
['Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.', 'We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.', 'We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.', 'Our annotation tool supplies efficient manipulation and immediate visualization of argument structures.']
['system', 'ROUGE-S*', 'Average_R:', '0.00067', '(95%-conf.int.', '0.00067', '-', '0.00067)']
['system', 'ROUGE-S*', 'Average_P:', '0.10000', '(95%-conf.int.', '0.10000', '-', '0.10000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00134', '(95%-conf.int.', '0.00134', '-', '0.00134)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:10', 'F:1']
['Syntactically annotated corpora of German have been missing until now.']
['In the first phase  the main functionality for building and displaying unordered trees is supplied.', 'For a description of the annotation tool see section 5.', 'We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.', 'These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.', 'In section 2  we examine the appropriateness of existing annotation schemes.']
['system', 'ROUGE-S*', 'Average_R:', '0.00135', '(95%-conf.int.', '0.00135', '-', '0.00135)']
['system', 'ROUGE-S*', 'Average_P:', '0.10000', '(95%-conf.int.', '0.10000', '-', '0.10000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00266', '(95%-conf.int.', '0.00266', '-', '0.00266)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:10', 'F:1']
['In addition the approach provides empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linea.rizations, and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.']
['In addition the approach provides empirical material for psycholinguistic investigation  since preferences for the choice of certain syntactic constructions  linea.rizations  and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.', u'The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- \xe2\u20ac\xa2lar representational strata.', 'Since the requirements for such a formalism differ from those posited for configurational languages  several features have been added  influencing the architecture of the scheme.', 'The data-drivenness of this approach presents a clear advantage over the traditional  idealised notion of competence grammar.', 'Due to the substantial differences between existing models of constituent structure  the question arises of how the theory independencf requirement  can be satisfied.']
['system', 'ROUGE-S*', 'Average_R:', '0.11712', '(95%-conf.int.', '0.11712', '-', '0.11712)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.20968', '(95%-conf.int.', '0.20968', '-', '0.20968)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:325', 'F:325']
['Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.']
['We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.', 'Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.', 'During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.', "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.", 'We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.']
['system', 'ROUGE-S*', 'Average_R:', '0.00190', '(95%-conf.int.', '0.00190', '-', '0.00190)']
['system', 'ROUGE-S*', 'Average_P:', '0.05714', '(95%-conf.int.', '0.05714', '-', '0.05714)']
['system', 'ROUGE-S*', 'Average_F:', '0.00368', '(95%-conf.int.', '0.00368', '-', '0.00368)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:105', 'F:6']
0.214775452593 0.0126281817034 0.0229527270641





input/ref/Task1/A00-2030_vardha.csv
input/res/Task1/A00-2030.csv
parsing: input/ref/Task1/A00-2030_vardha.csv
 <S sid="11" ssid="1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="10">Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'"]
'50'
['50']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
 <S sid="106" ssid="3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
 <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
 <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="2">The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
 <S sid="3" ssid="1">Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'"]
'3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="15">Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'"]
'32'
['32']
parsed_discourse_facet ['method_citation']
 <S sid="10" ssid="8">Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="13">Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
    <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
 <S sid="26" ssid="9">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'"]
'26'
['26']
parsed_discourse_facet ['method_citation']
    <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="2">Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="43" ssid = "3">Thus  we did not consider simply adding semantic labels to the existing Penn TREEBANK  which is drawn from a single source — the Wall Street Journal — and is impoverished in articles about rocket launches.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'10'", "'5'", "'43'", "'104'"]
'1'
'10'
'5'
'43'
'104'
['1', '10', '5', '43', '104']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S><S sid ="26" ssid = "9">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993)  and more recently  had begun using a generative statistical model for name finding (Bikel et al.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'28'", "'96'", "'26'"]
'2'
'6'
'28'
'96'
'26'
['2', '6', '28', '96', '26']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "1">Since 1995  a few statistical parsing algorithms (Magerman  1995; Collins  1996 and 1997; Charniak  1997; Rathnaparki  1997) demonstrated a breakthrough in parsing accuracy  as measured against the University of Pennsylvania TREEBANK as a gold standard.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'1'", "'104'", "'5'", "'96'"]
'3'
'1'
'104'
'5'
'96'
['3', '1', '104', '5', '96']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "2">Yet  relatively few have embedded one of these algorithms in a task.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="108" ssid = "5">This simple semantic annotation was the only source of task knowledge used to configure the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'105'", "'23'", "'28'", "'108'"]
'4'
'105'
'23'
'28'
'108'
['4', '105', '23', '28', '108']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'106'", "'1'", "'28'", "'10'"]
'5'
'106'
'1'
'28'
'10'
['5', '106', '1', '28', '10']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S><S sid ="2" ssid = "2">In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'104'", "'2'", "'51'", "'1'"]
'6'
'104'
'2'
'51'
'1'
['6', '104', '2', '51', '1']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "5">The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.</S><S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="50" ssid = "10">Figure 4 shows an example of the semantic annotation  which was the only type of manual annotation we performed.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'11'", "'28'", "'50'", "'107'"]
'7'
'11'
'28'
'50'
'107'
['7', '11', '28', '50', '107']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "6">Several technical challenges confronted us and were solved: TREEBANK on Wall Street Journal adequately train the algorithm for New York Times newswire  which includes dozens of newspapers?</S><S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'10'", "'5'", "'51'", "'1'"]
'8'
'10'
'5'
'51'
'1'
['8', '10', '5', '51', '1']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "7">Manually creating sourcespecific training data for syntax was not required.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="108" ssid = "5">This simple semantic annotation was the only source of task knowledge used to configure the model.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S><S sid ="41" ssid = "1">To train our integrated model  we required a large corpus of augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'106'", "'108'", "'107'", "'41'"]
'9'
'106'
'108'
'107'
'41'
['9', '106', '108', '107', '41']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="43" ssid = "3">Thus  we did not consider simply adding semantic labels to the existing Penn TREEBANK  which is drawn from a single source — the Wall Street Journal — and is impoverished in articles about rocket launches.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'1'", "'43'", "'51'", "'5'"]
'10'
'1'
'43'
'51'
'5'
['10', '1', '43', '51', '5']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S><S sid ="7" ssid = "5">The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'7'", "'101'", "'5'", "'96'"]
'11'
'7'
'101'
'5'
'96'
['11', '7', '101', '5', '96']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "2">The Template Element (TE) task identifies organizations  persons  locations  and some artifacts (rocket and airplane-related artifacts).</S><S sid ="98" ssid = "3">In both Template Entity (TE) and Template Relation (TR)  our system finished in second place among all entrants.</S><S sid ="16" ssid = "6">For the following example  the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S><S sid ="78" ssid = "19">If we generalize the tree components (constituent labels  words  tags  etc.) and treat them all as simply elements  e  and treat all the conditioning factors as the history  h  we can write:</S><S sid ="34" ssid = "2">In these trees  the standard TREEBANK structures are augmented to convey semantic information  that is  entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'98'", "'16'", "'78'", "'34'"]
'12'
'98'
'16'
'78'
'34'
['12', '98', '16', '78', '34']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="14" ssid = "4">For each person  one must find all of the person's names within the document  his/her type (civilian or military)  and any significant descriptions (e.g.  titles).</S><S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="58" ssid = "4">Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree  semantic pointer labels are attached to all of the intermediate nodes.</S><S sid ="100" ssid = "5">Given multiple constituents that cover identical spans in the chart  only those constituents with probabilities within a While our focus throughout the project was on TE and TR  we became curious about how well the model did at part-of-speech tagging  syntactic parsing  and at name finding.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'14'", "'15'", "'58'", "'100'"]
'13'
'14'
'15'
'58'
'100'
['13', '14', '15', '58', '100']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "4">For each person  one must find all of the person's names within the document  his/her type (civilian or military)  and any significant descriptions (e.g.  titles).</S><S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S><S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'13'", "'74'", "'15'", "'36'"]
'14'
'13'
'74'
'15'
'36'
['14', '13', '74', '15', '36']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="78" ssid = "19">If we generalize the tree components (constituent labels  words  tags  etc.) and treat them all as simply elements  e  and treat all the conditioning factors as the history  h  we can write:</S><S sid ="91" ssid = "10">The probability of generating a constituent of the specified category  starting at the topmost node.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="55" ssid = "1">syntactic modifier of the other  the inserted node serves to indicate the relation as well as the argument.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'78'", "'91'", "'82'", "'55'"]
'15'
'78'
'91'
'82'
'55'
['15', '78', '91', '82', '55']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "6">For the following example  the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S><S sid ="98" ssid = "3">In both Template Entity (TE) and Template Relation (TR)  our system finished in second place among all entrants.</S><S sid ="12" ssid = "2">The Template Element (TE) task identifies organizations  persons  locations  and some artifacts (rocket and airplane-related artifacts).</S><S sid ="94" ssid = "13">Given a new sentence  the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'74'", "'98'", "'12'", "'94'"]
'16'
'74'
'98'
'12'
'94'
['16', '74', '98', '12', '94']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "7">For the following example  the template relation in Figure 2 was to be generated: &quot;Donald M. Goldstein  a historian at the University of Pittsburgh who helped write...&quot;</S><S sid ="50" ssid = "10">Figure 4 shows an example of the semantic annotation  which was the only type of manual annotation we performed.</S><S sid ="35" ssid = "3">An example of an augmented parse tree is shown in Figure 3.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'50'", "'35'", "'82'", "'56'"]
'17'
'50'
'35'
'82'
'56'
['17', '50', '35', '82', '56']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "1">Almost all approaches to information extraction — even at the sentence level — are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.</S><S sid ="19" ssid = "2">Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S><S sid ="29" ssid = "12">Thus  each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="84" ssid = "3">Although mathematically the model predicts tree elements in a top-down fashion  we search the space bottom-up using a chartbased search.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'19'", "'29'", "'5'", "'84'"]
'18'
'19'
'29'
'5'
'84'
['18', '19', '29', '5', '84']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "2">Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S><S sid ="29" ssid = "12">Thus  each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.</S><S sid ="24" ssid = "7">For this reason  we focused on designing an integrated model in which tagging  namefinding  parsing  and semantic interpretation decisions all have the opportunity to mutually influence each other.</S><S sid ="90" ssid = "9">Thus  the scores used in pruning can be considered as the product of: 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'96'", "'29'", "'24'", "'90'"]
'19'
'96'
'29'
'24'
'90'
['19', '96', '29', '24', '90']
parsed_discourse_facet ['method_citation']
['In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['The probability of generating a constituent of the specified category  starting at the topmost node.', 'Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.', 'syntactic modifier of the other  the inserted node serves to indicate the relation as well as the argument.', 'For each location  one must also give its type (city  province  county  body of water  etc.).', 'If we generalize the tree components (constituent labels  words  tags  etc.) and treat them all as simply elements  e  and treat all the conditioning factors as the history  h  we can write:']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:91', 'F:0']
['We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al.']
['Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.', 'An example of an augmented parse tree is shown in Figure 3.', 'For example  in the phrase &quot;Lt. Cmdr.', 'Figure 4 shows an example of the semantic annotation  which was the only type of manual annotation we performed.', 'For the following example  the template relation in Figure 2 was to be generated: &quot;Donald M. Goldstein  a historian at the University of Pittsburgh who helped write...&quot;']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:153', 'F:0']
['In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.', u'Almost all approaches to information extraction \u2014 even at the sentence level \u2014 are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.', 'Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.', 'Thus  each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.', 'Although mathematically the model predicts tree elements in a top-down fashion  we search the space bottom-up using a chartbased search.']
['system', 'ROUGE-S*', 'Average_R:', '0.00121', '(95%-conf.int.', '0.00121', '-', '0.00121)']
['system', 'ROUGE-S*', 'Average_P:', '0.03297', '(95%-conf.int.', '0.03297', '-', '0.03297)']
['system', 'ROUGE-S*', 'Average_F:', '0.00233', '(95%-conf.int.', '0.00233', '-', '0.00233)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:91', 'F:3']
['We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.']
['Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.', 'We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.', 'Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.', 'Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.', 'Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.']
['system', 'ROUGE-S*', 'Average_R:', '0.01967', '(95%-conf.int.', '0.01967', '-', '0.01967)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.03859', '(95%-conf.int.', '0.03859', '-', '0.03859)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:36', 'F:36']
['In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.', 'In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.', 'We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.', 'In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.02549', '(95%-conf.int.', '0.02549', '-', '0.02549)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.04971', '(95%-conf.int.', '0.04971', '-', '0.04971)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3570', 'P:91', 'F:91']
['In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).', 'Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.', 'Figure 4 shows an example of the semantic annotation  which was the only type of manual annotation we performed.', 'The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.', 'The semantic training corpus was produced by students according to a simple set of guidelines.']
['system', 'ROUGE-S*', 'Average_R:', '0.00163', '(95%-conf.int.', '0.00163', '-', '0.00163)']
['system', 'ROUGE-S*', 'Average_P:', '0.02198', '(95%-conf.int.', '0.02198', '-', '0.02198)']
['system', 'ROUGE-S*', 'Average_F:', '0.00304', '(95%-conf.int.', '0.00304', '-', '0.00304)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:91', 'F:2']
['The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).']
['Manually creating sourcespecific training data for syntax was not required.', 'This simple semantic annotation was the only source of task knowledge used to configure the model.', 'We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.', 'The semantic training corpus was produced by students according to a simple set of guidelines.', 'To train our integrated model  we required a large corpus of augmented parse trees.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:78', 'F:0']
['Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.']
['Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.', 'Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.', 'Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.', 'To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.', u'Thus  we did not consider simply adding semantic labels to the existing Penn TREEBANK  which is drawn from a single source \u2014 the Wall Street Journal \u2014 and is impoverished in articles about rocket launches.']
['system', 'ROUGE-S*', 'Average_R:', '0.01951', '(95%-conf.int.', '0.01951', '-', '0.01951)']
['system', 'ROUGE-S*', 'Average_P:', '0.26877', '(95%-conf.int.', '0.26877', '-', '0.26877)']
['system', 'ROUGE-S*', 'Average_F:', '0.03637', '(95%-conf.int.', '0.03637', '-', '0.03637)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:253', 'F:68']
['Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.']
['Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.', 'For this reason  we focused on designing an integrated model in which tagging  namefinding  parsing  and semantic interpretation decisions all have the opportunity to mutually influence each other.', 'Thus  each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.', 'Thus  the scores used in pruning can be considered as the product of: 1.', 'Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.']
['system', 'ROUGE-S*', 'Average_R:', '0.12867', '(95%-conf.int.', '0.12867', '-', '0.12867)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.22800', '(95%-conf.int.', '0.22800', '-', '0.22800)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:276', 'F:276']
['Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.']
['In both Template Entity (TE) and Template Relation (TR)  our system finished in second place among all entrants.', 'In these trees  the standard TREEBANK structures are augmented to convey semantic information  that is  entities and relations.', 'The Template Element (TE) task identifies organizations  persons  locations  and some artifacts (rocket and airplane-related artifacts).', 'If we generalize the tree components (constituent labels  words  tags  etc.) and treat them all as simply elements  e  and treat all the conditioning factors as the history  h  we can write:', 'For the following example  the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:253', 'F:0']
['We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.']
['In both Template Entity (TE) and Template Relation (TR)  our system finished in second place among all entrants.', 'The Template Element (TE) task identifies organizations  persons  locations  and some artifacts (rocket and airplane-related artifacts).', u'The categories for head constituents  cl\u201e are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c\u201e _1  and the head word of their parent  wp.', 'Given a new sentence  the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.', 'For the following example  the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3570', 'P:78', 'F:0']
['Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.']
['For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.', u'The categories for head constituents  cl\u201e are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c\u201e _1  and the head word of their parent  wp.', 'For each location  one must also give its type (city  province  county  body of water  etc.).', "For each person  one must find all of the person's names within the document  his/her type (civilian or military)  and any significant descriptions (e.g.  titles).", 'The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:55', 'F:0']
['Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.']
['Given multiple constituents that cover identical spans in the chart  only those constituents with probabilities within a While our focus throughout the project was on TE and TR  we became curious about how well the model did at part-of-speech tagging  syntactic parsing  and at name finding.', 'For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.', 'For each location  one must also give its type (city  province  county  body of water  etc.).', "For each person  one must find all of the person's names within the document  his/her type (civilian or military)  and any significant descriptions (e.g.  titles).", 'Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree  semantic pointer labels are attached to all of the intermediate nodes.']
['system', 'ROUGE-S*', 'Average_R:', '0.00099', '(95%-conf.int.', '0.00099', '-', '0.00099)']
['system', 'ROUGE-S*', 'Average_P:', '0.01471', '(95%-conf.int.', '0.01471', '-', '0.01471)']
['system', 'ROUGE-S*', 'Average_F:', '0.00186', '(95%-conf.int.', '0.00186', '-', '0.00186)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:136', 'F:2']
0.256802305717 0.0151669229603 0.0276846151717





input/ref/Task1/P11-1061_swastika.csv
input/res/Task1/P11-1061.csv
parsing: input/ref/Task1/P11-1061_swastika.csv
    <S sid="144" ssid="7">For comparison, the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%, and goes up to 88.7% with a treebank dictionary.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
    <S sid="44" ssid="10">Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.</S
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
parsed_discourse_facet ['method_citation']
    <S sid="16" ssid="12">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (&#167;3), and then use graph label propagation to project syntactic information from English to the foreign language (&#167;4).</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="44" ssid="10">Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.</S
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
parsed_discourse_facet ['method_citation']
<S sid="110" ssid="10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S>
original cit marker offset is 0
new cit marker offset is 0



['110']
110
['110']
parsed_discourse_facet ['aim_citation']
    <S sid="115" ssid="15">The taggers were trained on datasets labeled with the universal tags.</S>
original cit marker offset is 0
new cit marker offset is 0



['115']
115
['115']
parsed_discourse_facet ['method_citation']
    <S sid="115" ssid="15">The taggers were trained on datasets labeled with the universal tags.</S>
original cit marker offset is 0
new cit marker offset is 0



['115']
115
['115']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['158']
158
['158']
parsed_discourse_facet ['result_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



['23']
23
['23']
parsed_discourse_facet ['result_citation']
<S sid="24" ssid="1">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['24']
24
['24']
parsed_discourse_facet ['aim_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['???']
???
['???']
parsed_discourse_facet ['method_citation']
    <S sid="16" ssid="12">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (&#167;3), and then use graph label propagation to project syntactic information from English to the foreign language (&#167;4).</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



['23']
23
['23']
parsed_discourse_facet ['result_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['158']
158
['158']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
<S sid="56" ssid="22">To define a similarity function between the English and the foreign vertices, we rely on high-confidence word alignments.</S>
original cit marker offset is 0
new cit marker offset is 0



['56']
56
['56']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



['161']
161
['161']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P11-1061.csv
<S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S><S sid ="8" ssid = "4">Unsupervised learning approaches appear to be a natural solution to this problem  as they require only unannotated text for training models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'160'", "'3'", "'13'", "'8'"]
'1'
'160'
'3'
'13'
'8'
['1', '160', '3', '13', '8']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed)  making it applicable to a wide array of resource-poor languages.</S><S sid ="9" ssid = "5">Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.</S><S sid ="126" ssid = "26">Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.</S><S sid ="41" ssid = "7">Because the information flow in our graph is asymmetric (from English to the foreign language)  we use different types of vertices for each language.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'9'", "'126'", "'41'", "'107'"]
'2'
'9'
'126'
'41'
'107'
['2', '9', '126', '41', '107']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="33" ssid = "10">The POS distributions over the foreign trigram types are used as features to learn a better unsupervised POS tagger (§5).</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="60" ssid = "26">To initialize the graph for label propagation we use a supervised English tagger to label the English side of the bitext.7 We then simply count the individual labels of the English tokens and normalize the counts to produce tag distributions over English word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'39'", "'33'", "'1'", "'60'"]
'3'
'39'
'33'
'1'
'60'
['3', '39', '33', '1', '60']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S><S sid ="23" ssid = "19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%)  and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S sid ="143" ssid = "6">Overall  it gives improvements ranging from 1.1% for German to 14.7% for Italian  for an average improvement of 8.3% over the unsupervised feature-HMM model.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S><S sid ="144" ssid = "7">For comparison  the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%  and goes up to 88.7% with a treebank dictionary.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'23'", "'143'", "'135'", "'144'"]
'4'
'23'
'143'
'135'
'144'
['4', '23', '143', '135', '144']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing  resulting in highly accurate systems.</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S><S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S><S sid ="15" ssid = "11">First  we use a novel graph-based framework for projecting syntactic information across language boundaries.</S><S sid ="18" ssid = "14">To make the projection practical  we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'158'", "'4'", "'15'", "'18'"]
'5'
'158'
'4'
'15'
'18'
['5', '158', '4', '15', '18']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Supervised part-of-speech (POS) taggers  for example  approach the level of inter-annotator agreement (Shen et al.  2007  97.3% accuracy for English).</S><S sid ="47" ssid = "13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S><S sid ="9" ssid = "5">Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S><S sid ="20" ssid = "16">Because there might be some controversy about the exact definitions of such universals  this set of coarse-grained POS categories is defined operationally  by collapsing language (or treebank) specific distinctions to a set of categories that exists across all languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'47'", "'9'", "'158'", "'20'"]
'6'
'47'
'9'
'158'
'20'
['6', '47', '9', '158', '20']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S><S sid ="27" ssid = "4">Graph construction does not require any labeled data  but makes use of two similarity functions.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'27'", "'107'", "'56'", "'37'"]
'7'
'27'
'107'
'56'
'37'
['7', '27', '107', '56', '37']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Unsupervised learning approaches appear to be a natural solution to this problem  as they require only unannotated text for training models.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'160'", "'1'", "'13'", "'3'"]
'8'
'160'
'1'
'13'
'3'
['8', '160', '1', '13', '3']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.</S><S sid ="47" ssid = "13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S><S sid ="126" ssid = "26">Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.</S><S sid ="2" ssid = "2">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed)  making it applicable to a wide array of resource-poor languages.</S><S sid ="57" ssid = "23">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'47'", "'126'", "'2'", "'57'"]
'9'
'47'
'126'
'2'
'57'
['9', '47', '126', '2', '57']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'39'", "'16'", "'26'", "'13'"]
'10'
'39'
'16'
'26'
'13'
['10', '39', '16', '26', '13']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This scenario is applicable to a large set of languages and has been considered by a number of authors in the past (Alshawi et al.  2000; Xi and Hwa  2005; Ganchev et al.  2009).</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="94" ssid = "25">We adopted this state-of-the-art model because it makes it easy to experiment with various ways of incorporating our novel constraint feature into the log-linear emission model.</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'19'", "'26'", "'94'", "'13'"]
'11'
'19'
'26'
'94'
'13'
['11', '19', '26', '94', '13']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Naseem et al. (2009) and Snyder et al.</S><S sid ="38" ssid = "4">More recently  Subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model.</S><S sid ="93" ssid = "24">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S><S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="123" ssid = "23">This can be seen as a rough approximation of Yarowsky and Ngai (2001).</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'38'", "'93'", "'37'", "'123'"]
'12'
'38'
'93'
'37'
'123'
['12', '38', '93', '37', '123']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="24" ssid = "1">The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'39'", "'160'", "'1'", "'24'"]
'13'
'39'
'160'
'1'
'24'
['13', '39', '160', '1', '24']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">Our work is closest to that of Yarowsky and Ngai (2001)  but differs in two important ways.</S><S sid ="123" ssid = "23">This can be seen as a rough approximation of Yarowsky and Ngai (2001).</S><S sid ="40" ssid = "6">We extend Subramanya et al.’s intuitions to our bilingual setup.</S><S sid ="88" ssid = "19">In our experiments  we used the same set of features as BergKirkpatrick et al. (2010): an indicator feature based In a traditional Markov model  the emission distribution PΘ(Xi = xi  |Zi = zi) is a set of multinomials.</S><S sid ="85" ssid = "16">We develop our POS induction model based on the feature-based HMM of Berg-Kirkpatrick et al. (2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'123'", "'40'", "'88'", "'85'"]
'14'
'123'
'40'
'88'
'85'
['14', '123', '40', '88', '85']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">First  we use a novel graph-based framework for projecting syntactic information across language boundaries.</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'158'", "'29'", "'56'", "'25'"]
'15'
'158'
'29'
'56'
'25'
['15', '158', '29', '56', '25']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="57" ssid = "23">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid ="70" ssid = "1">Given the bilingual graph described in the previous section  we can use label propagation to project the English POS labels to the foreign language.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'57'", "'70'", "'29'", "'26'"]
'16'
'57'
'70'
'29'
'26'
['16', '57', '70', '29', '26']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">Second  we treat the projected labels as features in an unsupervised model (§5)  rather than using them directly for supervised training.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="71" ssid = "2">We use label propagation in two stages to generate soft labels on all the vertices in the graph.</S><S sid ="126" ssid = "26">Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.</S><S sid ="52" ssid = "18">This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'97'", "'71'", "'126'", "'52'"]
'17'
'97'
'71'
'126'
'52'
['17', '97', '71', '126', '52']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">To make the projection practical  we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="111" ssid = "11">We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns)  VERB (verbs)  ADJ (adjectives)  ADV (adverbs)  PRON (pronouns)  DET (determiners)  ADP (prepositions or postpositions)  NUM (numerals)  CONJ (conjunctions)  PRT (particles)  PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).</S><S sid ="85" ssid = "16">We develop our POS induction model based on the feature-based HMM of Berg-Kirkpatrick et al. (2010).</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'56'", "'111'", "'85'", "'158'"]
'18'
'56'
'111'
'85'
'158'
['18', '56', '111', '85', '158']
parsed_discourse_facet ['method_citation']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.', 'To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.', 'First  we use a novel graph-based framework for projecting syntactic information across language boundaries.', u'To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (\u0e22\u0e073.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.02344', '(95%-conf.int.', '0.02344', '-', '0.02344)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.04581', '(95%-conf.int.', '0.04581', '-', '0.04581)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:55', 'F:55']
['For comparison, the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%, and goes up to 88.7% with a treebank dictionary.']
['We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.', 'Unsupervised learning approaches appear to be a natural solution to this problem  as they require only unannotated text for training models.', 'We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).', 'Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.', '(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.']
['system', 'ROUGE-S*', 'Average_R:', '0.00132', '(95%-conf.int.', '0.00132', '-', '0.00132)']
['system', 'ROUGE-S*', 'Average_P:', '0.02206', '(95%-conf.int.', '0.02206', '-', '0.02206)']
['system', 'ROUGE-S*', 'Average_F:', '0.00249', '(95%-conf.int.', '0.00249', '-', '0.00249)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:136', 'F:3']
['To this end, we construct a bilingual graph over word types to establish a connection between the two languages (&#167;3), and then use graph label propagation to project syntactic information from English to the foreign language (&#167;4).']
['We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.', 'The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.', '(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.', 'Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.', 'They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.']
['system', 'ROUGE-S*', 'Average_R:', '0.00396', '(95%-conf.int.', '0.00396', '-', '0.00396)']
['system', 'ROUGE-S*', 'Average_P:', '0.04762', '(95%-conf.int.', '0.04762', '-', '0.04762)']
['system', 'ROUGE-S*', 'Average_F:', '0.00732', '(95%-conf.int.', '0.00732', '-', '0.00732)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:231', 'F:11']
['Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).']
['Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.', 'Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).', u'Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don\u2019t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De\ufffdf.', 'Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed)  making it applicable to a wide array of resource-poor languages.', 'Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.']
['system', 'ROUGE-S*', 'Average_R:', '0.00582', '(95%-conf.int.', '0.00582', '-', '0.00582)']
['system', 'ROUGE-S*', 'Average_P:', '0.06653', '(95%-conf.int.', '0.06653', '-', '0.06653)']
['system', 'ROUGE-S*', 'Average_F:', '0.01070', '(95%-conf.int.', '0.01070', '-', '0.01070)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5671', 'P:496', 'F:33']
['To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.']
['This can be seen as a rough approximation of Yarowsky and Ngai (2001).', 'For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.', 'Naseem et al. (2009) and Snyder et al.', 'Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.', 'More recently  Subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00352', '(95%-conf.int.', '0.00352', '-', '0.00352)']
['system', 'ROUGE-S*', 'Average_P:', '0.01705', '(95%-conf.int.', '0.01705', '-', '0.01705)']
['system', 'ROUGE-S*', 'Average_F:', '0.00584', '(95%-conf.int.', '0.00584', '-', '0.00584)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:528', 'F:9']
['The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.']
['To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.', 'They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.', '(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.', u'As discussed in more detail in \u0e22\u0e073  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', u'To this end  we construct a bilingual graph over word types to establish a connection between the two languages (\u0e22\u0e073)  and then use graph label propagation to project syntactic information from English to the foreign language (\u0e22\u0e074).']
['system', 'ROUGE-S*', 'Average_R:', '0.00878', '(95%-conf.int.', '0.00878', '-', '0.00878)']
['system', 'ROUGE-S*', 'Average_P:', '0.51648', '(95%-conf.int.', '0.51648', '-', '0.51648)']
['system', 'ROUGE-S*', 'Average_F:', '0.01726', '(95%-conf.int.', '0.01726', '-', '0.01726)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5356', 'P:91', 'F:47']
['We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.']
['First  we use a novel graph-based framework for projecting syntactic information across language boundaries.', 'We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.', 'Supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing  resulting in highly accurate systems.', 'Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.', 'To make the projection practical  we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).']
['system', 'ROUGE-S*', 'Average_R:', '0.00043', '(95%-conf.int.', '0.00043', '-', '0.00043)']
['system', 'ROUGE-S*', 'Average_P:', '0.04762', '(95%-conf.int.', '0.04762', '-', '0.04762)']
['system', 'ROUGE-S*', 'Average_F:', '0.00084', '(95%-conf.int.', '0.00084', '-', '0.00084)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:21', 'F:1']
['Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.']
['Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.', 'Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed)  making it applicable to a wide array of resource-poor languages.', 'Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.', 'Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.', 'Because the information flow in our graph is asymmetric (from English to the foreign language)  we use different types of vertices for each language.']
['system', 'ROUGE-S*', 'Average_R:', '0.00088', '(95%-conf.int.', '0.00088', '-', '0.00088)']
['system', 'ROUGE-S*', 'Average_P:', '0.13333', '(95%-conf.int.', '0.13333', '-', '0.13333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00174', '(95%-conf.int.', '0.00174', '-', '0.00174)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:15', 'F:2']
['Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.', 'To make the projection practical  we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).', 'To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.', 'We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns)  VERB (verbs)  ADJ (adjectives)  ADV (adverbs)  PRON (pronouns)  DET (determiners)  ADP (prepositions or postpositions)  NUM (numerals)  CONJ (conjunctions)  PRT (particles)  PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).', 'We develop our POS induction model based on the feature-based HMM of Berg-Kirkpatrick et al. (2010).']
['system', 'ROUGE-S*', 'Average_R:', '0.00230', '(95%-conf.int.', '0.00230', '-', '0.00230)']
['system', 'ROUGE-S*', 'Average_P:', '0.06618', '(95%-conf.int.', '0.06618', '-', '0.06618)']
['system', 'ROUGE-S*', 'Average_F:', '0.00444', '(95%-conf.int.', '0.00444', '-', '0.00444)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3916', 'P:136', 'F:9']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.']
['We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.', 'Unsupervised learning approaches appear to be a natural solution to this problem  as they require only unannotated text for training models.', '(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.', 'Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.', 'We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).']
['system', 'ROUGE-S*', 'Average_R:', '0.00615', '(95%-conf.int.', '0.00615', '-', '0.00615)']
['system', 'ROUGE-S*', 'Average_P:', '0.25455', '(95%-conf.int.', '0.25455', '-', '0.25455)']
['system', 'ROUGE-S*', 'Average_F:', '0.01200', '(95%-conf.int.', '0.01200', '-', '0.01200)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:55', 'F:14']
['The taggers were trained on datasets labeled with the universal tags.']
['To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.', 'Graph construction does not require any labeled data  but makes use of two similarity functions.', 'Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.', 'Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.', 'However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:15', 'F:0']
0.197401816387 0.00514545449868 0.00985818172856





input/ref/Task1/W99-0623_vardha.csv
input/res/Task1/W99-0623.csv
parsing: input/ref/Task1/W99-0623_vardha.csv
    <S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="117" ssid="46">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'117'"]
'117'
['117']
parsed_discourse_facet ['method_citation']
<S sid="72" ssid="1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
    <S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'"]
'120'
['120']
parsed_discourse_facet ['method_citation']
  <S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'139'"]
'139'
['139']
parsed_discourse_facet ['method_citation']
 <S sid="84" ssid="13">The first shows how constituent features and context do not help in deciding which parser to trust.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'"]
'84'
['84']
parsed_discourse_facet ['method_citation']
    <S sid="76" ssid="5">The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'"]
'76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'"]
'38'
['38']
parsed_discourse_facet ['method_citation']
  <S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
 <S sid="72" ssid="1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
    <S sid="87" ssid="16">It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'"]
'87'
['87']
parsed_discourse_facet ['method_citation']
    <S sid="51" ssid="37">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
    <S sid="79" ssid="8">Precision is the portion of hypothesized constituents that are correct and recall is the portion of the Treebank constituents that are hypothesized.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
  <S sid="27" ssid="13">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'"]
'27'
['27']
parsed_discourse_facet ['method_citation']
 <S sid="77" ssid="6">Each parse is converted into a set of constituents represented as a tuples: (label, start, end).</S>
original cit marker offset is 0
new cit marker offset is 0



["'77'"]
'77'
['77']
parsed_discourse_facet ['method_citation']
  <S sid="11" ssid="7">Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
  <S sid="116" ssid="45">The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'116'"]
'116'
['116']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0623.csv
<S sid ="1" ssid = "1">Three state-of-the-art statistical parsers are combined to produce more accurate parses  as well as new bounds on achievable Treebank parsing accuracy.</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="129" ssid = "58">In the interest of testing the robustness of these combining techniques  we added a fourth  simple nonlexicalized PCFG parser.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'72'", "'38'", "'129'", "'130'"]
'1'
'72'
'38'
'129'
'130'
['1', '72', '38', '129', '130']
Error in Discourse Facet
<S sid ="2" ssid = "2">Two general approaches are presented and two combination techniques are described for each approach.</S><S sid ="59" ssid = "45">Once again we present both a non-parametric and a parametric technique for this task.</S><S sid ="3" ssid = "3">Both parametric and non-parametric models are explored.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'59'", "'3'", "'85'", "'139'"]
'2'
'59'
'3'
'85'
'139'
['2', '59', '3', '85', '139']
Error in Discourse Facet
<S sid ="3" ssid = "3">Both parametric and non-parametric models are explored.</S><S sid ="59" ssid = "45">Once again we present both a non-parametric and a parametric technique for this task.</S><S sid ="2" ssid = "2">Two general approaches are presented and two combination techniques are described for each approach.</S><S sid ="138" ssid = "67">Surprisingly  the non-parametric switching technique also exhibited robust behaviour in this situation.</S><S sid ="136" ssid = "65">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'59'", "'2'", "'138'", "'136'"]
'3'
'59'
'2'
'138'
'136'
['3', '59', '2', '138', '136']
Error in Discourse Facet
<S sid ="4" ssid = "4">The resulting parsers surpass the best previously published performance results for the Penn Treebank.</S><S sid ="76" ssid = "5">The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="134" ssid = "63">As seen by the drop in average individual parser performance baseline  the introduced parser does not perform very well.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'76'", "'85'", "'130'", "'134'"]
'4'
'76'
'85'
'130'
'134'
['4', '76', '85', '130', '134']
Error in Discourse Facet
<S sid ="5" ssid = "1">The natural language processing community is in the strong position of having many available approaches to solving some of its most fundamental problems.</S><S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid ="26" ssid = "12">This technique has the advantage of requiring no training  but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'6'", "'12'", "'13'", "'26'"]
'5'
'6'
'12'
'13'
'26'
['5', '6', '12', '13', '26']
Error in Discourse Facet
<S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="108" ssid = "37">From this we see that a finer-grained model for parser combination  at least for the features we have examined  will not give us any additional power.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'11'", "'9'", "'12'", "'108'"]
'6'
'11'
'9'
'12'
'108'
['6', '11', '9', '12', '108']
Error in Discourse Facet
<S sid ="7" ssid = "3">Their theoretical finding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent  accurate classifiers.</S><S sid ="30" ssid = "16">This is equivalent to the assumption used in probability estimation for naïve Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.</S><S sid ="116" ssid = "45">The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="41" ssid = "27">IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'30'", "'116'", "'38'", "'41'"]
'7'
'30'
'116'
'38'
'41'
['7', '30', '116', '38', '41']
Error in Discourse Facet
<S sid ="8" ssid = "4">The theory has also been validated empirically.</S><S sid ="36" ssid = "22">The estimation of the probabilities in the model is carried out as shown in Equation 4.</S><S sid ="83" ssid = "12">We performed three experiments to evaluate our techniques.</S><S sid ="121" ssid = "50">Table 3 contains the results for evaluating our systems on the test set (section 22).</S><S sid ="109" ssid = "38">The results in Table 2 were achieved on the development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'36'", "'83'", "'121'", "'109'"]
'8'
'36'
'83'
'121'
'109'
['8', '36', '83', '121', '109']
Error in Discourse Facet
<S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="88" ssid = "17">For example  one parser could be more accurate at predicting noun phrases than the other parsers.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'55'", "'88'", "'21'", "'139'"]
'9'
'55'
'88'
'21'
'139'
['9', '55', '88', '21', '139']
Error in Discourse Facet
<S sid ="10" ssid = "6">In both cases the investigators were able to achieve significant improvements over the previous best tagging results.</S><S sid ="136" ssid = "65">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="141" ssid = "3">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid ="134" ssid = "63">As seen by the drop in average individual parser performance baseline  the introduced parser does not perform very well.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'136'", "'85'", "'141'", "'134'"]
'10'
'136'
'85'
'141'
'134'
['10', '136', '85', '141', '134']
Error in Discourse Facet
<S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'9'", "'6'", "'12'", "'13'"]
'11'
'9'
'6'
'12'
'13'
['11', '9', '6', '12', '13']
Error in Discourse Facet
<S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'13'", "'11'", "'6'", "'72'"]
'12'
'13'
'11'
'6'
'72'
['12', '13', '11', '6', '72']
Error in Discourse Facet
<S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="4" ssid = "4">The resulting parsers surpass the best previously published performance results for the Penn Treebank.</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'12'", "'4'", "'72'", "'130'"]
'13'
'12'
'4'
'72'
'130'
['13', '12', '4', '72', '130']
Error in Discourse Facet
<S sid ="14" ssid = "10">We used these three parsers to explore parser combination techniques.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="126" ssid = "55">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S><S sid ="140" ssid = "2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'139'", "'85'", "'126'", "'140'"]
'14'
'139'
'85'
'126'
'140'
['14', '139', '85', '126', '140']
Error in Discourse Facet
<S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="88" ssid = "17">For example  one parser could be more accurate at predicting noun phrases than the other parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'55'", "'27'", "'85'", "'88'"]
'15'
'55'
'27'
'85'
'88'
['15', '55', '27', '85', '88']
Error in Discourse Facet
<S sid ="16" ssid = "2">We call this approach parse hybridization.</S><S sid ="58" ssid = "44">We call this approach parser switching.</S><S sid ="23" ssid = "9">We call this technique constituent voting.</S><S sid ="96" ssid = "25">We call such a constituent an isolated constituent.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'58'", "'23'", "'96'", "'139'"]
'16'
'58'
'23'
'96'
'139'
['16', '58', '23', '96', '139']
Error in Discourse Facet
<S sid ="17" ssid = "3">The substructures that are unanimously hypothesized by the parsers should be preserved after combination  and the combination technique should not foolishly create substructures for which there is no supporting evidence.</S><S sid ="49" ssid = "35">In general  the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.</S><S sid ="117" ssid = "46">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S><S sid ="125" ssid = "54">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S><S sid ="95" ssid = "24">One side of the decision making process is when we choose to believe a constituent should be in the parse  even though only one parser suggests it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'49'", "'117'", "'125'", "'95'"]
'17'
'49'
'117'
'125'
'95'
['17', '49', '117', '125', '95']
Error in Discourse Facet
['The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.']
['Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.', 'One side of the decision making process is when we choose to believe a constituent should be in the parse  even though only one parser suggests it.', u'The constituent voting and na\xc3\xafve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.', 'The substructures that are unanimously hypothesized by the parsers should be preserved after combination  and the combination technique should not foolishly create substructures for which there is no supporting evidence.', 'In general  the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.00627', '(95%-conf.int.', '0.00627', '-', '0.00627)']
['system', 'ROUGE-S*', 'Average_P:', '0.15152', '(95%-conf.int.', '0.15152', '-', '0.15152)']
['system', 'ROUGE-S*', 'Average_F:', '0.01203', '(95%-conf.int.', '0.01203', '-', '0.01203)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:66', 'F:10']
['Each parse is converted into a set of constituents represented as a tuples: (label, start, end).']
['We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.', 'For example  one parser could be more accurate at predicting noun phrases than the other parsers.', u'Another technique for parse hybridization is to use a na\xc3\xafve Bayes classifier to determine which constituents to include in the parse.', 'We are interested in combining the substructures of the input parses to produce a better parse.', 'We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.00106', '(95%-conf.int.', '0.00106', '-', '0.00106)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00204', '(95%-conf.int.', '0.00204', '-', '0.00204)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:36', 'F:1']
['One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure.']
['Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).', 'These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).', 'The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).', 'The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.', 'The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).']
['system', 'ROUGE-S*', 'Average_R:', '0.00031', '(95%-conf.int.', '0.00031', '-', '0.00031)']
['system', 'ROUGE-S*', 'Average_P:', '0.00735', '(95%-conf.int.', '0.00735', '-', '0.00735)']
['system', 'ROUGE-S*', 'Average_F:', '0.00059', '(95%-conf.int.', '0.00059', '-', '0.00059)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3240', 'P:136', 'F:1']
['Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.']
['Both parametric and non-parametric models are explored.', 'Two general approaches are presented and two combination techniques are described for each approach.', 'Once again we present both a non-parametric and a parametric technique for this task.', 'We have presented two general approaches to studying parser combination: parser switching and parse hybridization.', 'We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.00178', '(95%-conf.int.', '0.00178', '-', '0.00178)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00335', '(95%-conf.int.', '0.00335', '-', '0.00335)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:36', 'F:1']
['The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.']
[u'This is equivalent to the assumption used in probability estimation for na\xefve Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.', 'The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.', 'IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.', 'Their theoretical finding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent  accurate classifiers.', u'Under certain conditions the constituent voting and na\xefve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.']
['system', 'ROUGE-S*', 'Average_R:', '0.00045', '(95%-conf.int.', '0.00045', '-', '0.00045)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00088', '(95%-conf.int.', '0.00088', '-', '0.00088)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:55', 'F:1']
['Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.']
['The theory has also been validated empirically.', 'The results in Table 2 were achieved on the development set.', 'The estimation of the probabilities in the model is carried out as shown in Equation 4.', 'Table 3 contains the results for evaluating our systems on the test set (section 22).', 'We performed three experiments to evaluate our techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:406', 'P:120', 'F:0']
['The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.']
['Surprisingly  the non-parametric switching technique also exhibited robust behaviour in this situation.', 'Both parametric and non-parametric models are explored.', 'Two general approaches are presented and two combination techniques are described for each approach.', 'Once again we present both a non-parametric and a parametric technique for this task.', 'The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:136', 'F:0']
['It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.']
['Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).', 'Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).', 'These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).', 'The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).', 'The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).']
['system', 'ROUGE-S*', 'Average_R:', '0.00063', '(95%-conf.int.', '0.00063', '-', '0.00063)']
['system', 'ROUGE-S*', 'Average_P:', '0.04444', '(95%-conf.int.', '0.04444', '-', '0.04444)']
['system', 'ROUGE-S*', 'Average_F:', '0.00125', '(95%-conf.int.', '0.00125', '-', '0.00125)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:45', 'F:2']
0.0346312495671 0.00131249998359 0.00251749996853





input/ref/Task1/D09-1092_swastika.csv
input/res/Task1/D09-1092.csv
parsing: input/ref/Task1/D09-1092_swastika.csv
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="114" ssid="63">Ideally, the &#8220;glue&#8221; documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.</S>
original cit marker offset is 0
new cit marker offset is 0



['114']
114
['114']
parsed_discourse_facet ['aim_citation']
<S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



['138']
138
['138']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



['55']
55
['55']
parsed_discourse_facet ['method_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['192']
192
['192']
parsed_discourse_facet ['result_citation']
<S sid="119" ssid="68">The lower the divergence, the more similar the distributions are to each other.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['result_citation']
<S sid="148" ssid="97">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="163" ssid="112">Performance continues to improve with longer documents, most likely due to better topic inference.</S>
original cit marker offset is 0
new cit marker offset is 0



['163']
163
['163']
parsed_discourse_facet ['result_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="148" ssid="97">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['192']
192
['192']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['method_citation']
    <S sid="10" ssid="6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="105" ssid = "54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S><S sid ="194" ssid = "3">We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'25'", "'105'", "'194'", "'126'"]
'1'
'25'
'105'
'194'
'126'
['1', '25', '105', '194', '126']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Meanwhile  massive collections of interlinked documents in dozens of languages  such as Wikipedia  are now widely available  calling for tools that can characterize content in many languages.</S><S sid ="16" ssid = "12">However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.</S><S sid ="168" ssid = "2">However  the growth of the web  and in particular Wikipedia  has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid ="196" ssid = "5">When applied to comparable document collections such as Wikipedia  PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S><S sid ="105" ssid = "54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'16'", "'168'", "'196'", "'105'"]
'2'
'16'
'168'
'196'
'105'
['2', '16', '168', '196', '105']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="192" ssid = "1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="20" ssid = "16">We also explore how the characteristics of different languages affect topic model performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'18'", "'31'", "'20'"]
'3'
'192'
'18'
'31'
'20'
['3', '192', '18', '31', '20']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="22" ssid = "18">We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages  and to detect differences in topic emphasis between languages.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="10" ssid = "6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'22'", "'19'", "'10'", "'126'"]
'4'
'22'
'19'
'10'
'126'
['4', '22', '19', '10', '126']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Statistical topic models have emerged as an increasingly useful analysis tool for large text collections.</S><S sid ="15" ssid = "11">Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="6" ssid = "2">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'15'", "'18'", "'3'", "'6'"]
'5'
'15'
'18'
'3'
'6'
['5', '15', '18', '3', '6']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S><S sid ="26" ssid = "2">Tam  Lane and Schultz (Tam et al.  2007) also show improvements in machine translation using bilingual topic models.</S><S sid ="5" ssid = "1">Statistical topic models have emerged as an increasingly useful analysis tool for large text collections.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="29" ssid = "5">A recent extended abstract  developed concurrently by Ni et al. (Ni et al.  2009)  discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'26'", "'5'", "'18'", "'29'"]
'6'
'26'
'5'
'18'
'29'
['6', '26', '5', '18', '29']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Much of this work  however  has occurred in monolingual contexts.</S><S sid ="15" ssid = "11">Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'15'", "'31'", "'18'", "'161'"]
'7'
'15'
'31'
'18'
'161'
['7', '15', '31', '18', '161']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">In an increasingly connected world  the ability to access documents in many languages has become both a strategic asset and a personally enriching experience.</S><S sid ="16" ssid = "12">However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.</S><S sid ="168" ssid = "2">However  the growth of the web  and in particular Wikipedia  has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="108" ssid = "57">If we wish to perform topic-based bibliometric analysis  it is vital to be able to track the same topics across all languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'16'", "'168'", "'23'", "'108'"]
'8'
'16'
'168'
'23'
'108'
['8', '16', '168', '23', '108']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">In this paper  we present the polylingual topic model (PLTM).</S><S sid ="192" ssid = "1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="169" ssid = "3">In this section  we explore two questions relating to comparable text corpora and polylingual topic modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'192'", "'18'", "'3'", "'169'"]
'9'
'192'
'18'
'3'
'169'
['9', '192', '18', '3', '169']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="22" ssid = "18">We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages  and to detect differences in topic emphasis between languages.</S><S sid ="111" ssid = "60">In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts – i.e.  we put each of these documents in a single-document tuple.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'19'", "'22'", "'111'"]
'10'
'4'
'19'
'22'
'111'
['10', '4', '19', '22', '111']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">There are many potential applications for polylingual topic models.</S><S sid ="169" ssid = "3">In this section  we explore two questions relating to comparable text corpora and polylingual topic modeling.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'169'", "'3'", "'18'", "'31'"]
'11'
'169'
'3'
'18'
'31'
['11', '169', '3', '18', '31']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Although research literature is typically written in English  bibliographic databases often contain substantial quantities of work in other languages.</S><S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S><S sid ="174" ssid = "8">These versions of Wikipedia were selected to provide a diverse range of language families  geographic areas  and quantities of text.</S><S sid ="7" ssid = "3">Much of this work  however  has occurred in monolingual contexts.</S><S sid ="14" ssid = "10">Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'161'", "'174'", "'7'", "'14'"]
'12'
'161'
'174'
'7'
'14'
['12', '161', '174', '7', '14']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">To perform topic-based bibliometric analysis on these collections  it is necessary to have topic models that are aligned across languages.</S><S sid ="108" ssid = "57">If we wish to perform topic-based bibliometric analysis  it is vital to be able to track the same topics across all languages.</S><S sid ="196" ssid = "5">When applied to comparable document collections such as Wikipedia  PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S><S sid ="193" ssid = "2">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'108'", "'196'", "'126'", "'193'"]
'13'
'108'
'196'
'126'
'193'
['13', '108', '196', '126', '193']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.</S><S sid ="20" ssid = "16">We also explore how the characteristics of different languages affect topic model performance.</S><S sid ="170" ssid = "4">First  we explore whether comparable document tuples support the alignment of fine-grained topics  as demonstrated earlier using parallel documents.</S><S sid ="53" ssid = "2">In this case  we can be confident that the topic distribution is genuinely shared across all languages.</S><S sid ="68" ssid = "17">The third topic demonstrates differences in inflectional variation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'20'", "'170'", "'53'", "'68'"]
'14'
'20'
'170'
'53'
'68'
['14', '20', '170', '53', '68']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.</S><S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'52'", "'31'", "'3'", "'18'"]
'15'
'52'
'31'
'3'
'18'
['15', '52', '31', '3', '18']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.</S><S sid ="168" ssid = "2">However  the growth of the web  and in particular Wikipedia  has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid ="2" ssid = "2">Meanwhile  massive collections of interlinked documents in dozens of languages  such as Wikipedia  are now widely available  calling for tools that can characterize content in many languages.</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="21" ssid = "17">The second corpus  Wikipedia articles in twelve languages  contains sets of documents that are not translations of one another  but are very likely to be about similar concepts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'168'", "'2'", "'54'", "'21'"]
'16'
'168'
'2'
'54'
'21'
['16', '168', '2', '54', '21']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S><S sid ="194" ssid = "3">We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.</S><S sid ="105" ssid = "54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S><S sid ="13" ssid = "9">To perform topic-based bibliometric analysis on these collections  it is necessary to have topic models that are aligned across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'126'", "'194'", "'105'", "'13'"]
'17'
'126'
'194'
'105'
'13'
['17', '126', '194', '105', '13']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="169" ssid = "3">In this section  we explore two questions relating to comparable text corpora and polylingual topic modeling.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="9" ssid = "5">In this paper  we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'169'", "'3'", "'31'", "'9'"]
'18'
'169'
'3'
'31'
'9'
['18', '169', '3', '31', '9']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'148'", "'131'", "'4'", "'126'"]
'19'
'148'
'131'
'4'
'126'
['19', '148', '131', '4', '126']
parsed_discourse_facet ['method_citation']
['In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models.']
['We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.', 'We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).', 'These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.', u'We explore the model\xe2\u20ac\u2122s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.', 'We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages  and to detect differences in topic emphasis between languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00341', '(95%-conf.int.', '0.00341', '-', '0.00341)']
['system', 'ROUGE-S*', 'Average_P:', '0.17778', '(95%-conf.int.', '0.17778', '-', '0.17778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00669', '(95%-conf.int.', '0.00669', '-', '0.00669)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:45', 'F:8']
['We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.']
['In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'We also explore how the characteristics of different languages affect topic model performance.', 'They also provide little analysis of the differences between polylingual and single-language topic models.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:78', 'F:0']
['We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).']
['We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', 'These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.', u'We explore the model\u2019s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.', 'To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.']
['system', 'ROUGE-S*', 'Average_R:', '0.00500', '(95%-conf.int.', '0.00500', '-', '0.00500)']
['system', 'ROUGE-S*', 'Average_P:', '0.12500', '(95%-conf.int.', '0.12500', '-', '0.12500)']
['system', 'ROUGE-S*', 'Average_F:', '0.00961', '(95%-conf.int.', '0.00961', '-', '0.00961)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:120', 'F:15']
['Ideally, the &#8220;glue&#8221; documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.']
['When applied to comparable document collections such as Wikipedia  PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.', 'Meanwhile  massive collections of interlinked documents in dozens of languages  such as Wikipedia  are now widely available  calling for tools that can characterize content in many languages.', u'However  the growth of the web  and in particular Wikipedia  has made comparable text corpora \u2013 documents that are topically similar but are not direct translations of one another \u2013 considerably more abundant than true parallel corpora.', 'An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.', u'However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts\u2014documents that are topically similar but are not direct translations of one another\u2014considerably more abundant than ever before.']
['system', 'ROUGE-S*', 'Average_R:', '0.00864', '(95%-conf.int.', '0.00864', '-', '0.00864)']
['system', 'ROUGE-S*', 'Average_P:', '0.23333', '(95%-conf.int.', '0.23333', '-', '0.23333)']
['system', 'ROUGE-S*', 'Average_F:', '0.01667', '(95%-conf.int.', '0.01667', '-', '0.01667)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3240', 'P:120', 'F:28']
['The lower the divergence, the more similar the distributions are to each other.']
['In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.', 'They also provide little analysis of the differences between polylingual and single-language topic models.', 'Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.', 'As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.', 'Much of this work  however  has occurred in monolingual contexts.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:6', 'F:0']
['The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.']
['Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).', 'Statistical topic models have emerged as an increasingly useful analysis tool for large text collections.', 'In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.']
['system', 'ROUGE-S*', 'Average_R:', '0.00065', '(95%-conf.int.', '0.00065', '-', '0.00065)']
['system', 'ROUGE-S*', 'Average_P:', '0.01053', '(95%-conf.int.', '0.01053', '-', '0.01053)']
['system', 'ROUGE-S*', 'Average_F:', '0.00122', '(95%-conf.int.', '0.00122', '-', '0.00122)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:190', 'F:2']
['To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.']
['The third topic demonstrates differences in inflectional variation.', 'We also explore how the characteristics of different languages affect topic model performance.', 'Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.', 'In this case  we can be confident that the topic distribution is genuinely shared across all languages.', 'First  we explore whether comparable document tuples support the alignment of fine-grained topics  as demonstrated earlier using parallel documents.']
['system', 'ROUGE-S*', 'Average_R:', '0.02008', '(95%-conf.int.', '0.02008', '-', '0.02008)']
['system', 'ROUGE-S*', 'Average_P:', '0.09048', '(95%-conf.int.', '0.09048', '-', '0.09048)']
['system', 'ROUGE-S*', 'Average_F:', '0.03287', '(95%-conf.int.', '0.03287', '-', '0.03287)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:210', 'F:19']
['Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.']
['In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.', 'They also provide little analysis of the differences between polylingual and single-language topic models.', 'Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'Our first set of experiments focuses on document tuples that are known to consist of direct translations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00245', '(95%-conf.int.', '0.00245', '-', '0.00245)']
['system', 'ROUGE-S*', 'Average_P:', '0.01299', '(95%-conf.int.', '0.01299', '-', '0.01299)']
['system', 'ROUGE-S*', 'Average_F:', '0.00412', '(95%-conf.int.', '0.00412', '-', '0.00412)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:231', 'F:3']
['We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.']
['Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.', 'Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).', 'We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.', 'These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.', 'An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00538', '(95%-conf.int.', '0.00538', '-', '0.00538)']
['system', 'ROUGE-S*', 'Average_P:', '0.28889', '(95%-conf.int.', '0.28889', '-', '0.28889)']
['system', 'ROUGE-S*', 'Average_F:', '0.01057', '(95%-conf.int.', '0.01057', '-', '0.01057)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:45', 'F:13']
['We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.', 'To perform topic-based bibliometric analysis on these collections  it is necessary to have topic models that are aligned across languages.', 'These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.', 'We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.', 'An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.01331', '(95%-conf.int.', '0.01331', '-', '0.01331)']
['system', 'ROUGE-S*', 'Average_P:', '0.57778', '(95%-conf.int.', '0.57778', '-', '0.57778)']
['system', 'ROUGE-S*', 'Average_F:', '0.02603', '(95%-conf.int.', '0.02603', '-', '0.02603)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:45', 'F:26']
0.151677998483 0.00589199994108 0.0107779998922





input/ref/Task1/D09-1092_sweta.csv
input/res/Task1/D09-1092.csv
parsing: input/ref/Task1/D09-1092_sweta.csv
 <S sid="32" ssid="8">Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="5">Additionally, PLTM assumes that each &#8220;topic&#8221; consists of a set of discrete distributions over words&#8212;one for each language l = 1, ... , L. In other words, rather than using a single set of topics &#934; = {&#966;1, ... , &#966;T}, as in LDA, there are L sets of language-specific topics, &#934;1, ... , &#934;L, each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter &#946;l.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



["138'"]
138'
['138']
parsed_discourse_facet ['method_citation']
 <S sid="196" ssid="5">When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>
original cit marker offset is 0
new cit marker offset is 0



["196'"]
196'
['196']
parsed_discourse_facet ['method_citation']
<S sid="111" ssid="60">In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts &#8211; i.e., we put each of these documents in a single-document tuple.</S>
original cit marker offset is 0
new cit marker offset is 0



["111'"]
111'
['111']
parsed_discourse_facet ['method_citation']
<S sid="128" ssid="77">Although the PLTM is clearly not a substitute for a machine translation system&#8212;it has no way to represent syntax or even multi-word phrases&#8212;it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.</S>
original cit marker offset is 0
new cit marker offset is 0



["128'"]
128'
['128']
parsed_discourse_facet ['method_citation']
<S sid="122" ssid="71">Divergence drops significantly when the proportion of &#8220;glue&#8221; tuples increases from 0.01 to 0.25.</S>
original cit marker offset is 0
new cit marker offset is 0



["122'"]
122'
['122']
parsed_discourse_facet ['method_citation']
 <S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
 <S sid="110" ssid="59">Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S>
original cit marker offset is 0
new cit marker offset is 0



["110'"]
110'
['110']
parsed_discourse_facet ['method_citation']
<S sid="30" ssid="6">However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["30'"]
30'
['30']
parsed_discourse_facet ['method_citation']
    <S sid="146" ssid="95">In addition to enhancing lexicons by aligning topic-specific vocabulary, PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["146'"]
146'
['146']
parsed_discourse_facet ['method_citation']
<S sid="77" ssid="26">Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.</
original cit marker offset is 0
new cit marker offset is 0



["77'"]
77'
['77']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="105">We use both Jensen-Shannon divergence and cosine distance.</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="7">They also provide little analysis of the differences between polylingual and single-language topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>
original cit marker offset is 0
new cit marker offset is 0



["19'"]
19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="131" ssid="80">We evaluate sets of high-probability words in each topic and multilingual &#8220;synsets&#8221; by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["131'"]
131'
['131']
parsed_discourse_facet ['method_citation']
<S sid="196" ssid="5">When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>
original cit marker offset is 0
new cit marker offset is 0



["196'"]
196'
['196']
parsed_discourse_facet ['method_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["192'"]
192'
['192']
parsed_discourse_facet ['method_citation']
<S sid="195" ssid="4">Additionally, PLTM can support the creation of bilingual lexica for low resource language pairs, providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["195'"]
195'
['195']
parsed_discourse_facet ['method_citation']
<S sid="6" ssid="2">Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["6'"]
6'
['6']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="105" ssid = "54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S><S sid ="194" ssid = "3">We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'25'", "'105'", "'194'", "'126'"]
'1'
'25'
'105'
'194'
'126'
['1', '25', '105', '194', '126']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Meanwhile  massive collections of interlinked documents in dozens of languages  such as Wikipedia  are now widely available  calling for tools that can characterize content in many languages.</S><S sid ="16" ssid = "12">However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.</S><S sid ="168" ssid = "2">However  the growth of the web  and in particular Wikipedia  has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid ="196" ssid = "5">When applied to comparable document collections such as Wikipedia  PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S><S sid ="105" ssid = "54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'16'", "'168'", "'196'", "'105'"]
'2'
'16'
'168'
'196'
'105'
['2', '16', '168', '196', '105']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="192" ssid = "1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="20" ssid = "16">We also explore how the characteristics of different languages affect topic model performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'18'", "'31'", "'20'"]
'3'
'192'
'18'
'31'
'20'
['3', '192', '18', '31', '20']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="22" ssid = "18">We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages  and to detect differences in topic emphasis between languages.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="10" ssid = "6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'22'", "'19'", "'10'", "'126'"]
'4'
'22'
'19'
'10'
'126'
['4', '22', '19', '10', '126']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Statistical topic models have emerged as an increasingly useful analysis tool for large text collections.</S><S sid ="15" ssid = "11">Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="6" ssid = "2">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'15'", "'18'", "'3'", "'6'"]
'5'
'15'
'18'
'3'
'6'
['5', '15', '18', '3', '6']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S><S sid ="26" ssid = "2">Tam  Lane and Schultz (Tam et al.  2007) also show improvements in machine translation using bilingual topic models.</S><S sid ="5" ssid = "1">Statistical topic models have emerged as an increasingly useful analysis tool for large text collections.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="29" ssid = "5">A recent extended abstract  developed concurrently by Ni et al. (Ni et al.  2009)  discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'26'", "'5'", "'18'", "'29'"]
'6'
'26'
'5'
'18'
'29'
['6', '26', '5', '18', '29']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Much of this work  however  has occurred in monolingual contexts.</S><S sid ="15" ssid = "11">Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'15'", "'31'", "'18'", "'161'"]
'7'
'15'
'31'
'18'
'161'
['7', '15', '31', '18', '161']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">In an increasingly connected world  the ability to access documents in many languages has become both a strategic asset and a personally enriching experience.</S><S sid ="16" ssid = "12">However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.</S><S sid ="168" ssid = "2">However  the growth of the web  and in particular Wikipedia  has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="108" ssid = "57">If we wish to perform topic-based bibliometric analysis  it is vital to be able to track the same topics across all languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'16'", "'168'", "'23'", "'108'"]
'8'
'16'
'168'
'23'
'108'
['8', '16', '168', '23', '108']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">In this paper  we present the polylingual topic model (PLTM).</S><S sid ="192" ssid = "1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="169" ssid = "3">In this section  we explore two questions relating to comparable text corpora and polylingual topic modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'192'", "'18'", "'3'", "'169'"]
'9'
'192'
'18'
'3'
'169'
['9', '192', '18', '3', '169']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="22" ssid = "18">We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages  and to detect differences in topic emphasis between languages.</S><S sid ="111" ssid = "60">In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts – i.e.  we put each of these documents in a single-document tuple.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'19'", "'22'", "'111'"]
'10'
'4'
'19'
'22'
'111'
['10', '4', '19', '22', '111']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">There are many potential applications for polylingual topic models.</S><S sid ="169" ssid = "3">In this section  we explore two questions relating to comparable text corpora and polylingual topic modeling.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'169'", "'3'", "'18'", "'31'"]
'11'
'169'
'3'
'18'
'31'
['11', '169', '3', '18', '31']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Although research literature is typically written in English  bibliographic databases often contain substantial quantities of work in other languages.</S><S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S><S sid ="174" ssid = "8">These versions of Wikipedia were selected to provide a diverse range of language families  geographic areas  and quantities of text.</S><S sid ="7" ssid = "3">Much of this work  however  has occurred in monolingual contexts.</S><S sid ="14" ssid = "10">Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'161'", "'174'", "'7'", "'14'"]
'12'
'161'
'174'
'7'
'14'
['12', '161', '174', '7', '14']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">To perform topic-based bibliometric analysis on these collections  it is necessary to have topic models that are aligned across languages.</S><S sid ="108" ssid = "57">If we wish to perform topic-based bibliometric analysis  it is vital to be able to track the same topics across all languages.</S><S sid ="196" ssid = "5">When applied to comparable document collections such as Wikipedia  PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S><S sid ="193" ssid = "2">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'108'", "'196'", "'126'", "'193'"]
'13'
'108'
'196'
'126'
'193'
['13', '108', '196', '126', '193']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.</S><S sid ="20" ssid = "16">We also explore how the characteristics of different languages affect topic model performance.</S><S sid ="170" ssid = "4">First  we explore whether comparable document tuples support the alignment of fine-grained topics  as demonstrated earlier using parallel documents.</S><S sid ="53" ssid = "2">In this case  we can be confident that the topic distribution is genuinely shared across all languages.</S><S sid ="68" ssid = "17">The third topic demonstrates differences in inflectional variation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'20'", "'170'", "'53'", "'68'"]
'14'
'20'
'170'
'53'
'68'
['14', '20', '170', '53', '68']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.</S><S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'52'", "'31'", "'3'", "'18'"]
'15'
'52'
'31'
'3'
'18'
['15', '52', '31', '3', '18']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.</S><S sid ="168" ssid = "2">However  the growth of the web  and in particular Wikipedia  has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid ="2" ssid = "2">Meanwhile  massive collections of interlinked documents in dozens of languages  such as Wikipedia  are now widely available  calling for tools that can characterize content in many languages.</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="21" ssid = "17">The second corpus  Wikipedia articles in twelve languages  contains sets of documents that are not translations of one another  but are very likely to be about similar concepts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'168'", "'2'", "'54'", "'21'"]
'16'
'168'
'2'
'54'
'21'
['16', '168', '2', '54', '21']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S><S sid ="194" ssid = "3">We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.</S><S sid ="105" ssid = "54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S><S sid ="13" ssid = "9">To perform topic-based bibliometric analysis on these collections  it is necessary to have topic models that are aligned across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'126'", "'194'", "'105'", "'13'"]
'17'
'126'
'194'
'105'
'13'
['17', '126', '194', '105', '13']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="169" ssid = "3">In this section  we explore two questions relating to comparable text corpora and polylingual topic modeling.</S><S sid ="3" ssid = "3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid ="9" ssid = "5">In this paper  we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'169'", "'3'", "'31'", "'9'"]
'18'
'169'
'3'
'31'
'9'
['18', '169', '3', '31', '9']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="126" ssid = "75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'148'", "'131'", "'4'", "'126'"]
'19'
'148'
'131'
'4'
'126'
['19', '148', '131', '4', '126']
parsed_discourse_facet ['method_citation']
['When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.']
['We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.', 'We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).', 'These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.', u'We explore the model\xe2\u20ac\u2122s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.', 'We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages  and to detect differences in topic emphasis between languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.01705', '(95%-conf.int.', '0.01705', '-', '0.01705)']
['system', 'ROUGE-S*', 'Average_P:', '0.33333', '(95%-conf.int.', '0.33333', '-', '0.33333)']
['system', 'ROUGE-S*', 'Average_F:', '0.03244', '(95%-conf.int.', '0.03244', '-', '0.03244)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:120', 'F:40']
['We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.']
['In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'We also explore how the characteristics of different languages affect topic model performance.', 'They also provide little analysis of the differences between polylingual and single-language topic models.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:78', 'F:0']
['Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).']
['We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', 'These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.', u'We explore the model\u2019s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.', 'To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.']
['system', 'ROUGE-S*', 'Average_R:', '0.00966', '(95%-conf.int.', '0.00966', '-', '0.00966)']
['system', 'ROUGE-S*', 'Average_P:', '0.04603', '(95%-conf.int.', '0.04603', '-', '0.04603)']
['system', 'ROUGE-S*', 'Average_F:', '0.01596', '(95%-conf.int.', '0.01596', '-', '0.01596)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:630', 'F:29']
['Additionally, PLTM assumes that each &#8220;topic&#8221; consists of a set of discrete distributions over words&#8212;one for each language l = 1, ... , L. In other words, rather than using a single set of topics &#934; = {&#966;1, ... , &#966;T}, as in LDA, there are L sets of language-specific topics, &#934;1, ... , &#934;L, each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter &#946;l.']
['When applied to comparable document collections such as Wikipedia  PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.', 'Meanwhile  massive collections of interlinked documents in dozens of languages  such as Wikipedia  are now widely available  calling for tools that can characterize content in many languages.', u'However  the growth of the web  and in particular Wikipedia  has made comparable text corpora \u2013 documents that are topically similar but are not direct translations of one another \u2013 considerably more abundant than true parallel corpora.', 'An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.', u'However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts\u2014documents that are topically similar but are not direct translations of one another\u2014considerably more abundant than ever before.']
['system', 'ROUGE-S*', 'Average_R:', '0.00556', '(95%-conf.int.', '0.00556', '-', '0.00556)']
['system', 'ROUGE-S*', 'Average_P:', '0.02560', '(95%-conf.int.', '0.02560', '-', '0.02560)']
['system', 'ROUGE-S*', 'Average_F:', '0.00913', '(95%-conf.int.', '0.00913', '-', '0.00913)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3240', 'P:703', 'F:18']
['Divergence drops significantly when the proportion of &#8220;glue&#8221; tuples increases from 0.01 to 0.25.']
['In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.', 'They also provide little analysis of the differences between polylingual and single-language topic models.', 'Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.', 'As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.', 'Much of this work  however  has occurred in monolingual contexts.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:78', 'F:0']
['In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts &#8211; i.e., we put each of these documents in a single-document tuple.']
['Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).', 'Statistical topic models have emerged as an increasingly useful analysis tool for large text collections.', 'In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.']
['system', 'ROUGE-S*', 'Average_R:', '0.00032', '(95%-conf.int.', '0.00032', '-', '0.00032)']
['system', 'ROUGE-S*', 'Average_P:', '0.00585', '(95%-conf.int.', '0.00585', '-', '0.00585)']
['system', 'ROUGE-S*', 'Average_F:', '0.00062', '(95%-conf.int.', '0.00062', '-', '0.00062)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:171', 'F:1']
['We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.']
['The third topic demonstrates differences in inflectional variation.', 'We also explore how the characteristics of different languages affect topic model performance.', 'Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.', 'In this case  we can be confident that the topic distribution is genuinely shared across all languages.', 'First  we explore whether comparable document tuples support the alignment of fine-grained topics  as demonstrated earlier using parallel documents.']
['system', 'ROUGE-S*', 'Average_R:', '0.00211', '(95%-conf.int.', '0.00211', '-', '0.00211)']
['system', 'ROUGE-S*', 'Average_P:', '0.02198', '(95%-conf.int.', '0.02198', '-', '0.02198)']
['system', 'ROUGE-S*', 'Average_F:', '0.00386', '(95%-conf.int.', '0.00386', '-', '0.00386)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:91', 'F:2']
['We evaluate sets of high-probability words in each topic and multilingual &#8220;synsets&#8221; by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).']
['In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.', 'They also provide little analysis of the differences between polylingual and single-language topic models.', 'Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'Our first set of experiments focuses on document tuples that are known to consist of direct translations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00082', '(95%-conf.int.', '0.00082', '-', '0.00082)']
['system', 'ROUGE-S*', 'Average_P:', '0.00585', '(95%-conf.int.', '0.00585', '-', '0.00585)']
['system', 'ROUGE-S*', 'Average_F:', '0.00143', '(95%-conf.int.', '0.00143', '-', '0.00143)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:171', 'F:1']
['Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.']
['Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.', 'Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).', 'We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.', 'These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.', 'An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00124', '(95%-conf.int.', '0.00124', '-', '0.00124)']
['system', 'ROUGE-S*', 'Average_P:', '0.02500', '(95%-conf.int.', '0.02500', '-', '0.02500)']
['system', 'ROUGE-S*', 'Average_F:', '0.00237', '(95%-conf.int.', '0.00237', '-', '0.00237)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:120', 'F:3']
['We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.', 'To perform topic-based bibliometric analysis on these collections  it is necessary to have topic models that are aligned across languages.', 'These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.', 'We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.', 'An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.01331', '(95%-conf.int.', '0.01331', '-', '0.01331)']
['system', 'ROUGE-S*', 'Average_P:', '0.57778', '(95%-conf.int.', '0.57778', '-', '0.57778)']
['system', 'ROUGE-S*', 'Average_F:', '0.02603', '(95%-conf.int.', '0.02603', '-', '0.02603)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:45', 'F:26']
0.104141998959 0.00500699994993 0.00918399990816





input/ref/Task1/W99-0613_vardha.csv
input/res/Task1/W99-0613.csv
parsing: input/ref/Task1/W99-0613_vardha.csv
    <S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="35" ssid="29">AdaBoost finds a weighted combination of simple (weak) classifiers, where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
    <S sid="134" ssid="1">This section describes an algorithm based on boosting algorithms, which were previously developed for supervised machine learning problems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'"]
'134'
['134']
parsed_discourse_facet ['method_citation']
    <S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'"]
'236'
['236']
parsed_discourse_facet ['method_citation']
    <S sid="8" ssid="2">Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
    <S sid="42" ssid="36">(Riloff and Shepherd 97) describe a bootstrapping approach for acquiring nouns in particular categories (such as &amp;quot;vehicle&amp;quot; or &amp;quot;weapon&amp;quot; categories).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
    <S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'"]
'236'
['236']
parsed_discourse_facet ['method_citation']
    <S sid="222" ssid="1">The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'222'"]
'222'
['222']
parsed_discourse_facet ['method_citation']
    <S sid="30" ssid="24">(Blum and Mitchell 98) offer a promising formulation of redundancy, also prove some results about how the use of can help classification, and suggest an objective function when training with unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'"]
'30'
['30']
parsed_discourse_facet ['method_citation']
 <S sid="26" ssid="20">We present two algorithms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'"]
'26'
['26']
parsed_discourse_facet ['method_citation']
    <S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="26">The algorithm can be viewed as heuristically optimizing an objective function suggested by (Blum and Mitchell 98); empirically it is shown to be quite successful in optimizing this criterion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'"]
'32'
['32']
parsed_discourse_facet ['method_citation']
    <S sid="47" ssid="1">971,746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'"]
'47'
['47']
parsed_discourse_facet ['method_citation']
    <S sid="127" ssid="60">The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'"]
'127'
['127']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="1" ssid = "1">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="9" ssid = "3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="136" ssid = "3">We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'250'", "'136'", "'163'"]
'1'
'9'
'250'
'136'
'163'
['1', '9', '250', '136', '163']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.</S><S sid ="17" ssid = "11">At first glance  the problem seems quite complex: a large number of rules is needed to cover the domain  suggesting that a large number of labeled examples is required to train an accurate classifier.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="105" ssid = "38">Thus the method makes the fairly strong assumption that the features can be partitioned into two types such that each type alone is sufficient for classification.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'17'", "'250'", "'105'", "'248'"]
'2'
'17'
'250'
'105'
'248'
['2', '17', '250', '105', '248']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid ="92" ssid = "25">Thus an explicit assumption about the redundancy of the features — that either the spelling or context alone should be sufficient to build a classifier — has been built into the algorithm.</S><S sid ="121" ssid = "54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).</S><S sid ="58" ssid = "12">Having found (spelling  context) pairs in the parsed data  a number of features are extracted.</S><S sid ="232" ssid = "11">For example  the independence assumptions mean that the model fails to capture the dependence between specific and more general features (for example the fact that the feature full.-string=New_York is always seen with the features contains (New) and The baseline method tags all entities as the most frequent class type (organization). contains (York) and is never seen with a feature such as contains (Group) ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'92'", "'121'", "'58'", "'232'"]
'3'
'92'
'121'
'58'
'232'
['3', '92', '121', '58', '232']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "20">We present two algorithms.</S><S sid ="4" ssid = "4">We present two algorithms.</S><S sid ="68" ssid = "1">The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S><S sid ="88" ssid = "21">We can now compare this algorithm to that of (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'4'", "'68'", "'163'", "'88'"]
'26'
'4'
'68'
'163'
'88'
['26', '4', '68', '163', '88']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid ="6" ssid = "6">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="68" ssid = "1">The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'31'", "'6'", "'97'", "'68'"]
'5'
'31'
'6'
'97'
'68'
['5', '31', '6', '97', '68']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "6">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid ="5" ssid = "5">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="30" ssid = "24">(Blum and Mitchell 98) offer a promising formulation of redundancy  also prove some results about how the use of unlabeled examples can help classification  and suggest an objective function when training with unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'31'", "'5'", "'204'", "'30'"]
'6'
'31'
'5'
'204'
'30'
['6', '31', '5', '204', '30']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision  in the form of labeled training examples.</S><S sid ="2" ssid = "2">A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="3" ssid = "3">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid ="130" ssid = "63">So the success of the algorithm may well be due to its success in maximizing the number of unlabeled examples on which the two decision lists agree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'2'", "'250'", "'3'", "'130'"]
'7'
'2'
'250'
'3'
'130'
['7', '2', '250', '3', '130']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "2">Recent results (e.g.  (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S><S sid ="27" ssid = "21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S><S sid ="120" ssid = "53">(Blum and Mitchell 98) go on to give PAC results for learning in the cotraining case.</S><S sid ="40" ssid = "34">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid ="88" ssid = "21">We can now compare this algorithm to that of (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'27'", "'120'", "'40'", "'88'"]
'8'
'27'
'120'
'40'
'88'
['8', '27', '120', '40', '88']
parsed_discourse_facet ['results_citation']
<S sid ="1" ssid = "1">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="9" ssid = "3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="136" ssid = "3">We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'250'", "'136'", "'163'"]
'1'
'9'
'250'
'136'
'163'
['1', '9', '250', '136', '163']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "4">The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.</S><S sid ="147" ssid = "14">The weak learner for two-class problems computes a weak hypothesis h from the input space into the reals (h : 2x -4 R)  where the sign4 of h(x) is interpreted as the predicted label and the magnitude I h(x)I is the confidence in the prediction: large numbers for I h(x)I indicate high confidence in the prediction  and numbers close to zero indicate low confidence.</S><S sid ="35" ssid = "29">AdaBoost finds a weighted combination of simple (weak) classifiers  where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.</S><S sid ="224" ssid = "3">The model was parameterized such that the joint probability of a (label  feature-set) pair P(yi  xi) is written as The model assumes that (y  x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).</S><S sid ="74" ssid = "7">Output of the learning algorithm: a function h:Xxy [0  1] where h(x  y) is an estimate of the conditional probability p(y1x) of seeing label y given that feature x is present.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'147'", "'35'", "'224'", "'74'"]
'10'
'147'
'35'
'224'
'74'
['10', '147', '35', '224', '74']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "5">For example  a good classifier would identify Mrs. Frank as a person  Steptoe & Johnson as a company  and Honduras as a location.</S><S sid ="40" ssid = "34">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid ="23" ssid = "17">For example  in ..  says Mr. Cooper  a vice president of.. both a spelling feature (that the string contains Mr.) and a contextual feature (that president modifies the string) are strong indications that Mr. Cooper is of type Person.</S><S sid ="48" ssid = "2">For example  take ...  says Maury Cooper  a vice president at S.&P.</S><S sid ="55" ssid = "9">In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'40'", "'23'", "'48'", "'55'"]
'11'
'40'
'23'
'48'
'55'
['11', '40', '23', '48', '55']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "6">The approach uses both spelling and contextual rules.</S><S sid ="58" ssid = "12">Having found (spelling  context) pairs in the parsed data  a number of features are extracted.</S><S sid ="22" ssid = "16">In many cases  inspection of either the spelling or context alone is sufficient to classify an example.</S><S sid ="165" ssid = "32">In the namedentity problem each example is a (spelling context) pair.</S><S sid ="55" ssid = "9">In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'58'", "'22'", "'165'", "'55'"]
'12'
'58'
'22'
'165'
'55'
['12', '58', '22', '165', '55']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "7">A spelling rule might be a simple look-up for the string (e.g.  a rule that Honduras is a location) or a rule that looks at words within a string (e.g.  a rule that any string containing Mr. is a person).</S><S sid ="14" ssid = "8">A contextual rule considers words surrounding the string in the sentence in which it appears (e.g.  a rule that any proper name modified by an appositive whose head is president is a person).</S><S sid ="61" ssid = "15">The following features were used: full-string=x The full string (e.g.  for Maury Cooper  full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word  this feature applies for any words that the string contains (e.g.  Maury Cooper contributes two such features  contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g.  IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods  and contains at least one period.</S><S sid ="22" ssid = "16">In many cases  inspection of either the spelling or context alone is sufficient to classify an example.</S><S sid ="165" ssid = "32">In the namedentity problem each example is a (spelling context) pair.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'14'", "'61'", "'22'", "'165'"]
'13'
'14'
'61'
'22'
'165'
['13', '14', '61', '22', '165']
parsed_discourse_facet ['method_citation']
['This section describes an algorithm based on boosting algorithms, which were previously developed for supervised machine learning problems.']
['They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).', 'The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.', u'Thus an explicit assumption about the redundancy of the features \u2014 that either the spelling or context alone should be sufficient to build a classifier \u2014 has been built into the algorithm.', 'Having found (spelling  context) pairs in the parsed data  a number of features are extracted.', 'For example  the independence assumptions mean that the model fails to capture the dependence between specific and more general features (for example the fact that the feature full.-string=New_York is always seen with the features contains (New) and The baseline method tags all entities as the most frequent class type (organization). contains (York) and is never seen with a feature such as contains (Group) ).']
['system', 'ROUGE-S*', 'Average_R:', '0.00036', '(95%-conf.int.', '0.00036', '-', '0.00036)']
['system', 'ROUGE-S*', 'Average_P:', '0.01515', '(95%-conf.int.', '0.01515', '-', '0.01515)']
['system', 'ROUGE-S*', 'Average_F:', '0.00070', '(95%-conf.int.', '0.00070', '-', '0.00070)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:66', 'F:1']
['We present two algorithms.']
['The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.', 'Output of the learning algorithm: a function h:Xxy [0  1] where h(x  y) is an estimate of the conditional probability p(y1x) of seeing label y given that feature x is present.', 'The model was parameterized such that the joint probability of a (label  feature-set) pair P(yi  xi) is written as The model assumes that (y  x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).', 'AdaBoost finds a weighted combination of simple (weak) classifiers  where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.', 'The weak learner for two-class problems computes a weak hypothesis h from the input space into the reals (h : 2x -4 R)  where the sign4 of h(x) is interpreted as the predicted label and the magnitude I h(x)I is the confidence in the prediction: large numbers for I h(x)I indicate high confidence in the prediction  and numbers close to zero indicate low confidence.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5671', 'P:1', 'F:0']
['We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.']
['The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).', "Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).", 'The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', 'The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:45', 'F:0']
['The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.']
['The following features were used: full-string=x The full string (e.g.  for Maury Cooper  full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word  this feature applies for any words that the string contains (e.g.  Maury Cooper contributes two such features  contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g.  IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods  and contains at least one period.', 'A spelling rule might be a simple look-up for the string (e.g.  a rule that Honduras is a location) or a rule that looks at words within a string (e.g.  a rule that any string containing Mr. is a person).', 'A contextual rule considers words surrounding the string in the sentence in which it appears (e.g.  a rule that any proper name modified by an appositive whose head is president is a person).', 'In many cases  inspection of either the spelling or context alone is sufficient to classify an example.', 'In the namedentity problem each example is a (spelling context) pair.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:36', 'F:0']
['We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.']
['So the success of the algorithm may well be due to its success in maximizing the number of unlabeled examples on which the two decision lists agree.', 'Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.', 'A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.', 'The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.', 'Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision  in the form of labeled training examples.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:45', 'F:0']
['The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.']
['(Blum and Mitchell 98) go on to give PAC results for learning in the cotraining case.', 'Recent results (e.g.  (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.', 'We can now compare this algorithm to that of (Yarowsky 95).', 'The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).', '(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.']
['system', 'ROUGE-S*', 'Average_R:', '0.00063', '(95%-conf.int.', '0.00063', '-', '0.00063)']
['system', 'ROUGE-S*', 'Average_P:', '0.00654', '(95%-conf.int.', '0.00654', '-', '0.00654)']
['system', 'ROUGE-S*', 'Average_F:', '0.00114', '(95%-conf.int.', '0.00114', '-', '0.00114)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:153', 'F:1']
['AdaBoost finds a weighted combination of simple (weak) classifiers, where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.']
['Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.', 'A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.', 'With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.', 'At first glance  the problem seems quite complex: a large number of rules is needed to cover the domain  suggesting that a large number of labeled examples is required to train an accurate classifier.', 'Thus the method makes the fairly strong assumption that the features can be partitioned into two types such that each type alone is sufficient for classification.']
['system', 'ROUGE-S*', 'Average_R:', '0.00342', '(95%-conf.int.', '0.00342', '-', '0.00342)']
['system', 'ROUGE-S*', 'Average_P:', '0.07353', '(95%-conf.int.', '0.07353', '-', '0.07353)']
['system', 'ROUGE-S*', 'Average_F:', '0.00653', '(95%-conf.int.', '0.00653', '-', '0.00653)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:136', 'F:10']
['The algorithm can be viewed as heuristically optimizing an objective function suggested by (Blum and Mitchell 98); empirically it is shown to be quite successful in optimizing this criterion.']
['In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.', 'In the namedentity problem each example is a (spelling context) pair.', 'The approach uses both spelling and contextual rules.', 'Having found (spelling  context) pairs in the parsed data  a number of features are extracted.', 'In many cases  inspection of either the spelling or context alone is sufficient to classify an example.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:105', 'F:0']
['Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.']
["Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).", '(Blum and Mitchell 98) offer a promising formulation of redundancy  also prove some results about how the use of unlabeled examples can help classification  and suggest an objective function when training with unlabeled examples.', 'The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).', 'In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.', 'The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).']
['system', 'ROUGE-S*', 'Average_R:', '0.01997', '(95%-conf.int.', '0.01997', '-', '0.01997)']
['system', 'ROUGE-S*', 'Average_P:', '0.37143', '(95%-conf.int.', '0.37143', '-', '0.37143)']
['system', 'ROUGE-S*', 'Average_F:', '0.03790', '(95%-conf.int.', '0.03790', '-', '0.03790)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:105', 'F:39']
0.0518499994239 0.00270888885879 0.00514111105399





input/ref/Task1/W99-0613_sweta.csv
input/res/Task1/W99-0613.csv
parsing: input/ref/Task1/W99-0613_sweta.csv
<S sid="121" ssid="54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page, and other pages pointing to the page).</S>
original cit marker offset is 0
new cit marker offset is 0



["121'"]
121'
['121']
parsed_discourse_facet ['method_citation']
<S sid="252" ssid="3">The method uses a &amp;quot;soft&amp;quot; measure of the agreement between two classifiers as an objective function; we described an algorithm which directly optimizes this function.</S>
original cit marker offset is 0
new cit marker offset is 0



["252'"]
252'
['252']
parsed_discourse_facet ['method_citation']
 <S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



["213'"]
213'
['213']
parsed_discourse_facet ['method_citation']
 <S sid="250" ssid="1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S>
original cit marker offset is 0
new cit marker offset is 0



["250'"]
250'
['250']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="33">(Brin 98) ,describes a system for extracting (author, book-title) pairs from the World Wide Web using an approach that bootstraps from an initial seed set of examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="202" ssid="69">The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels, and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.</S>
original cit marker offset is 0
new cit marker offset is 0



["202'"]
202'
['202']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="15">The following features were used: full-string=x The full string (e.g., for Maury Cooper, full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word, this feature applies for any words that the string contains (e.g., Maury Cooper contributes two such features, contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g., IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods, and contains at least one period.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="176" ssid="43">(7) is at 0 when: 1) Vi : sign(gi (xi)) = sign(g2 (xi)); 2) Ig3(xi)l oo; and 3) sign(gi (xi)) = yi for i = 1, , m. In fact, Zco provides a bound on the sum of the classification error of the labeled examples and the number of disagreements between the two classifiers on the unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["176'"]
176'
['176']
parsed_discourse_facet ['method_citation']
 <S sid="108" ssid="41">In the cotraining case, (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples, and (2) must agree with each other on the unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S>
    <S sid="28" ssid="22">(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'", "'28'"]
27'
'28'
['27', '28']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["7'"]
7'
['7']
parsed_discourse_facet ['method_citation']
 <S sid="172" ssid="39">To see this, note thai the first two terms in the above equation correspond to the function that AdaBoost attempts to minimize in the standard supervised setting (Equ.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["85'"]
85'
['85']
parsed_discourse_facet ['method_citation']
 <S sid="214" ssid="81">This modification brings the method closer to the DL-CoTrain algorithm described earlier, and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples, preventing one label from dominating &#8212; this deserves more theoretical investigation.</S>
original cit marker offset is 0
new cit marker offset is 0



["214'"]
214'
['214']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="1" ssid = "1">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="9" ssid = "3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="136" ssid = "3">We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'250'", "'136'", "'163'"]
'1'
'9'
'250'
'136'
'163'
['1', '9', '250', '136', '163']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.</S><S sid ="17" ssid = "11">At first glance  the problem seems quite complex: a large number of rules is needed to cover the domain  suggesting that a large number of labeled examples is required to train an accurate classifier.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="105" ssid = "38">Thus the method makes the fairly strong assumption that the features can be partitioned into two types such that each type alone is sufficient for classification.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'17'", "'250'", "'105'", "'248'"]
'2'
'17'
'250'
'105'
'248'
['2', '17', '250', '105', '248']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid ="92" ssid = "25">Thus an explicit assumption about the redundancy of the features — that either the spelling or context alone should be sufficient to build a classifier — has been built into the algorithm.</S><S sid ="121" ssid = "54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).</S><S sid ="58" ssid = "12">Having found (spelling  context) pairs in the parsed data  a number of features are extracted.</S><S sid ="232" ssid = "11">For example  the independence assumptions mean that the model fails to capture the dependence between specific and more general features (for example the fact that the feature full.-string=New_York is always seen with the features contains (New) and The baseline method tags all entities as the most frequent class type (organization). contains (York) and is never seen with a feature such as contains (Group) ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'92'", "'121'", "'58'", "'232'"]
'3'
'92'
'121'
'58'
'232'
['3', '92', '121', '58', '232']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "20">We present two algorithms.</S><S sid ="4" ssid = "4">We present two algorithms.</S><S sid ="68" ssid = "1">The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S><S sid ="88" ssid = "21">We can now compare this algorithm to that of (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'4'", "'68'", "'163'", "'88'"]
'26'
'4'
'68'
'163'
'88'
['26', '4', '68', '163', '88']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid ="6" ssid = "6">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="68" ssid = "1">The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'31'", "'6'", "'97'", "'68'"]
'5'
'31'
'6'
'97'
'68'
['5', '31', '6', '97', '68']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "6">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid ="5" ssid = "5">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="30" ssid = "24">(Blum and Mitchell 98) offer a promising formulation of redundancy  also prove some results about how the use of unlabeled examples can help classification  and suggest an objective function when training with unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'31'", "'5'", "'204'", "'30'"]
'6'
'31'
'5'
'204'
'30'
['6', '31', '5', '204', '30']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision  in the form of labeled training examples.</S><S sid ="2" ssid = "2">A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="3" ssid = "3">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid ="130" ssid = "63">So the success of the algorithm may well be due to its success in maximizing the number of unlabeled examples on which the two decision lists agree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'2'", "'250'", "'3'", "'130'"]
'7'
'2'
'250'
'3'
'130'
['7', '2', '250', '3', '130']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "2">Recent results (e.g.  (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S><S sid ="27" ssid = "21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S><S sid ="120" ssid = "53">(Blum and Mitchell 98) go on to give PAC results for learning in the cotraining case.</S><S sid ="40" ssid = "34">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid ="88" ssid = "21">We can now compare this algorithm to that of (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'27'", "'120'", "'40'", "'88'"]
'8'
'27'
'120'
'40'
'88'
['8', '27', '120', '40', '88']
parsed_discourse_facet ['results_citation']
<S sid ="1" ssid = "1">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="9" ssid = "3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="136" ssid = "3">We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'250'", "'136'", "'163'"]
'1'
'9'
'250'
'136'
'163'
['1', '9', '250', '136', '163']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "4">The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.</S><S sid ="147" ssid = "14">The weak learner for two-class problems computes a weak hypothesis h from the input space into the reals (h : 2x -4 R)  where the sign4 of h(x) is interpreted as the predicted label and the magnitude I h(x)I is the confidence in the prediction: large numbers for I h(x)I indicate high confidence in the prediction  and numbers close to zero indicate low confidence.</S><S sid ="35" ssid = "29">AdaBoost finds a weighted combination of simple (weak) classifiers  where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.</S><S sid ="224" ssid = "3">The model was parameterized such that the joint probability of a (label  feature-set) pair P(yi  xi) is written as The model assumes that (y  x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).</S><S sid ="74" ssid = "7">Output of the learning algorithm: a function h:Xxy [0  1] where h(x  y) is an estimate of the conditional probability p(y1x) of seeing label y given that feature x is present.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'147'", "'35'", "'224'", "'74'"]
'10'
'147'
'35'
'224'
'74'
['10', '147', '35', '224', '74']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "5">For example  a good classifier would identify Mrs. Frank as a person  Steptoe & Johnson as a company  and Honduras as a location.</S><S sid ="40" ssid = "34">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid ="23" ssid = "17">For example  in ..  says Mr. Cooper  a vice president of.. both a spelling feature (that the string contains Mr.) and a contextual feature (that president modifies the string) are strong indications that Mr. Cooper is of type Person.</S><S sid ="48" ssid = "2">For example  take ...  says Maury Cooper  a vice president at S.&P.</S><S sid ="55" ssid = "9">In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'40'", "'23'", "'48'", "'55'"]
'11'
'40'
'23'
'48'
'55'
['11', '40', '23', '48', '55']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "6">The approach uses both spelling and contextual rules.</S><S sid ="58" ssid = "12">Having found (spelling  context) pairs in the parsed data  a number of features are extracted.</S><S sid ="22" ssid = "16">In many cases  inspection of either the spelling or context alone is sufficient to classify an example.</S><S sid ="165" ssid = "32">In the namedentity problem each example is a (spelling context) pair.</S><S sid ="55" ssid = "9">In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'58'", "'22'", "'165'", "'55'"]
'12'
'58'
'22'
'165'
'55'
['12', '58', '22', '165', '55']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "7">A spelling rule might be a simple look-up for the string (e.g.  a rule that Honduras is a location) or a rule that looks at words within a string (e.g.  a rule that any string containing Mr. is a person).</S><S sid ="14" ssid = "8">A contextual rule considers words surrounding the string in the sentence in which it appears (e.g.  a rule that any proper name modified by an appositive whose head is president is a person).</S><S sid ="61" ssid = "15">The following features were used: full-string=x The full string (e.g.  for Maury Cooper  full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word  this feature applies for any words that the string contains (e.g.  Maury Cooper contributes two such features  contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g.  IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods  and contains at least one period.</S><S sid ="22" ssid = "16">In many cases  inspection of either the spelling or context alone is sufficient to classify an example.</S><S sid ="165" ssid = "32">In the namedentity problem each example is a (spelling context) pair.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'14'", "'61'", "'22'", "'165'"]
'13'
'14'
'61'
'22'
'165'
['13', '14', '61', '22', '165']
parsed_discourse_facet ['method_citation']
['There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.']
['They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).', 'The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.', u'Thus an explicit assumption about the redundancy of the features \u2014 that either the spelling or context alone should be sufficient to build a classifier \u2014 has been built into the algorithm.', 'Having found (spelling  context) pairs in the parsed data  a number of features are extracted.', 'For example  the independence assumptions mean that the model fails to capture the dependence between specific and more general features (for example the fact that the feature full.-string=New_York is always seen with the features contains (New) and The baseline method tags all entities as the most frequent class type (organization). contains (York) and is never seen with a feature such as contains (Group) ).']
['system', 'ROUGE-S*', 'Average_R:', '0.00613', '(95%-conf.int.', '0.00613', '-', '0.00613)']
['system', 'ROUGE-S*', 'Average_P:', '0.21795', '(95%-conf.int.', '0.21795', '-', '0.21795)']
['system', 'ROUGE-S*', 'Average_F:', '0.01192', '(95%-conf.int.', '0.01192', '-', '0.01192)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:78', 'F:17']
['The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).', '(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance.']
['The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.', 'Output of the learning algorithm: a function h:Xxy [0  1] where h(x  y) is an estimate of the conditional probability p(y1x) of seeing label y given that feature x is present.', 'The model was parameterized such that the joint probability of a (label  feature-set) pair P(yi  xi) is written as The model assumes that (y  x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).', 'AdaBoost finds a weighted combination of simple (weak) classifiers  where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.', 'The weak learner for two-class problems computes a weak hypothesis h from the input space into the reals (h : 2x -4 R)  where the sign4 of h(x) is interpreted as the predicted label and the magnitude I h(x)I is the confidence in the prediction: large numbers for I h(x)I indicate high confidence in the prediction  and numbers close to zero indicate low confidence.']
['system', 'ROUGE-S*', 'Average_R:', '0.00035', '(95%-conf.int.', '0.00035', '-', '0.00035)']
['system', 'ROUGE-S*', 'Average_P:', '0.00866', '(95%-conf.int.', '0.00866', '-', '0.00866)']
['system', 'ROUGE-S*', 'Average_F:', '0.00068', '(95%-conf.int.', '0.00068', '-', '0.00068)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5671', 'P:231', 'F:2']
['In the cotraining case, (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples, and (2) must agree with each other on the unlabeled examples.']
['This paper discusses the use of unlabeled examples for the problem of named entity classification.', 'This paper discusses the use of unlabeled examples for the problem of named entity classification.', 'Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.', 'We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.', 'We now describe the CoBoost algorithm for the named entity problem.']
['system', 'ROUGE-S*', 'Average_R:', '0.00332', '(95%-conf.int.', '0.00332', '-', '0.00332)']
['system', 'ROUGE-S*', 'Average_P:', '0.01299', '(95%-conf.int.', '0.01299', '-', '0.01299)']
['system', 'ROUGE-S*', 'Average_F:', '0.00529', '(95%-conf.int.', '0.00529', '-', '0.00529)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:231', 'F:3']
['Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.']
['The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).', "Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).", 'The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', 'The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).']
['system', 'ROUGE-S*', 'Average_R:', '0.00065', '(95%-conf.int.', '0.00065', '-', '0.00065)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00125', '(95%-conf.int.', '0.00125', '-', '0.00125)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:55', 'F:1']
['This modification brings the method closer to the DL-CoTrain algorithm described earlier, and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples, preventing one label from dominating &#8212; this deserves more theoretical investigation.']
['The following features were used: full-string=x The full string (e.g.  for Maury Cooper  full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word  this feature applies for any words that the string contains (e.g.  Maury Cooper contributes two such features  contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g.  IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods  and contains at least one period.', 'A spelling rule might be a simple look-up for the string (e.g.  a rule that Honduras is a location) or a rule that looks at words within a string (e.g.  a rule that any string containing Mr. is a person).', 'A contextual rule considers words surrounding the string in the sentence in which it appears (e.g.  a rule that any proper name modified by an appositive whose head is president is a person).', 'In many cases  inspection of either the spelling or context alone is sufficient to classify an example.', 'In the namedentity problem each example is a (spelling context) pair.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:231', 'F:0']
['The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels, and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.']
['So the success of the algorithm may well be due to its success in maximizing the number of unlabeled examples on which the two decision lists agree.', 'Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.', 'A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.', 'The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.', 'Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision  in the form of labeled training examples.']
['system', 'ROUGE-S*', 'Average_R:', '0.00325', '(95%-conf.int.', '0.00325', '-', '0.00325)']
['system', 'ROUGE-S*', 'Average_P:', '0.10989', '(95%-conf.int.', '0.10989', '-', '0.10989)']
['system', 'ROUGE-S*', 'Average_F:', '0.00631', '(95%-conf.int.', '0.00631', '-', '0.00631)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:91', 'F:10']
['The following features were used: full-string=x The full string (e.g., for Maury Cooper, full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word, this feature applies for any words that the string contains (e.g., Maury Cooper contributes two such features, contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g., IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods, and contains at least one period.']
['(Blum and Mitchell 98) go on to give PAC results for learning in the cotraining case.', 'Recent results (e.g.  (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.', 'We can now compare this algorithm to that of (Yarowsky 95).', 'The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).', '(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:903', 'F:0']
['The method uses a &quot;soft&quot; measure of the agreement between two classifiers as an objective function; we described an algorithm which directly optimizes this function.']
['Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.', 'A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.', 'With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.', 'At first glance  the problem seems quite complex: a large number of rules is needed to cover the domain  suggesting that a large number of labeled examples is required to train an accurate classifier.', 'Thus the method makes the fairly strong assumption that the features can be partitioned into two types such that each type alone is sufficient for classification.']
['system', 'ROUGE-S*', 'Average_R:', '0.00205', '(95%-conf.int.', '0.00205', '-', '0.00205)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.00399', '(95%-conf.int.', '0.00399', '-', '0.00399)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:78', 'F:6']
['(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.']
['In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.', 'In the namedentity problem each example is a (spelling context) pair.', 'The approach uses both spelling and contextual rules.', 'Having found (spelling  context) pairs in the parsed data  a number of features are extracted.', 'In many cases  inspection of either the spelling or context alone is sufficient to classify an example.']
['system', 'ROUGE-S*', 'Average_R:', '0.00178', '(95%-conf.int.', '0.00178', '-', '0.00178)']
['system', 'ROUGE-S*', 'Average_P:', '0.00159', '(95%-conf.int.', '0.00159', '-', '0.00159)']
['system', 'ROUGE-S*', 'Average_F:', '0.00168', '(95%-conf.int.', '0.00168', '-', '0.00168)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:630', 'F:1']
['Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.']
["Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).", '(Blum and Mitchell 98) offer a promising formulation of redundancy  also prove some results about how the use of unlabeled examples can help classification  and suggest an objective function when training with unlabeled examples.', 'The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).', 'In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.', 'The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).']
['system', 'ROUGE-S*', 'Average_R:', '0.00307', '(95%-conf.int.', '0.00307', '-', '0.00307)']
['system', 'ROUGE-S*', 'Average_P:', '0.10909', '(95%-conf.int.', '0.10909', '-', '0.10909)']
['system', 'ROUGE-S*', 'Average_F:', '0.00598', '(95%-conf.int.', '0.00598', '-', '0.00598)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:55', 'F:6']
0.0555269994447 0.0020599999794 0.0037099999629





input/ref/Task1/D10-1044_aakansha.csv
input/res/Task1/D10-1044.csv
parsing: input/ref/Task1/D10-1044_aakansha.csv
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S>
    <S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'96'"]
'95'
'96'
['95', '96']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'"]
'75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="28">For comparison to information-retrieval inspired baselines, eg (L&#168;u et al., 2007), we select sentences from OUT using language model perplexities from IN.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'"]
'22'
['22']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="23">The 2nd block contains the IR system, which was tuned by selecting text in multiples of the size of the EMEA training corpus, according to dev set performance.</S>
    <S sid="120" ssid="24">This significantly underperforms log-linear combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'", "'120'"]
'119'
'120'
['119', '120']
parsed_discourse_facet ['result_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
    <S sid="24" ssid="21">Sentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="4">We focus here instead on adapting the two most important features: the language model (LM), which estimates the probability p(wIh) of a target word w following an ngram h; and the translation models (TM) p(slt) and p(t1s), which give the probability of source phrase s translating to target phrase t, and vice versa.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D10-1044.csv
<S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="145" ssid = "2">Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="62" ssid = "26">To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'65'", "'145'", "'64'", "'62'"]
'1'
'65'
'145'
'64'
'62'
['1', '65', '145', '64', '62']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S><S sid ="23" ssid = "20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="153" ssid = "10">Finally  we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.</S><S sid ="68" ssid = "5">First  we learn weights on individual phrase pairs rather than sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'23'", "'20'", "'153'", "'68'"]
'2'
'23'
'20'
'153'
'68'
['2', '23', '20', '153', '68']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">We incorporate instance weighting into a mixture-model framework  and find that it yields consistent improvements over a wide range of baselines.</S><S sid ="150" ssid = "7">In both cases  the instanceweighting approach improved over a wide range of baselines  giving gains of over 2 BLEU points over the best non-adapted baseline  and gains of between 0.6 and 1.8 over an equivalent mixture model (with an identical training procedure but without instance weighting).</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'150'", "'65'", "'38'", "'1'"]
'3'
'150'
'65'
'38'
'1'
['3', '150', '65', '38', '1']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'10'", "'6'", "'24'", "'17'"]
'4'
'10'
'6'
'24'
'17'
['4', '10', '6', '24', '17']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "2">Even when there is training data available in the domain of interest  there is often additional data from other domains that could in principle be used to improve performance.</S><S sid ="138" ssid = "7">However  for multinomial models like our LMs and TMs  there is a one to one correspondence between instances and features  eg the correspondence between a phrase pair (s  t) and its conditional multinomial probability p(s1t).</S><S sid ="9" ssid = "6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="39" ssid = "3">Thus  provided at least this amount of IN data is available—as it is in our setting—adapting these weights is straightforward.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'138'", "'9'", "'6'", "'39'"]
'5'
'138'
'9'
'6'
'39'
['5', '138', '9', '6', '39']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'17'", "'24'", "'18'", "'4'"]
'6'
'17'
'24'
'18'
'4'
['6', '17', '24', '18', '4']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="9" ssid = "6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'24'", "'21'", "'4'", "'9'"]
'7'
'24'
'21'
'4'
'9'
['7', '24', '21', '4', '9']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'10'", "'18'", "'6'", "'20'"]
'8'
'10'
'18'
'6'
'20'
['8', '10', '18', '6', '20']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S><S sid ="144" ssid = "1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S><S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="42" ssid = "6">The natural baseline approach is to concatenate data from IN and OUT.</S><S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'144'", "'1'", "'42'", "'2'"]
'9'
'144'
'1'
'42'
'2'
['9', '144', '1', '42', '2']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="29" ssid = "26">This is a simple and effective alternative to setting weights discriminatively to maximize a metric such as BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'18'", "'6'", "'29'"]
'10'
'4'
'18'
'6'
'29'
['10', '4', '18', '6', '29']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="44" ssid = "8">When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.</S><S sid ="92" ssid = "29">6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'18'", "'6'", "'44'", "'92'"]
'11'
'18'
'6'
'44'
'92'
['11', '18', '6', '44', '92']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "9">For simplicity  we assume that OUT is homogeneous.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="92" ssid = "29">6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.</S><S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'18'", "'11'", "'92'", "'16'"]
'12'
'18'
'11'
'92'
'16'
['12', '18', '11', '92', '16']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "10">The techniques we develop can be extended in a relatively straightforward manner to the more general case when OUT consists of multiple sub-domains.</S><S sid ="139" ssid = "8">As mentioned above  it is not obvious how to apply Daum´e’s approach to multinomials  which do not have a mechanism for combining split features.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="82" ssid = "19">This is not unreasonable given the application to phrase pairs from OUT  but it suggests that an interesting alternative might be to use a plain log-linear weighting function exp(Ei Aifi(s  t))  with outputs in [0  oo].</S><S sid ="32" ssid = "29">This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'139'", "'21'", "'82'", "'32'"]
'13'
'139'
'21'
'82'
'32'
['13', '139', '21', '82', '32']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "11">There is a fairly large body of work on SMT adaptation.</S><S sid ="141" ssid = "10">Moving beyond directly related work  major themes in SMT adaptation include the IR (Hildebrand et al.  2005; L¨u et al.  2007; Zhao et al.  2004) and mixture (Finch and Sumita  2008; Foster and Kuhn  2007; Koehn and Schroeder  2007; L¨u et al.  2007) approaches for LMs and TMs described above  as well as methods for exploiting monolingual in-domain text  typically by translating it automatically and then performing self training (Bertoldi and Federico  2009; Ueffing et al.  2007; Schwenk and Senellart  2009).</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="34" ssid = "31">Section 2 describes our baseline techniques for SMT adaptation  and section 3 describes the instance-weighting approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'141'", "'4'", "'10'", "'34'"]
'14'
'141'
'4'
'10'
'34'
['14', '141', '4', '10', '34']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "12">We introduce several new ideas.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="27" ssid = "24">Finally  we make some improvements to baseline approaches.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="88" ssid = "25">We have not explored this strategy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'67'", "'27'", "'20'", "'88'"]
'15'
'67'
'27'
'20'
'88'
['15', '67', '27', '20', '88']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="42" ssid = "6">The natural baseline approach is to concatenate data from IN and OUT.</S><S sid ="68" ssid = "5">First  we learn weights on individual phrase pairs rather than sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'17'", "'24'", "'42'", "'68'"]
'16'
'17'
'24'
'42'
'68'
['16', '17', '24', '42', '68']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S><S sid ="100" ssid = "4">Figure 1 shows sample sentences from these domains  which are widely divergent.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'6'", "'24'", "'16'", "'100'"]
'17'
'6'
'24'
'16'
'100'
['17', '6', '24', '16', '100']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="44" ssid = "8">When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'11'", "'44'", "'6'", "'10'"]
'18'
'11'
'44'
'6'
'10'
['18', '11', '44', '6', '10']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "16">The idea of distinguishing between general and domain-specific examples is due to Daum´e and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daum´e’s (2007) work that splits each feature into domain-specific and general copies.</S><S sid ="73" ssid = "10">Jiang and Zhai (2007) suggest the following derivation  making use of the true OUT distribution po(s  t): where each fi(s  t) is a feature intended to charac- !0ˆ = argmax pf(s  t) log pθ(s|t) (8) terize the usefulness of (s  t)  weighted by Ai. θ s t pf(s  t)po(s  t) log pθ(s|t) The mixing parameters and feature weights (col- != argmax po (s  t) lectively 0) are optimized simultaneously using dev- θ s t pf(s  t)co(s  t) log pθ(s|t)  set maximum likelihood as before: !�argmax po (s  t) ! θ s t �ˆ = argmax ˜p(s  t) log p(s|t; 0).</S><S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'136'", "'73'", "'140'", "'64'"]
'19'
'136'
'73'
'140'
'64'
['19', '136', '73', '140', '64']
parsed_discourse_facet ['method_citation']
['Our second contribution is to apply instance weighting at the level of phrase pairs.']
[u'Daum\xc2\xb4e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.', 'We have not explored this strategy.', 'Finally  we make some improvements to baseline approaches.', 'We introduce several new ideas.', 'We extend the Matsoukas et al approach in several ways.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:325', 'P:21', 'F:0']
['To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.']
['Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.', 'This is a standard adaptation problem for SMT.', 'Domain adaptation is a common concern when optimizing empirical NLP applications.', 'Previous approaches have tried to find examples that are similar to the target domain.', 'Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.']
['system', 'ROUGE-S*', 'Average_R:', '0.01080', '(95%-conf.int.', '0.01080', '-', '0.01080)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01858', '(95%-conf.int.', '0.01858', '-', '0.01858)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:120', 'F:8']
['Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.', 'We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.']
['First  we learn weights on individual phrase pairs rather than sentences.', 'Finally  we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.', 'This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.', u'Daum\xc2\xb4e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.', 'Our second contribution is to apply instance weighting at the level of phrase pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.04299', '(95%-conf.int.', '0.04299', '-', '0.04299)']
['system', 'ROUGE-S*', 'Average_P:', '0.08108', '(95%-conf.int.', '0.08108', '-', '0.08108)']
['system', 'ROUGE-S*', 'Average_F:', '0.05619', '(95%-conf.int.', '0.05619', '-', '0.05619)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:703', 'F:57']
['In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.', 'To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.', 'Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.', 'The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.', 'Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.']
['system', 'ROUGE-S*', 'Average_R:', '0.00768', '(95%-conf.int.', '0.00768', '-', '0.00768)']
['system', 'ROUGE-S*', 'Average_P:', '0.19231', '(95%-conf.int.', '0.19231', '-', '0.19231)']
['system', 'ROUGE-S*', 'Average_F:', '0.01477', '(95%-conf.int.', '0.01477', '-', '0.01477)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:78', 'F:15']
['We focus here instead on adapting the two most important features: the language model (LM), which estimates the probability p(wIh) of a target word w following an ngram h; and the translation models (TM) p(slt) and p(t1s), which give the probability of source phrase s translating to target phrase t, and vice versa.']
[u'The idea of distinguishing between general and domain-specific examples is due to Daum\xb4e and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.', 'The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.', u'Jiang and Zhai (2007) suggest the following derivation  making use of the true OUT distribution po(s  t): where each fi(s  t) is a feature intended to charac- !0\u02c6 = argmax pf(s  t) log p\u03b8(s|t) (8) terize the usefulness of (s  t)  weighted by Ai. \u03b8 s t pf(s  t)po(s  t) log p\u03b8(s|t) The mixing parameters and feature weights (col- != argmax po (s  t) lectively 0) are optimized simultaneously using dev- \u03b8 s t pf(s  t)co(s  t) log p\u03b8(s|t)  set maximum likelihood as before: !\ufffdargmax po (s  t) ! \u03b8 s t \ufffd\u02c6 = argmax \u02dcp(s  t) log p(s|t; 0).', u'It is also worth pointing out a connection with Daum\xb4e\u2019s (2007) work that splits each feature into domain-specific and general copies.', u'Recent work by Finkel and Manning (2009) which re-casts Daum\xb4e\u2019s approach in a hierarchical MAP framework may be applicable to this problem.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4753', 'P:351', 'F:0']
['In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.']
[u'In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material\u2014though adequate for reasonable performance\u2014is also available.', 'We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.', 'In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.', 'This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.', 'The natural baseline approach is to concatenate data from IN and OUT.']
['system', 'ROUGE-S*', 'Average_R:', '0.07507', '(95%-conf.int.', '0.07507', '-', '0.07507)']
['system', 'ROUGE-S*', 'Average_P:', '0.81429', '(95%-conf.int.', '0.81429', '-', '0.81429)']
['system', 'ROUGE-S*', 'Average_F:', '0.13746', '(95%-conf.int.', '0.13746', '-', '0.13746)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:210', 'F:171']
['Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.']
['This is a standard adaptation problem for SMT.', 'Domain adaptation is a common concern when optimizing empirical NLP applications.', 'Section 2 describes our baseline techniques for SMT adaptation  and section 3 describes the instance-weighting approach.', 'There is a fairly large body of work on SMT adaptation.', u'Moving beyond directly related work  major themes in SMT adaptation include the IR (Hildebrand et al.  2005; L\xa8u et al.  2007; Zhao et al.  2004) and mixture (Finch and Sumita  2008; Foster and Kuhn  2007; Koehn and Schroeder  2007; L\xa8u et al.  2007) approaches for LMs and TMs described above  as well as methods for exploiting monolingual in-domain text  typically by translating it automatically and then performing self training (Bertoldi and Federico  2009; Ueffing et al.  2007; Schwenk and Senellart  2009).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3403', 'P:78', 'F:0']
['Sentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language.', 'Our second contribution is to apply instance weighting at the level of phrase pairs.']
['It is difficult when IN and OUT are dissimilar  as they are in the cases we study.', 'This is less effective in our setting  where IN and OUT are disparate.', 'When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.', 'This is a standard adaptation problem for SMT.', 'Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00493', '(95%-conf.int.', '0.00493', '-', '0.00493)']
['system', 'ROUGE-S*', 'Average_P:', '0.01307', '(95%-conf.int.', '0.01307', '-', '0.01307)']
['system', 'ROUGE-S*', 'Average_F:', '0.00716', '(95%-conf.int.', '0.00716', '-', '0.00716)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:406', 'P:153', 'F:2']
['In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.', 'In both cases  the instanceweighting approach improved over a wide range of baselines  giving gains of over 2 BLEU points over the best non-adapted baseline  and gains of between 0.6 and 1.8 over an equivalent mixture model (with an identical training procedure but without instance weighting).', 'We incorporate instance weighting into a mixture-model framework  and find that it yields consistent improvements over a wide range of baselines.', 'The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.', 'Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.']
['system', 'ROUGE-S*', 'Average_R:', '0.00235', '(95%-conf.int.', '0.00235', '-', '0.00235)']
['system', 'ROUGE-S*', 'Average_P:', '0.03810', '(95%-conf.int.', '0.03810', '-', '0.03810)']
['system', 'ROUGE-S*', 'Average_F:', '0.00443', '(95%-conf.int.', '0.00443', '-', '0.00443)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3403', 'P:210', 'F:8']
0.133946665178 0.0159799998224 0.0265099997054





input/ref/Task1/P11-1061_aakansha.csv
input/res/Task1/P11-1061.csv
parsing: input/ref/Task1/P11-1061_aakansha.csv
<S sid="40" ssid="6">We extend Subramanya et al.&#8217;s intuitions to our bilingual setup.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="11">First, we use a novel graph-based framework for projecting syntactic information across language boundaries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="29" ssid="6">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'"]
'158'
['158']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'"]
'158'
['158']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="1">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="17" ssid="13">Second, we treat the projected labels as features in an unsupervised model (&#167;5), rather than using them directly for supervised training.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="111" ssid="11">We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).</S>
original cit marker offset is 0
new cit marker offset is 0



["'111'"]
'111'
['111']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="29" ssid="6">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'161'"]
'161'
['161']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1061.csv
<S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S><S sid ="8" ssid = "4">Unsupervised learning approaches appear to be a natural solution to this problem  as they require only unannotated text for training models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'160'", "'3'", "'13'", "'8'"]
'1'
'160'
'3'
'13'
'8'
['1', '160', '3', '13', '8']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed)  making it applicable to a wide array of resource-poor languages.</S><S sid ="9" ssid = "5">Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.</S><S sid ="126" ssid = "26">Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.</S><S sid ="41" ssid = "7">Because the information flow in our graph is asymmetric (from English to the foreign language)  we use different types of vertices for each language.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'9'", "'126'", "'41'", "'107'"]
'2'
'9'
'126'
'41'
'107'
['2', '9', '126', '41', '107']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="33" ssid = "10">The POS distributions over the foreign trigram types are used as features to learn a better unsupervised POS tagger (§5).</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="60" ssid = "26">To initialize the graph for label propagation we use a supervised English tagger to label the English side of the bitext.7 We then simply count the individual labels of the English tokens and normalize the counts to produce tag distributions over English word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'39'", "'33'", "'1'", "'60'"]
'3'
'39'
'33'
'1'
'60'
['3', '39', '33', '1', '60']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S><S sid ="23" ssid = "19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%)  and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S sid ="143" ssid = "6">Overall  it gives improvements ranging from 1.1% for German to 14.7% for Italian  for an average improvement of 8.3% over the unsupervised feature-HMM model.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S><S sid ="144" ssid = "7">For comparison  the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%  and goes up to 88.7% with a treebank dictionary.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'23'", "'143'", "'135'", "'144'"]
'4'
'23'
'143'
'135'
'144'
['4', '23', '143', '135', '144']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing  resulting in highly accurate systems.</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S><S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S><S sid ="15" ssid = "11">First  we use a novel graph-based framework for projecting syntactic information across language boundaries.</S><S sid ="18" ssid = "14">To make the projection practical  we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'158'", "'4'", "'15'", "'18'"]
'5'
'158'
'4'
'15'
'18'
['5', '158', '4', '15', '18']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Supervised part-of-speech (POS) taggers  for example  approach the level of inter-annotator agreement (Shen et al.  2007  97.3% accuracy for English).</S><S sid ="47" ssid = "13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S><S sid ="9" ssid = "5">Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S><S sid ="20" ssid = "16">Because there might be some controversy about the exact definitions of such universals  this set of coarse-grained POS categories is defined operationally  by collapsing language (or treebank) specific distinctions to a set of categories that exists across all languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'47'", "'9'", "'158'", "'20'"]
'6'
'47'
'9'
'158'
'20'
['6', '47', '9', '158', '20']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S><S sid ="27" ssid = "4">Graph construction does not require any labeled data  but makes use of two similarity functions.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'27'", "'107'", "'56'", "'37'"]
'7'
'27'
'107'
'56'
'37'
['7', '27', '107', '56', '37']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Unsupervised learning approaches appear to be a natural solution to this problem  as they require only unannotated text for training models.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'160'", "'1'", "'13'", "'3'"]
'8'
'160'
'1'
'13'
'3'
['8', '160', '1', '13', '3']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.</S><S sid ="47" ssid = "13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S><S sid ="126" ssid = "26">Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.</S><S sid ="2" ssid = "2">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed)  making it applicable to a wide array of resource-poor languages.</S><S sid ="57" ssid = "23">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'47'", "'126'", "'2'", "'57'"]
'9'
'47'
'126'
'2'
'57'
['9', '47', '126', '2', '57']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'39'", "'16'", "'26'", "'13'"]
'10'
'39'
'16'
'26'
'13'
['10', '39', '16', '26', '13']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This scenario is applicable to a large set of languages and has been considered by a number of authors in the past (Alshawi et al.  2000; Xi and Hwa  2005; Ganchev et al.  2009).</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="94" ssid = "25">We adopted this state-of-the-art model because it makes it easy to experiment with various ways of incorporating our novel constraint feature into the log-linear emission model.</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'19'", "'26'", "'94'", "'13'"]
'11'
'19'
'26'
'94'
'13'
['11', '19', '26', '94', '13']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Naseem et al. (2009) and Snyder et al.</S><S sid ="38" ssid = "4">More recently  Subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model.</S><S sid ="93" ssid = "24">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S><S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="123" ssid = "23">This can be seen as a rough approximation of Yarowsky and Ngai (2001).</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'38'", "'93'", "'37'", "'123'"]
'12'
'38'
'93'
'37'
'123'
['12', '38', '93', '37', '123']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="24" ssid = "1">The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'39'", "'160'", "'1'", "'24'"]
'13'
'39'
'160'
'1'
'24'
['13', '39', '160', '1', '24']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">Our work is closest to that of Yarowsky and Ngai (2001)  but differs in two important ways.</S><S sid ="123" ssid = "23">This can be seen as a rough approximation of Yarowsky and Ngai (2001).</S><S sid ="40" ssid = "6">We extend Subramanya et al.’s intuitions to our bilingual setup.</S><S sid ="88" ssid = "19">In our experiments  we used the same set of features as BergKirkpatrick et al. (2010): an indicator feature based In a traditional Markov model  the emission distribution PΘ(Xi = xi  |Zi = zi) is a set of multinomials.</S><S sid ="85" ssid = "16">We develop our POS induction model based on the feature-based HMM of Berg-Kirkpatrick et al. (2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'123'", "'40'", "'88'", "'85'"]
'14'
'123'
'40'
'88'
'85'
['14', '123', '40', '88', '85']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">First  we use a novel graph-based framework for projecting syntactic information across language boundaries.</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'158'", "'29'", "'56'", "'25'"]
'15'
'158'
'29'
'56'
'25'
['15', '158', '29', '56', '25']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="57" ssid = "23">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid ="70" ssid = "1">Given the bilingual graph described in the previous section  we can use label propagation to project the English POS labels to the foreign language.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'57'", "'70'", "'29'", "'26'"]
'16'
'57'
'70'
'29'
'26'
['16', '57', '70', '29', '26']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">Second  we treat the projected labels as features in an unsupervised model (§5)  rather than using them directly for supervised training.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="71" ssid = "2">We use label propagation in two stages to generate soft labels on all the vertices in the graph.</S><S sid ="126" ssid = "26">Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.</S><S sid ="52" ssid = "18">This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'97'", "'71'", "'126'", "'52'"]
'17'
'97'
'71'
'126'
'52'
['17', '97', '71', '126', '52']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">To make the projection practical  we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="111" ssid = "11">We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns)  VERB (verbs)  ADJ (adjectives)  ADV (adverbs)  PRON (pronouns)  DET (determiners)  ADP (prepositions or postpositions)  NUM (numerals)  CONJ (conjunctions)  PRT (particles)  PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).</S><S sid ="85" ssid = "16">We develop our POS induction model based on the feature-based HMM of Berg-Kirkpatrick et al. (2010).</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'56'", "'111'", "'85'", "'158'"]
'18'
'56'
'111'
'85'
'158'
['18', '56', '111', '85', '158']
parsed_discourse_facet ['method_citation']
['We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.', 'To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.', 'First  we use a novel graph-based framework for projecting syntactic information across language boundaries.', u'To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (\u0e22\u0e073.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00043', '(95%-conf.int.', '0.00043', '-', '0.00043)']
['system', 'ROUGE-S*', 'Average_P:', '0.00111', '(95%-conf.int.', '0.00111', '-', '0.00111)']
['system', 'ROUGE-S*', 'Average_F:', '0.00062', '(95%-conf.int.', '0.00062', '-', '0.00062)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:903', 'F:1']
['We extend Subramanya et al.&#8217;s intuitions to our bilingual setup.']
['We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.', 'Unsupervised learning approaches appear to be a natural solution to this problem  as they require only unannotated text for training models.', 'We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).', 'Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.', '(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:21', 'F:0']
['The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.']
['We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.', 'The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.', '(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.', 'Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.', 'They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.']
['system', 'ROUGE-S*', 'Average_R:', '0.03279', '(95%-conf.int.', '0.03279', '-', '0.03279)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.06350', '(95%-conf.int.', '0.06350', '-', '0.06350)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:91', 'F:91']
['These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.']
['Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.', 'Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).', u'Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don\u2019t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De\ufffdf.', 'Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed)  making it applicable to a wide array of resource-poor languages.', 'Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.']
['system', 'ROUGE-S*', 'Average_R:', '0.00353', '(95%-conf.int.', '0.00353', '-', '0.00353)']
['system', 'ROUGE-S*', 'Average_P:', '0.07246', '(95%-conf.int.', '0.07246', '-', '0.07246)']
['system', 'ROUGE-S*', 'Average_F:', '0.00673', '(95%-conf.int.', '0.00673', '-', '0.00673)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5671', 'P:276', 'F:20']
['To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.']
['This can be seen as a rough approximation of Yarowsky and Ngai (2001).', 'For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.', 'Naseem et al. (2009) and Snyder et al.', 'Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.', 'More recently  Subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00352', '(95%-conf.int.', '0.00352', '-', '0.00352)']
['system', 'ROUGE-S*', 'Average_P:', '0.01705', '(95%-conf.int.', '0.01705', '-', '0.01705)']
['system', 'ROUGE-S*', 'Average_F:', '0.00584', '(95%-conf.int.', '0.00584', '-', '0.00584)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:528', 'F:9']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.']
['To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.', 'They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.', '(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.', u'As discussed in more detail in \u0e22\u0e073  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', u'To this end  we construct a bilingual graph over word types to establish a connection between the two languages (\u0e22\u0e073)  and then use graph label propagation to project syntactic information from English to the foreign language (\u0e22\u0e074).']
['system', 'ROUGE-S*', 'Average_R:', '0.00392', '(95%-conf.int.', '0.00392', '-', '0.00392)']
['system', 'ROUGE-S*', 'Average_P:', '0.38182', '(95%-conf.int.', '0.38182', '-', '0.38182)']
['system', 'ROUGE-S*', 'Average_F:', '0.00776', '(95%-conf.int.', '0.00776', '-', '0.00776)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5356', 'P:55', 'F:21']
['To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.']
['First  we use a novel graph-based framework for projecting syntactic information across language boundaries.', 'We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.', 'Supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing  resulting in highly accurate systems.', 'Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.', 'To make the projection practical  we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).']
['system', 'ROUGE-S*', 'Average_R:', '0.00341', '(95%-conf.int.', '0.00341', '-', '0.00341)']
['system', 'ROUGE-S*', 'Average_P:', '0.02279', '(95%-conf.int.', '0.02279', '-', '0.02279)']
['system', 'ROUGE-S*', 'Average_F:', '0.00593', '(95%-conf.int.', '0.00593', '-', '0.00593)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:351', 'F:8']
['To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.']
['Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.', 'Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed)  making it applicable to a wide array of resource-poor languages.', 'Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.', 'Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.', 'Because the information flow in our graph is asymmetric (from English to the foreign language)  we use different types of vertices for each language.']
['system', 'ROUGE-S*', 'Average_R:', '0.04346', '(95%-conf.int.', '0.04346', '-', '0.04346)']
['system', 'ROUGE-S*', 'Average_P:', '0.18750', '(95%-conf.int.', '0.18750', '-', '0.18750)']
['system', 'ROUGE-S*', 'Average_F:', '0.07056', '(95%-conf.int.', '0.07056', '-', '0.07056)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:528', 'F:99']
['Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.', 'To make the projection practical  we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).', 'To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.', 'We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns)  VERB (verbs)  ADJ (adjectives)  ADV (adverbs)  PRON (pronouns)  DET (determiners)  ADP (prepositions or postpositions)  NUM (numerals)  CONJ (conjunctions)  PRT (particles)  PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).', 'We develop our POS induction model based on the feature-based HMM of Berg-Kirkpatrick et al. (2010).']
['system', 'ROUGE-S*', 'Average_R:', '0.00230', '(95%-conf.int.', '0.00230', '-', '0.00230)']
['system', 'ROUGE-S*', 'Average_P:', '0.06618', '(95%-conf.int.', '0.06618', '-', '0.06618)']
['system', 'ROUGE-S*', 'Average_F:', '0.00444', '(95%-conf.int.', '0.00444', '-', '0.00444)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3916', 'P:136', 'F:9']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.']
['We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.', 'Unsupervised learning approaches appear to be a natural solution to this problem  as they require only unannotated text for training models.', '(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.', 'Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.', 'We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).']
['system', 'ROUGE-S*', 'Average_R:', '0.00615', '(95%-conf.int.', '0.00615', '-', '0.00615)']
['system', 'ROUGE-S*', 'Average_P:', '0.25455', '(95%-conf.int.', '0.25455', '-', '0.25455)']
['system', 'ROUGE-S*', 'Average_F:', '0.01200', '(95%-conf.int.', '0.01200', '-', '0.01200)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:55', 'F:14']
['To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).']
['To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.', 'Graph construction does not require any labeled data  but makes use of two similarity functions.', 'Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.', 'Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.', 'However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.']
['system', 'ROUGE-S*', 'Average_R:', '0.00349', '(95%-conf.int.', '0.00349', '-', '0.00349)']
['system', 'ROUGE-S*', 'Average_P:', '0.07576', '(95%-conf.int.', '0.07576', '-', '0.07576)']
['system', 'ROUGE-S*', 'Average_F:', '0.00668', '(95%-conf.int.', '0.00668', '-', '0.00668)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:66', 'F:5']
0.189019998282 0.00936363627851 0.0167327271206





input/ref/Task1/P05-1013_aakansha.csv
input/res/Task1/P05-1013.csv
parsing: input/ref/Task1/P05-1013_aakansha.csv
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="17">However, the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech, with a best performance of 73% UAS (Holan, 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="86" ssid="13">As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'"]
'86'
['86']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="7">As shown in Table 3, the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.</S>
    <S sid="81"  ssid="8">However, the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'81'"]
'80'
'81'
['80', '81']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'"]
'49'
['49']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="10">While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P05-1013.csv
<S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="13" ssid = "9">The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="16" ssid = "12">Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.</S><S sid ="80" ssid = "7">As shown in Table 3  the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'13'", "'7'", "'16'", "'80'"]
'1'
'13'
'7'
'16'
'80'
['1', '13', '7', '16', '80']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="44" ssid = "15">Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="30" ssid = "1">We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.</S><S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'44'", "'36'", "'30'", "'8'"]
'2'
'44'
'36'
'30'
'8'
['2', '44', '36', '30', '8']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy.</S><S sid ="28" ssid = "24">First  in section 4  we evaluate the graph transformation techniques in themselves  with data from the Prague Dependency Treebank and the Danish Dependency Treebank.</S><S sid ="29" ssid = "25">In section 5  we then evaluate the entire parsing system by training and evaluating on data from the Prague Dependency Treebank.</S><S sid ="62" ssid = "1">In the experiments below  we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs 3 previously tested on Swedish (Nivre et al.  2004) and English (Nivre and Scholz  2004).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'28'", "'29'", "'62'", "'75'"]
'3'
'28'
'29'
'62'
'75'
['3', '28', '29', '62', '75']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'104'", "'106'", "'78'", "'8'"]
'4'
'104'
'106'
'78'
'8'
['4', '104', '106', '78', '8']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order  where discontinuous syntactic constructions are more common than in languages like English (Mel’ˇcuk  1988; Covington  1990).</S><S sid ="94" ssid = "5">The first thing to note is that projectivizing helps in itself  even if no encoding is used  as seen from the fact that the projective baseline outperforms the non-projective training condition by more than half a percentage point on attachment score  although the gain is much smaller with respect to exact match.</S><S sid ="99" ssid = "10">This may seem surprising  given the experiments reported in section 4  but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift  where the encoding of path information is often redundant.</S><S sid ="9" ssid = "5">This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).</S><S sid ="47" ssid = "18">In practice  we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'94'", "'99'", "'9'", "'47'"]
'5'
'94'
'99'
'9'
'47'
['5', '94', '99', '9', '47']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.</S><S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="44" ssid = "15">Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.</S><S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'2'", "'36'", "'44'", "'89'"]
'6'
'2'
'36'
'44'
'89'
['6', '2', '36', '44', '89']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="16" ssid = "12">Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.</S><S sid ="19" ssid = "15">Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).</S><S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="47" ssid = "18">In practice  we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'16'", "'19'", "'1'", "'47'"]
'7'
'16'
'19'
'1'
'47'
['7', '16', '19', '1', '47']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S><S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="30" ssid = "1">We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'2'", "'104'", "'4'", "'30'"]
'8'
'2'
'104'
'4'
'30'
['8', '2', '104', '4', '30']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).</S><S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="17" ssid = "13">There exist a few robust broad-coverage parsers that produce non-projective dependency structures  notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English  Foth et al. (2004) for German  and Holan (2004) for Czech.</S><S sid ="105" ssid = "16">Although the best published results for the Collins parser is 80% UAS (Collins  1999)  this parser reaches 82% when trained on the entire training data set  and an adapted version of Charniak’s parser (Charniak  2000) performs at 84% (Jan Hajiˇc  pers. comm.).</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'10'", "'17'", "'105'", "'24'"]
'9'
'10'
'17'
'105'
'24'
['9', '10', '17', '105', '24']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="42" ssid = "13">Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="12" ssid = "8">Prague Dependency Treebank (Hajiˇc et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'42'", "'36'", "'4'", "'12'"]
'10'
'42'
'36'
'4'
'12'
['10', '42', '36', '4', '12']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="6" ssid = "2">However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'6'", "'78'", "'10'", "'36'"]
'11'
'6'
'78'
'10'
'36'
['11', '6', '78', '10', '36']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Prague Dependency Treebank (Hajiˇc et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S><S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="42" ssid = "13">Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'10'", "'36'", "'42'", "'78'"]
'12'
'10'
'36'
'42'
'78'
['12', '10', '36', '42', '78']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.</S><S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'2'", "'95'", "'1'", "'104'"]
'13'
'2'
'95'
'1'
'104'
['13', '2', '95', '1', '104']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 15–25%  the total proportion of non-projective arcs is normally only 1–2%.</S><S sid ="80" ssid = "7">As shown in Table 3  the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S><S sid ="100" ssid = "11">It is likely that the more complex cases  where path information could make a difference  are beyond the reach of the parser in most cases.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'80'", "'106'", "'81'", "'100'"]
'14'
'80'
'106'
'81'
'100'
['14', '80', '106', '81', '100']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">As long as the main evaluation metric is dependency accuracy per word  with state-of-the-art accuracy mostly below 90%  the penalty for not handling non-projective constructions is almost negligible.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="101" ssid = "12">However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.</S><S sid ="110" ssid = "2">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="105" ssid = "16">Although the best published results for the Collins parser is 80% UAS (Collins  1999)  this parser reaches 82% when trained on the entire training data set  and an adapted version of Charniak’s parser (Charniak  2000) performs at 84% (Jan Hajiˇc  pers. comm.).</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'106'", "'101'", "'110'", "'105'"]
'15'
'106'
'101'
'110'
'105'
['15', '106', '101', '110', '105']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="110" ssid = "2">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="19" ssid = "15">Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'7'", "'1'", "'110'", "'19'"]
'16'
'7'
'1'
'110'
'19'
['16', '7', '1', '110', '19']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'104'", "'106'", "'78'", "'8'"]
'4'
'104'
'106'
'78'
'8'
['4', '104', '106', '78', '8']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">In addition  there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington  1990; Kahane et al.  1998; Duchier and Debusmann  2001; Holan et al.  2001; Hellwig  2003).</S><S sid ="17" ssid = "13">There exist a few robust broad-coverage parsers that produce non-projective dependency structures  notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English  Foth et al. (2004) for German  and Holan (2004) for Czech.</S><S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S><S sid ="100" ssid = "11">It is likely that the more complex cases  where path information could make a difference  are beyond the reach of the parser in most cases.</S><S sid ="109" ssid = "1">We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'17'", "'20'", "'100'", "'109'"]
'18'
'17'
'20'
'100'
'109'
['18', '17', '20', '100', '109']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "15">Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="13" ssid = "9">The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.</S><S sid ="107" ssid = "18">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'7'", "'95'", "'13'", "'107'"]
'19'
'7'
'95'
'13'
'107'
['19', '7', '95', '13', '107']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S><S sid ="109" ssid = "1">We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'109'", "'24'", "'60'", "'95'"]
'20'
'109'
'24'
'60'
'95'
['20', '109', '24', '60', '95']
parsed_discourse_facet ['aim_citation']
['We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).']
['It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).', u'Prague Dependency Treebank (Haji\u02c7c et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.', 'Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', 'The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.']
['system', 'ROUGE-S*', 'Average_R:', '0.00179', '(95%-conf.int.', '0.00179', '-', '0.00179)']
['system', 'ROUGE-S*', 'Average_P:', '0.12727', '(95%-conf.int.', '0.12727', '-', '0.12727)']
['system', 'ROUGE-S*', 'Average_F:', '0.00353', '(95%-conf.int.', '0.00353', '-', '0.00353)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3916', 'P:55', 'F:7']
['By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.']
['This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).', 'It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).', 'We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).', u'There exist a few robust broad-coverage parsers that produce non-projective dependency structures  notably Tapanainen and J\xa8arvinen (1997) and Wang and Harper (2004) for English  Foth et al. (2004) for German  and Holan (2004) for Czech.', u'Although the best published results for the Collins parser is 80% UAS (Collins  1999)  this parser reaches 82% when trained on the entire training data set  and an adapted version of Charniak\u2019s parser (Charniak  2000) performs at 84% (Jan Haji\u02c7c  pers. comm.).']
['system', 'ROUGE-S*', 'Average_R:', '0.00385', '(95%-conf.int.', '0.00385', '-', '0.00385)']
['system', 'ROUGE-S*', 'Average_P:', '0.04234', '(95%-conf.int.', '0.04234', '-', '0.04234)']
['system', 'ROUGE-S*', 'Average_F:', '0.00705', '(95%-conf.int.', '0.00705', '-', '0.00705)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5460', 'P:496', 'F:21']
['As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.']
['Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.', 'However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).', 'This leads to the best reported performance for robust non-projective parsing of Czech.', 'The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.', 'The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.']
['system', 'ROUGE-S*', 'Average_R:', '0.00202', '(95%-conf.int.', '0.00202', '-', '0.00202)']
['system', 'ROUGE-S*', 'Average_P:', '0.03297', '(95%-conf.int.', '0.03297', '-', '0.03297)']
['system', 'ROUGE-S*', 'Average_F:', '0.00381', '(95%-conf.int.', '0.00381', '-', '0.00381)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:91', 'F:3']
['As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.']
['Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.', 'We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.', 'This leads to the best reported performance for robust non-projective parsing of Czech.', 'The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.', 'We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.01584', '(95%-conf.int.', '0.01584', '-', '0.01584)']
['system', 'ROUGE-S*', 'Average_P:', '0.05556', '(95%-conf.int.', '0.05556', '-', '0.05556)']
['system', 'ROUGE-S*', 'Average_F:', '0.02465', '(95%-conf.int.', '0.02465', '-', '0.02465)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:378', 'F:21']
['While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.']
['In addition  there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington  1990; Kahane et al.  1998; Duchier and Debusmann  2001; Holan et al.  2001; Hellwig  2003).', 'It is likely that the more complex cases  where path information could make a difference  are beyond the reach of the parser in most cases.', 'We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.', 'In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.', u'There exist a few robust broad-coverage parsers that produce non-projective dependency structures  notably Tapanainen and J\xc2\xa8arvinen (1997) and Wang and Harper (2004) for English  Foth et al. (2004) for German  and Holan (2004) for Czech.']
['system', 'ROUGE-S*', 'Average_R:', '0.00090', '(95%-conf.int.', '0.00090', '-', '0.00090)']
['system', 'ROUGE-S*', 'Average_P:', '0.03297', '(95%-conf.int.', '0.03297', '-', '0.03297)']
['system', 'ROUGE-S*', 'Average_F:', '0.00176', '(95%-conf.int.', '0.00176', '-', '0.00176)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3321', 'P:91', 'F:3']
['We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).']
['It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).', u'Prague Dependency Treebank (Haji\u02c7c et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.', 'Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', 'This leads to the best reported performance for robust non-projective parsing of Czech.']
['system', 'ROUGE-S*', 'Average_R:', '0.00247', '(95%-conf.int.', '0.00247', '-', '0.00247)']
['system', 'ROUGE-S*', 'Average_P:', '0.14545', '(95%-conf.int.', '0.14545', '-', '0.14545)']
['system', 'ROUGE-S*', 'Average_F:', '0.00486', '(95%-conf.int.', '0.00486', '-', '0.00486)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3240', 'P:55', 'F:8']
['In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
['We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).', 'In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.', 'We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.', 'In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.', 'The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.']
['system', 'ROUGE-S*', 'Average_R:', '0.03768', '(95%-conf.int.', '0.03768', '-', '0.03768)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.07263', '(95%-conf.int.', '0.07263', '-', '0.07263)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:91', 'F:91']
['In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
['This may seem surprising  given the experiments reported in section 4  but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift  where the encoding of path information is often redundant.', 'This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).', u'It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order  where discontinuous syntactic constructions are more common than in languages like English (Mel\u2019\u02c7cuk  1988; Covington  1990).', 'The first thing to note is that projectivizing helps in itself  even if no encoding is used  as seen from the fact that the projective baseline outperforms the non-projective training condition by more than half a percentage point on attachment score  although the gain is much smaller with respect to exact match.', 'In practice  we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00161', '(95%-conf.int.', '0.00161', '-', '0.00161)']
['system', 'ROUGE-S*', 'Average_P:', '0.10989', '(95%-conf.int.', '0.10989', '-', '0.10989)']
['system', 'ROUGE-S*', 'Average_F:', '0.00317', '(95%-conf.int.', '0.00317', '-', '0.00317)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6216', 'P:91', 'F:10']
['However, the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech, with a best performance of 73% UAS (Holan, 2004).']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\xcb\u2021c  1998).', 'Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy.', 'First  in section 4  we evaluate the graph transformation techniques in themselves  with data from the Prague Dependency Treebank and the Danish Dependency Treebank.', 'In section 5  we then evaluate the entire parsing system by training and evaluating on data from the Prague Dependency Treebank.', 'In the experiments below  we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs 3 previously tested on Swedish (Nivre et al.  2004) and English (Nivre and Scholz  2004).']
['system', 'ROUGE-S*', 'Average_R:', '0.00201', '(95%-conf.int.', '0.00201', '-', '0.00201)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00390', '(95%-conf.int.', '0.00390', '-', '0.00390)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:105', 'F:7']
['We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).']
['Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.', 'Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.', 'We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', 'We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00639', '(95%-conf.int.', '0.00639', '-', '0.00639)']
['system', 'ROUGE-S*', 'Average_P:', '0.27273', '(95%-conf.int.', '0.27273', '-', '0.27273)']
['system', 'ROUGE-S*', 'Average_F:', '0.01249', '(95%-conf.int.', '0.01249', '-', '0.01249)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:55', 'F:15']
['The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.']
['In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.', 'Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).', 'The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.', 'From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.', 'Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.']
['system', 'ROUGE-S*', 'Average_R:', '0.00602', '(95%-conf.int.', '0.00602', '-', '0.00602)']
['system', 'ROUGE-S*', 'Average_P:', '0.04184', '(95%-conf.int.', '0.04184', '-', '0.04184)']
['system', 'ROUGE-S*', 'Average_F:', '0.01052', '(95%-conf.int.', '0.01052', '-', '0.01052)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5151', 'P:741', 'F:31']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).', 'In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.', 'In practice  we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.', 'From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.', 'Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.']
['system', 'ROUGE-S*', 'Average_R:', '0.00614', '(95%-conf.int.', '0.00614', '-', '0.00614)']
['system', 'ROUGE-S*', 'Average_P:', '0.26667', '(95%-conf.int.', '0.26667', '-', '0.26667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01200', '(95%-conf.int.', '0.01200', '-', '0.01200)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4560', 'P:105', 'F:28']
['In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
['In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.', 'The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.', 'As shown in Table 3  the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.', 'From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.', 'Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.']
['system', 'ROUGE-S*', 'Average_R:', '0.00439', '(95%-conf.int.', '0.00439', '-', '0.00439)']
['system', 'ROUGE-S*', 'Average_P:', '0.10989', '(95%-conf.int.', '0.10989', '-', '0.10989)']
['system', 'ROUGE-S*', 'Average_F:', '0.00844', '(95%-conf.int.', '0.00844', '-', '0.00844)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:91', 'F:10']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.', 'However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).', 'This leads to the best reported performance for robust non-projective parsing of Czech.', 'The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.', 'The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.']
['system', 'ROUGE-S*', 'Average_R:', '0.01481', '(95%-conf.int.', '0.01481', '-', '0.01481)']
['system', 'ROUGE-S*', 'Average_P:', '0.20952', '(95%-conf.int.', '0.20952', '-', '0.20952)']
['system', 'ROUGE-S*', 'Average_F:', '0.02767', '(95%-conf.int.', '0.02767', '-', '0.02767)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:105', 'F:22']
['As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.']
['Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.', 'We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.', 'The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', 'However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.']
['system', 'ROUGE-S*', 'Average_R:', '0.12422', '(95%-conf.int.', '0.12422', '-', '0.12422)']
['system', 'ROUGE-S*', 'Average_P:', '0.79365', '(95%-conf.int.', '0.79365', '-', '0.79365)']
['system', 'ROUGE-S*', 'Average_F:', '0.21482', '(95%-conf.int.', '0.21482', '-', '0.21482)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:378', 'F:300']
['As shown in Table 3, the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.', 'However, the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.']
['This is in contrast to dependency treebanks  e.g.', 'It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).', 'The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', 'However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.']
['system', 'ROUGE-S*', 'Average_R:', '0.00241', '(95%-conf.int.', '0.00241', '-', '0.00241)']
['system', 'ROUGE-S*', 'Average_P:', '0.03509', '(95%-conf.int.', '0.03509', '-', '0.03509)']
['system', 'ROUGE-S*', 'Average_F:', '0.00452', '(95%-conf.int.', '0.00452', '-', '0.00452)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:171', 'F:6']
0.208906873694 0.0145343749092 0.0259887498376





input/ref/Task1/P08-1043_aakansha.csv
input/res/Task1/P08-1043.csv
parsing: input/ref/Task1/P08-1043_aakansha.csv
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="69" ssid="1">We represent all morphological analyses of a given utterance using a lattice structure.</S>
    <S sid="70" ssid="2">Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'70'"]
'69'
'70'
['69', '70']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="17">The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty, 2006) and (Cohen and Smith, 2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="11">Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
    <S sid="134" ssid="12">Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training 	similar to (Cohen and Smith, 2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'134'"]
'133'
'134'
['133', '134']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="17">The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="155" ssid="33">Evaluation We use 8 different measures to evaluate the performance of our system on the joint disambiguation task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S sid ="1" ssid = "1">Morphological processes in Semitic languages deliver space-delimited words which introduce multiple  distinct  syntactic units into the structure of the input sentence.</S><S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="39" ssid = "18">Such ambiguities cause discrepancies between token boundaries (indexed as white spaces) and constituent boundaries (imposed by syntactic categories) with respect to a surface form.</S><S sid ="34" ssid = "13">Lexical and Morphological Ambiguity The rich morphological processes for deriving Hebrew stems give rise to a high degree of ambiguity for Hebrew space-delimited tokens.</S><S sid ="24" ssid = "3">Co-occurrences among the particles themselves are subject to further syntactic and lexical constraints relative to the stem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'14'", "'39'", "'34'", "'24'"]
'1'
'14'
'39'
'34'
'24'
['1', '14', '39', '34', '24']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.</S><S sid ="16" ssid = "12">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S><S sid ="94" ssid = "26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S><S sid ="20" ssid = "16">We claim that no particular morphological segmentation is a-priory more likely for surface forms before exploring the compositional nature of syntactic structures  including manifestations of various long-distance dependencies.</S><S sid ="191" ssid = "5">In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'16'", "'94'", "'20'", "'191'"]
'2'
'16'
'94'
'20'
'191'
['2', '16', '94', '20', '191']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S><S sid ="19" ssid = "15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S sid ="187" ssid = "1">Employing a PCFG-based generative framework to make both syntactic and morphological disambiguation decisions is not only theoretically clean and linguistically justified and but also probabilistically apropriate and empirically sound.</S><S sid ="189" ssid = "3">Better grammars are shown here to improve performance on both morphological and syntactic tasks  providing support for the advantage of a joint framework over pipelined or factorized ones.</S><S sid ="21" ssid = "17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'19'", "'187'", "'189'", "'21'"]
'3'
'19'
'187'
'189'
'21'
['3', '19', '187', '189', '21']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Using a treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined  integrated or factorized systems for Hebrew morphological and syntactic processing  yielding an error reduction of 12% over the best published results so far.</S><S sid ="21" ssid = "17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S><S sid ="185" ssid = "23">In contrast  our morphological probabilities are based on a unigram  lexeme-based model  and all other (local and non-local) contextual considerations are delegated to the PCFG.</S><S sid ="188" ssid = "2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S><S sid ="191" ssid = "5">In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'21'", "'185'", "'188'", "'191'"]
'4'
'21'
'185'
'188'
'191'
['4', '21', '185', '188', '191']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  ‘tokens’) that constitute the unanalyzed surface forms (utterances).</S><S sid ="12" ssid = "8">This leads to word- and constituent-boundaries discrepancy  which breaks the assumptions underlying current state-of-the-art statistical parsers.</S><S sid ="94" ssid = "26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S><S sid ="21" ssid = "17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S><S sid ="16" ssid = "12">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'12'", "'94'", "'21'", "'16'"]
'5'
'12'
'94'
'21'
'16'
['5', '12', '94', '21', '16']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">In Semitic languages the situation is very different.</S><S sid ="78" ssid = "10">A similar structure is used in speech recognition.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="99" ssid = "31">This is done using a simple PCFG which is lexemebased.</S><S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'78'", "'92'", "'99'", "'180'"]
'6'
'78'
'92'
'99'
'180'
['6', '78', '92', '99', '180']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).</S><S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="17" ssid = "13">Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task.</S><S sid ="8" ssid = "4">The Hebrew token ‘bcl’1  for example  stands for the complete prepositional phrase 'We adopt here the transliteration of (Sima’an et al.  2001).</S><S sid ="29" ssid = "8">A less canonical representation of segmental morphology is triggered by a morpho-phonological process of omitting the definite article h when occurring after the particles b or l. This process triggers ambiguity as for the definiteness status of Nouns following these particles.We refer to such cases in which the concatenation of elements does not strictly correspond to the original surface form as super-segmental morphology.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'14'", "'17'", "'8'", "'29'"]
'7'
'14'
'17'
'8'
'29'
['7', '14', '17', '8', '29']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">The Hebrew token ‘bcl’1  for example  stands for the complete prepositional phrase 'We adopt here the transliteration of (Sima’an et al.  2001).</S><S sid ="35" ssid = "14">The form fmnh  for example  can be understood as the verb “lubricated”  the possessed noun “her oil”  the adjective “fat” or the verb “got fat”.</S><S sid ="62" ssid = "9">When a token fmnh is to be interpreted as the lexeme sequence f/REL mnh/VB  the analysis introduces two distinct entities  the relativizer f (“that”) and the verb mnh (“counted”)  and not as the complex entity “that counted”.</S><S sid ="7" ssid = "3">In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).</S><S sid ="15" ssid = "11">The aforementioned surface form bcl  for example  may also stand for the lexical item “onion”  a Noun.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'35'", "'62'", "'7'", "'15'"]
'8'
'35'
'62'
'7'
'15'
['8', '35', '62', '7', '15']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">“in the shadow”.</S><S sid ="184" ssid = "22">Oracle results).</S><S sid ="109" ssid = "41">The latter arcs correspond to OOV words in English.</S><S sid ="113" ssid = "45">The remaining arcs are marked OOV.</S><S sid ="78" ssid = "10">A similar structure is used in speech recognition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'184'", "'109'", "'113'", "'78'"]
'9'
'184'
'109'
'113'
'78'
['9', '184', '109', '113', '78']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">This token may further embed into a larger utterance  e.g.  ‘bcl hneim’ (literally “in-the-shadow the-pleasant”  meaning roughly “in the pleasant shadow”) in which the dominated Noun is modified by a proceeding space-delimited adjective.</S><S sid ="15" ssid = "11">The aforementioned surface form bcl  for example  may also stand for the lexical item “onion”  a Noun.</S><S sid ="76" ssid = "8">Furthermore  some of the arcs represent lexemes not present in the input tokens (e.g. h/DT  fl/POS)  however these are parts of valid analyses of the token (cf. super-segmental morphology section 2).</S><S sid ="58" ssid = "5">Such tag sequences are often treated as “complex tags” (e.g.</S><S sid ="108" ssid = "40">Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p —* (s  p)) > 0  while other segments have never been observed as a lexical event before.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'15'", "'76'", "'58'", "'108'"]
'10'
'15'
'76'
'58'
'108'
['10', '15', '76', '58', '108']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">It should be clear from the onset that the particle b (“in”) in ‘bcl’ may then attach higher than the bare noun cl (“shadow”).</S><S sid ="37" ssid = "16">The same form fmnh can be segmented as f-mnh  f (“that”) functioning as a reletivizer with the form mnh.</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="38" ssid = "17">The form mnh itself can be read as at least three different verbs (“counted”  “appointed”  “was appointed”)  a noun (“a portion”)  and a possessed noun (“her kind”).</S><S sid ="26" ssid = "5">The relativizer f(“that”) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'37'", "'57'", "'38'", "'26'"]
'11'
'37'
'57'
'38'
'26'
['11', '37', '57', '38', '26']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">This leads to word- and constituent-boundaries discrepancy  which breaks the assumptions underlying current state-of-the-art statistical parsers.</S><S sid ="5" ssid = "1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  ‘tokens’) that constitute the unanalyzed surface forms (utterances).</S><S sid ="13" ssid = "9">One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).</S><S sid ="2" ssid = "2">These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.</S><S sid ="19" ssid = "15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'5'", "'13'", "'2'", "'19'"]
'12'
'5'
'13'
'2'
'19'
['12', '5', '13', '2', '19']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).</S><S sid ="33" ssid = "12">The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).</S><S sid ="140" ssid = "18">For these models we limit the options provided for OOV words by not considering the entire token as a valid segmentation in case at least some prefix segmentation exists.</S><S sid ="139" ssid = "17">To control for the effect of the HSPELL-based pruning  we also experimented with a morphological analyzer that does not perform this pruning.</S><S sid ="3" ssid = "3">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'33'", "'140'", "'139'", "'3'"]
'13'
'33'
'140'
'139'
'3'
['13', '33', '140', '139', '3']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="7" ssid = "3">In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).</S><S sid ="140" ssid = "18">For these models we limit the options provided for OOV words by not considering the entire token as a valid segmentation in case at least some prefix segmentation exists.</S><S sid ="17" ssid = "13">Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task.</S><S sid ="121" ssid = "53">In such cases we use the non-pruned lattice including all (possibly ungrammatical) segmentation  and let the statistics (including OOV) decide.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'7'", "'140'", "'17'", "'121'"]
'14'
'7'
'140'
'17'
'121'
['14', '7', '140', '17', '121']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">The aforementioned surface form bcl  for example  may also stand for the lexical item “onion”  a Noun.</S><S sid ="32" ssid = "11">The additional morphological material in such cases appears after the stem and realizes the extended meaning.</S><S sid ="28" ssid = "7">The same argument holds for resolving PP attachment of a prefixed preposition or marking conjunction of elements of any kind.</S><S sid ="10" ssid = "6">This token may further embed into a larger utterance  e.g.  ‘bcl hneim’ (literally “in-the-shadow the-pleasant”  meaning roughly “in the pleasant shadow”) in which the dominated Noun is modified by a proceeding space-delimited adjective.</S><S sid ="30" ssid = "9">An additional case of super-segmental morphology is the case of Pronominal Clitics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'32'", "'28'", "'10'", "'30'"]
'15'
'32'
'28'
'10'
'30'
['15', '32', '28', '10', '30']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S><S sid ="94" ssid = "26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S><S sid ="188" ssid = "2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S><S sid ="13" ssid = "9">One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).</S><S sid ="2" ssid = "2">These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'94'", "'188'", "'13'", "'2'"]
'16'
'94'
'188'
'13'
'2'
['16', '94', '188', '13', '2']
parsed_discourse_facet ['method_citation']
['The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.']
[u"The Hebrew token \u2018bcl\u20191  for example  stands for the complete prepositional phrase 'We adopt here the transliteration of (Sima\u2019an et al.  2001).", u'When a token fmnh is to be interpreted as the lexeme sequence f/REL mnh/VB  the analysis introduces two distinct entities  the relativizer f (\u201cthat\u201d) and the verb mnh (\u201ccounted\u201d)  and not as the complex entity \u201cthat counted\u201d.', u'The aforementioned surface form bcl  for example  may also stand for the lexical item \u201conion\u201d  a Noun.', 'In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).', u'The form fmnh  for example  can be understood as the verb \u201clubricated\u201d  the possessed noun \u201cher oil\u201d  the adjective \u201cfat\u201d or the verb \u201cgot fat\u201d.']
['system', 'ROUGE-S*', 'Average_R:', '0.00078', '(95%-conf.int.', '0.00078', '-', '0.00078)']
['system', 'ROUGE-S*', 'Average_P:', '0.02564', '(95%-conf.int.', '0.02564', '-', '0.02564)']
['system', 'ROUGE-S*', 'Average_F:', '0.00152', '(95%-conf.int.', '0.00152', '-', '0.00152)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:78', 'F:2']
['Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.']
['Morphological processes in Semitic languages deliver space-delimited words which introduce multiple  distinct  syntactic units into the structure of the input sentence.', 'Co-occurrences among the particles themselves are subject to further syntactic and lexical constraints relative to the stem.', 'Such ambiguities cause discrepancies between token boundaries (indexed as white spaces) and constituent boundaries (imposed by syntactic categories) with respect to a surface form.', 'The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).', 'Lexical and Morphological Ambiguity The rich morphological processes for deriving Hebrew stems give rise to a high degree of ambiguity for Hebrew space-delimited tokens.']
['system', 'ROUGE-S*', 'Average_R:', '0.00316', '(95%-conf.int.', '0.00316', '-', '0.00316)']
['system', 'ROUGE-S*', 'Average_P:', '0.05882', '(95%-conf.int.', '0.05882', '-', '0.05882)']
['system', 'ROUGE-S*', 'Average_F:', '0.00599', '(95%-conf.int.', '0.00599', '-', '0.00599)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:153', 'F:9']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
[u'This token may further embed into a larger utterance  e.g.  \u2018bcl hneim\u2019 (literally \u201cin-the-shadow the-pleasant\u201d  meaning roughly \u201cin the pleasant shadow\u201d) in which the dominated Noun is modified by a proceeding space-delimited adjective.', u'Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p \u2014* (s  p)) > 0  while other segments have never been observed as a lexical event before.', u'The aforementioned surface form bcl  for example  may also stand for the lexical item \u201conion\u201d  a Noun.', u'Such tag sequences are often treated as \u201ccomplex tags\u201d (e.g.', 'Furthermore  some of the arcs represent lexemes not present in the input tokens (e.g. h/DT  fl/POS)  however these are parts of valid analyses of the token (cf. super-segmental morphology section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00090', '(95%-conf.int.', '0.00090', '-', '0.00090)']
['system', 'ROUGE-S*', 'Average_P:', '0.01667', '(95%-conf.int.', '0.01667', '-', '0.01667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00172', '(95%-conf.int.', '0.00172', '-', '0.00172)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:120', 'F:2']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.', 'In contrast  our morphological probabilities are based on a unigram  lexeme-based model  and all other (local and non-local) contextual considerations are delegated to the PCFG.', 'In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.', 'Using a treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined  integrated or factorized systems for Hebrew morphological and syntactic processing  yielding an error reduction of 12% over the best published results so far.', 'Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2']
['system', 'ROUGE-S*', 'Average_R:', '0.00223', '(95%-conf.int.', '0.00223', '-', '0.00223)']
['system', 'ROUGE-S*', 'Average_P:', '0.14167', '(95%-conf.int.', '0.14167', '-', '0.14167)']
['system', 'ROUGE-S*', 'Average_F:', '0.00439', '(95%-conf.int.', '0.00439', '-', '0.00439)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:7626', 'P:120', 'F:17']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.', 'Better grammars are shown here to improve performance on both morphological and syntactic tasks  providing support for the advantage of a joint framework over pipelined or factorized ones.', 'Employing a PCFG-based generative framework to make both syntactic and morphological disambiguation decisions is not only theoretically clean and linguistically justified and but also probabilistically apropriate and empirically sound.', 'Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2']
['system', 'ROUGE-S*', 'Average_R:', '0.02525', '(95%-conf.int.', '0.02525', '-', '0.02525)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.04925', '(95%-conf.int.', '0.04925', '-', '0.04925)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4753', 'P:120', 'F:120']
['Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.', 'These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.', 'We claim that no particular morphological segmentation is a-priory more likely for surface forms before exploring the compositional nature of syntactic structures  including manifestations of various long-distance dependencies.', 'Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.', 'The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.']
['system', 'ROUGE-S*', 'Average_R:', '0.00430', '(95%-conf.int.', '0.00430', '-', '0.00430)']
['system', 'ROUGE-S*', 'Average_P:', '0.02910', '(95%-conf.int.', '0.02910', '-', '0.02910)']
['system', 'ROUGE-S*', 'Average_F:', '0.00750', '(95%-conf.int.', '0.00750', '-', '0.00750)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:378', 'F:11']
['We represent all morphological analyses of a given utterance using a lattice structure.', 'Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.']
[u"The Hebrew token \u2018bcl\u20191  for example  stands for the complete prepositional phrase 'We adopt here the transliteration of (Sima\u2019an et al.  2001).", 'Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task.', 'The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).', 'In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).', 'A less canonical representation of segmental morphology is triggered by a morpho-phonological process of omitting the definite article h when occurring after the particles b or l. This process triggers ambiguity as for the definiteness status of Nouns following these particles.We refer to such cases in which the concatenation of elements does not strictly correspond to the original surface form as super-segmental morphology.']
['system', 'ROUGE-S*', 'Average_R:', '0.00370', '(95%-conf.int.', '0.00370', '-', '0.00370)']
['system', 'ROUGE-S*', 'Average_P:', '0.12281', '(95%-conf.int.', '0.12281', '-', '0.12281)']
['system', 'ROUGE-S*', 'Average_F:', '0.00719', '(95%-conf.int.', '0.00719', '-', '0.00719)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5671', 'P:171', 'F:21']
['Evaluation We use 8 different measures to evaluate the performance of our system on the joint disambiguation task.']
['The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.', 'These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.', 'One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).', 'Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.', 'The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.']
['system', 'ROUGE-S*', 'Average_R:', '0.00079', '(95%-conf.int.', '0.00079', '-', '0.00079)']
['system', 'ROUGE-S*', 'Average_P:', '0.11111', '(95%-conf.int.', '0.11111', '-', '0.11111)']
['system', 'ROUGE-S*', 'Average_F:', '0.00157', '(95%-conf.int.', '0.00157', '-', '0.00157)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5050', 'P:36', 'F:4']
['Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.', 'This leads to word- and constituent-boundaries discrepancy  which breaks the assumptions underlying current state-of-the-art statistical parsers.', u'Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  \u2018tokens\u2019) that constitute the unanalyzed surface forms (utterances).', 'Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.', 'Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2']
['system', 'ROUGE-S*', 'Average_R:', '0.01335', '(95%-conf.int.', '0.01335', '-', '0.01335)']
['system', 'ROUGE-S*', 'Average_P:', '0.22751', '(95%-conf.int.', '0.22751', '-', '0.22751)']
['system', 'ROUGE-S*', 'Average_F:', '0.02522', '(95%-conf.int.', '0.02522', '-', '0.02522)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6441', 'P:378', 'F:86']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
[u'\u201cin the shadow\u201d.', 'The latter arcs correspond to OOV words in English.', 'The remaining arcs are marked OOV.', 'Oracle results).', 'A similar structure is used in speech recognition.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:120', 'P:120', 'F:0']
0.173332998267 0.00544599994554 0.0104349998957





input/ref/Task1/J01-2004_aakansha.csv
input/res/Task1/J01-2004.csv
parsing: input/ref/Task1/J01-2004_aakansha.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'"]
'372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="17" ssid="5">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



["'302'"]
'302'
['302']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="37">This underspecification of the nonterminal predictions (e.g., VP-VBD in the example in Figure 2, as opposed to NP), allows lexical items to become part of the left context, and so be used to condition production probabilities, even the production probabilities of constituents that dominate them in the unfactored tree.</S>
    <S sid="80" ssid="38">It also brings words further downstream into the look-ahead at the point of specification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'", "'80'"]
'79'
'80'
['79', '80']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="42">Our approach is found to yield very accurate parses efficiently, and, in addition, to lend itself straightforwardly to estimating word probabilities on-line, that is, in a single pass from left to right.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="291" ssid="47">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'291'"]
'291'
['291']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['method_citation']
<S sid="209" ssid="113">This parser is essentially a stochastic version of the top-down parser described in Aho, Sethi, and Ullman (1986).</S>
    <S sid="210" ssid="114">It uses a PCFG with a conditional probability model of the sort defined in the previous section.</S>
original cit marker offset is 0
new cit marker offset is 0



["'209'", "'210'"]
'209'
'210'
['209', '210']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'"]
'372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="8">Two features of our top-down parsing approach will emerge as key to its success.</S>
    <S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S><S sid="32" ssid="20">A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'21'", "'32'"]
'20'
'21'
'32'
['20', '21', '32']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="21">Because the rooted partial derivation is fully connected, all of the conditioning information that might be extracted from the top-down left context has already been specified, and a conditional probability model built on this information will not impose any additional burden on the search.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'"]
'33'
['33']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S sid ="1" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="7" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="209" ssid = "113">This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).</S><S sid ="132" ssid = "36">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid ="277" ssid = "33">The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989 860 words  39 832 sentences) of the Penn Treebank (Marcus  Santorini  and Marcinkiewicz 1993) served as the training data  Section 24 (34 199 words  1 346 sentences) as the held-out data for parameter estimation  and Section 23 (59 100 words  2 416 sentences) as the test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'7'", "'209'", "'132'", "'277'"]
'1'
'7'
'209'
'132'
'277'
['1', '7', '209', '132', '277']
parsed_discourse_facet ['aim_citation']
<S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="41" ssid = "29">There will also be a brief review of previous work using syntactic information for language modeling  before we introduce our model in Section 4.</S><S sid ="18" ssid = "6">Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture long-distance dependencies for language modeling will be reviewed.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'2'", "'41'", "'18'", "'4'"]
'8'
'2'
'41'
'18'
'4'
['8', '2', '41', '18', '4']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="9" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'390'", "'10'", "'4'"]
'3'
'9'
'390'
'10'
'4'
['3', '9', '390', '10', '4']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="100" ssid = "4">The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'100'", "'2'", "'8'"]
'10'
'4'
'100'
'2'
'8'
['10', '4', '100', '2', '8']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="11" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="379" ssid = "135">While the improvements over the trigram model in these trials are modest  they do indicate that our model is robust enough to provide good information even in the face of noisy input.</S><S sid ="300" ssid = "56">The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'11'", "'379'", "'300'", "'404'"]
'5'
'11'
'379'
'300'
'404'
['5', '11', '379', '300', '404']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="6" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="39" ssid = "27">The top-down guidance that is provided makes this approach quite efficient in practice.</S><S sid ="388" ssid = "1">The empirical results presented above are quite encouraging  and the potential of this kind of approach both for parsing and language modeling seems very promising.</S><S sid ="114" ssid = "18">He was able to reduce both sentence and word error rates on the ATIS corpus using this method.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'6'", "'39'", "'388'", "'114'"]
'12'
'6'
'39'
'388'
'114'
['12', '6', '39', '388', '114']
parsed_discourse_facet ['method_citation']
<S sid ="1" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="7" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="209" ssid = "113">This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).</S><S sid ="132" ssid = "36">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid ="277" ssid = "33">The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989 860 words  39 832 sentences) of the Penn Treebank (Marcus  Santorini  and Marcinkiewicz 1993) served as the training data  Section 24 (34 199 words  1 346 sentences) as the held-out data for parameter estimation  and Section 23 (59 100 words  2 416 sentences) as the test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'7'", "'209'", "'132'", "'277'"]
'1'
'7'
'209'
'132'
'277'
['1', '7', '209', '132', '277']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="41" ssid = "29">There will also be a brief review of previous work using syntactic information for language modeling  before we introduce our model in Section 4.</S><S sid ="18" ssid = "6">Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture long-distance dependencies for language modeling will be reviewed.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'2'", "'41'", "'18'", "'4'"]
'8'
'2'
'41'
'18'
'4'
['8', '2', '41', '18', '4']
parsed_discourse_facet ['results_citation']
<S sid ="3" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="9" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'390'", "'10'", "'4'"]
'3'
'9'
'390'
'10'
'4'
['3', '9', '390', '10', '4']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="100" ssid = "4">The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'100'", "'2'", "'8'"]
'10'
'4'
'100'
'2'
'8'
['10', '4', '100', '2', '8']
parsed_discourse_facet ['method_citation']
<S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="11" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="379" ssid = "135">While the improvements over the trigram model in these trials are modest  they do indicate that our model is robust enough to provide good information even in the face of noisy input.</S><S sid ="300" ssid = "56">The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'11'", "'379'", "'300'", "'404'"]
'5'
'11'
'379'
'300'
'404'
['5', '11', '379', '300', '404']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="6" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="39" ssid = "27">The top-down guidance that is provided makes this approach quite efficient in practice.</S><S sid ="388" ssid = "1">The empirical results presented above are quite encouraging  and the potential of this kind of approach both for parsing and language modeling seems very promising.</S><S sid ="114" ssid = "18">He was able to reduce both sentence and word error rates on the ATIS corpus using this method.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'6'", "'39'", "'388'", "'114'"]
'12'
'6'
'39'
'388'
'114'
['12', '6', '39', '388', '114']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "1">With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.</S><S sid ="14" ssid = "2">Perhaps one reason for this is that  until relatively recently  few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.</S><S sid ="15" ssid = "3">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S><S sid ="383" ssid = "139">It was selected with the goal of high parser accuracy; but in this new domain  parser accuracy is a secondary measure of performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'14'", "'15'", "'404'", "'383'"]
'13'
'14'
'15'
'404'
'383'
['13', '14', '15', '404', '383']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "2">Perhaps one reason for this is that  until relatively recently  few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.</S><S sid ="13" ssid = "1">With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.</S><S sid ="300" ssid = "56">The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.</S><S sid ="15" ssid = "3">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid ="319" ssid = "75">The hope  however  is that the improved parsing model provided by our conditional probability model will cause the distribution over structures to be more peaked  thus enabling us to capture more of the total probability mass  and making this a fairly snug upper bound on the perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'13'", "'300'", "'15'", "'319'"]
'14'
'13'
'300'
'15'
'319'
['14', '13', '300', '15', '319']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "3">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid ="97" ssid = "1">There have been attempts to jump over adjacent words to words farther back in the left context  without the use of dependency links or syntactic structure  for example Saul and Pereira (1997) and Rosenfeld (1996  1997).</S><S sid ="13" ssid = "1">With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.</S><S sid ="38" ssid = "26">This has lead us to a formulation of the conditional probability model in terms of values returned from tree-walking functions that themselves are contextually sensitive.</S><S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'97'", "'13'", "'38'", "'5'"]
'15'
'97'
'13'
'38'
'5'
['15', '97', '13', '38', '5']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "4">While language models built around shallow local dependencies are still the standard in state-of-the-art speech recognition systems  there is reason to hope that better language models can and will be developed by computational linguists for this task.</S><S sid ="397" ssid = "10">A top-down parser  in contrast to a standard bottom-up chart parser  has enough information to predict empty categories only where they are likely to occur.</S><S sid ="139" ssid = "43">This on-line characteristic allows our language model to be interpolated on a word-by-word basis with other models  such as the trigram  yielding further improvements.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'397'", "'139'", "'4'", "'10'"]
'16'
'397'
'139'
'4'
'10'
['16', '397', '139', '4', '10']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "5">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S sid ="132" ssid = "36">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid ="88" ssid = "46">This section will briefly introduce language modeling for statistical speech recognition.'</S><S sid ="40" ssid = "28">The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.</S><S sid ="247" ssid = "3">Perplexity is a standard measure within the speech recognition community for comparing language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'132'", "'88'", "'40'", "'247'"]
'17'
'132'
'88'
'40'
'247'
['17', '132', '88', '40', '247']
parsed_discourse_facet ['method_citation']
['Because the rooted partial derivation is fully connected, all of the conditioning information that might be extracted from the top-down left context has already been specified, and a conditional probability model built on this information will not impose any additional burden on the search.']
['Perplexity is a standard measure within the speech recognition community for comparing language models.', 'The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.', "This section will briefly introduce language modeling for statistical speech recognition.'", 'The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.', 'This paper will examine language modeling for speech recognition from a natural language processing point of view.']
['system', 'ROUGE-S*', 'Average_R:', '0.00078', '(95%-conf.int.', '0.00078', '-', '0.00078)']
['system', 'ROUGE-S*', 'Average_P:', '0.00526', '(95%-conf.int.', '0.00526', '-', '0.00526)']
['system', 'ROUGE-S*', 'Average_F:', '0.00137', '(95%-conf.int.', '0.00137', '-', '0.00137)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:190', 'F:1']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.', 'Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.', 'In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.', 'Perhaps one reason for this is that  until relatively recently  few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.', 'It was selected with the goal of high parser accuracy; but in this new domain  parser accuracy is a secondary measure of performance.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:91', 'F:0']
['It also brings words further downstream into the look-ahead at the point of specification.', 'This underspecification of the nonterminal predictions (e.g., VP-VBD in the example in Figure 2, as opposed to NP), allows lexical items to become part of the left context, and so be used to condition production probabilities, even the production probabilities of constituents that dominate them in the unfactored tree.']
['This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.', 'This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).', 'The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989 860 words  39 832 sentences) of the Penn Treebank (Marcus  Santorini  and Marcinkiewicz 1993) served as the training data  Section 24 (34 199 words  1 346 sentences) as the held-out data for parameter estimation  and Section 23 (59 100 words  2 416 sentences) as the test data.', 'This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.', 'The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.']
['system', 'ROUGE-S*', 'Average_R:', '0.00020', '(95%-conf.int.', '0.00020', '-', '0.00020)']
['system', 'ROUGE-S*', 'Average_P:', '0.00246', '(95%-conf.int.', '0.00246', '-', '0.00246)']
['system', 'ROUGE-S*', 'Average_F:', '0.00037', '(95%-conf.int.', '0.00037', '-', '0.00037)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4950', 'P:406', 'F:1']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.', 'This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).', 'The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989 860 words  39 832 sentences) of the Penn Treebank (Marcus  Santorini  and Marcinkiewicz 1993) served as the training data  Section 24 (34 199 words  1 346 sentences) as the held-out data for parameter estimation  and Section 23 (59 100 words  2 416 sentences) as the test data.', 'This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.', 'The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.']
['system', 'ROUGE-S*', 'Average_R:', '0.00061', '(95%-conf.int.', '0.00061', '-', '0.00061)']
['system', 'ROUGE-S*', 'Average_P:', '0.03297', '(95%-conf.int.', '0.03297', '-', '0.03297)']
['system', 'ROUGE-S*', 'Average_F:', '0.00119', '(95%-conf.int.', '0.00119', '-', '0.00119)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4950', 'P:91', 'F:3']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.', 'While the improvements over the trigram model in these trials are modest  they do indicate that our model is robust enough to provide good information even in the face of noisy input.', 'The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.', 'Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.', 'Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00032', '(95%-conf.int.', '0.00032', '-', '0.00032)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00063', '(95%-conf.int.', '0.00063', '-', '0.00063)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:91', 'F:1']
['Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.']
['A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.', 'The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.', 'The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.', 'A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.', 'The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.']
['system', 'ROUGE-S*', 'Average_R:', '0.00327', '(95%-conf.int.', '0.00327', '-', '0.00327)']
['system', 'ROUGE-S*', 'Average_P:', '0.03218', '(95%-conf.int.', '0.03218', '-', '0.03218)']
['system', 'ROUGE-S*', 'Average_F:', '0.00594', '(95%-conf.int.', '0.00594', '-', '0.00594)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4278', 'P:435', 'F:14']
['This parser is essentially a stochastic version of the top-down parser described in Aho, Sethi, and Ullman (1986).', 'It uses a PCFG with a conditional probability model of the sort defined in the previous section.']
['The empirical results presented above are quite encouraging  and the potential of this kind of approach both for parsing and language modeling seems very promising.', 'He was able to reduce both sentence and word error rates on the ATIS corpus using this method.', 'The top-down guidance that is provided makes this approach quite efficient in practice.', 'A small recognition experiment also demonstrates the utility of the model.', 'A small recognition experiment also demonstrates the utility of the model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00142', '(95%-conf.int.', '0.00142', '-', '0.00142)']
['system', 'ROUGE-S*', 'Average_P:', '0.00654', '(95%-conf.int.', '0.00654', '-', '0.00654)']
['system', 'ROUGE-S*', 'Average_F:', '0.00234', '(95%-conf.int.', '0.00234', '-', '0.00234)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:153', 'F:1']
['This paper will examine language modeling for speech recognition from a natural language processing point of view.']
['The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.', 'The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.', 'A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.', 'Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture long-distance dependencies for language modeling will be reviewed.', 'There will also be a brief review of previous work using syntactic information for language modeling  before we introduce our model in Section 4.']
['system', 'ROUGE-S*', 'Average_R:', '0.00216', '(95%-conf.int.', '0.00216', '-', '0.00216)']
['system', 'ROUGE-S*', 'Average_P:', '0.10909', '(95%-conf.int.', '0.10909', '-', '0.10909)']
['system', 'ROUGE-S*', 'Average_F:', '0.00424', '(95%-conf.int.', '0.00424', '-', '0.00424)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:55', 'F:6']
0.0249362496883 0.00109499998631 0.00200999997488





input/ref/Task1/P08-1102_swastika.csv
input/res/Task1/P08-1102.csv
parsing: input/ref/Task1/P08-1102_swastika.csv
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="10">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="8">All feature templates and their instances are shown in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



['36']
36
['36']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
<S sid="97" ssid="8">Figure 3 shows their learning curves depicting the F-measure on the development set after 1 to 10 training iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



['97']
97
['97']
parsed_discourse_facet ['result_citation']
<S sid="91" ssid="2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['aim_citation']
<S sid="16" ssid="12">Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



['34']
34
['34']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="130" ssid = "1">We proposed a cascaded linear model for Chinese Joint S&T.</S><S sid ="76" ssid = "1">Sequence segmentation and labelling problem can be solved through a viterbi style decoding procedure.</S><S sid ="24" ssid = "20">It is a better idea to perform segmentation and POS tagging jointly in a uniform framework.</S><S sid ="4" ssid = "4">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'130'", "'76'", "'24'", "'4'"]
'1'
'130'
'76'
'24'
'4'
['1', '130', '76', '24', '4']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="64" ssid = "15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'16'", "'133'", "'64'", "'134'"]
'2'
'16'
'133'
'64'
'134'
['2', '16', '133', '64', '134']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid ="21" ssid = "17">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S sid ="106" ssid = "17">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.</S><S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="4" ssid = "4">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'21'", "'106'", "'1'", "'4'"]
'3'
'21'
'106'
'1'
'4'
['3', '21', '106', '1', '4']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S><S sid ="127" ssid = "38">Finally  the word count penalty gives improvement to the cascaded model  0.13 points on segmentation and 0.16 points on Joint S&T.</S><S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="123" ssid = "34">Without it  the F-measure on segmentation and Joint S&T both suffer a decrement of 0.2 points.</S><S sid ="118" ssid = "29">We find that the cascaded model achieves a F-measure increment of about 0.5 points on segmentation and about 0.9 points on Joint S&T  over the perceptron-only model POS+.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'127'", "'1'", "'123'", "'118'"]
'4'
'127'
'1'
'123'
'118'
['4', '127', '1', '123', '118']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages.</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="1" ssid = "1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid ="51" ssid = "2">Additional features most widely used are related to word or POS ngrams.</S><S sid ="114" ssid = "25">We used SRI Language Modelling Toolkit (Stolcke and Andreas  2002) to train a 3gram word LM with modified Kneser-Ney smoothing (Chen and Goodman  1998)  and a 4-gram POS LM with Witten-Bell smoothing  and we trained a word-POS co-occurrence model simply by MLE without smoothing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'31'", "'1'", "'51'", "'114'"]
'5'
'31'
'1'
'51'
'114'
['5', '31', '1', '51', '114']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Several models were introduced for these problems  for example  the Hidden Markov Model (HMM) (Rabiner  1989)  Maximum Entropy Model (ME) (Ratnaparkhi and Adwait  1996)  and Conditional Random Fields (CRFs) (Lafferty et al.  2001).</S><S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="15" ssid = "11">To cope with this problem  we propose a cascaded linear model inspired by the log-linear model (Och and Ney  2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'8'", "'134'", "'15'", "'45'"]
'6'
'8'
'134'
'15'
'45'
['6', '8', '134', '15', '45']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S><S sid ="100" ssid = "11">Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS  CityU and MSR).</S><S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'8'", "'100'", "'14'", "'31'"]
'7'
'8'
'100'
'14'
'31'
['7', '8', '100', '14', '31']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="29" ssid = "1">The perceptron algorithm introduced into NLP by Collins (2002)  is a simple but effective discriminative training method.</S><S sid ="115" ssid = "26">To obtain their corresponding weights  we adapted the minimum-error-rate training algorithm (Och  2003) to train the outside-layer model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'45'", "'49'", "'29'", "'115'"]
'8'
'45'
'49'
'29'
'115'
['8', '45', '49', '29', '115']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid ="23" ssid = "19">We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag.</S><S sid ="11" ssid = "7">Compared to performing segmentation and POS tagging one at a time  Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low  2004).</S><S sid ="80" ssid = "5">At each position i  we enumerate all possible word-POS pairs by assigning each POS to each possible word formed from the character subsequence spanning length l = L. min(i  K) (K is assigned 20 in all our experiments) and ending at position i  then we derive all candidate results by attaching each word-POS pair p (of length l) to the tail of each candidate result at the prior position of p (position i−l)  and select for position i a N-best list of candidate results from all these candidates.</S><S sid ="81" ssid = "6">When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p  we calculate the scores of the word LM  the POS LM  the labelling probability and the generating probability  Algorithm 2 Decoding algorithm. as well as the score of the perceptron model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'23'", "'11'", "'80'", "'81'"]
'9'
'23'
'11'
'80'
'81'
['9', '23', '11', '80', '81']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen  2003)  Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low  2004).</S><S sid ="25" ssid = "21">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S><S sid ="23" ssid = "19">We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="106" ssid = "17">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'25'", "'23'", "'7'", "'106'"]
'10'
'25'
'23'
'7'
'106'
['10', '25', '23', '7', '106']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">Compared to performing segmentation and POS tagging one at a time  Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low  2004).</S><S sid ="9" ssid = "5">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid ="3" ssid = "3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid ="26" ssid = "22">In order to perform POS tagging at the same time  we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004).</S><S sid ="106" ssid = "17">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'9'", "'3'", "'26'", "'106'"]
'11'
'9'
'3'
'26'
'106'
['11', '9', '3', '26', '106']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Besides the usual character-based features  additional features dependent on POS’s or words can also be employed to improve the performance.</S><S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid ="54" ssid = "5">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S><S sid ="51" ssid = "2">Additional features most widely used are related to word or POS ngrams.</S><S sid ="82" ssid = "7">In addition  we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'14'", "'54'", "'51'", "'82'"]
'12'
'14'
'54'
'51'
'82'
['12', '14', '54', '51', '82']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">However  as such features are generated dynamically during the decoding procedure  two limitation arise: on the one hand  the amount of parameters increases rapidly  which is apt to overfit on training corpus; on the other hand  exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.</S><S sid ="52" ssid = "3">However  such features are generated dynamically during the decoding procedure so that the feature space enlarges much more rapidly.</S><S sid ="55" ssid = "6">In addition  even though these higher grams were managed to be used  there still remains another problem: as the current predication relies on the results of prior ones  the decoding procedure has to resort to approximate inference by maintaining a list of N-best candidates at each predication position  which evokes a potential risk to depress the training.</S><S sid ="131" ssid = "2">Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.</S><S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'52'", "'55'", "'131'", "'14'"]
'13'
'52'
'55'
'131'
'14'
['13', '52', '55', '131', '14']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid ="12" ssid = "8">Besides the usual character-based features  additional features dependent on POS’s or words can also be employed to improve the performance.</S><S sid ="131" ssid = "2">Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.</S><S sid ="132" ssid = "3">This is a substitute method to use both local and non-local features  and it would be especially useful when the training corpus is very large.</S><S sid ="54" ssid = "5">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'12'", "'131'", "'132'", "'54'"]
'14'
'12'
'131'
'132'
'54'
['14', '12', '131', '132', '54']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">To cope with this problem  we propose a cascaded linear model inspired by the log-linear model (Och and Ney  2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.</S><S sid ="56" ssid = "7">To alleviate the drawbacks  we propose a cascaded linear model.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="8" ssid = "4">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'56'", "'49'", "'45'", "'8'"]
'15'
'56'
'49'
'45'
'8'
['15', '56', '49', '45', '8']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="65" ssid = "16">As shown in Figure 1  the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.</S><S sid ="57" ssid = "8">It has a two-layer architecture  with a perceptron as the core and another linear model as the outside-layer.</S><S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="78" ssid = "3">Given a Chinese character sequence C1:n  the decoding procedure can proceed in a left-right fashion with a dynamic programming approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'65'", "'57'", "'2'", "'78'"]
'16'
'65'
'57'
'2'
'78'
['16', '65', '57', '2', '78']
parsed_discourse_facet ['method_citation']
['Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.']
['We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.', 'Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.', u'Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS\xe2\u20ac\u2122s are correctly labelled.', 'Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.', 'On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:66', 'F:0']
['All feature templates and their instances are shown in Table 1.']
['Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.', 'If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?', 'To cope with this problem  we propose a cascaded linear model inspired by the log-linear model (Och and Ney  2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.', u'We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x \u2208 X to outputs y \u2208 Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.', 'Several models were introduced for these problems  for example  the Hidden Markov Model (HMM) (Rabiner  1989)  Maximum Entropy Model (ME) (Ratnaparkhi and Adwait  1996)  and Conditional Random Fields (CRFs) (Lafferty et al.  2001).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5460', 'P:15', 'F:0']
['The feature templates we adopted are selected from those of Ng and Low (2004).']
['Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.', 'To alleviate the drawbacks  we propose a cascaded linear model.', 'To cope with this problem  we propose a cascaded linear model inspired by the log-linear model (Och and Ney  2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.', u'We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x \u2208 X to outputs y \u2208 Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.', u'To alleviate overfitting on the training examples  we use the refinement strategy called \u201caveraged parameters\u201d (Collins  2002) to the algorithm in Algorithm 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:21', 'F:0']
['The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.']
['We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.', 'Additional features most widely used are related to word or POS ngrams.', 'The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.', 'Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages.', 'We used SRI Language Modelling Toolkit (Stolcke and Andreas  2002) to train a 3gram word LM with modified Kneser-Ney smoothing (Chen and Goodman  1998)  and a 4-gram POS LM with Witten-Bell smoothing  and we trained a word-POS co-occurrence model simply by MLE without smoothing.']
['system', 'ROUGE-S*', 'Average_R:', '0.00222', '(95%-conf.int.', '0.00222', '-', '0.00222)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.00431', '(95%-conf.int.', '0.00431', '-', '0.00431)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:91', 'F:7']
['The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.']
['Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen  2003)  Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low  2004).', 'We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag.', u'Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS\xe2\u20ac\u2122s are correctly labelled.', 'CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.', 'According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.']
['system', 'ROUGE-S*', 'Average_R:', '0.00125', '(95%-conf.int.', '0.00125', '-', '0.00125)']
['system', 'ROUGE-S*', 'Average_P:', '0.05495', '(95%-conf.int.', '0.05495', '-', '0.05495)']
['system', 'ROUGE-S*', 'Average_F:', '0.00244', '(95%-conf.int.', '0.00244', '-', '0.00244)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4005', 'P:91', 'F:5']
['We trained a character-based perceptron for Chinese Joint S&T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.']
['Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.', 'Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS  CityU and MSR).', 'The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.', 'As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.', 'CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.']
['system', 'ROUGE-S*', 'Average_R:', '0.00938', '(95%-conf.int.', '0.00938', '-', '0.00938)']
['system', 'ROUGE-S*', 'Average_P:', '0.24176', '(95%-conf.int.', '0.24176', '-', '0.24176)']
['system', 'ROUGE-S*', 'Average_F:', '0.01805', '(95%-conf.int.', '0.01805', '-', '0.01805)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:91', 'F:22']
['The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.']
['It has a two-layer architecture  with a perceptron as the core and another linear model as the outside-layer.', 'With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.', 'As shown in Figure 1  the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.', 'Given a Chinese character sequence C1:n  the decoding procedure can proceed in a left-right fashion with a dynamic programming approach.', 'Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.']
['system', 'ROUGE-S*', 'Average_R:', '0.00136', '(95%-conf.int.', '0.00136', '-', '0.00136)']
['system', 'ROUGE-S*', 'Average_P:', '0.03297', '(95%-conf.int.', '0.03297', '-', '0.03297)']
['system', 'ROUGE-S*', 'Average_F:', '0.00261', '(95%-conf.int.', '0.00261', '-', '0.00261)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:91', 'F:3']
0.0580857134559 0.002029999971 0.00391571422978





input/ref/Task1/W11-2123_aakansha.csv
input/res/Task1/W11-2123.csv
parsing: input/ref/Task1/W11-2123_aakansha.csv
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="136" ssid="8">We offer a state function s(wn1) = wn&#65533; where substring wn&#65533; is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'"]
'136'
['136']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="3">Queries take the form p(wn|wn&#8722;1 1 ) where wn1 is an n-gram.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="205" ssid="24">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al., 2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'205'"]
'205'
['205']
parsed_discourse_facet ['method_citation']
<S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
<S sid="204" ssid="23">For RandLM, we used the settings in the documentation: 8 bits per value and false positive probability 1 256.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'"]
'204'
['204']
parsed_discourse_facet ['method_citation']
<S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="229" ssid="48">Then we ran binary search to determine the least amount of memory with which it would run.</S>
original cit marker offset is 0
new cit marker offset is 0



["'229'"]
'229'
['229']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="93" ssid="71">The cost of storing these averages, in bits, is Because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.</S>
original cit marker offset is 0
new cit marker offset is 0



["'93'"]
'93'
['93']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W11-2123.csv
<S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid ="7" ssid = "2">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'274'", "'7'", "'259'", "'45'"]
'1'
'274'
'7'
'259'
'45'
['1', '274', '7', '259', '45']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">The structure uses linear probing hash tables and is designed for speed.</S><S sid ="194" ssid = "13">Further  the probing hash table does only one random lookup per query  explaining why it is faster on large data.</S><S sid ="23" ssid = "1">We implement two data structures: PROBING  designed for speed  and TRIE  optimized for memory.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S><S sid ="120" ssid = "24">The compressed variant uses block compression and is rather slow as a result.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'194'", "'23'", "'45'", "'120'"]
'2'
'194'
'23'
'45'
'120'
['2', '194', '23', '45', '120']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S><S sid ="128" ssid = "32">These models generally outperform our memory consumption but are much slower  even when cached.</S><S sid ="161" ssid = "33">In both cases  SRILM walks its trie an additional time to minimize context as mentioned in Section 4.1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'275'", "'276'", "'128'", "'161'"]
'3'
'275'
'276'
'128'
'161'
['3', '275', '276', '128', '161']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Our code is thread-safe  and integrated into the Moses  cdec  and Joshua translation systems.</S><S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="169" ssid = "41">In our case multi-threading is trivial because our data structures are read-only and uncached.</S><S sid ="282" ssid = "3">Adam Pauls provided a pre-release comparison to BerkeleyLM and an initial Java interface.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'279'", "'169'", "'282'", "'266'"]
'4'
'279'
'169'
'282'
'266'
['4', '279', '169', '282', '266']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid ="26" ssid = "4">We use two common techniques  hash tables and sorted arrays  describing each before the model that uses the technique.</S><S sid ="129" ssid = "1">In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'274'", "'26'", "'129'"]
'5'
'1'
'274'
'26'
'129'
['5', '1', '274', '26', '129']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "1">Language models are widely applied in natural language processing  and applications such as machine translation make very frequent queries.</S><S sid ="130" ssid = "2">Applications such as machine translation use language model probability as a feature to assist in choosing between hypotheses.</S><S sid ="184" ssid = "3">Sparse lookup is a key subproblem of language model queries.</S><S sid ="7" ssid = "2">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'130'", "'184'", "'7'", "'259'"]
'6'
'130'
'184'
'7'
'259'
['6', '130', '184', '7', '259']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "2">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'1'", "'274'", "'259'", "'45'"]
'7'
'1'
'274'
'259'
'45'
['7', '1', '274', '259', '45']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "3">Queries take the form p(wn|wn−1 1 ) where wn1 is an n-gram.</S><S sid ="84" ssid = "62">In a model we built with default settings  1.2% of n + 1-grams were missing their ngram suffix.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="72" ssid = "50">We maintain a separate array for each length n containing all n-gram entries sorted in suffix order.</S><S sid ="46" ssid = "24">Unigram lookup is dense so we use an array of probability and backoff values.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'84'", "'88'", "'72'", "'46'"]
'8'
'84'
'88'
'72'
'46'
['8', '84', '88', '72', '46']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "4">Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf   returning where the probability p(wn|wn−1 f ) and backoff penalties b(wn−1 i ) are given by an already-estimated model.</S><S sid ="127" ssid = "31">Minimal perfect hashing is used to find the index at which a quantized probability and possibly backoff are stored.</S><S sid ="136" ssid = "8">We offer a state function s(wn1) = wn� where substring wn� is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.</S><S sid ="78" ssid = "56">Unigram records store probability  backoff  and an index in the bigram table.</S><S sid ="154" ssid = "26">To optimize left-to-right queries  we extend state to store backoff information: where m is the minimal context from Section 4.1 and b is the backoff penalty.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'127'", "'136'", "'78'", "'154'"]
'9'
'127'
'136'
'78'
'154'
['9', '127', '136', '78', '154']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "5">The problem is to store these two values for a large and sparse set of n-grams in a way that makes queries efficient.</S><S sid ="25" ssid = "3">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S><S sid ="24" ssid = "2">The set of n-grams appearing in a model is sparse  and we want to efficiently find their associated probabilities and backoff penalties.</S><S sid ="93" ssid = "71">The cost of storing these averages  in bits  is Because there are comparatively few unigrams  we elected to store them byte-aligned and unquantized  making every query faster.</S><S sid ="85" ssid = "63">This causes a problem for reverse trie implementations  including SRILM itself  because it leaves n+1-grams without an n-gram node pointing to them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'25'", "'24'", "'93'", "'85'"]
'10'
'25'
'24'
'93'
'85'
['10', '25', '24', '93', '85']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "6">Many packages perform language model queries.</S><S sid ="68" ssid = "46">The trie data structure is commonly used for language modeling.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'68'", "'266'", "'259'", "'45'"]
'11'
'68'
'266'
'259'
'45'
['11', '68', '266', '259', '45']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke  2002) is a popular toolkit based on tries used in several decoders.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="97" ssid = "1">SRILM (Stolcke  2002) is widely used within academia.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'13'", "'14'", "'103'", "'97'"]
'12'
'13'
'14'
'103'
'97'
['12', '13', '14', '103', '97']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'103'", "'15'", "'14'", "'262'"]
'13'
'103'
'15'
'14'
'262'
['13', '103', '15', '14', '262']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="97" ssid = "1">SRILM (Stolcke  2002) is widely used within academia.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'15'", "'103'", "'97'", "'266'"]
'14'
'15'
'103'
'97'
'266'
['14', '15', '103', '97', '266']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'14'", "'103'", "'13'", "'262'"]
'15'
'14'
'103'
'13'
'262'
['15', '14', '103', '13', '262']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "11">BerkeleyLM revision 152 (Pauls and Klein  2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.</S><S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="51" ssid = "29">This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'15'", "'51'", "'262'", "'13'"]
'16'
'15'
'51'
'262'
'13'
['16', '15', '51', '262', '13']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "12">Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.</S><S sid ="18" ssid = "13">TPT Germann et al. (2009) describe tries with better locality properties  but did not release code.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="215" ssid = "34">Another option is the closedsource data structures from Sheffield (Guthrie and Hepple  2010).</S><S sid ="114" ssid = "18">Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'18'", "'262'", "'215'", "'114'"]
'17'
'18'
'262'
'215'
'114'
['17', '18', '262', '215', '114']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "13">TPT Germann et al. (2009) describe tries with better locality properties  but did not release code.</S><S sid ="17" ssid = "12">Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.</S><S sid ="232" ssid = "51">Memory usage is likely much lower than ours. fThe original paper (Germann et al.  2009) provided only 2s of query timing and compared with SRI when it exceeded available RAM.</S><S sid ="114" ssid = "18">Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'17'", "'232'", "'114'", "'262'"]
'18'
'17'
'232'
'114'
'262'
['18', '17', '232', '114', '262']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "14">These packages are further described in Section 3.</S><S sid ="129" ssid = "1">In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="5" ssid = "5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S sid ="150" ssid = "22">Section 4.1 explained that state s is stored by applications with partial hypotheses to determine when they can be recombined.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'129'", "'266'", "'5'", "'150'"]
'19'
'129'
'266'
'5'
'150'
['19', '129', '266', '5', '150']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "15">We substantially outperform all of them on query speed and offer lower memory consumption than lossless alternatives.</S><S sid ="128" ssid = "32">These models generally outperform our memory consumption but are much slower  even when cached.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S><S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'128'", "'276'", "'1'", "'275'"]
'20'
'128'
'276'
'1'
'275'
['20', '128', '276', '1', '275']
parsed_discourse_facet ['aim_citation']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.', 'IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.', 'RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.', 'MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.']
['system', 'ROUGE-S*', 'Average_R:', '0.00395', '(95%-conf.int.', '0.00395', '-', '0.00395)']
['system', 'ROUGE-S*', 'Average_P:', '0.08974', '(95%-conf.int.', '0.08974', '-', '0.08974)']
['system', 'ROUGE-S*', 'Average_F:', '0.00758', '(95%-conf.int.', '0.00758', '-', '0.00758)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:78', 'F:7']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.', 'IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.', 'Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke  2002) is a popular toolkit based on tries used in several decoders.', 'MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.', 'SRILM (Stolcke  2002) is widely used within academia.']
['system', 'ROUGE-S*', 'Average_R:', '0.00135', '(95%-conf.int.', '0.00135', '-', '0.00135)']
['system', 'ROUGE-S*', 'Average_P:', '0.02564', '(95%-conf.int.', '0.02564', '-', '0.02564)']
['system', 'ROUGE-S*', 'Average_F:', '0.00256', '(95%-conf.int.', '0.00256', '-', '0.00256)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:78', 'F:2']
['The cost of storing these averages, in bits, is Because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.']
['Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', 'TPT Germann et al. (2009) describe tries with better locality properties  but did not release code.', 'Memory usage is likely much lower than ours. fThe original paper (Germann et al.  2009) provided only 2s of query timing and compared with SRI when it exceeded available RAM.', 'Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.', 'Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:91', 'F:0']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.', 'We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.', 'We use two common techniques  hash tables and sorted arrays  describing each before the model that uses the technique.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.', 'In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.']
['system', 'ROUGE-S*', 'Average_R:', '0.01412', '(95%-conf.int.', '0.01412', '-', '0.01412)']
['system', 'ROUGE-S*', 'Average_P:', '0.32727', '(95%-conf.int.', '0.32727', '-', '0.32727)']
['system', 'ROUGE-S*', 'Average_F:', '0.02707', '(95%-conf.int.', '0.02707', '-', '0.02707)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:55', 'F:18']
['We offer a state function s(wn1) = wn&#65533; where substring wn&#65533; is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.']
['These models generally outperform our memory consumption but are much slower  even when cached.', 'Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', 'The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.', 'The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.', 'In both cases  SRILM walks its trie an additional time to minimize context as mentioned in Section 4.1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:105', 'F:0']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.', 'Adam Pauls provided a pre-release comparison to BerkeleyLM and an initial Java interface.', 'Our code is thread-safe  and integrated into the Moses  cdec  and Joshua translation systems.', 'Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.', 'In our case multi-threading is trivial because our data structures are read-only and uncached.']
['system', 'ROUGE-S*', 'Average_R:', '0.00606', '(95%-conf.int.', '0.00606', '-', '0.00606)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.01124', '(95%-conf.int.', '0.01124', '-', '0.01124)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:78', 'F:6']
['We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.']
['Many packages perform language model queries.', 'The trie data structure is commonly used for language modeling.', 'The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.', 'There any many techniques for improving language model speed and reducing memory consumption.', 'Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.']
['system', 'ROUGE-S*', 'Average_R:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_P:', '0.22222', '(95%-conf.int.', '0.22222', '-', '0.22222)']
['system', 'ROUGE-S*', 'Average_F:', '0.02424', '(95%-conf.int.', '0.02424', '-', '0.02424)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:45', 'F:10']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.', 'TPT Germann et al. (2009) describe tries with better locality properties  but did not release code.', 'Another option is the closedsource data structures from Sheffield (Guthrie and Hepple  2010).', 'Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.']
['system', 'ROUGE-S*', 'Average_R:', '0.00116', '(95%-conf.int.', '0.00116', '-', '0.00116)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00213', '(95%-conf.int.', '0.00213', '-', '0.00213)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:78', 'F:1']
['The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.']
['The compressed variant uses block compression and is rather slow as a result.', 'Further  the probing hash table does only one random lookup per query  explaining why it is faster on large data.', 'The structure uses linear probing hash tables and is designed for speed.', 'The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.', 'We implement two data structures: PROBING  designed for speed  and TRIE  optimized for memory.']
['system', 'ROUGE-S*', 'Average_R:', '0.06091', '(95%-conf.int.', '0.06091', '-', '0.06091)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.11482', '(95%-conf.int.', '0.11482', '-', '0.11482)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:55', 'F:55']
['Then we ran binary search to determine the least amount of memory with which it would run.']
['This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.', 'IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.', 'RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', 'BerkeleyLM revision 152 (Pauls and Klein  2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:21', 'F:0']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.', 'There any many techniques for improving language model speed and reducing memory consumption.', 'We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.', 'The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.', 'This paper presents methods to query N-gram language models  minimizing time and space costs.']
['system', 'ROUGE-S*', 'Average_R:', '0.03991', '(95%-conf.int.', '0.03991', '-', '0.03991)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.07676', '(95%-conf.int.', '0.07676', '-', '0.07676)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:55', 'F:55']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.', 'There any many techniques for improving language model speed and reducing memory consumption.', 'We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.', 'The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.', 'This paper presents methods to query N-gram language models  minimizing time and space costs.']
['system', 'ROUGE-S*', 'Average_R:', '0.05660', '(95%-conf.int.', '0.05660', '-', '0.05660)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.10714', '(95%-conf.int.', '0.10714', '-', '0.10714)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:78', 'F:78']
['Queries take the form p(wn|wn&#8722;1 1 ) where wn1 is an n-gram.']
['There any many techniques for improving language model speed and reducing memory consumption.', 'Applications such as machine translation use language model probability as a feature to assist in choosing between hypotheses.', 'This paper presents methods to query N-gram language models  minimizing time and space costs.', 'Sparse lookup is a key subproblem of language model queries.', 'Language models are widely applied in natural language processing  and applications such as machine translation make very frequent queries.']
['system', 'ROUGE-S*', 'Average_R:', '0.00085', '(95%-conf.int.', '0.00085', '-', '0.00085)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00165', '(95%-conf.int.', '0.00165', '-', '0.00165)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:36', 'F:1']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.', 'IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.', 'RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.', 'MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.']
['system', 'ROUGE-S*', 'Average_R:', '0.00395', '(95%-conf.int.', '0.00395', '-', '0.00395)']
['system', 'ROUGE-S*', 'Average_P:', '0.08974', '(95%-conf.int.', '0.08974', '-', '0.08974)']
['system', 'ROUGE-S*', 'Average_F:', '0.00758', '(95%-conf.int.', '0.00758', '-', '0.00758)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:78', 'F:7']
['For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.']
['These models generally outperform our memory consumption but are much slower  even when cached.', 'We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.', 'We substantially outperform all of them on query speed and offer lower memory consumption than lossless alternatives.', 'The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.', 'The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.']
['system', 'ROUGE-S*', 'Average_R:', '0.00177', '(95%-conf.int.', '0.00177', '-', '0.00177)']
['system', 'ROUGE-S*', 'Average_P:', '0.00615', '(95%-conf.int.', '0.00615', '-', '0.00615)']
['system', 'ROUGE-S*', 'Average_F:', '0.00275', '(95%-conf.int.', '0.00275', '-', '0.00275)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:325', 'F:2']
['We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al., 2009).']
['Unigram lookup is dense so we use an array of probability and backoff values.', u'Queries take the form p(wn|wn\u22121 1 ) where wn1 is an n-gram.', u'By contrast  BerkeleyLM\u2019s hash and compressed variants will return incorrect results based on an n \u22121-gram.', 'We maintain a separate array for each length n containing all n-gram entries sorted in suffix order.', 'In a model we built with default settings  1.2% of n + 1-grams were missing their ngram suffix.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:136', 'F:0']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.', 'RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.', 'MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.', 'SRILM (Stolcke  2002) is widely used within academia.', 'Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.']
['system', 'ROUGE-S*', 'Average_R:', '0.00603', '(95%-conf.int.', '0.00603', '-', '0.00603)']
['system', 'ROUGE-S*', 'Average_P:', '0.10256', '(95%-conf.int.', '0.10256', '-', '0.10256)']
['system', 'ROUGE-S*', 'Average_F:', '0.01140', '(95%-conf.int.', '0.01140', '-', '0.01140)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:78', 'F:8']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['These packages are further described in Section 3.', 'In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.', 'Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.', 'Section 4.1 explained that state s is stored by applications with partial hypotheses to determine when they can be recombined.']
['system', 'ROUGE-S*', 'Average_R:', '0.00664', '(95%-conf.int.', '0.00664', '-', '0.00664)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.01223', '(95%-conf.int.', '0.01223', '-', '0.01223)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:78', 'F:6']
0.225431109859 0.0120066666 0.0227305554293





input/ref/Task1/P87-1015_vardha.csv
input/res/Task1/P87-1015.csv
parsing: input/ref/Task1/P87-1015_vardha.csv
<S sid="165" ssid="50">This class of formalisms have the properties that their derivation trees are local sets, and manipulate objects, using a finite number of composition operations that use a finite number of symbols.</S>
original cit marker offset is 0
new cit marker offset is 0



["'165'"]
'165'
['165']
parsed_discourse_facet ['method_citation']
 <S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'"]
'119'
['119']
parsed_discourse_facet ['method_citation']
    <S sid="3" ssid="1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'"]
'3'
['3']
parsed_discourse_facet ['method_citation']
    <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
    <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
    <S sid="133" ssid="18">To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'"]
'133'
['133']
parsed_discourse_facet ['method_citation']
 <S sid="138" ssid="23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
    <S sid="156" ssid="41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'"]
'156'
['156']
parsed_discourse_facet ['method_citation']
 <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
 <S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'"]
'119'
['119']
parsed_discourse_facet ['method_citation']
  <S sid="133" ssid="18">To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'"]
'133'
['133']
parsed_discourse_facet ['method_citation']
 <S sid="164" ssid="49">Although embedding this version of LCFRS's in the framework of ILFP developed by Rounds (1985) is straightforward, our motivation was to capture properties shared by a family of grammatical systems and generalize them defining a class of related formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'164'"]
'164'
['164']
parsed_discourse_facet ['method_citation']
 <S sid="204" ssid="10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's, HG's, TAG's, and MCTAG's are all local sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'"]
'204'
['204']
parsed_discourse_facet ['method_citation']
    <S sid="9" ssid="7">We examine both the complexity of the paths of trees in the tree sets, and the kinds of dependencies that the formalisms can impose between paths.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="28" ssid="13">A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
 <S sid="138" ssid="23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P87-1015.csv
<S sid ="1" ssid = "1">We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.</S><S sid ="9" ssid = "7">We examine both the complexity of the paths of trees in the tree sets  and the kinds of dependencies that the formalisms can impose between paths.</S><S sid ="232" ssid = "38">In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="110" ssid = "16">The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'232'", "'195'", "'110'"]
'1'
'9'
'232'
'195'
'110'
['1', '9', '232', '195', '110']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">In considering the relationship between formalisms  we show that it is useful to abstract away from the details of the formalism  and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation  we describe a class of formalisms which we call Linear Context- Free Rewriting Systems  and show they are recognizable in polynomial time and generate only semilinear languages.</S><S sid ="203" ssid = "9">In order to observe the similarity between these constrained systems  it is crucial to abstract away from the details of the structures and operations used by the system.</S><S sid ="232" ssid = "38">In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.</S><S sid ="11" ssid = "9">By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'203'", "'232'", "'11'", "'116'"]
'2'
'203'
'232'
'11'
'116'
['2', '203', '232', '11', '116']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S><S sid ="48" ssid = "33">There has been recent interest in the application of Indexed Grammars (IG's) to natural languages.</S><S sid ="23" ssid = "8">He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.</S><S sid ="56" ssid = "41">Gazdar (1985) argues that sharing of stacks can be used to give analyses for coordination.</S><S sid ="65" ssid = "50">While the generative power of CG's is greater that of CFG's  it appears to be highly constrained.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'48'", "'23'", "'56'", "'65'"]
'3'
'48'
'23'
'56'
'65'
['3', '48', '23', '56', '65']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "2">Little attention  however  has been paid to the structural descriptions that these formalisms can assign to strings  i.e. their strong generative capacity.</S><S sid ="3" ssid = "1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S><S sid ="66" ssid = "51">Hence  their relationship to formalisms such as HG's and TAG's is of interest.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="48" ssid = "33">There has been recent interest in the application of Indexed Grammars (IG's) to natural languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'3'", "'66'", "'195'", "'48'"]
'4'
'3'
'66'
'195'
'48'
['4', '3', '66', '195', '48']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "3">This aspect of the formalism is both linguistically and computationally important.</S><S sid ="198" ssid = "4">This property reflects an important aspect of the underlying linguistic theory associated with the formalism.</S><S sid ="10" ssid = "8">These two properties of the tree sets are not only linguistically relevant  but also have computational importance.</S><S sid ="123" ssid = "8">A second restriction on the formalisms is that choices during the derivation are independent of the context in the derivation.</S><S sid ="3" ssid = "1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'198'", "'10'", "'123'", "'3'"]
'5'
'198'
'10'
'123'
'3'
['5', '198', '10', '123', '3']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">For example  Gazdar (1985) discusses the applicability of Indexed Grammars (IG's) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar (LFG) and Government and Bindings grammars (GB).</S><S sid ="24" ssid = "9">Head Grammars (HG's)  introduced by Pollard (1984)  is a formalism that manipulates headed strings: i.e.  strings  one of whose symbols is distinguished as the head.</S><S sid ="35" ssid = "20">TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish  for cross serial dependencies in Dutch subordinate clauses  and for the nestings of paired English complementizers.</S><S sid ="64" ssid = "49">Steedman (1986) considers Categorial Grammars in which both the operations of function application and composition may be used  and in which function can specify whether they take their arguments from their right or left.</S><S sid ="22" ssid = "7">Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'24'", "'35'", "'64'", "'22'"]
'6'
'24'
'35'
'64'
'22'
['6', '24', '35', '64', '22']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "5">The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG's and IG's.</S><S sid ="50" ssid = "35">The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.</S><S sid ="16" ssid = "1">From Thatcher's (1973) work  it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S><S sid ="98" ssid = "4">Thatcher (1973) describes a tee pumping lemma for recognizable sets related to the string pumping lemma for regular sets.</S><S sid ="24" ssid = "9">Head Grammars (HG's)  introduced by Pollard (1984)  is a formalism that manipulates headed strings: i.e.  strings  one of whose symbols is distinguished as the head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'50'", "'16'", "'98'", "'24'"]
'7'
'50'
'16'
'98'
'24'
['7', '50', '16', '98', '24']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "6">We consider properties of the tree sets generated by CFG's  Tree Adjoining Grammars (TAG's)  Head Grammars (HG's)  Categorial Grammars (CG's)  and IG's.</S><S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="27" ssid = "12">Thus  the tree sets generated by HG's are similar to those of CFG's  with each node annotated by the operation (concatenation or wrapping) used to combine the headed strings derived by the daughters of Tree Adjoining Grammars  a tree rewriting formalism  was introduced by Joshi  Levy and Takahashi (1975) and Joshi (1983/85).</S><S sid ="94" ssid = "79">The semilinearity of Tree Adjoining Languages (TAL's)  MCTAL's  and Head Languages (HL's) can be proved using this property  with suitable restrictions on the composition operations.</S><S sid ="208" ssid = "14">This family represents an attempt to generalize the properties shared by CFG's  HG's  TAG's  and MCTAG's.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'151'", "'27'", "'94'", "'208'"]
'8'
'151'
'27'
'94'
'208'
['8', '151', '27', '94', '208']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "7">We examine both the complexity of the paths of trees in the tree sets  and the kinds of dependencies that the formalisms can impose between paths.</S><S sid ="1" ssid = "1">We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.</S><S sid ="8" ssid = "6">We consider properties of the tree sets generated by CFG's  Tree Adjoining Grammars (TAG's)  Head Grammars (HG's)  Categorial Grammars (CG's)  and IG's.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="110" ssid = "16">The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'1'", "'8'", "'195'", "'110'"]
'9'
'1'
'8'
'195'
'110'
['9', '1', '8', '195', '110']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "8">These two properties of the tree sets are not only linguistically relevant  but also have computational importance.</S><S sid ="5" ssid = "3">This aspect of the formalism is both linguistically and computationally important.</S><S sid ="198" ssid = "4">This property reflects an important aspect of the underlying linguistic theory associated with the formalism.</S><S sid ="68" ssid = "53">This kind of dependency arises from the use of the composition operation to compose two arbitrarily large categories.</S><S sid ="23" ssid = "8">He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'5'", "'198'", "'68'", "'23'"]
'10'
'5'
'198'
'68'
'23'
['10', '5', '198', '68', '23']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "9">By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.</S><S sid ="2" ssid = "2">In considering the relationship between formalisms  we show that it is useful to abstract away from the details of the formalism  and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation  we describe a class of formalisms which we call Linear Context- Free Rewriting Systems  and show they are recognizable in polynomial time and generate only semilinear languages.</S><S sid ="232" ssid = "38">In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.</S><S sid ="14" ssid = "12">This suggests that by generalizing the notion of context-freeness in CFG's  we can define a class of grammatical formalisms that manipulate more complex structures.</S><S sid ="203" ssid = "9">In order to observe the similarity between these constrained systems  it is crucial to abstract away from the details of the structures and operations used by the system.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'2'", "'232'", "'14'", "'203'"]
'11'
'2'
'232'
'14'
'203'
['11', '2', '232', '14', '203']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "10">We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.</S><S sid ="137" ssid = "22">In considering recognition of LCFRS's  we make further assumption concerning the contribution of each structure to the input string  and how the composition operations combine structures in this respect.</S><S sid ="200" ssid = "6">The importance of this property becomes clear in contrasting theories underlying GPSG (Gazdar  Klein  Pulluna  and Sag  1985)  and GB (as described by Berwick  1984) with those underlying LFG and FUG.</S><S sid ="22" ssid = "7">Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.</S><S sid ="60" ssid = "45">Gazdar (1985) considers a restriction of IG's in which no more than one nonterminal on the right-hand-side of a production can inherit the stack from the left-hand-side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'137'", "'200'", "'22'", "'60'"]
'12'
'137'
'200'
'22'
'60'
['12', '137', '200', '22', '60']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "11">It is striking that from this point of view many formalisms can be grouped together as having identically structured derivation tree sets.</S><S sid ="15" ssid = "13">In this paper  we outline how such family of formalisms can be defined  and show that like CFG's  each member possesses a number of desirable linguistic and computational properties: in particular  the constant growth property and polynomial recognizability.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S><S sid ="202" ssid = "8">As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.</S><S sid ="78" ssid = "63">Each member of a set of trees can be adjoined into distinct nodes of trees in a single elementary tree set  i.e  derivations always involve the adjunction of a derived auxiliary tree set into an elementary tree set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'15'", "'116'", "'202'", "'78'"]
'13'
'15'
'116'
'202'
'78'
['13', '15', '116', '202', '78']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "12">This suggests that by generalizing the notion of context-freeness in CFG's  we can define a class of grammatical formalisms that manipulate more complex structures.</S><S sid ="119" ssid = "4">In defining LCFRS's  we hope to generalize the definition of CFG's to formalisms manipulating any structure  e.g. strings  trees  or graphs.</S><S sid ="53" ssid = "38">We focus on this difference between the tree sets of CFG's and IG's  and formalize the notion of dependence between paths in a tree set in Section 3.</S><S sid ="142" ssid = "27">Although this property is not structural  it depends on the structural property that sentences can be built from a finite set of clauses of bounded structure as noted by Joshi (1983/85).</S><S sid ="159" ssid = "44">Similarly  for all the LCFRS's  discussed in Section 2  we can define the relationship between a structure and the sequence of substrings it spans  and the effect of the composition operations on sequences of substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'119'", "'53'", "'142'", "'159'"]
'14'
'119'
'53'
'142'
'159'
['14', '119', '53', '142', '159']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "13">In this paper  we outline how such family of formalisms can be defined  and show that like CFG's  each member possesses a number of desirable linguistic and computational properties: in particular  the constant growth property and polynomial recognizability.</S><S sid ="202" ssid = "8">As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.</S><S sid ="117" ssid = "2">Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S><S sid ="13" ssid = "11">It is striking that from this point of view many formalisms can be grouped together as having identically structured derivation tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'202'", "'117'", "'116'", "'13'"]
'15'
'202'
'117'
'116'
'13'
['15', '202', '117', '116', '13']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "1">From Thatcher's (1973) work  it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S><S sid ="50" ssid = "35">The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.</S><S sid ="19" ssid = "4">It can be easily shown from Thatcher's result that the path set of every local set is a regular set.</S><S sid ="229" ssid = "35">In addition  the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars  and the hierarchy defined by Weir).</S><S sid ="166" ssid = "51">With the additional assumptions  inspired by Rounds (1985)  we can show that members of this class can be recognized in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'50'", "'19'", "'229'", "'166'"]
'16'
'50'
'19'
'229'
'166'
['16', '50', '19', '229', '166']
parsed_discourse_facet ['method_citation']
["We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments."]
["Head Grammars (HG's)  introduced by Pollard (1984)  is a formalism that manipulates headed strings: i.e.  strings  one of whose symbols is distinguished as the head.", 'Thatcher (1973) describes a tee pumping lemma for recognizable sets related to the string pumping lemma for regular sets.', "The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.", "The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG's and IG's.", "From Thatcher's (1973) work  it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'."]
['system', 'ROUGE-S*', 'Average_R:', '0.00121', '(95%-conf.int.', '0.00121', '-', '0.00121)']
['system', 'ROUGE-S*', 'Average_P:', '0.04545', '(95%-conf.int.', '0.04545', '-', '0.04545)']
['system', 'ROUGE-S*', 'Average_F:', '0.00235', '(95%-conf.int.', '0.00235', '-', '0.00235)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:66', 'F:3']
['This class of formalisms have the properties that their derivation trees are local sets, and manipulate objects, using a finite number of composition operations that use a finite number of symbols.']
['We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.', 'We examine both the complexity of the paths of trees in the tree sets  and the kinds of dependencies that the formalisms can impose between paths.', 'We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.', 'The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .', 'In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.']
['system', 'ROUGE-S*', 'Average_R:', '0.00161', '(95%-conf.int.', '0.00161', '-', '0.00161)']
['system', 'ROUGE-S*', 'Average_P:', '0.03333', '(95%-conf.int.', '0.03333', '-', '0.03333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00307', '(95%-conf.int.', '0.00307', '-', '0.00307)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:120', 'F:4']
["In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs."]
["By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.", 'In order to observe the similarity between these constrained systems  it is crucial to abstract away from the details of the structures and operations used by the system.', 'In considering the relationship between formalisms  we show that it is useful to abstract away from the details of the formalism  and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation  we describe a class of formalisms which we call Linear Context- Free Rewriting Systems  and show they are recognizable in polynomial time and generate only semilinear languages.', 'From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).', 'In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.']
['system', 'ROUGE-S*', 'Average_R:', '0.00159', '(95%-conf.int.', '0.00159', '-', '0.00159)']
['system', 'ROUGE-S*', 'Average_P:', '0.16667', '(95%-conf.int.', '0.16667', '-', '0.16667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00316', '(95%-conf.int.', '0.00316', '-', '0.00316)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6903', 'P:66', 'F:11']
["We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments."]
["It can be easily shown from Thatcher's result that the path set of every local set is a regular set.", 'With the additional assumptions  inspired by Rounds (1985)  we can show that members of this class can be recognized in polynomial time.', "The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.", "In addition  the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars  and the hierarchy defined by Weir).", "From Thatcher's (1973) work  it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'."]
['system', 'ROUGE-S*', 'Average_R:', '0.00190', '(95%-conf.int.', '0.00190', '-', '0.00190)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.00372', '(95%-conf.int.', '0.00372', '-', '0.00372)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:66', 'F:6']
['A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.']
['From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).', 'Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.', "As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.", "In this paper  we outline how such family of formalisms can be defined  and show that like CFG's  each member possesses a number of desirable linguistic and computational properties: in particular  the constant growth property and polynomial recognizability.", 'It is striking that from this point of view many formalisms can be grouped together as having identically structured derivation tree sets.']
['system', 'ROUGE-S*', 'Average_R:', '0.00341', '(95%-conf.int.', '0.00341', '-', '0.00341)']
['system', 'ROUGE-S*', 'Average_P:', '0.17778', '(95%-conf.int.', '0.17778', '-', '0.17778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00669', '(95%-conf.int.', '0.00669', '-', '0.00669)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:45', 'F:8']
['We examine both the complexity of the paths of trees in the tree sets, and the kinds of dependencies that the formalisms can impose between paths.']
["Similarly  for all the LCFRS's  discussed in Section 2  we can define the relationship between a structure and the sequence of substrings it spans  and the effect of the composition operations on sequences of substrings.", 'Although this property is not structural  it depends on the structural property that sentences can be built from a finite set of clauses of bounded structure as noted by Joshi (1983/85).', "In defining LCFRS's  we hope to generalize the definition of CFG's to formalisms manipulating any structure  e.g. strings  trees  or graphs.", "This suggests that by generalizing the notion of context-freeness in CFG's  we can define a class of grammatical formalisms that manipulate more complex structures.", "We focus on this difference between the tree sets of CFG's and IG's  and formalize the notion of dependence between paths in a tree set in Section 3."]
['system', 'ROUGE-S*', 'Average_R:', '0.00926', '(95%-conf.int.', '0.00926', '-', '0.00926)']
['system', 'ROUGE-S*', 'Average_P:', '0.41818', '(95%-conf.int.', '0.41818', '-', '0.41818)']
['system', 'ROUGE-S*', 'Average_F:', '0.01811', '(95%-conf.int.', '0.01811', '-', '0.01811)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:55', 'F:23']
['To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.']
["Head Grammars (HG's)  introduced by Pollard (1984)  is a formalism that manipulates headed strings: i.e.  strings  one of whose symbols is distinguished as the head.", 'Steedman (1986) considers Categorial Grammars in which both the operations of function application and composition may be used  and in which function can specify whether they take their arguments from their right or left.', 'Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.', "TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish  for cross serial dependencies in Dutch subordinate clauses  and for the nestings of paired English complementizers.", "For example  Gazdar (1985) discusses the applicability of Indexed Grammars (IG's) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar (LFG) and Government and Bindings grammars (GB)."]
['system', 'ROUGE-S*', 'Average_R:', '0.00086', '(95%-conf.int.', '0.00086', '-', '0.00086)']
['system', 'ROUGE-S*', 'Average_P:', '0.02206', '(95%-conf.int.', '0.02206', '-', '0.02206)']
['system', 'ROUGE-S*', 'Average_F:', '0.00166', '(95%-conf.int.', '0.00166', '-', '0.00166)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:136', 'F:3']
['To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.']
["By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.", 'In order to observe the similarity between these constrained systems  it is crucial to abstract away from the details of the structures and operations used by the system.', 'In considering the relationship between formalisms  we show that it is useful to abstract away from the details of the formalism  and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation  we describe a class of formalisms which we call Linear Context- Free Rewriting Systems  and show they are recognizable in polynomial time and generate only semilinear languages.', "This suggests that by generalizing the notion of context-freeness in CFG's  we can define a class of grammatical formalisms that manipulate more complex structures.", 'In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.']
['system', 'ROUGE-S*', 'Average_R:', '0.00406', '(95%-conf.int.', '0.00406', '-', '0.00406)']
['system', 'ROUGE-S*', 'Average_P:', '0.20588', '(95%-conf.int.', '0.20588', '-', '0.20588)']
['system', 'ROUGE-S*', 'Average_F:', '0.00796', '(95%-conf.int.', '0.00796', '-', '0.00796)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6903', 'P:136', 'F:28']
['Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.']
['Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.', 'Gazdar (1985) argues that sharing of stacks can be used to give analyses for coordination.', "While the generative power of CG's is greater that of CFG's  it appears to be highly constrained.", "There has been recent interest in the application of Indexed Grammars (IG's) to natural languages.", 'He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.']
['system', 'ROUGE-S*', 'Average_R:', '0.05314', '(95%-conf.int.', '0.05314', '-', '0.05314)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.10092', '(95%-conf.int.', '0.10092', '-', '0.10092)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:55', 'F:55']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
['We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.', 'Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.', 'Little attention  however  has been paid to the structural descriptions that these formalisms can assign to strings  i.e. their strong generative capacity.', "Hence  their relationship to formalisms such as HG's and TAG's is of interest.", "There has been recent interest in the application of Indexed Grammars (IG's) to natural languages."]
['system', 'ROUGE-S*', 'Average_R:', '0.00082', '(95%-conf.int.', '0.00082', '-', '0.00082)']
['system', 'ROUGE-S*', 'Average_P:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00149', '(95%-conf.int.', '0.00149', '-', '0.00149)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:120', 'F:1']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
['We examine both the complexity of the paths of trees in the tree sets  and the kinds of dependencies that the formalisms can impose between paths.', 'We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.', 'We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.', 'The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .', "We consider properties of the tree sets generated by CFG's  Tree Adjoining Grammars (TAG's)  Head Grammars (HG's)  Categorial Grammars (CG's)  and IG's."]
['system', 'ROUGE-S*', 'Average_R:', '0.00038', '(95%-conf.int.', '0.00038', '-', '0.00038)']
['system', 'ROUGE-S*', 'Average_P:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00073', '(95%-conf.int.', '0.00073', '-', '0.00073)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:120', 'F:1']
0.197901816383 0.00711272720807 0.0136236362398





input/ref/Task1/A00-2018_akanksha.csv
input/res/Task1/A00-2018.csv
parsing: input/ref/Task1/A00-2018_akanksha.csv
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'"]
'90'
'91'
['90', '91']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="17">Maximum-entropy models have two benefits for a parser builder.</S>
    <S sid="49" ssid="18">First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &amp;quot;features&amp;quot; suggests that the probability model should be easily changeable &#8212; just change the set of features used.</S>
    <S sid="51" ssid="20">Second, and this is a point we have not yet mentioned, the features used in these models need have no particular independence of one another.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'49'", "'51'"]
'48'
'49'
'51'
['48', '49', '51']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
    <S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
    <S sid="93" ssid="4">This allows the second pass to see expansions not present in the training corpus.</S>
    <S sid="94" ssid="5">We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'", "'92'", "'93'", "'94'"]
'90'
'91'
'92'
'93'
'94'
['90', '91', '92', '93', '94']
parsed_discourse_facet ['method_citation']
NA
original cit marker offset is 0
new cit marker offset is 0



['0']
0
['0']
Error in Reference Offset
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
    <S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
    <S sid="93" ssid="4">This allows the second pass to see expansions not present in the training corpus.</S>
    <S sid="94" ssid="5">We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'", "'92'", "'93'", "'94'"]
'90'
'91'
'92'
'93'
'94'
['90', '91', '92', '93', '94']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="7">To compute a probability in a log-linear model one first defines a set of &amp;quot;features&amp;quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.</S>
    <S sid="39" ssid="8">In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.</S>
    <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'39'", "'40'"]
'38'
'39'
'40'
['38', '39', '40']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['result_citation']
<S sid="85" ssid="54">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models, this simplifies the model significantly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="32">As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &amp;quot;before&amp;quot;), and the label of the grandparent of c (la).</S><S sid="143" ssid="34">The first is simply that if we first guess the pre-terminal, when we go to guess the head the first thing we can condition upon is the pre-terminal, i.e., we compute p(h I t).</S><S sid="146" ssid="37">The second major reason why first guessing the pre-terminal makes so much difference is that it can be used when backing off the lexical head in computing the probability of the rule expansion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'143'", "'146'"]
'63'
'143'
'146'
['63', '143', '146']
parsed_discourse_facet ['method_citation']
???<S sid="78" ssid="47">With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.</S>                                        <S sid="79" ssid="48">We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.</S>
original cit marker offset is 0
new cit marker offset is 0



["'78'", "'79'"]
'78'
'79'
['78', '79']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/A00-2018.csv
<S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'5'", "'2'", "'6'", "'125'"]
'1'
'5'
'2'
'6'
'125'
['1', '5', '2', '6', '125']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="175" ssid = "2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'1'", "'125'", "'175'"]
'2'
'6'
'1'
'125'
'175'
['2', '6', '1', '125', '175']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.</S><S sid ="182" ssid = "9">As noted above  the main methodological innovation presented here is our &quot;maximumentropy-inspired&quot; model for conditioning and smoothing.</S><S sid ="128" ssid = "19">The model labeled &quot;Old&quot; attempts to recreate the Char97 system using the current program.</S><S sid ="118" ssid = "9">First  it uses a clustering scheme on words to give the system a &quot;soft&quot; clustering of heads and sub-heads.</S><S sid ="116" ssid = "7">This is as opposed to the &quot;Markovgrammar&quot; approach used in the current parser.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'182'", "'128'", "'118'", "'116'"]
'3'
'182'
'128'
'118'
'116'
['3', '182', '128', '118', '116']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid ="139" ssid = "30">In Figure 2 we show that this one factor improves performance by nearly 2%.</S><S sid ="164" ssid = "55">When we do so using our maximum-entropy-inspired conditioning  we get another 0.45% improvement in average precision/recall  as indicated in Figure 2 on the line labeled &quot;MaxEnt-Inspired'.</S><S sid ="17" ssid = "6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S><S sid ="94" ssid = "5">We use the gathered statistics for all observed words  even those with very low counts  though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'139'", "'164'", "'17'", "'94'"]
'4'
'139'
'164'
'17'
'94'
['4', '139', '164', '17', '94']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'2'", "'6'", "'7'"]
'5'
'1'
'2'
'6'
'7'
['5', '1', '2', '6', '7']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="175" ssid = "2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'1'", "'125'", "'175'"]
'2'
'6'
'1'
'125'
'175'
['2', '6', '1', '125', '175']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid ="171" ssid = "62">As already noted  our best model uses a Markov-grammar approach.</S><S sid ="90" ssid = "1">We created a parser based upon the maximumentropy-inspired model of the last section  smoothed using standard deleted interpolation.</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'171'", "'90'", "'1'", "'5'"]
'7'
'171'
'90'
'1'
'5'
['7', '171', '90', '1', '5']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">That is  for all sentences s and all parses 7r  the parser assigns a probability p(s   7r) = p(r)  the equality holding when we restrict consideration to 7r whose yield * This research was supported in part by NSF grant LIS SBR 9720368.</S><S sid ="12" ssid = "1">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).</S><S sid ="10" ssid = "6">That is  the parser implements the function arg maxrp(7r s) = arg maxirp(7r  s) = arg maxrp(w).</S><S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S><S sid ="147" ssid = "38">For example  when we first guess the lexical head we can move from computing p(r I 1 1p  h) to p(r I /  t  /p  h).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'12'", "'10'", "'41'", "'147'"]
'8'
'12'
'10'
'41'
'147'
['8', '12', '10', '41', '147']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.</S><S sid ="185" ssid = "12">We expect that as we experiment with other  more semantic conditioning information  the importance of this aspect of the model will increase.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="171" ssid = "62">As already noted  our best model uses a Markov-grammar approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'185'", "'5'", "'1'", "'171'"]
'9'
'185'
'5'
'1'
'171'
['9', '185', '5', '1', '171']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">That is  the parser implements the function arg maxrp(7r s) = arg maxirp(7r  s) = arg maxrp(w).</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid ="83" ssid = "52">In the simple (content-free) form (Equation 6)  it is clear that Z(H) = 1.</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="56" ssid = "25">In max-entropy models one can simply include features for all three events f1 (a  b  c)  f2 (a  b)  and f3(a  c) and combine them in the model according to Equation 3  or equivalently  Equation 4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'13'", "'83'", "'54'", "'56'"]
'10'
'13'
'83'
'54'
'56'
['10', '13', '83', '54', '56']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">What fundamentally distinguishes probabilistic generative parsers is how they compute p(r)  and it is to that topic we turn next.</S><S sid ="53" ssid = "22">That is  suppose we want to compute a conditional probability p(a b  c)  but we are not sure that we have enough examples of the conditioning event b  c in the training corpus to ensure that the empirically obtained probability P (a I b  c) is accurate.</S><S sid ="82" ssid = "51">The major advantage of using Equation 7 is that one can generally get away without computing the partition function Z(H).</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="188" ssid = "15">Ultimately it is this flexibility that let us try the various conditioning events  to move on to a Markov grammar approach  and to try several Markov grammars of different orders  without significant programming.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'53'", "'82'", "'54'", "'188'"]
'11'
'53'
'82'
'54'
'188'
['11', '53', '82', '54', '188']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "1">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).</S><S sid ="18" ssid = "7">The method that gives the best results  however  uses a Markov grammar — a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6 10 15].</S><S sid ="134" ssid = "25">As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.</S><S sid ="147" ssid = "38">For example  when we first guess the lexical head we can move from computing p(r I 1 1p  h) to p(r I /  t  /p  h).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'18'", "'134'", "'147'", "'33'"]
'12'
'18'
'134'
'147'
'33'
['12', '18', '134', '147', '33']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid ="16" ssid = "5">In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.</S><S sid ="45" ssid = "14">The function Z(H)  called the partition function  is a normalizing constant (for fixed H)  so the probabilities over all a sum to one.</S><S sid ="83" ssid = "52">In the simple (content-free) form (Equation 6)  it is clear that Z(H) = 1.</S><S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'16'", "'45'", "'83'", "'41'"]
'13'
'16'
'45'
'83'
'41'
['13', '16', '45', '83', '41']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "3">Much of the interesting work is determining what goes into H (c).</S><S sid ="73" ssid = "42">We make one more point on the connection of Equation 7 to a maximum entropy formulation.</S><S sid ="60" ssid = "29">But let us look at how it works for a particular case in our parsing scheme.</S><S sid ="159" ssid = "50">Something very much like this is done in [15].</S><S sid ="151" ssid = "42">We have already noted the importance of conditioning on the parent label /p.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'73'", "'60'", "'159'", "'151'"]
'14'
'73'
'60'
'159'
'151'
['14', '73', '60', '159', '151']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "4">Whenever it is clear to which constituent we are referring we omit the (c) in  e.g.  h(c).</S><S sid ="67" ssid = "36">For example  it does not seem to make much sense to condition on  say  hp without first conditioning on ti .</S><S sid ="151" ssid = "42">We have already noted the importance of conditioning on the parent label /p.</S><S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="140" ssid = "31">It may not be obvious why this should make so great a difference  since most words are effectively unambiguous.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'67'", "'151'", "'157'", "'140'"]
'15'
'67'
'151'
'157'
'140'
['15', '67', '151', '157', '140']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "5">In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.</S><S sid ="65" ssid = "34">We can now rewrite this in the form of Equation 5 as follows: Here we have sequentially conditioned on steadily increasing portions of c's history.</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid ="58" ssid = "27">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid ="46" ssid = "15">Now for our purposes it is useful to rewrite this as a sequence of multiplicative functions gi(a  H) for 0 < i < j: Here go(a  H) = 11Z (H) and gi(a  H) = eAi(a H) fi(° 11).</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'65'", "'13'", "'58'", "'46'"]
'16'
'65'
'13'
'58'
'46'
['16', '65', '13', '58', '46']
parsed_discourse_facet ['method_citation']
['As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.', 'We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.']
['We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.', 'We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.00769', '(95%-conf.int.', '0.00769', '-', '0.00769)']
['system', 'ROUGE-S*', 'Average_P:', '0.01961', '(95%-conf.int.', '0.01961', '-', '0.01961)']
['system', 'ROUGE-S*', 'Average_F:', '0.01104', '(95%-conf.int.', '0.01104', '-', '0.01104)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:561', 'F:11']
['In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.', 'To compute a probability in a log-linear model one first defines a set of &quot;features&quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.', 'In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.']
['The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.', 'We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length ', 'We expect that as we experiment with other  more semantic conditioning information  the importance of this aspect of the model will increase.']
['system', 'ROUGE-S*', 'Average_R:', '0.00348', '(95%-conf.int.', '0.00348', '-', '0.00348)']
['system', 'ROUGE-S*', 'Average_P:', '0.00317', '(95%-conf.int.', '0.00317', '-', '0.00317)']
['system', 'ROUGE-S*', 'Average_F:', '0.00332', '(95%-conf.int.', '0.00332', '-', '0.00332)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:946', 'F:3']
['Second, and this is a point we have not yet mentioned, the features used in these models need have no particular independence of one another.', 'Maximum-entropy models have two benefits for a parser builder.', 'First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &quot;features&quot; suggests that the probability model should be easily changeable &#8212; just change the set of features used.']
["When we do so using our maximum-entropy-inspired conditioning  we get another 0.45% improvement in average precision/recall  as indicated in Figure 2 on the line labeled &quot;MaxEnt-Inspired'.", 'In Figure 2 we show that this one factor improves performance by nearly 2%.', "We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.", 'In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.', 'We use the gathered statistics for all observed words  even those with very low counts  though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.']
['system', 'ROUGE-S*', 'Average_R:', '0.00342', '(95%-conf.int.', '0.00342', '-', '0.00342)']
['system', 'ROUGE-S*', 'Average_P:', '0.01935', '(95%-conf.int.', '0.01935', '-', '0.01935)']
['system', 'ROUGE-S*', 'Average_F:', '0.00582', '(95%-conf.int.', '0.00582', '-', '0.00582)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:465', 'F:9']
['We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.']
['We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.', 'We created a parser based upon the maximumentropy-inspired model of the last section  smoothed using standard deleted interpolation.', 'We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length ', 'As already noted  our best model uses a Markov-grammar approach.', 'Following [5 10]  our parser is based upon a probabilistic generative model.']
['system', 'ROUGE-S*', 'Average_R:', '0.02198', '(95%-conf.int.', '0.02198', '-', '0.02198)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.04301', '(95%-conf.int.', '0.04301', '-', '0.04301)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:66', 'F:66']
['We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.']
['We have already noted the importance of conditioning on the parent label /p.', 'Whenever it is clear to which constituent we are referring we omit the (c) in  e.g.  h(c).', 'It may not be obvious why this should make so great a difference  since most words are effectively unambiguous.', 'Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.', 'For example  it does not seem to make much sense to condition on  say  hp without first conditioning on ti .']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:66', 'F:0']
no Reference Text in gold A00-2018
['We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.']
['As noted above  the main methodological innovation presented here is our &quot;maximumentropy-inspired&quot; model for conditioning and smoothing.', 'The model labeled &quot;Old&quot; attempts to recreate the Char97 system using the current program.', 'The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.', 'This is as opposed to the &quot;Markovgrammar&quot; approach used in the current parser.', 'First  it uses a clustering scheme on words to give the system a &quot;soft&quot; clustering of heads and sub-heads.']
['system', 'ROUGE-S*', 'Average_R:', '0.00376', '(95%-conf.int.', '0.00376', '-', '0.00376)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.00722', '(95%-conf.int.', '0.00722', '-', '0.00722)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:66', 'F:6']
['As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.', 'We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.', 'This allows the second pass to see expansions not present in the training corpus.', 'For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.', 'We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.']
['We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.01307', '(95%-conf.int.', '0.01307', '-', '0.01307)']
['system', 'ROUGE-S*', 'Average_P:', '0.00090', '(95%-conf.int.', '0.00090', '-', '0.00090)']
['system', 'ROUGE-S*', 'Average_F:', '0.00169', '(95%-conf.int.', '0.00169', '-', '0.00169)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:153', 'P:2211', 'F:2']
['We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.']
['We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.', 'This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].', 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].', 'For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.', 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].']
['system', 'ROUGE-S*', 'Average_R:', '0.16653', '(95%-conf.int.', '0.16653', '-', '0.16653)']
['system', 'ROUGE-S*', 'Average_P:', '0.65856', '(95%-conf.int.', '0.65856', '-', '0.65856)']
['system', 'ROUGE-S*', 'Average_F:', '0.26584', '(95%-conf.int.', '0.26584', '-', '0.26584)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3741', 'P:946', 'F:623']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.']
['Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.', 'Now for our purposes it is useful to rewrite this as a sequence of multiplicative functions gi(a  H) for 0 ', u'Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c \u2014 information outside c that our probability model deems important in determining the probability in question.', "We can now rewrite this in the form of Equation 5 as follows: Here we have sequentially conditioned on steadily increasing portions of c's history.", 'In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.']
['system', 'ROUGE-S*', 'Average_R:', '0.00180', '(95%-conf.int.', '0.00180', '-', '0.00180)']
['system', 'ROUGE-S*', 'Average_P:', '0.01232', '(95%-conf.int.', '0.01232', '-', '0.01232)']
['system', 'ROUGE-S*', 'Average_F:', '0.00314', '(95%-conf.int.', '0.00314', '-', '0.00314)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:406', 'F:5']
['As partition-function calculation is typically the major on-line computational problem for maximum-entropy models, this simplifies the model significantly.']
['For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).', u'The method that gives the best results  however  uses a Markov grammar \xe2\u20ac\u201d a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6 10 15].', 'For example  when we first guess the lexical head we can move from computing p(r I 1 1p  h) to p(r I /  t  /p  h).', 'The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).', 'As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.']
['system', 'ROUGE-S*', 'Average_R:', '0.00047', '(95%-conf.int.', '0.00047', '-', '0.00047)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00089', '(95%-conf.int.', '0.00089', '-', '0.00089)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:91', 'F:1']
['We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.', 'With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.']
['But let us look at how it works for a particular case in our parsing scheme.', 'Something very much like this is done in [15].', 'We have already noted the importance of conditioning on the parent label /p.', 'We make one more point on the connection of Equation 7 to a maximum entropy formulation.', 'Much of the interesting work is determining what goes into H (c).']
['system', 'ROUGE-S*', 'Average_R:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_P:', '0.00430', '(95%-conf.int.', '0.00430', '-', '0.00430)']
['system', 'ROUGE-S*', 'Average_F:', '0.00593', '(95%-conf.int.', '0.00593', '-', '0.00593)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:465', 'F:2']
0.16546454395 0.021065454354 0.0316272724398





input/ref/Task1/A00-2030_aakansha.csv
input/res/Task1/A00-2030.csv
parsing: input/ref/Task1/A00-2030_aakansha.csv
<S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="9">By necessity, we adopted the strategy of hand marking only the semantics.</S>
    <S sid="50" ssid="10">Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'50'"]
'49'
'50'
['49', '50']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="8">Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'"]
'16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S><S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'", "'33'", "'34'"]
'23'
'24'
'33'
'34'
['23', '24', '33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="60" ssid="1">In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).</S>
    <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'61'"]
'60'
'61'
['60', '61']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'"]
'33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).</S>
    <S sid="12" ssid="2">The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).</S><S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'12'", "'16'"]
'11'
'12'
'16'
['11', '12', '16']
parsed_discourse_facet ['method_citation']
<S sid="105" ssid="2">A single model proved capable of performing all necessary sentential processing, both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'"]
'105'
['105']
parsed_discourse_facet ['result_citation']
<S sid="60" ssid="1">In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).</S>
    <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'61'"]
'60'
'61'
['60', '61']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'"]
'16'
['16']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="43" ssid = "3">Thus  we did not consider simply adding semantic labels to the existing Penn TREEBANK  which is drawn from a single source — the Wall Street Journal — and is impoverished in articles about rocket launches.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'10'", "'5'", "'43'", "'104'"]
'1'
'10'
'5'
'43'
'104'
['1', '10', '5', '43', '104']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S><S sid ="26" ssid = "9">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993)  and more recently  had begun using a generative statistical model for name finding (Bikel et al.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'28'", "'96'", "'26'"]
'2'
'6'
'28'
'96'
'26'
['2', '6', '28', '96', '26']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "1">Since 1995  a few statistical parsing algorithms (Magerman  1995; Collins  1996 and 1997; Charniak  1997; Rathnaparki  1997) demonstrated a breakthrough in parsing accuracy  as measured against the University of Pennsylvania TREEBANK as a gold standard.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'1'", "'104'", "'5'", "'96'"]
'3'
'1'
'104'
'5'
'96'
['3', '1', '104', '5', '96']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "2">Yet  relatively few have embedded one of these algorithms in a task.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="108" ssid = "5">This simple semantic annotation was the only source of task knowledge used to configure the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'105'", "'23'", "'28'", "'108'"]
'4'
'105'
'23'
'28'
'108'
['4', '105', '23', '28', '108']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'106'", "'1'", "'28'", "'10'"]
'5'
'106'
'1'
'28'
'10'
['5', '106', '1', '28', '10']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S><S sid ="2" ssid = "2">In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'104'", "'2'", "'51'", "'1'"]
'6'
'104'
'2'
'51'
'1'
['6', '104', '2', '51', '1']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "5">The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.</S><S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="50" ssid = "10">Figure 4 shows an example of the semantic annotation  which was the only type of manual annotation we performed.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'11'", "'28'", "'50'", "'107'"]
'7'
'11'
'28'
'50'
'107'
['7', '11', '28', '50', '107']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "6">Several technical challenges confronted us and were solved: TREEBANK on Wall Street Journal adequately train the algorithm for New York Times newswire  which includes dozens of newspapers?</S><S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'10'", "'5'", "'51'", "'1'"]
'8'
'10'
'5'
'51'
'1'
['8', '10', '5', '51', '1']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "7">Manually creating sourcespecific training data for syntax was not required.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="108" ssid = "5">This simple semantic annotation was the only source of task knowledge used to configure the model.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S><S sid ="41" ssid = "1">To train our integrated model  we required a large corpus of augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'106'", "'108'", "'107'", "'41'"]
'9'
'106'
'108'
'107'
'41'
['9', '106', '108', '107', '41']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "8">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid ="1" ssid = "1">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid ="43" ssid = "3">Thus  we did not consider simply adding semantic labels to the existing Penn TREEBANK  which is drawn from a single source — the Wall Street Journal — and is impoverished in articles about rocket launches.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'1'", "'43'", "'51'", "'5'"]
'10'
'1'
'43'
'51'
'5'
['10', '1', '43', '51', '5']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S><S sid ="7" ssid = "5">The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'7'", "'101'", "'5'", "'96'"]
'11'
'7'
'101'
'5'
'96'
['11', '7', '101', '5', '96']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "2">The Template Element (TE) task identifies organizations  persons  locations  and some artifacts (rocket and airplane-related artifacts).</S><S sid ="98" ssid = "3">In both Template Entity (TE) and Template Relation (TR)  our system finished in second place among all entrants.</S><S sid ="16" ssid = "6">For the following example  the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S><S sid ="78" ssid = "19">If we generalize the tree components (constituent labels  words  tags  etc.) and treat them all as simply elements  e  and treat all the conditioning factors as the history  h  we can write:</S><S sid ="34" ssid = "2">In these trees  the standard TREEBANK structures are augmented to convey semantic information  that is  entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'98'", "'16'", "'78'", "'34'"]
'12'
'98'
'16'
'78'
'34'
['12', '98', '16', '78', '34']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="14" ssid = "4">For each person  one must find all of the person's names within the document  his/her type (civilian or military)  and any significant descriptions (e.g.  titles).</S><S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="58" ssid = "4">Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree  semantic pointer labels are attached to all of the intermediate nodes.</S><S sid ="100" ssid = "5">Given multiple constituents that cover identical spans in the chart  only those constituents with probabilities within a While our focus throughout the project was on TE and TR  we became curious about how well the model did at part-of-speech tagging  syntactic parsing  and at name finding.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'14'", "'15'", "'58'", "'100'"]
'13'
'14'
'15'
'58'
'100'
['13', '14', '15', '58', '100']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "4">For each person  one must find all of the person's names within the document  his/her type (civilian or military)  and any significant descriptions (e.g.  titles).</S><S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S><S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'13'", "'74'", "'15'", "'36'"]
'14'
'13'
'74'
'15'
'36'
['14', '13', '74', '15', '36']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="78" ssid = "19">If we generalize the tree components (constituent labels  words  tags  etc.) and treat them all as simply elements  e  and treat all the conditioning factors as the history  h  we can write:</S><S sid ="91" ssid = "10">The probability of generating a constituent of the specified category  starting at the topmost node.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="55" ssid = "1">syntactic modifier of the other  the inserted node serves to indicate the relation as well as the argument.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'78'", "'91'", "'82'", "'55'"]
'15'
'78'
'91'
'82'
'55'
['15', '78', '91', '82', '55']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "6">For the following example  the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S><S sid ="98" ssid = "3">In both Template Entity (TE) and Template Relation (TR)  our system finished in second place among all entrants.</S><S sid ="12" ssid = "2">The Template Element (TE) task identifies organizations  persons  locations  and some artifacts (rocket and airplane-related artifacts).</S><S sid ="94" ssid = "13">Given a new sentence  the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'74'", "'98'", "'12'", "'94'"]
'16'
'74'
'98'
'12'
'94'
['16', '74', '98', '12', '94']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "7">For the following example  the template relation in Figure 2 was to be generated: &quot;Donald M. Goldstein  a historian at the University of Pittsburgh who helped write...&quot;</S><S sid ="50" ssid = "10">Figure 4 shows an example of the semantic annotation  which was the only type of manual annotation we performed.</S><S sid ="35" ssid = "3">An example of an augmented parse tree is shown in Figure 3.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'50'", "'35'", "'82'", "'56'"]
'17'
'50'
'35'
'82'
'56'
['17', '50', '35', '82', '56']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "1">Almost all approaches to information extraction — even at the sentence level — are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.</S><S sid ="19" ssid = "2">Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S><S sid ="29" ssid = "12">Thus  each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="84" ssid = "3">Although mathematically the model predicts tree elements in a top-down fashion  we search the space bottom-up using a chartbased search.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'19'", "'29'", "'5'", "'84'"]
'18'
'19'
'29'
'5'
'84'
['18', '19', '29', '5', '84']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "2">Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S><S sid ="29" ssid = "12">Thus  each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.</S><S sid ="24" ssid = "7">For this reason  we focused on designing an integrated model in which tagging  namefinding  parsing  and semantic interpretation decisions all have the opportunity to mutually influence each other.</S><S sid ="90" ssid = "9">Thus  the scores used in pruning can be considered as the product of: 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'96'", "'29'", "'24'", "'90'"]
'19'
'96'
'29'
'24'
'90'
['19', '96', '29', '24', '90']
parsed_discourse_facet ['method_citation']
Length 0 input/ref/Task1/A00-2030_aakansha.csv
0.0 0.0 0.0





input/ref/Task1/W06-3114_sweta.csv
input/res/Task1/W06-3114.csv
parsing: input/ref/Task1/W06-3114_sweta.csv
 <S sid="108" ssid="1">The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="27">For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="44">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages, as demonstrated by the results for English-German and English-French.</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
 <S sid="172" ssid="3">Due to many similarly performing systems, we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
 <S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["36'"]
36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="103" ssid="19">Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["103'"]
103'
['103']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="60">One annotator suggested that this was the case for as much as 10% of our test sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["102'"]
102'
['102']
parsed_discourse_facet ['method_citation']
<S sid="123" ssid="16">For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.</S>
original cit marker offset is 0
new cit marker offset is 0



["123'"]
123'
['123']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="27">For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="1">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="19">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["126'"]
126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="173" ssid="4">The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["173'"]
173'
['173']
parsed_discourse_facet ['method_citation']
 <S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["170'"]
170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="84" ssid="23">The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["84'"]
84'
['84']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="143" ssid = "36">For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.</S><S sid ="129" ssid = "22">All systems (except for Systran  which was not tuned to Europarl) did considerably worse on outof-domain training data.</S><S sid ="121" ssid = "14">For the automatic scoring method BLEU  we can distinguish three quarters of the systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'136'", "'143'", "'129'", "'121'"]
'1'
'136'
'143'
'129'
'121'
['1', '136', '143', '129', '121']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S sid ="86" ssid = "2">The average fluency judgement per judge ranged from 2.33 to 3.67  the average adequacy judgement ranged from 2.56 to 4.13.</S><S sid ="150" ssid = "43">For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French)  at least as measured by BLEU.</S><S sid ="95" ssid = "11">The judgement of 4 in the first case will go to a vastly better system output than in the second case.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'86'", "'150'", "'95'", "'136'"]
'2'
'86'
'150'
'95'
'136'
['2', '86', '150', '95', '136']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "1">was done by the participants.</S><S sid ="67" ssid = "6">Participants and other volunteers contributed about 180 hours of labor in the manual evaluation.</S><S sid ="29" ssid = "22">About half of the participants of last year’s shared task participated again.</S><S sid ="78" ssid = "17">Judges where excluded from assessing the quality of MT systems that were submitted by their institution.</S><S sid ="168" ssid = "61">Annotators argued for the importance of having correct and even multiple references.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'67'", "'29'", "'78'", "'168'"]
'3'
'67'
'29'
'78'
'168'
['3', '67', '29', '78', '168']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "2">This revealed interesting clues about the properties of automatic and manual scoring.</S><S sid ="146" ssid = "39">This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.</S><S sid ="168" ssid = "61">Annotators argued for the importance of having correct and even multiple references.</S><S sid ="176" ssid = "7">Human judges also pointed out difficulties with the evaluation of long sentences.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'146'", "'168'", "'176'", "'134'"]
'4'
'146'
'168'
'176'
'134'
['4', '146', '168', '176', '134']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "3">• We evaluated translation from English  in addition to into English.</S><S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S><S sid ="152" ssid = "45">The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'6'", "'152'", "'170'", "'162'"]
'5'
'6'
'152'
'170'
'162'
['5', '6', '152', '170', '162']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S><S sid ="5" ssid = "3">• We evaluated translation from English  in addition to into English.</S><S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="79" ssid = "18">Sentences and systems were randomly selected and randomly shuffled for presentation.</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'5'", "'163'", "'79'", "'162'"]
'6'
'5'
'163'
'79'
'162'
['6', '5', '163', '79', '162']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "5">We dropped  however  one of the languages  Finnish  partly to keep the number of tracks manageable  partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation.</S><S sid ="165" ssid = "58">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid ="66" ssid = "5">In this shared task  we were also confronted with this problem  and since we had no funding for paying human judgements  we asked participants in the evaluation to share the burden.</S><S sid ="47" ssid = "13">Because of this  we retokenized and lowercased submitted output with our own tokenizer  which was also used to prepare the training and test data.</S><S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'165'", "'66'", "'47'", "'125'"]
'7'
'165'
'66'
'47'
'125'
['7', '165', '66', '47', '125']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "1">The evaluation framework for the shared task is similar to the one used in last year’s shared task.</S><S sid ="57" ssid = "23">We are therefore applying a different method  which has been used at the 2005 DARPA/NIST evaluation.</S><S sid ="29" ssid = "22">About half of the participants of last year’s shared task participated again.</S><S sid ="31" ssid = "24">Compared to last year’s shared task  the participants represent more long-term research efforts.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'57'", "'29'", "'31'", "'28'"]
'8'
'57'
'29'
'31'
'28'
['8', '57', '29', '31', '28']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "2">Training and testing is based on the Europarl corpus.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="143" ssid = "36">For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.</S><S sid ="21" ssid = "14">The out-of-domain test set differs from the Europarl data in various ways.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'28'", "'170'", "'143'", "'21'"]
'9'
'28'
'170'
'143'
'21'
['9', '28', '170', '143', '21']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "3">Figure 1 provides some statistics about this corpus.</S><S sid ="20" ssid = "13">For statistics on this test set  refer to Figure 1.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="4" ssid = "2">This revealed interesting clues about the properties of automatic and manual scoring.</S><S sid ="120" ssid = "13">In Figure 4  we displayed the number of system comparisons  for which we concluded statistical significance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'20'", "'134'", "'4'", "'120'"]
'10'
'20'
'134'
'4'
'120'
['10', '20', '134', '4', '120']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "4">To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="161" ssid = "54">Since all long sentence translation are somewhat muddled  even a contrastive evaluation between systems was difficult.</S><S sid ="152" ssid = "45">The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.</S><S sid ="31" ssid = "24">Compared to last year’s shared task  the participants represent more long-term research efforts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'28'", "'161'", "'152'", "'31'"]
'11'
'28'
'161'
'152'
'31'
['11', '28', '161', '152', '31']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "5">To summarize  we provided: The performance of the baseline system is similar to the best submissions in last year’s shared task.</S><S sid ="141" ssid = "34">In-domain Systran scores on this metric are lower than all statistical systems  even the ones that have much worse human scores.</S><S sid ="8" ssid = "1">The evaluation framework for the shared task is similar to the one used in last year’s shared task.</S><S sid ="81" ssid = "20">This is less than the 694 judgements 2004 DARPA/NIST evaluation  or the 532 judgements in the 2005 DARPA/NIST evaluation.</S><S sid ="151" ssid = "44">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages  as demonstrated by the results for English-German and English-French.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'141'", "'8'", "'81'", "'151'"]
'12'
'141'
'8'
'81'
'151'
['12', '141', '8', '81', '151']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "6">We are currently working on a complete open source implementation of a training and decoding system  which should become available over the summer. pus  from which also the in-domain test set is taken.</S><S sid ="33" ssid = "26">While building a machine translation system is a serious undertaking  in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.</S><S sid ="43" ssid = "9">At the very least  we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.</S><S sid ="74" ssid = "13">While we had up to 11 submissions for a translation direction  we did decide against presenting all 11 system outputs to the human judge.</S><S sid ="42" ssid = "8">It was our hope that this competition  which included the manual and automatic evaluation of statistical systems and one rulebased commercial system  will give further insight into the relation between automatic and manual evaluation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'33'", "'43'", "'74'", "'42'"]
'13'
'33'
'43'
'74'
'42'
['13', '33', '43', '74', '42']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "7">There is twice as much language modelling data  since training data for the machine translation system is filtered against sentences of length larger than 40 words.</S><S sid ="165" ssid = "58">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid ="58" ssid = "24">We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set  53 blocks for the out-of-domain test set)  check for each block  if one system has a higher BLEU score than the other  and then use the sign test.</S><S sid ="151" ssid = "44">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages  as demonstrated by the results for English-German and English-French.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'165'", "'58'", "'151'", "'155'"]
'14'
'165'
'58'
'151'
'155'
['14', '165', '58', '151', '155']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "8">Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.</S><S sid ="21" ssid = "14">The out-of-domain test set differs from the Europarl data in various ways.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="142" ssid = "35">Surprisingly  this effect is much less obvious for out-of-domain test data.</S><S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'21'", "'28'", "'142'", "'18'"]
'15'
'21'
'28'
'142'
'18'
['15', '21', '28', '142', '18']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="165" ssid = "58">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid ="14" ssid = "7">There is twice as much language modelling data  since training data for the machine translation system is filtered against sentences of length larger than 40 words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'126'", "'18'", "'165'", "'14'"]
'16'
'126'
'18'
'165'
'14'
['16', '126', '18', '165', '14']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="69" ssid = "8">We settled on contrastive evaluations of 5 system outputs for a single test sentence.</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="19" ssid = "12">We aligned the texts at a sentence level across all four languages  resulting in 1064 sentence per language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'69'", "'162'", "'170'", "'19'"]
'17'
'69'
'162'
'170'
'19'
['17', '69', '162', '170', '19']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="15" ssid = "8">Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'126'", "'16'", "'15'", "'155'"]
'18'
'126'
'16'
'15'
'155'
['18', '126', '16', '15', '155']
parsed_discourse_facet ['method_citation']
['The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.']
['In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.', 'For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.', 'Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00055', '(95%-conf.int.', '0.00055', '-', '0.00055)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00107', '(95%-conf.int.', '0.00107', '-', '0.00107)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:45', 'F:1']
['The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'All systems (except for Systran  which was not tuned to Europarl) did considerably worse on outof-domain training data.', 'The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).', 'For the automatic scoring method BLEU  we can distinguish three quarters of the systems.', 'For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.']
['system', 'ROUGE-S*', 'Average_R:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Average_P:', '0.16364', '(95%-conf.int.', '0.16364', '-', '0.16364)']
['system', 'ROUGE-S*', 'Average_F:', '0.00002', '(95%-conf.int.', '0.00002', '-', '0.00002)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:973710', 'P:55', 'F:9']
['One annotator suggested that this was the case for as much as 10% of our test sentences.']
['Training and testing is based on the Europarl corpus.', 'Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.', 'For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.', 'The out-of-domain test set differs from the Europarl data in various ways.', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:15', 'F:0']
['In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.']
['was done by the participants.', 'Judges where excluded from assessing the quality of MT systems that were submitted by their institution.', 'Annotators argued for the importance of having correct and even multiple references.', u'About half of the participants of last year\xe2\u20ac\u2122s shared task participated again.', 'Participants and other volunteers contributed about 180 hours of labor in the manual evaluation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00230', '(95%-conf.int.', '0.00230', '-', '0.00230)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00380', '(95%-conf.int.', '0.00380', '-', '0.00380)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:91', 'F:1']
['The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages, as demonstrated by the results for English-German and English-French.']
['This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.', 'Human judges also pointed out difficulties with the evaluation of long sentences.', 'Annotators argued for the importance of having correct and even multiple references.', 'This revealed interesting clues about the properties of automatic and manual scoring.', 'Again  we can compute average scores for all systems for the different language pairs (Figure 6).']
['system', 'ROUGE-S*', 'Average_R:', '0.00135', '(95%-conf.int.', '0.00135', '-', '0.00135)']
['system', 'ROUGE-S*', 'Average_P:', '0.00735', '(95%-conf.int.', '0.00735', '-', '0.00735)']
['system', 'ROUGE-S*', 'Average_F:', '0.00228', '(95%-conf.int.', '0.00228', '-', '0.00228)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:136', 'F:1']
['While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.']
['While building a machine translation system is a serious undertaking  in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.', 'We are currently working on a complete open source implementation of a training and decoding system  which should become available over the summer. pus  from which also the in-domain test set is taken.', 'While we had up to 11 submissions for a translation direction  we did decide against presenting all 11 system outputs to the human judge.', 'It was our hope that this competition  which included the manual and automatic evaluation of statistical systems and one rulebased commercial system  will give further insight into the relation between automatic and manual evaluation.', 'At the very least  we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.']
['system', 'ROUGE-S*', 'Average_R:', '0.00769', '(95%-conf.int.', '0.00769', '-', '0.00769)']
['system', 'ROUGE-S*', 'Average_P:', '0.06926', '(95%-conf.int.', '0.06926', '-', '0.06926)']
['system', 'ROUGE-S*', 'Average_F:', '0.01385', '(95%-conf.int.', '0.01385', '-', '0.01385)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:231', 'F:16']
['The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.', 'The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.', u'\xe2\u20ac\xa2 We evaluated translation from English  in addition to into English.', 'A few annotators suggested to break up long sentences into clauses and evaluate these separately.', 'English was again paired with German  French  and Spanish.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:66', 'F:0']
['For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.']
['The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages  as demonstrated by the results for English-German and English-French.', u'The evaluation framework for the shared task is similar to the one used in last year\u2019s shared task.', u'To summarize  we provided: The performance of the baseline system is similar to the best submissions in last year\u2019s shared task.', 'This is less than the 694 judgements 2004 DARPA/NIST evaluation  or the 532 judgements in the 2005 DARPA/NIST evaluation.', 'In-domain Systran scores on this metric are lower than all statistical systems  even the ones that have much worse human scores.']
['system', 'ROUGE-S*', 'Average_R:', '0.00058', '(95%-conf.int.', '0.00058', '-', '0.00058)']
['system', 'ROUGE-S*', 'Average_P:', '0.03571', '(95%-conf.int.', '0.03571', '-', '0.03571)']
['system', 'ROUGE-S*', 'Average_F:', '0.00115', '(95%-conf.int.', '0.00115', '-', '0.00115)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:28', 'F:1']
['The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data.']
['Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.', 'Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.', 'The out-of-domain test set differs from the Europarl data in various ways.', 'In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.', 'Surprisingly  this effect is much less obvious for out-of-domain test data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00580', '(95%-conf.int.', '0.00580', '-', '0.00580)']
['system', 'ROUGE-S*', 'Average_P:', '0.13333', '(95%-conf.int.', '0.13333', '-', '0.13333)']
['system', 'ROUGE-S*', 'Average_F:', '0.01111', '(95%-conf.int.', '0.01111', '-', '0.01111)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:45', 'F:6']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
[u'However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate \xe2\u20ac\u201d due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).', 'In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.', 'There is twice as much language modelling data  since training data for the machine translation system is filtered against sentences of length larger than 40 words.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00167', '(95%-conf.int.', '0.00167', '-', '0.00167)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.00327', '(95%-conf.int.', '0.00327', '-', '0.00327)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:55', 'F:5']
['For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.']
['To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.', u'Compared to last year\xe2\u20ac\u2122s shared task  the participants represent more long-term research efforts.', 'The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.', 'Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.', 'Since all long sentence translation are somewhat muddled  even a contrastive evaluation between systems was difficult.']
['system', 'ROUGE-S*', 'Average_R:', '0.00070', '(95%-conf.int.', '0.00070', '-', '0.00070)']
['system', 'ROUGE-S*', 'Average_P:', '0.03571', '(95%-conf.int.', '0.03571', '-', '0.03571)']
['system', 'ROUGE-S*', 'Average_F:', '0.00137', '(95%-conf.int.', '0.00137', '-', '0.00137)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:28', 'F:1']
['For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.']
['The judgement of 4 in the first case will go to a vastly better system output than in the second case.', 'For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French)  at least as measured by BLEU.', u'0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 \u2022systran \u2022 ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency \u2022systran \u2022nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p \ufffd 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain \u2022upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv 23 24 25 26 27 28 29 30 31 32 \u2022upc-mr \u2022utd \u2022upc-jmc \u2022uedin-birch \u2022ntt \u2022rali \u2022uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy \u2022upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency \u2022ntt \u2022nrc \u2022rali \u2022uedin-birch -0.2 -0.3 -0.5 \u2022upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr \u2022 rali Fluency -0.4 \u2022upc-mr utd \u2022upc-jmc -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 \u2022upv 0.5 0.4 \u2022systran \u2022upc-mr \u2022 \u2022rali 0.3 \u2022ntt 0.2 0.1 -0.0 -0.1 \u2022systran \u2022upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022upv \u2022systran \u2022upc-mr \u2022 Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022systran \u2022ntt', 'The average fluency judgement per judge ranged from 2.33 to 3.67  the average adequacy judgement ranged from 2.56 to 4.13.', 'The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).']
['system', 'ROUGE-S*', 'Average_R:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Average_P:', '0.03571', '(95%-conf.int.', '0.03571', '-', '0.03571)']
['system', 'ROUGE-S*', 'Average_F:', '0.00002', '(95%-conf.int.', '0.00002', '-', '0.00002)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:116403', 'P:28', 'F:1']
['The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:']
['We aligned the texts at a sentence level across all four languages  resulting in 1064 sentence per language.', 'We settled on contrastive evaluations of 5 system outputs for a single test sentence.', 'A few annotators suggested to break up long sentences into clauses and evaluate these separately.', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:28', 'F:0']
0.0465253842575 0.00158923075701 0.00291846151601





input/ref/Task1/J01-2004_swastika.csv
input/res/Task1/J01-2004.csv
parsing: input/ref/Task1/J01-2004_swastika.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="3">In the past few years, however, some improvements have been made over these language models through the use of statistical methods of natural language processing, and the development of innovative, linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
<S sid="215" ssid="119">The first word in the string remaining to be parsed, w1, we will call the look-ahead word.</S>
original cit marker offset is 0
new cit marker offset is 0



['215']
215
['215']
parsed_discourse_facet ['method_citation']
    <S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



['302']
302
['302']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="100" ssid="4">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['method_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S sid ="1" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="7" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="209" ssid = "113">This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).</S><S sid ="132" ssid = "36">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid ="277" ssid = "33">The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989 860 words  39 832 sentences) of the Penn Treebank (Marcus  Santorini  and Marcinkiewicz 1993) served as the training data  Section 24 (34 199 words  1 346 sentences) as the held-out data for parameter estimation  and Section 23 (59 100 words  2 416 sentences) as the test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'7'", "'209'", "'132'", "'277'"]
'1'
'7'
'209'
'132'
'277'
['1', '7', '209', '132', '277']
parsed_discourse_facet ['aim_citation']
<S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="41" ssid = "29">There will also be a brief review of previous work using syntactic information for language modeling  before we introduce our model in Section 4.</S><S sid ="18" ssid = "6">Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture long-distance dependencies for language modeling will be reviewed.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'2'", "'41'", "'18'", "'4'"]
'8'
'2'
'41'
'18'
'4'
['8', '2', '41', '18', '4']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="9" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'390'", "'10'", "'4'"]
'3'
'9'
'390'
'10'
'4'
['3', '9', '390', '10', '4']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="100" ssid = "4">The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'100'", "'2'", "'8'"]
'10'
'4'
'100'
'2'
'8'
['10', '4', '100', '2', '8']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="11" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="379" ssid = "135">While the improvements over the trigram model in these trials are modest  they do indicate that our model is robust enough to provide good information even in the face of noisy input.</S><S sid ="300" ssid = "56">The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'11'", "'379'", "'300'", "'404'"]
'5'
'11'
'379'
'300'
'404'
['5', '11', '379', '300', '404']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="6" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="39" ssid = "27">The top-down guidance that is provided makes this approach quite efficient in practice.</S><S sid ="388" ssid = "1">The empirical results presented above are quite encouraging  and the potential of this kind of approach both for parsing and language modeling seems very promising.</S><S sid ="114" ssid = "18">He was able to reduce both sentence and word error rates on the ATIS corpus using this method.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'6'", "'39'", "'388'", "'114'"]
'12'
'6'
'39'
'388'
'114'
['12', '6', '39', '388', '114']
parsed_discourse_facet ['method_citation']
<S sid ="1" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="7" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="209" ssid = "113">This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).</S><S sid ="132" ssid = "36">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid ="277" ssid = "33">The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989 860 words  39 832 sentences) of the Penn Treebank (Marcus  Santorini  and Marcinkiewicz 1993) served as the training data  Section 24 (34 199 words  1 346 sentences) as the held-out data for parameter estimation  and Section 23 (59 100 words  2 416 sentences) as the test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'7'", "'209'", "'132'", "'277'"]
'1'
'7'
'209'
'132'
'277'
['1', '7', '209', '132', '277']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="41" ssid = "29">There will also be a brief review of previous work using syntactic information for language modeling  before we introduce our model in Section 4.</S><S sid ="18" ssid = "6">Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture long-distance dependencies for language modeling will be reviewed.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'2'", "'41'", "'18'", "'4'"]
'8'
'2'
'41'
'18'
'4'
['8', '2', '41', '18', '4']
parsed_discourse_facet ['results_citation']
<S sid ="3" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="9" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'390'", "'10'", "'4'"]
'3'
'9'
'390'
'10'
'4'
['3', '9', '390', '10', '4']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="100" ssid = "4">The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'100'", "'2'", "'8'"]
'10'
'4'
'100'
'2'
'8'
['10', '4', '100', '2', '8']
parsed_discourse_facet ['method_citation']
<S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="11" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="379" ssid = "135">While the improvements over the trigram model in these trials are modest  they do indicate that our model is robust enough to provide good information even in the face of noisy input.</S><S sid ="300" ssid = "56">The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'11'", "'379'", "'300'", "'404'"]
'5'
'11'
'379'
'300'
'404'
['5', '11', '379', '300', '404']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="6" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="39" ssid = "27">The top-down guidance that is provided makes this approach quite efficient in practice.</S><S sid ="388" ssid = "1">The empirical results presented above are quite encouraging  and the potential of this kind of approach both for parsing and language modeling seems very promising.</S><S sid ="114" ssid = "18">He was able to reduce both sentence and word error rates on the ATIS corpus using this method.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'6'", "'39'", "'388'", "'114'"]
'12'
'6'
'39'
'388'
'114'
['12', '6', '39', '388', '114']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "1">With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.</S><S sid ="14" ssid = "2">Perhaps one reason for this is that  until relatively recently  few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.</S><S sid ="15" ssid = "3">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S><S sid ="383" ssid = "139">It was selected with the goal of high parser accuracy; but in this new domain  parser accuracy is a secondary measure of performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'14'", "'15'", "'404'", "'383'"]
'13'
'14'
'15'
'404'
'383'
['13', '14', '15', '404', '383']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "2">Perhaps one reason for this is that  until relatively recently  few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.</S><S sid ="13" ssid = "1">With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.</S><S sid ="300" ssid = "56">The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.</S><S sid ="15" ssid = "3">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid ="319" ssid = "75">The hope  however  is that the improved parsing model provided by our conditional probability model will cause the distribution over structures to be more peaked  thus enabling us to capture more of the total probability mass  and making this a fairly snug upper bound on the perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'13'", "'300'", "'15'", "'319'"]
'14'
'13'
'300'
'15'
'319'
['14', '13', '300', '15', '319']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "3">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid ="97" ssid = "1">There have been attempts to jump over adjacent words to words farther back in the left context  without the use of dependency links or syntactic structure  for example Saul and Pereira (1997) and Rosenfeld (1996  1997).</S><S sid ="13" ssid = "1">With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.</S><S sid ="38" ssid = "26">This has lead us to a formulation of the conditional probability model in terms of values returned from tree-walking functions that themselves are contextually sensitive.</S><S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'97'", "'13'", "'38'", "'5'"]
'15'
'97'
'13'
'38'
'5'
['15', '97', '13', '38', '5']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "4">While language models built around shallow local dependencies are still the standard in state-of-the-art speech recognition systems  there is reason to hope that better language models can and will be developed by computational linguists for this task.</S><S sid ="397" ssid = "10">A top-down parser  in contrast to a standard bottom-up chart parser  has enough information to predict empty categories only where they are likely to occur.</S><S sid ="139" ssid = "43">This on-line characteristic allows our language model to be interpolated on a word-by-word basis with other models  such as the trigram  yielding further improvements.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'397'", "'139'", "'4'", "'10'"]
'16'
'397'
'139'
'4'
'10'
['16', '397', '139', '4', '10']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "5">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S sid ="132" ssid = "36">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid ="88" ssid = "46">This section will briefly introduce language modeling for statistical speech recognition.'</S><S sid ="40" ssid = "28">The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.</S><S sid ="247" ssid = "3">Perplexity is a standard measure within the speech recognition community for comparing language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'132'", "'88'", "'40'", "'247'"]
'17'
'132'
'88'
'40'
'247'
['17', '132', '88', '40', '247']
parsed_discourse_facet ['method_citation']
['First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.']
['Perplexity is a standard measure within the speech recognition community for comparing language models.', 'The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.', "This section will briefly introduce language modeling for statistical speech recognition.'", 'The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.', 'This paper will examine language modeling for speech recognition from a natural language processing point of view.']
['system', 'ROUGE-S*', 'Average_R:', '0.00235', '(95%-conf.int.', '0.00235', '-', '0.00235)']
['system', 'ROUGE-S*', 'Average_P:', '0.00923', '(95%-conf.int.', '0.00923', '-', '0.00923)']
['system', 'ROUGE-S*', 'Average_F:', '0.00375', '(95%-conf.int.', '0.00375', '-', '0.00375)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:325', 'F:3']
['The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.']
['With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.', 'Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.', 'In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.', 'Perhaps one reason for this is that  until relatively recently  few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.', 'It was selected with the goal of high parser accuracy; but in this new domain  parser accuracy is a secondary measure of performance.']
['system', 'ROUGE-S*', 'Average_R:', '0.00316', '(95%-conf.int.', '0.00316', '-', '0.00316)']
['system', 'ROUGE-S*', 'Average_P:', '0.02849', '(95%-conf.int.', '0.02849', '-', '0.02849)']
['system', 'ROUGE-S*', 'Average_F:', '0.00570', '(95%-conf.int.', '0.00570', '-', '0.00570)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:351', 'F:10']
['The first word in the string remaining to be parsed, w1, we will call the look-ahead word.']
['This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.', 'This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).', 'The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989 860 words  39 832 sentences) of the Penn Treebank (Marcus  Santorini  and Marcinkiewicz 1993) served as the training data  Section 24 (34 199 words  1 346 sentences) as the held-out data for parameter estimation  and Section 23 (59 100 words  2 416 sentences) as the test data.', 'This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.', 'The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.']
['system', 'ROUGE-S*', 'Average_R:', '0.00121', '(95%-conf.int.', '0.00121', '-', '0.00121)']
['system', 'ROUGE-S*', 'Average_P:', '0.16667', '(95%-conf.int.', '0.16667', '-', '0.16667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00241', '(95%-conf.int.', '0.00241', '-', '0.00241)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4950', 'P:36', 'F:6']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.', 'This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).', 'The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989 860 words  39 832 sentences) of the Penn Treebank (Marcus  Santorini  and Marcinkiewicz 1993) served as the training data  Section 24 (34 199 words  1 346 sentences) as the held-out data for parameter estimation  and Section 23 (59 100 words  2 416 sentences) as the test data.', 'This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.', 'The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.']
['system', 'ROUGE-S*', 'Average_R:', '0.00061', '(95%-conf.int.', '0.00061', '-', '0.00061)']
['system', 'ROUGE-S*', 'Average_P:', '0.03297', '(95%-conf.int.', '0.03297', '-', '0.03297)']
['system', 'ROUGE-S*', 'Average_F:', '0.00119', '(95%-conf.int.', '0.00119', '-', '0.00119)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4950', 'P:91', 'F:3']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.', 'While the improvements over the trigram model in these trials are modest  they do indicate that our model is robust enough to provide good information even in the face of noisy input.', 'The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.', 'Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.', 'Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00032', '(95%-conf.int.', '0.00032', '-', '0.00032)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00063', '(95%-conf.int.', '0.00063', '-', '0.00063)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:91', 'F:1']
["In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds."]
['A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.', 'The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.', 'The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.', 'A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.', 'The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.']
['system', 'ROUGE-S*', 'Average_R:', '0.00538', '(95%-conf.int.', '0.00538', '-', '0.00538)']
['system', 'ROUGE-S*', 'Average_P:', '0.29487', '(95%-conf.int.', '0.29487', '-', '0.29487)']
['system', 'ROUGE-S*', 'Average_F:', '0.01056', '(95%-conf.int.', '0.01056', '-', '0.01056)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4278', 'P:78', 'F:23']
['Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).']
['The empirical results presented above are quite encouraging  and the potential of this kind of approach both for parsing and language modeling seems very promising.', 'He was able to reduce both sentence and word error rates on the ATIS corpus using this method.', 'The top-down guidance that is provided makes this approach quite efficient in practice.', 'A small recognition experiment also demonstrates the utility of the model.', 'A small recognition experiment also demonstrates the utility of the model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:300', 'F:0']
['In the past few years, however, some improvements have been made over these language models through the use of statistical methods of natural language processing, and the development of innovative, linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.']
['The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.', 'The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.', 'A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.', 'Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture long-distance dependencies for language modeling will be reviewed.', 'There will also be a brief review of previous work using syntactic information for language modeling  before we introduce our model in Section 4.']
['system', 'ROUGE-S*', 'Average_R:', '0.00649', '(95%-conf.int.', '0.00649', '-', '0.00649)']
['system', 'ROUGE-S*', 'Average_P:', '0.06000', '(95%-conf.int.', '0.06000', '-', '0.06000)']
['system', 'ROUGE-S*', 'Average_F:', '0.01171', '(95%-conf.int.', '0.01171', '-', '0.01171)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:300', 'F:18']
0.0754024990575 0.0024399999695 0.00449374994383





input/ref/Task1/D10-1044_swastika.csv
input/res/Task1/D10-1044.csv
parsing: input/ref/Task1/D10-1044_swastika.csv
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
<S sid="71" ssid="8">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c&#955;(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



['71']
71
['71']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
<S sid="71" ssid="8">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c&#955;(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



['71']
71
['71']
parsed_discourse_facet ['result_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="9">An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (Och, 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



['45']
45
['45']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



['75']
75
['75']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
  <S sid="42" ssid="6">The natural baseline approach is to concatenate data from IN and OUT.</S>
original cit marker offset is 0
new cit marker offset is 0



['42']
42
['42']
parsed_discourse_facet ['aim_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/D10-1044.csv
<S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="145" ssid = "2">Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="62" ssid = "26">To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'65'", "'145'", "'64'", "'62'"]
'1'
'65'
'145'
'64'
'62'
['1', '65', '145', '64', '62']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S><S sid ="23" ssid = "20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="153" ssid = "10">Finally  we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.</S><S sid ="68" ssid = "5">First  we learn weights on individual phrase pairs rather than sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'23'", "'20'", "'153'", "'68'"]
'2'
'23'
'20'
'153'
'68'
['2', '23', '20', '153', '68']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">We incorporate instance weighting into a mixture-model framework  and find that it yields consistent improvements over a wide range of baselines.</S><S sid ="150" ssid = "7">In both cases  the instanceweighting approach improved over a wide range of baselines  giving gains of over 2 BLEU points over the best non-adapted baseline  and gains of between 0.6 and 1.8 over an equivalent mixture model (with an identical training procedure but without instance weighting).</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'150'", "'65'", "'38'", "'1'"]
'3'
'150'
'65'
'38'
'1'
['3', '150', '65', '38', '1']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'10'", "'6'", "'24'", "'17'"]
'4'
'10'
'6'
'24'
'17'
['4', '10', '6', '24', '17']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "2">Even when there is training data available in the domain of interest  there is often additional data from other domains that could in principle be used to improve performance.</S><S sid ="138" ssid = "7">However  for multinomial models like our LMs and TMs  there is a one to one correspondence between instances and features  eg the correspondence between a phrase pair (s  t) and its conditional multinomial probability p(s1t).</S><S sid ="9" ssid = "6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="39" ssid = "3">Thus  provided at least this amount of IN data is available—as it is in our setting—adapting these weights is straightforward.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'138'", "'9'", "'6'", "'39'"]
'5'
'138'
'9'
'6'
'39'
['5', '138', '9', '6', '39']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'17'", "'24'", "'18'", "'4'"]
'6'
'17'
'24'
'18'
'4'
['6', '17', '24', '18', '4']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="9" ssid = "6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'24'", "'21'", "'4'", "'9'"]
'7'
'24'
'21'
'4'
'9'
['7', '24', '21', '4', '9']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'10'", "'18'", "'6'", "'20'"]
'8'
'10'
'18'
'6'
'20'
['8', '10', '18', '6', '20']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S><S sid ="144" ssid = "1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S><S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="42" ssid = "6">The natural baseline approach is to concatenate data from IN and OUT.</S><S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'144'", "'1'", "'42'", "'2'"]
'9'
'144'
'1'
'42'
'2'
['9', '144', '1', '42', '2']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="29" ssid = "26">This is a simple and effective alternative to setting weights discriminatively to maximize a metric such as BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'18'", "'6'", "'29'"]
'10'
'4'
'18'
'6'
'29'
['10', '4', '18', '6', '29']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="44" ssid = "8">When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.</S><S sid ="92" ssid = "29">6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'18'", "'6'", "'44'", "'92'"]
'11'
'18'
'6'
'44'
'92'
['11', '18', '6', '44', '92']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "9">For simplicity  we assume that OUT is homogeneous.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="92" ssid = "29">6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.</S><S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'18'", "'11'", "'92'", "'16'"]
'12'
'18'
'11'
'92'
'16'
['12', '18', '11', '92', '16']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "10">The techniques we develop can be extended in a relatively straightforward manner to the more general case when OUT consists of multiple sub-domains.</S><S sid ="139" ssid = "8">As mentioned above  it is not obvious how to apply Daum´e’s approach to multinomials  which do not have a mechanism for combining split features.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="82" ssid = "19">This is not unreasonable given the application to phrase pairs from OUT  but it suggests that an interesting alternative might be to use a plain log-linear weighting function exp(Ei Aifi(s  t))  with outputs in [0  oo].</S><S sid ="32" ssid = "29">This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'139'", "'21'", "'82'", "'32'"]
'13'
'139'
'21'
'82'
'32'
['13', '139', '21', '82', '32']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "11">There is a fairly large body of work on SMT adaptation.</S><S sid ="141" ssid = "10">Moving beyond directly related work  major themes in SMT adaptation include the IR (Hildebrand et al.  2005; L¨u et al.  2007; Zhao et al.  2004) and mixture (Finch and Sumita  2008; Foster and Kuhn  2007; Koehn and Schroeder  2007; L¨u et al.  2007) approaches for LMs and TMs described above  as well as methods for exploiting monolingual in-domain text  typically by translating it automatically and then performing self training (Bertoldi and Federico  2009; Ueffing et al.  2007; Schwenk and Senellart  2009).</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="34" ssid = "31">Section 2 describes our baseline techniques for SMT adaptation  and section 3 describes the instance-weighting approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'141'", "'4'", "'10'", "'34'"]
'14'
'141'
'4'
'10'
'34'
['14', '141', '4', '10', '34']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "12">We introduce several new ideas.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="27" ssid = "24">Finally  we make some improvements to baseline approaches.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="88" ssid = "25">We have not explored this strategy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'67'", "'27'", "'20'", "'88'"]
'15'
'67'
'27'
'20'
'88'
['15', '67', '27', '20', '88']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="42" ssid = "6">The natural baseline approach is to concatenate data from IN and OUT.</S><S sid ="68" ssid = "5">First  we learn weights on individual phrase pairs rather than sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'17'", "'24'", "'42'", "'68'"]
'16'
'17'
'24'
'42'
'68'
['16', '17', '24', '42', '68']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S><S sid ="100" ssid = "4">Figure 1 shows sample sentences from these domains  which are widely divergent.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'6'", "'24'", "'16'", "'100'"]
'17'
'6'
'24'
'16'
'100'
['17', '6', '24', '16', '100']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="44" ssid = "8">When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'11'", "'44'", "'6'", "'10'"]
'18'
'11'
'44'
'6'
'10'
['18', '11', '44', '6', '10']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "16">The idea of distinguishing between general and domain-specific examples is due to Daum´e and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daum´e’s (2007) work that splits each feature into domain-specific and general copies.</S><S sid ="73" ssid = "10">Jiang and Zhai (2007) suggest the following derivation  making use of the true OUT distribution po(s  t): where each fi(s  t) is a feature intended to charac- !0ˆ = argmax pf(s  t) log pθ(s|t) (8) terize the usefulness of (s  t)  weighted by Ai. θ s t pf(s  t)po(s  t) log pθ(s|t) The mixing parameters and feature weights (col- != argmax po (s  t) lectively 0) are optimized simultaneously using dev- θ s t pf(s  t)co(s  t) log pθ(s|t)  set maximum likelihood as before: !�argmax po (s  t) ! θ s t �ˆ = argmax ˜p(s  t) log p(s|t; 0).</S><S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'136'", "'73'", "'140'", "'64'"]
'19'
'136'
'73'
'140'
'64'
['19', '136', '73', '140', '64']
parsed_discourse_facet ['method_citation']
['To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.']
['Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.', 'This is a standard adaptation problem for SMT.', 'Domain adaptation is a common concern when optimizing empirical NLP applications.', 'Previous approaches have tried to find examples that are similar to the target domain.', 'Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.']
['system', 'ROUGE-S*', 'Average_R:', '0.01080', '(95%-conf.int.', '0.01080', '-', '0.01080)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01858', '(95%-conf.int.', '0.01858', '-', '0.01858)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:120', 'F:8']
['In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.', 'To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.', 'Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.', 'The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.', 'Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.']
['system', 'ROUGE-S*', 'Average_R:', '0.00410', '(95%-conf.int.', '0.00410', '-', '0.00410)']
['system', 'ROUGE-S*', 'Average_P:', '0.03810', '(95%-conf.int.', '0.03810', '-', '0.03810)']
['system', 'ROUGE-S*', 'Average_F:', '0.00740', '(95%-conf.int.', '0.00740', '-', '0.00740)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:210', 'F:8']
['Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.']
['This is a standard adaptation problem for SMT.', 'Domain adaptation is a common concern when optimizing empirical NLP applications.', 'Section 2 describes our baseline techniques for SMT adaptation  and section 3 describes the instance-weighting approach.', 'There is a fairly large body of work on SMT adaptation.', u'Moving beyond directly related work  major themes in SMT adaptation include the IR (Hildebrand et al.  2005; L\xa8u et al.  2007; Zhao et al.  2004) and mixture (Finch and Sumita  2008; Foster and Kuhn  2007; Koehn and Schroeder  2007; L\xa8u et al.  2007) approaches for LMs and TMs described above  as well as methods for exploiting monolingual in-domain text  typically by translating it automatically and then performing self training (Bertoldi and Federico  2009; Ueffing et al.  2007; Schwenk and Senellart  2009).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3403', 'P:78', 'F:0']
['To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.']
[u'In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material\u2014though adequate for reasonable performance\u2014is also available.', 'We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.', 'In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.', 'This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.', 'The natural baseline approach is to concatenate data from IN and OUT.']
['system', 'ROUGE-S*', 'Average_R:', '0.00263', '(95%-conf.int.', '0.00263', '-', '0.00263)']
['system', 'ROUGE-S*', 'Average_P:', '0.05000', '(95%-conf.int.', '0.05000', '-', '0.05000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00500', '(95%-conf.int.', '0.00500', '-', '0.00500)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:120', 'F:6']
['We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.']
[u'Daum\xc2\xb4e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.', 'We have not explored this strategy.', 'Finally  we make some improvements to baseline approaches.', 'We introduce several new ideas.', 'We extend the Matsoukas et al approach in several ways.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:325', 'P:45', 'F:0']
['We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.']
['It is difficult when IN and OUT are dissimilar  as they are in the cases we study.', 'This is less effective in our setting  where IN and OUT are disparate.', 'When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.', 'This is a standard adaptation problem for SMT.', 'Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:406', 'P:45', 'F:0']
['In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.', 'In both cases  the instanceweighting approach improved over a wide range of baselines  giving gains of over 2 BLEU points over the best non-adapted baseline  and gains of between 0.6 and 1.8 over an equivalent mixture model (with an identical training procedure but without instance weighting).', 'We incorporate instance weighting into a mixture-model framework  and find that it yields consistent improvements over a wide range of baselines.', 'The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.', 'Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.']
['system', 'ROUGE-S*', 'Average_R:', '0.00235', '(95%-conf.int.', '0.00235', '-', '0.00235)']
['system', 'ROUGE-S*', 'Average_P:', '0.03810', '(95%-conf.int.', '0.03810', '-', '0.03810)']
['system', 'ROUGE-S*', 'Average_F:', '0.00443', '(95%-conf.int.', '0.00443', '-', '0.00443)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3403', 'P:210', 'F:8']
0.0275528567492 0.00283999995943 0.00505857135631





input/ref/Task1/P11-1060_sweta.csv
input/res/Task1/P11-1060.csv
parsing: input/ref/Task1/P11-1060_sweta.csv
<S sid="8" ssid="4">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
 <S sid="45" ssid="21">The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["45'"]
45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="27">It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent&#8217;s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
 <S sid="166" ssid="51">Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language&#8212;think grounded al., 2010). compositional semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
 <S sid="8" ssid="4">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
 <S sid="11" ssid="7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["11'"]
11'
['11']
parsed_discourse_facet ['method_citation']
  <S sid="115" ssid="91">After training, given a new utterance x, our system outputs the most likely y, summing out the latent logical form z: argmaxy p&#952;(T)(y  |x, z &#8712; &#732;ZL,&#952;(T)).</S>
original cit marker offset is 0
new cit marker offset is 0



["115'"]
115'
['115']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="70">We now turn to the task of mapping natural language For the example in Figure 4(b), the de- utterances to DCS trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["94'"]
94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="157" ssid="42">Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["157'"]
157'
['157']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["106'"]
106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="52">In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="23">Next, we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S>
original cit marker offset is 0
new cit marker offset is 0



["138'"]
138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="16">The CSP has two types of constraints: (i) x &#8712; w(p) for each node x labeled with predicate p &#8712; P; and (ii) xj = yj0 (the j-th component of x must equal the j'-th component of y) for each edge (x, y) labeled with j0j &#8712; R. A solution to the CSP is an assignment of nodes to values that satisfies all the constraints.</S>
original cit marker offset is 0
new cit marker offset is 0



["40'"]
40'
['40']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["171'"]
171'
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S sid ="1" ssid = "1">Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="2" ssid = "2">In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'141'", "'15'", "'6'", "'2'"]
'1'
'141'
'15'
'6'
'2'
['1', '141', '15', '6', '2']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="117" ssid = "2">In each dataset  each sentence x is annotated with a Prolog logical form  which we use only to evaluate and get an answer y.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="149" ssid = "34">Think of DCS as a higher-level Our system learns lexical associations between programming language tailored to natural language  words and predicates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'3'", "'117'", "'100'", "'149'"]
'2'
'3'
'117'
'100'
'149'
['2', '3', '117', '100', '149']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="48" ssid = "24">In addition  trees enable efficient computation  thereby establishing a new connection between dependency syntax and efficient semantic evaluation.</S><S sid ="149" ssid = "34">Think of DCS as a higher-level Our system learns lexical associations between programming language tailored to natural language  words and predicates.</S><S sid ="2" ssid = "2">In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.</S><S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parent’s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'48'", "'149'", "'2'", "'51'"]
'3'
'48'
'149'
'2'
'51'
['3', '48', '149', '2', '51']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">On two stansemantic parsing benchmarks our system obtains the highest published accuracies  despite requiring no annotated logical forms.</S><S sid ="24" ssid = "20">Our system outperforms all existing systems despite using no annotated logical forms.</S><S sid ="136" ssid = "21">In fact  DCS performs comparably to even the version of SEMRESP trained using logical forms.</S><S sid ="139" ssid = "24">All other systems require logical forms as training data  whereas ours does not.</S><S sid ="135" ssid = "20">SEMRESP requires a lexicon of 1.42 words per non-value predicate  WordNet features  and syntactic parse trees; DCS requires only words for the domain-independent predicates (overall  around 0.5 words per non-value predicate)  POS tags  and very simple indicator features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'24'", "'136'", "'139'", "'135'"]
'4'
'24'
'136'
'139'
'135'
['4', '24', '136', '139', '135']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">What is the total population of the ten largest capitals in the US?</S><S sid ="52" ssid = "28">The denotation of the middle node is {s}  where s is all major cities.</S><S sid ="105" ssid = "81">The feature vector φ(x  z) is defined by sums of five simple indicator feature templates: (F1) a word triggers a predicate (e.g.  [city  city]); (F2) a word is under a relation (e.g.  [that  11]); (F3) a word is under a trace predicate (e.g.  [in  loc]); (F4) two predicates are linked via a relation in the left or right direction (e.g.  [city 11  loc  RIGHT]); and (F5) a predicate has a child relation (e.g.  [city  11]).</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S><S sid ="50" ssid = "26">For example  consider the phrase number of major cities  and suppose that number corresponds to the count predicate.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'52'", "'105'", "'62'", "'50'"]
'5'
'52'
'105'
'62'
'50'
['5', '52', '105', '62', '50']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="48" ssid = "24">In addition  trees enable efficient computation  thereby establishing a new connection between dependency syntax and efficient semantic evaluation.</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'169'", "'58'", "'48'", "'165'"]
'6'
'169'
'58'
'48'
'165'
['6', '169', '58', '48', '165']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Supervised semantic parsers (Zelle and Mooney  1996; Tang and Mooney  2001; Ge and Mooney  2005; Zettlemoyer and Collins  2005; Kate and Mooney  2007; Zettlemoyer and Collins  2007; Wong and Mooney  2007; Kwiatkowski et al.  2010) rely on manual annotation of logical forms  which is expensive.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="166" ssid = "51">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid ="125" ssid = "10">We used the same training-test splits as Zettlemoyer and Collins (2005) (600+280 for GEO and 500+140 for JOBS).</S><S sid ="161" ssid = "46">CCG (Steedman  2000)  in which semantic pars- The integration of natural language with denotaing is driven from the lexicon.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'8'", "'166'", "'125'", "'161'"]
'7'
'8'
'166'
'125'
'161'
['7', '8', '166', '125', '161']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="138" ssid = "23">Next  we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S><S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid ="18" ssid = "14">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'138'", "'9'", "'165'", "'18'"]
'8'
'138'
'9'
'165'
'18'
['8', '138', '9', '165', '18']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="20" ssid = "16">At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'58'", "'165'", "'26'", "'20'"]
'9'
'58'
'165'
'26'
'20'
['9', '58', '165', '26', '20']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="37" ssid = "13">The basic version of DCS restricts R to join and aggregate relations (see Table 1).</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'58'", "'169'", "'37'", "'26'"]
'10'
'58'
'169'
'37'
'26'
['10', '58', '169', '37', '26']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts)  producing an answer y.</S><S sid ="118" ssid = "3">This evaluation is done with respect to a world w. Recall that a world w maps each predicate p ∈ P to a set of tuples w(p).</S><S sid ="31" ssid = "7">Conceptually  a world is a relational database where each predicate is a relation (possibly infinite).</S><S sid ="38" ssid = "14">Let us start by considering a DCS tree z with only join relations.</S><S sid ="29" ssid = "5">Let P be a set of predicates (e.g.  state  count ∈ P)  which are just symbols.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'118'", "'31'", "'38'", "'29'"]
'11'
'118'
'31'
'38'
'29'
['11', '118', '31', '38', '29']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="45" ssid = "21">The logical forms in DCS are called DCS trees  where nodes are labeled with predicates  and edges are labeled with relations.</S><S sid ="32" ssid = "8">Define a special predicate ø with w(ø) = V. We represent functions by a set of inputoutput pairs  e.g.  w(count) = {(S  n) : n = |S|}.</S><S sid ="85" ssid = "61">Extraction allows us to return the set of consistent values of a marked non-root node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'169'", "'45'", "'32'", "'85'"]
'12'
'169'
'45'
'32'
'85'
['12', '169', '45', '32', '85']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x  y)  which is much cheaper to obtain than (x  z) pairs.</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S><S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="103" ssid = "79">To define the features  we technically need to augment each tree z ∈ ZL(x) with alignment information—namely  for each predicate in z  the span in x (if any) that triggered it.</S><S sid ="108" ssid = "84">However  in order to learn  we need to sum over {z ∈ ZL(x) : JzKw = y}  and unfortunately  the additional constraint JzKw = y does not factorize.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'62'", "'12'", "'103'", "'108'"]
'13'
'62'
'12'
'103'
'108'
['13', '62', '12', '103', '108']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.</S><S sid ="43" ssid = "19">Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter  2003).</S><S sid ="107" ssid = "83">Our model is arc-factored  so we can sum over all DCS trees in ZL(x) using dynamic programming.</S><S sid ="47" ssid = "23">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.</S><S sid ="59" ssid = "35">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'43'", "'107'", "'47'", "'59'"]
'14'
'43'
'107'
'47'
'59'
['14', '43', '107', '47', '59']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="142" ssid = "27">This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.</S><S sid ="163" ssid = "48">Feedback from the context; for example  the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x  y)  which means (Schuler  2003) and semantic parsing (Popescu et borders looks right for the first argument and left al.  2003; Clarke et al.  2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'100'", "'141'", "'142'", "'163'"]
'15'
'100'
'141'
'142'
'163'
['15', '100', '141', '142', '163']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Which one should we use?</S><S sid ="144" ssid = "29">Intuitions How is our system learning?</S><S sid ="142" ssid = "27">This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.</S><S sid ="95" ssid = "71">Our first question is: given notation of the DCS tree before execution is an utterance x  what trees z ∈ Z are permissible?</S><S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'144'", "'142'", "'95'", "'15'"]
'16'
'144'
'142'
'95'
'15'
['16', '144', '142', '95', '15']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">The dominant paradigm in compositional semantics is Montague semantics  which constructs lambda calculus forms in a bottom-up manner.</S><S sid ="36" ssid = "12">It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.</S><S sid ="37" ssid = "13">The basic version of DCS restricts R to join and aggregate relations (see Table 1).</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="25" ssid = "1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS)  which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'36'", "'37'", "'58'", "'25'"]
'17'
'36'
'37'
'58'
'25'
['17', '36', '37', '58', '25']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="20" ssid = "16">At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.</S><S sid ="167" ssid = "52">In DCS  we start with lexical triggers  which are 6 Conclusion more basic than CCG lexical entries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'165'", "'26'", "'20'", "'167'"]
'18'
'165'
'26'
'20'
'167'
['18', '165', '26', '20', '167']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "15">However  the logical forms there can become quite complex  and in the context of program induction  this would lead to an unwieldy search space.</S><S sid ="122" ssid = "7">For JOBS  if we use the standard Jobs database  close to half the y’s are empty  which makes it uninteresting.</S><S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parent’s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid ="14" ssid = "10">The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.</S><S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'122'", "'51'", "'14'", "'9'"]
'19'
'122'
'51'
'14'
'9'
['19', '122', '51', '14', '9']
parsed_discourse_facet ['method_citation']
['Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem.']
['This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.', 'Our model is arc-factored  so we can sum over all DCS trees in ZL(x) using dynamic programming.', 'The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).', 'The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.', 'Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter  2003).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:120', 'F:0']
['It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent&#8217;s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.']
['Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.', u'The feature vector \xcf\u2020(x  z) is defined by sums of five simple indicator feature templates: (F1) a word triggers a predicate (e.g.  [city  city]); (F2) a word is under a relation (e.g.  [that  11]); (F3) a word is under a trace predicate (e.g.  [in  loc]); (F4) two predicates are linked via a relation in the left or right direction (e.g.  [city 11  loc  RIGHT]); and (F5) a predicate has a child relation (e.g.  [city  11]).', 'What is the total population of the ten largest capitals in the US?', 'For example  consider the phrase number of major cities  and suppose that number corresponds to the count predicate.', 'The denotation of the middle node is {s}  where s is all major cities.']
['system', 'ROUGE-S*', 'Average_R:', '0.00290', '(95%-conf.int.', '0.00290', '-', '0.00290)']
['system', 'ROUGE-S*', 'Average_P:', '0.01724', '(95%-conf.int.', '0.01724', '-', '0.01724)']
['system', 'ROUGE-S*', 'Average_F:', '0.00496', '(95%-conf.int.', '0.00496', '-', '0.00496)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:406', 'F:7']
['Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.']
['However  we still model the logical form (now as a latent variable) to capture the complexities of language.', 'We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.', 'The basic version of DCS restricts R to join and aggregate relations (see Table 1).', 'We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.', 'The combination rules are encoded in the tems  despite using no annotated logical forms.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:105', 'F:0']
['Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.']
['As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.', 'On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.', 'Next  we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).', 'CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).', 'These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:55', 'F:0']
['This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.']
['However  the logical forms there can become quite complex  and in the context of program induction  this would lead to an unwieldy search space.', 'As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.', u'For JOBS  if we use the standard Jobs database  close to half the y\u2019s are empty  which makes it uninteresting.', u'It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parent\u2019s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.', 'The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.']
['system', 'ROUGE-S*', 'Average_R:', '0.00070', '(95%-conf.int.', '0.00070', '-', '0.00070)']
['system', 'ROUGE-S*', 'Average_P:', '0.01053', '(95%-conf.int.', '0.01053', '-', '0.01053)']
['system', 'ROUGE-S*', 'Average_F:', '0.00132', '(95%-conf.int.', '0.00132', '-', '0.00132)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:190', 'F:2']
["The CSP has two types of constraints: (i) x &#8712; w(p) for each node x labeled with predicate p &#8712; P; and (ii) xj = yj0 (the j-th component of x must equal the j'-th component of y) for each edge (x, y) labeled with j0j &#8712; R. A solution to the CSP is an assignment of nodes to values that satisfies all the constraints."]
['CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).', 'We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.', 'In DCS  we start with lexical triggers  which are 6 Conclusion more basic than CCG lexical entries.', 'At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.', 'These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:300', 'F:0']
['After training, given a new utterance x, our system outputs the most likely y, summing out the latent logical form z: argmaxy p&#952;(T)(y  |x, z &#8712; &#732;ZL,&#952;(T)).']
['As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.', 'These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.', 'We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.', 'At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.', 'We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.']
['system', 'ROUGE-S*', 'Average_R:', '0.00032', '(95%-conf.int.', '0.00032', '-', '0.00032)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00063', '(95%-conf.int.', '0.00063', '-', '0.00063)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:91', 'F:1']
['On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.']
['On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.', u'Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language\xe2\u20ac\u201dthink grounded al.  2010). compositional semantics.', 'We used the same training-test splits as Zettlemoyer and Collins (2005) (600+280 for GEO and 500+140 for JOBS).', 'Supervised semantic parsers (Zelle and Mooney  1996; Tang and Mooney  2001; Ge and Mooney  2005; Zettlemoyer and Collins  2005; Kate and Mooney  2007; Zettlemoyer and Collins  2007; Wong and Mooney  2007; Kwiatkowski et al.  2010) rely on manual annotation of logical forms  which is expensive.', 'CCG (Steedman  2000)  in which semantic pars- The integration of natural language with denotaing is driven from the lexicon.']
['system', 'ROUGE-S*', 'Average_R:', '0.02454', '(95%-conf.int.', '0.02454', '-', '0.02454)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.04791', '(95%-conf.int.', '0.04791', '-', '0.04791)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4278', 'P:105', 'F:105']
['In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries.']
['Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.', u'Our first question is: given notation of the DCS tree before execution is an utterance x  what trees z \xe2\u02c6\u02c6 Z are permissible?', 'Intuitions How is our system learning?', 'This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.', 'Which one should we use?']
['system', 'ROUGE-S*', 'Average_R:', '0.00568', '(95%-conf.int.', '0.00568', '-', '0.00568)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01047', '(95%-conf.int.', '0.01047', '-', '0.01047)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:528', 'P:45', 'F:3']
['On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.']
['Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.', 'In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.', 'Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).', 'Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.', 'Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.']
['system', 'ROUGE-S*', 'Average_R:', '0.00043', '(95%-conf.int.', '0.00043', '-', '0.00043)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00082', '(95%-conf.int.', '0.00082', '-', '0.00082)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:105', 'F:1']
['Next, we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).']
['We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS)  which captures the core idea of using trees to represent formal semantics.', 'The basic version of DCS restricts R to join and aggregate relations (see Table 1).', 'It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.', 'The dominant paradigm in compositional semantics is Montague semantics  which constructs lambda calculus forms in a bottom-up manner.', 'We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.']
['system', 'ROUGE-S*', 'Average_R:', '0.00461', '(95%-conf.int.', '0.00461', '-', '0.00461)']
['system', 'ROUGE-S*', 'Average_P:', '0.09890', '(95%-conf.int.', '0.09890', '-', '0.09890)']
['system', 'ROUGE-S*', 'Average_F:', '0.00881', '(95%-conf.int.', '0.00881', '-', '0.00881)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:91', 'F:9']
['Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language&#8212;think grounded al., 2010). compositional semantics.']
['We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.', 'These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.', 'The combination rules are encoded in the tems  despite using no annotated logical forms.', 'In addition  trees enable efficient computation  thereby establishing a new connection between dependency syntax and efficient semantic evaluation.', 'Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).']
['system', 'ROUGE-S*', 'Average_R:', '0.00702', '(95%-conf.int.', '0.00702', '-', '0.00702)']
['system', 'ROUGE-S*', 'Average_P:', '0.06324', '(95%-conf.int.', '0.06324', '-', '0.06324)']
['system', 'ROUGE-S*', 'Average_F:', '0.01264', '(95%-conf.int.', '0.01264', '-', '0.01264)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:253', 'F:16']
0.10642416578 0.00384999996792 0.00729666660586





input/ref/Task1/W06-3114_aakansha.csv
input/res/Task1/W06-3114.csv
parsing: input/ref/Task1/W06-3114_aakansha.csv
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="2">Training and testing is based on the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="2">Training and testing is based on the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'"]
'102'
['102']
parsed_discourse_facet ['method_citation']
<S sid="84" ssid="23">The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'"]
'84'
['84']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="4">To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="19">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="6">Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="3">&#8226; We evaluated translation from English, in addition to into English.</S>
    <S sid="6" ssid="4">English was again paired with German, French, and Spanish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'6'"]
'5'
'6'
['5', '6']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="143" ssid = "36">For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.</S><S sid ="129" ssid = "22">All systems (except for Systran  which was not tuned to Europarl) did considerably worse on outof-domain training data.</S><S sid ="121" ssid = "14">For the automatic scoring method BLEU  we can distinguish three quarters of the systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'136'", "'143'", "'129'", "'121'"]
'1'
'136'
'143'
'129'
'121'
['1', '136', '143', '129', '121']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S sid ="86" ssid = "2">The average fluency judgement per judge ranged from 2.33 to 3.67  the average adequacy judgement ranged from 2.56 to 4.13.</S><S sid ="150" ssid = "43">For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French)  at least as measured by BLEU.</S><S sid ="95" ssid = "11">The judgement of 4 in the first case will go to a vastly better system output than in the second case.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'86'", "'150'", "'95'", "'136'"]
'2'
'86'
'150'
'95'
'136'
['2', '86', '150', '95', '136']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "1">was done by the participants.</S><S sid ="67" ssid = "6">Participants and other volunteers contributed about 180 hours of labor in the manual evaluation.</S><S sid ="29" ssid = "22">About half of the participants of last year’s shared task participated again.</S><S sid ="78" ssid = "17">Judges where excluded from assessing the quality of MT systems that were submitted by their institution.</S><S sid ="168" ssid = "61">Annotators argued for the importance of having correct and even multiple references.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'67'", "'29'", "'78'", "'168'"]
'3'
'67'
'29'
'78'
'168'
['3', '67', '29', '78', '168']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "2">This revealed interesting clues about the properties of automatic and manual scoring.</S><S sid ="146" ssid = "39">This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.</S><S sid ="168" ssid = "61">Annotators argued for the importance of having correct and even multiple references.</S><S sid ="176" ssid = "7">Human judges also pointed out difficulties with the evaluation of long sentences.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'146'", "'168'", "'176'", "'134'"]
'4'
'146'
'168'
'176'
'134'
['4', '146', '168', '176', '134']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "3">• We evaluated translation from English  in addition to into English.</S><S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S><S sid ="152" ssid = "45">The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'6'", "'152'", "'170'", "'162'"]
'5'
'6'
'152'
'170'
'162'
['5', '6', '152', '170', '162']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S><S sid ="5" ssid = "3">• We evaluated translation from English  in addition to into English.</S><S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="79" ssid = "18">Sentences and systems were randomly selected and randomly shuffled for presentation.</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'5'", "'163'", "'79'", "'162'"]
'6'
'5'
'163'
'79'
'162'
['6', '5', '163', '79', '162']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "5">We dropped  however  one of the languages  Finnish  partly to keep the number of tracks manageable  partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation.</S><S sid ="165" ssid = "58">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid ="66" ssid = "5">In this shared task  we were also confronted with this problem  and since we had no funding for paying human judgements  we asked participants in the evaluation to share the burden.</S><S sid ="47" ssid = "13">Because of this  we retokenized and lowercased submitted output with our own tokenizer  which was also used to prepare the training and test data.</S><S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'165'", "'66'", "'47'", "'125'"]
'7'
'165'
'66'
'47'
'125'
['7', '165', '66', '47', '125']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "1">The evaluation framework for the shared task is similar to the one used in last year’s shared task.</S><S sid ="57" ssid = "23">We are therefore applying a different method  which has been used at the 2005 DARPA/NIST evaluation.</S><S sid ="29" ssid = "22">About half of the participants of last year’s shared task participated again.</S><S sid ="31" ssid = "24">Compared to last year’s shared task  the participants represent more long-term research efforts.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'57'", "'29'", "'31'", "'28'"]
'8'
'57'
'29'
'31'
'28'
['8', '57', '29', '31', '28']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "2">Training and testing is based on the Europarl corpus.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="143" ssid = "36">For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.</S><S sid ="21" ssid = "14">The out-of-domain test set differs from the Europarl data in various ways.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'28'", "'170'", "'143'", "'21'"]
'9'
'28'
'170'
'143'
'21'
['9', '28', '170', '143', '21']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "3">Figure 1 provides some statistics about this corpus.</S><S sid ="20" ssid = "13">For statistics on this test set  refer to Figure 1.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="4" ssid = "2">This revealed interesting clues about the properties of automatic and manual scoring.</S><S sid ="120" ssid = "13">In Figure 4  we displayed the number of system comparisons  for which we concluded statistical significance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'20'", "'134'", "'4'", "'120'"]
'10'
'20'
'134'
'4'
'120'
['10', '20', '134', '4', '120']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "4">To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="161" ssid = "54">Since all long sentence translation are somewhat muddled  even a contrastive evaluation between systems was difficult.</S><S sid ="152" ssid = "45">The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.</S><S sid ="31" ssid = "24">Compared to last year’s shared task  the participants represent more long-term research efforts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'28'", "'161'", "'152'", "'31'"]
'11'
'28'
'161'
'152'
'31'
['11', '28', '161', '152', '31']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "5">To summarize  we provided: The performance of the baseline system is similar to the best submissions in last year’s shared task.</S><S sid ="141" ssid = "34">In-domain Systran scores on this metric are lower than all statistical systems  even the ones that have much worse human scores.</S><S sid ="8" ssid = "1">The evaluation framework for the shared task is similar to the one used in last year’s shared task.</S><S sid ="81" ssid = "20">This is less than the 694 judgements 2004 DARPA/NIST evaluation  or the 532 judgements in the 2005 DARPA/NIST evaluation.</S><S sid ="151" ssid = "44">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages  as demonstrated by the results for English-German and English-French.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'141'", "'8'", "'81'", "'151'"]
'12'
'141'
'8'
'81'
'151'
['12', '141', '8', '81', '151']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "6">We are currently working on a complete open source implementation of a training and decoding system  which should become available over the summer. pus  from which also the in-domain test set is taken.</S><S sid ="33" ssid = "26">While building a machine translation system is a serious undertaking  in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.</S><S sid ="43" ssid = "9">At the very least  we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.</S><S sid ="74" ssid = "13">While we had up to 11 submissions for a translation direction  we did decide against presenting all 11 system outputs to the human judge.</S><S sid ="42" ssid = "8">It was our hope that this competition  which included the manual and automatic evaluation of statistical systems and one rulebased commercial system  will give further insight into the relation between automatic and manual evaluation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'33'", "'43'", "'74'", "'42'"]
'13'
'33'
'43'
'74'
'42'
['13', '33', '43', '74', '42']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "7">There is twice as much language modelling data  since training data for the machine translation system is filtered against sentences of length larger than 40 words.</S><S sid ="165" ssid = "58">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid ="58" ssid = "24">We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set  53 blocks for the out-of-domain test set)  check for each block  if one system has a higher BLEU score than the other  and then use the sign test.</S><S sid ="151" ssid = "44">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages  as demonstrated by the results for English-German and English-French.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'165'", "'58'", "'151'", "'155'"]
'14'
'165'
'58'
'151'
'155'
['14', '165', '58', '151', '155']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "8">Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.</S><S sid ="21" ssid = "14">The out-of-domain test set differs from the Europarl data in various ways.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="142" ssid = "35">Surprisingly  this effect is much less obvious for out-of-domain test data.</S><S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'21'", "'28'", "'142'", "'18'"]
'15'
'21'
'28'
'142'
'18'
['15', '21', '28', '142', '18']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="165" ssid = "58">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid ="14" ssid = "7">There is twice as much language modelling data  since training data for the machine translation system is filtered against sentences of length larger than 40 words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'126'", "'18'", "'165'", "'14'"]
'16'
'126'
'18'
'165'
'14'
['16', '126', '18', '165', '14']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="69" ssid = "8">We settled on contrastive evaluations of 5 system outputs for a single test sentence.</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="19" ssid = "12">We aligned the texts at a sentence level across all four languages  resulting in 1064 sentence per language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'69'", "'162'", "'170'", "'19'"]
'17'
'69'
'162'
'170'
'19'
['17', '69', '162', '170', '19']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="15" ssid = "8">Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'126'", "'16'", "'15'", "'155'"]
'18'
'126'
'16'
'15'
'155'
['18', '126', '16', '15', '155']
parsed_discourse_facet ['method_citation']
['&#8226; We evaluated translation from English, in addition to into English.', 'English was again paired with German, French, and Spanish.']
['In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.', 'For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.', 'Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:55', 'F:0']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'All systems (except for Systran  which was not tuned to Europarl) did considerably worse on outof-domain training data.', 'The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).', 'For the automatic scoring method BLEU  we can distinguish three quarters of the systems.', 'For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.']
['system', 'ROUGE-S*', 'Average_R:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Average_P:', '0.20000', '(95%-conf.int.', '0.20000', '-', '0.20000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00002', '(95%-conf.int.', '0.00002', '-', '0.00002)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:973710', 'P:55', 'F:11']
['We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.']
['Training and testing is based on the Europarl corpus.', 'Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.', 'For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.', 'The out-of-domain test set differs from the Europarl data in various ways.', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_P:', '0.12821', '(95%-conf.int.', '0.12821', '-', '0.12821)']
['system', 'ROUGE-S*', 'Average_F:', '0.02331', '(95%-conf.int.', '0.02331', '-', '0.02331)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:78', 'F:10']
['Training and testing is based on the Europarl corpus.']
['was done by the participants.', 'Judges where excluded from assessing the quality of MT systems that were submitted by their institution.', 'Annotators argued for the importance of having correct and even multiple references.', u'About half of the participants of last year\xe2\u20ac\u2122s shared task participated again.', 'Participants and other volunteers contributed about 180 hours of labor in the manual evaluation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:10', 'F:0']
['The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.']
['This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.', 'Human judges also pointed out difficulties with the evaluation of long sentences.', 'Annotators argued for the importance of having correct and even multiple references.', 'This revealed interesting clues about the properties of automatic and manual scoring.', 'Again  we can compute average scores for all systems for the different language pairs (Figure 6).']
['system', 'ROUGE-S*', 'Average_R:', '0.00270', '(95%-conf.int.', '0.00270', '-', '0.00270)']
['system', 'ROUGE-S*', 'Average_P:', '0.02564', '(95%-conf.int.', '0.02564', '-', '0.02564)']
['system', 'ROUGE-S*', 'Average_F:', '0.00488', '(95%-conf.int.', '0.00488', '-', '0.00488)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:78', 'F:2']
['We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.']
['While building a machine translation system is a serious undertaking  in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.', 'We are currently working on a complete open source implementation of a training and decoding system  which should become available over the summer. pus  from which also the in-domain test set is taken.', 'While we had up to 11 submissions for a translation direction  we did decide against presenting all 11 system outputs to the human judge.', 'It was our hope that this competition  which included the manual and automatic evaluation of statistical systems and one rulebased commercial system  will give further insight into the relation between automatic and manual evaluation.', 'At the very least  we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:78', 'F:0']
['Training and testing is based on the Europarl corpus.']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.', 'The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.', u'\xe2\u20ac\xa2 We evaluated translation from English  in addition to into English.', 'A few annotators suggested to break up long sentences into clauses and evaluate these separately.', 'English was again paired with German  French  and Spanish.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:10', 'F:0']
['To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources.']
['The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages  as demonstrated by the results for English-German and English-French.', u'The evaluation framework for the shared task is similar to the one used in last year\u2019s shared task.', u'To summarize  we provided: The performance of the baseline system is similar to the best submissions in last year\u2019s shared task.', 'This is less than the 694 judgements 2004 DARPA/NIST evaluation  or the 532 judgements in the 2005 DARPA/NIST evaluation.', 'In-domain Systran scores on this metric are lower than all statistical systems  even the ones that have much worse human scores.']
['system', 'ROUGE-S*', 'Average_R:', '0.00409', '(95%-conf.int.', '0.00409', '-', '0.00409)']
['system', 'ROUGE-S*', 'Average_P:', '0.12727', '(95%-conf.int.', '0.12727', '-', '0.12727)']
['system', 'ROUGE-S*', 'Average_F:', '0.00793', '(95%-conf.int.', '0.00793', '-', '0.00793)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:55', 'F:7']
['Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary.']
['Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.', 'Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.', 'The out-of-domain test set differs from the Europarl data in various ways.', 'In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.', 'Surprisingly  this effect is much less obvious for out-of-domain test data.']
['system', 'ROUGE-S*', 'Average_R:', '0.04348', '(95%-conf.int.', '0.04348', '-', '0.04348)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.08333', '(95%-conf.int.', '0.08333', '-', '0.08333)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:45', 'F:45']
['The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.']
[u'However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate \xe2\u20ac\u201d due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).', 'In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.', 'There is twice as much language modelling data  since training data for the machine translation system is filtered against sentences of length larger than 40 words.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00033', '(95%-conf.int.', '0.00033', '-', '0.00033)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00066', '(95%-conf.int.', '0.00066', '-', '0.00066)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:45', 'F:1']
['The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:']
['To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.', u'Compared to last year\xe2\u20ac\u2122s shared task  the participants represent more long-term research efforts.', 'The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.', 'Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.', 'Since all long sentence translation are somewhat muddled  even a contrastive evaluation between systems was difficult.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:28', 'F:0']
['The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.']
['The judgement of 4 in the first case will go to a vastly better system output than in the second case.', 'For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French)  at least as measured by BLEU.', u'0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 \u2022systran \u2022 ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency \u2022systran \u2022nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p \ufffd 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain \u2022upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv 23 24 25 26 27 28 29 30 31 32 \u2022upc-mr \u2022utd \u2022upc-jmc \u2022uedin-birch \u2022ntt \u2022rali \u2022uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy \u2022upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency \u2022ntt \u2022nrc \u2022rali \u2022uedin-birch -0.2 -0.3 -0.5 \u2022upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr \u2022 rali Fluency -0.4 \u2022upc-mr utd \u2022upc-jmc -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 \u2022upv 0.5 0.4 \u2022systran \u2022upc-mr \u2022 \u2022rali 0.3 \u2022ntt 0.2 0.1 -0.0 -0.1 \u2022systran \u2022upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022upv \u2022systran \u2022upc-mr \u2022 Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022systran \u2022ntt', 'The average fluency judgement per judge ranged from 2.33 to 3.67  the average adequacy judgement ranged from 2.56 to 4.13.', 'The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:116403', 'P:45', 'F:0']
['Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.']
['We aligned the texts at a sentence level across all four languages  resulting in 1064 sentence per language.', 'We settled on contrastive evaluations of 5 system outputs for a single test sentence.', 'A few annotators suggested to break up long sentences into clauses and evaluate these separately.', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00355', '(95%-conf.int.', '0.00355', '-', '0.00355)']
['system', 'ROUGE-S*', 'Average_P:', '0.06061', '(95%-conf.int.', '0.06061', '-', '0.06061)']
['system', 'ROUGE-S*', 'Average_F:', '0.00670', '(95%-conf.int.', '0.00670', '-', '0.00670)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:66', 'F:4']
0.120303845228 0.00515230765267 0.00975615377111





input/ref/Task1/P11-1061_sweta.csv
input/res/Task1/P11-1061.csv
parsing: input/ref/Task1/P11-1061_sweta.csv
 <S sid="9" ssid="5">Unfortunately, the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al., 2010), making its practical usability questionable at best.</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="47" ssid="13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["47'"]
47'
['47']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["10'"]
10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="1">Given the bilingual graph described in the previous section, we can use label propagation to project the English POS labels to the foreign language.</S>
original cit marker offset is 0
new cit marker offset is 0



["70'"]
70'
['70']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="18">This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
 <S sid="83" ssid="14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value &#964;: We describe how we choose &#964; in &#167;6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["83'"]
83'
['83']
parsed_discourse_facet ['method_citation']
<S sid="113" ssid="13">For each language under consideration, Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["113'"]
113'
['113']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
 <S sid="13" ssid="9">(2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available.</S>
original cit marker offset is 0
new cit marker offset is 0



["13'"]
13'
['13']
parsed_discourse_facet ['method_citation']
 <S sid="3" ssid="3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="20">We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Syntactic universals are a well studied concept in linguistics (Carnie, 2002; Newmeyer, 2005), and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["19'"]
19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="16">Figure 2 shows an excerpt of a sentence from the Italian test set and the tags assigned by four different models, as well as the gold tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



["161'"]
161'
['161']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["23'"]
23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["23'"]
23'
['23']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1061.csv
<S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S><S sid ="8" ssid = "4">Unsupervised learning approaches appear to be a natural solution to this problem  as they require only unannotated text for training models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'160'", "'3'", "'13'", "'8'"]
'1'
'160'
'3'
'13'
'8'
['1', '160', '3', '13', '8']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed)  making it applicable to a wide array of resource-poor languages.</S><S sid ="9" ssid = "5">Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.</S><S sid ="126" ssid = "26">Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.</S><S sid ="41" ssid = "7">Because the information flow in our graph is asymmetric (from English to the foreign language)  we use different types of vertices for each language.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'9'", "'126'", "'41'", "'107'"]
'2'
'9'
'126'
'41'
'107'
['2', '9', '126', '41', '107']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="33" ssid = "10">The POS distributions over the foreign trigram types are used as features to learn a better unsupervised POS tagger (§5).</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="60" ssid = "26">To initialize the graph for label propagation we use a supervised English tagger to label the English side of the bitext.7 We then simply count the individual labels of the English tokens and normalize the counts to produce tag distributions over English word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'39'", "'33'", "'1'", "'60'"]
'3'
'39'
'33'
'1'
'60'
['3', '39', '33', '1', '60']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S><S sid ="23" ssid = "19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%)  and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S sid ="143" ssid = "6">Overall  it gives improvements ranging from 1.1% for German to 14.7% for Italian  for an average improvement of 8.3% over the unsupervised feature-HMM model.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S><S sid ="144" ssid = "7">For comparison  the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%  and goes up to 88.7% with a treebank dictionary.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'23'", "'143'", "'135'", "'144'"]
'4'
'23'
'143'
'135'
'144'
['4', '23', '143', '135', '144']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing  resulting in highly accurate systems.</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S><S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S><S sid ="15" ssid = "11">First  we use a novel graph-based framework for projecting syntactic information across language boundaries.</S><S sid ="18" ssid = "14">To make the projection practical  we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'158'", "'4'", "'15'", "'18'"]
'5'
'158'
'4'
'15'
'18'
['5', '158', '4', '15', '18']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Supervised part-of-speech (POS) taggers  for example  approach the level of inter-annotator agreement (Shen et al.  2007  97.3% accuracy for English).</S><S sid ="47" ssid = "13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S><S sid ="9" ssid = "5">Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S><S sid ="20" ssid = "16">Because there might be some controversy about the exact definitions of such universals  this set of coarse-grained POS categories is defined operationally  by collapsing language (or treebank) specific distinctions to a set of categories that exists across all languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'47'", "'9'", "'158'", "'20'"]
'6'
'47'
'9'
'158'
'20'
['6', '47', '9', '158', '20']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S><S sid ="27" ssid = "4">Graph construction does not require any labeled data  but makes use of two similarity functions.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'27'", "'107'", "'56'", "'37'"]
'7'
'27'
'107'
'56'
'37'
['7', '27', '107', '56', '37']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Unsupervised learning approaches appear to be a natural solution to this problem  as they require only unannotated text for training models.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'160'", "'1'", "'13'", "'3'"]
'8'
'160'
'1'
'13'
'3'
['8', '160', '1', '13', '3']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.</S><S sid ="47" ssid = "13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S><S sid ="126" ssid = "26">Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.</S><S sid ="2" ssid = "2">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed)  making it applicable to a wide array of resource-poor languages.</S><S sid ="57" ssid = "23">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'47'", "'126'", "'2'", "'57'"]
'9'
'47'
'126'
'2'
'57'
['9', '47', '126', '2', '57']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'39'", "'16'", "'26'", "'13'"]
'10'
'39'
'16'
'26'
'13'
['10', '39', '16', '26', '13']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This scenario is applicable to a large set of languages and has been considered by a number of authors in the past (Alshawi et al.  2000; Xi and Hwa  2005; Ganchev et al.  2009).</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="94" ssid = "25">We adopted this state-of-the-art model because it makes it easy to experiment with various ways of incorporating our novel constraint feature into the log-linear emission model.</S><S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'19'", "'26'", "'94'", "'13'"]
'11'
'19'
'26'
'94'
'13'
['11', '19', '26', '94', '13']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Naseem et al. (2009) and Snyder et al.</S><S sid ="38" ssid = "4">More recently  Subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model.</S><S sid ="93" ssid = "24">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S><S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="123" ssid = "23">This can be seen as a rough approximation of Yarowsky and Ngai (2001).</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'38'", "'93'", "'37'", "'123'"]
'12'
'38'
'93'
'37'
'123'
['12', '38', '93', '37', '123']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="24" ssid = "1">The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'39'", "'160'", "'1'", "'24'"]
'13'
'39'
'160'
'1'
'24'
['13', '39', '160', '1', '24']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">Our work is closest to that of Yarowsky and Ngai (2001)  but differs in two important ways.</S><S sid ="123" ssid = "23">This can be seen as a rough approximation of Yarowsky and Ngai (2001).</S><S sid ="40" ssid = "6">We extend Subramanya et al.’s intuitions to our bilingual setup.</S><S sid ="88" ssid = "19">In our experiments  we used the same set of features as BergKirkpatrick et al. (2010): an indicator feature based In a traditional Markov model  the emission distribution PΘ(Xi = xi  |Zi = zi) is a set of multinomials.</S><S sid ="85" ssid = "16">We develop our POS induction model based on the feature-based HMM of Berg-Kirkpatrick et al. (2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'123'", "'40'", "'88'", "'85'"]
'14'
'123'
'40'
'88'
'85'
['14', '123', '40', '88', '85']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">First  we use a novel graph-based framework for projecting syntactic information across language boundaries.</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'158'", "'29'", "'56'", "'25'"]
'15'
'158'
'29'
'56'
'25'
['15', '158', '29', '56', '25']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="57" ssid = "23">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid ="70" ssid = "1">Given the bilingual graph described in the previous section  we can use label propagation to project the English POS labels to the foreign language.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'57'", "'70'", "'29'", "'26'"]
'16'
'57'
'70'
'29'
'26'
['16', '57', '70', '29', '26']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">Second  we treat the projected labels as features in an unsupervised model (§5)  rather than using them directly for supervised training.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="71" ssid = "2">We use label propagation in two stages to generate soft labels on all the vertices in the graph.</S><S sid ="126" ssid = "26">Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.</S><S sid ="52" ssid = "18">This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'97'", "'71'", "'126'", "'52'"]
'17'
'97'
'71'
'126'
'52'
['17', '97', '71', '126', '52']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">To make the projection practical  we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="111" ssid = "11">We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns)  VERB (verbs)  ADJ (adjectives)  ADV (adverbs)  PRON (pronouns)  DET (determiners)  ADP (prepositions or postpositions)  NUM (numerals)  CONJ (conjunctions)  PRT (particles)  PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).</S><S sid ="85" ssid = "16">We develop our POS induction model based on the feature-based HMM of Berg-Kirkpatrick et al. (2010).</S><S sid ="158" ssid = "1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'56'", "'111'", "'85'", "'158'"]
'18'
'56'
'111'
'85'
'158'
['18', '56', '111', '85', '158']
parsed_discourse_facet ['method_citation']
['To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.', 'To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.', 'First  we use a novel graph-based framework for projecting syntactic information across language boundaries.', u'To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (\u0e22\u0e073.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00171', '(95%-conf.int.', '0.00171', '-', '0.00171)']
['system', 'ROUGE-S*', 'Average_P:', '0.06061', '(95%-conf.int.', '0.06061', '-', '0.06061)']
['system', 'ROUGE-S*', 'Average_F:', '0.00332', '(95%-conf.int.', '0.00332', '-', '0.00332)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:66', 'F:4']
['Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).']
['We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.', 'Unsupervised learning approaches appear to be a natural solution to this problem  as they require only unannotated text for training models.', 'We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).', 'Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.', '(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.']
['system', 'ROUGE-S*', 'Average_R:', '0.00044', '(95%-conf.int.', '0.00044', '-', '0.00044)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00086', '(95%-conf.int.', '0.00086', '-', '0.00086)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:55', 'F:1']
['Syntactic universals are a well studied concept in linguistics (Carnie, 2002; Newmeyer, 2005), and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.']
['We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.', 'The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.', '(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.', 'Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.', 'They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.']
['system', 'ROUGE-S*', 'Average_R:', '0.00252', '(95%-conf.int.', '0.00252', '-', '0.00252)']
['system', 'ROUGE-S*', 'Average_P:', '0.04575', '(95%-conf.int.', '0.04575', '-', '0.04575)']
['system', 'ROUGE-S*', 'Average_F:', '0.00478', '(95%-conf.int.', '0.00478', '-', '0.00478)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:153', 'F:7']
['(2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available.']
['Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.', 'Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).', u'Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don\u2019t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De\ufffdf.', 'Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed)  making it applicable to a wide array of resource-poor languages.', 'Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.']
['system', 'ROUGE-S*', 'Average_R:', '0.00053', '(95%-conf.int.', '0.00053', '-', '0.00053)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00105', '(95%-conf.int.', '0.00105', '-', '0.00105)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5671', 'P:55', 'F:3']
['Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.']
['This can be seen as a rough approximation of Yarowsky and Ngai (2001).', 'For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.', 'Naseem et al. (2009) and Snyder et al.', 'Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.', 'More recently  Subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:105', 'F:0']
['We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).']
['To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.', 'They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.', '(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.', u'As discussed in more detail in \u0e22\u0e073  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', u'To this end  we construct a bilingual graph over word types to establish a connection between the two languages (\u0e22\u0e073)  and then use graph label propagation to project syntactic information from English to the foreign language (\u0e22\u0e074).']
['system', 'ROUGE-S*', 'Average_R:', '0.00243', '(95%-conf.int.', '0.00243', '-', '0.00243)']
['system', 'ROUGE-S*', 'Average_P:', '0.09559', '(95%-conf.int.', '0.09559', '-', '0.09559)']
['system', 'ROUGE-S*', 'Average_F:', '0.00473', '(95%-conf.int.', '0.00473', '-', '0.00473)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5356', 'P:136', 'F:13']
['We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value &#964;: We describe how we choose &#964; in &#167;6.4.']
['First  we use a novel graph-based framework for projecting syntactic information across language boundaries.', 'We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.', 'Supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing  resulting in highly accurate systems.', 'Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.', 'To make the projection practical  we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).']
['system', 'ROUGE-S*', 'Average_R:', '0.00043', '(95%-conf.int.', '0.00043', '-', '0.00043)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00082', '(95%-conf.int.', '0.00082', '-', '0.00082)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:105', 'F:1']
['To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.']
['Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.', 'Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed)  making it applicable to a wide array of resource-poor languages.', 'Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.', 'Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.', 'Because the information flow in our graph is asymmetric (from English to the foreign language)  we use different types of vertices for each language.']
['system', 'ROUGE-S*', 'Average_R:', '0.04346', '(95%-conf.int.', '0.04346', '-', '0.04346)']
['system', 'ROUGE-S*', 'Average_P:', '0.18750', '(95%-conf.int.', '0.18750', '-', '0.18750)']
['system', 'ROUGE-S*', 'Average_F:', '0.07056', '(95%-conf.int.', '0.07056', '-', '0.07056)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:528', 'F:99']
['Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.', 'To make the projection practical  we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).', 'To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.', 'We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns)  VERB (verbs)  ADJ (adjectives)  ADV (adverbs)  PRON (pronouns)  DET (determiners)  ADP (prepositions or postpositions)  NUM (numerals)  CONJ (conjunctions)  PRT (particles)  PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).', 'We develop our POS induction model based on the feature-based HMM of Berg-Kirkpatrick et al. (2010).']
['system', 'ROUGE-S*', 'Average_R:', '0.00460', '(95%-conf.int.', '0.00460', '-', '0.00460)']
['system', 'ROUGE-S*', 'Average_P:', '0.03629', '(95%-conf.int.', '0.03629', '-', '0.03629)']
['system', 'ROUGE-S*', 'Average_F:', '0.00816', '(95%-conf.int.', '0.00816', '-', '0.00816)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3916', 'P:496', 'F:18']
['To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).']
['We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.', 'Unsupervised learning approaches appear to be a natural solution to this problem  as they require only unannotated text for training models.', '(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.', 'Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.', 'We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).']
['system', 'ROUGE-S*', 'Average_R:', '0.00176', '(95%-conf.int.', '0.00176', '-', '0.00176)']
['system', 'ROUGE-S*', 'Average_P:', '0.06061', '(95%-conf.int.', '0.06061', '-', '0.06061)']
['system', 'ROUGE-S*', 'Average_F:', '0.00341', '(95%-conf.int.', '0.00341', '-', '0.00341)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:66', 'F:4']
['We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).']
['To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.', 'Graph construction does not require any labeled data  but makes use of two similarity functions.', 'Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.', 'Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.', 'However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.']
['system', 'ROUGE-S*', 'Average_R:', '0.00559', '(95%-conf.int.', '0.00559', '-', '0.00559)']
['system', 'ROUGE-S*', 'Average_P:', '0.05882', '(95%-conf.int.', '0.05882', '-', '0.05882)']
['system', 'ROUGE-S*', 'Average_F:', '0.01021', '(95%-conf.int.', '0.01021', '-', '0.01021)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:136', 'F:8']
0.0570381812997 0.00576999994755 0.00980909081992





input/ref/Task1/E03-1005_aakansha.csv
input/res/Task1/E03-1005.csv
parsing: input/ref/Task1/E03-1005_aakansha.csv
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'"]
'105'
['105']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'"]
'145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'"]
'80'
['80']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'"]
'143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid="141" ssid="6">This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'141'"]
'140'
'141'
['140', '141']
parsed_discourse_facet ['result_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="102" ssid="5">In case the shortest derivation is not unique, Bod (2000b) proposes to back off to a frequency ordering of the subtrees.</S>
    <S sid="103" ssid="6">That is, all subtrees of each root label are assigned a rank according to their frequency in the treebank: the most frequent subtree (or subtrees) of each root label gets rank 1, the second most frequent subtree gets rank 2, etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'", "'103'"]
'102'
'103'
['102', '103']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="46" ssid="43">Most previous notions of best parse tree in DOP1 were based on a probabilistic metric, with Bod (2000b) as a notable exception, who used a simplicity metric based on the shortest derivation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'"]
'46'
['46']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="37">For example, Bod (2001) samples a fixed number of subtrees of each depth, which has the effect of assigning roughly equal weight to each node in the training data, and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S>
    <S sid="86" ssid="38">Bod reports state-of-the-art results with this method, and observes no decrease in parse accuracy when larger subtrees are included (using subtrees up to depth 14).</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'", "'86'"]
'85'
'86'
['85', '86']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="115" ssid="18">The only thing that needs to be changed for Simplicity-DOP is that all subtrees should be assigned equal probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S sid ="1" ssid = "1">Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.</S><S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S><S sid ="76" ssid = "28">Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.</S><S sid ="26" ssid = "23">Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation  which in some cases is a reasonable approximation of the most probable parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'108'", "'28'", "'76'", "'26'"]
'1'
'108'
'28'
'76'
'26'
['1', '108', '28', '76', '26']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">This paper proposes an integration of the two models which outperforms each of them separately.</S><S sid ="107" ssid = "10">This suggests that a model which combines these two notions of best parse may boost the accuracy.</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S><S sid ="94" ssid = "46">This paper presents the first published results with this estimator on the WSJ.</S><S sid ="56" ssid = "8">Here we will only sketch this PCFG-reduction  which is heavily based on Goodman (2002).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'107'", "'45'", "'94'", "'56'"]
'2'
'107'
'45'
'94'
'56'
['2', '107', '45', '94', '56']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models  and an average processing time of 3.6 seconds per WSJ sentence.</S><S sid ="38" ssid = "35">Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task  the parsing time was reported to be over 200 seconds per sentence (Bod 2003).</S><S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="92" ssid = "44">Tested on the OVIS corpus  Bonnema et al. 's proposal obtains results that are comparable to Sima'an (1999) -- see Bonnema et al.</S><S sid ="41" ssid = "38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'38'", "'130'", "'92'", "'41'"]
'3'
'38'
'130'
'92'
'41'
['3', '38', '130', '92', '41']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S><S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="6" ssid = "3">To appreciate these innovations  it should be noted that the model was radically different from all other statistical parsing models at the time.</S><S sid ="143" ssid = "8">While SL-DOP and LS-DOP have been compared before in Bod (2002)  especially in the context of musical parsing  this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules  and (2) the use of arbitrarily large fragments rather than restricted ones.</S><S sid ="1" ssid = "1">Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'108'", "'6'", "'143'", "'1'"]
'4'
'108'
'6'
'143'
'1'
['4', '108', '6', '143', '1']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "2">Fragments include  for instance  subtrees of depth 1 (corresponding to context-free rules) as well as entire trees.</S><S sid ="62" ssid = "14">Let a represent the number of subtrees headed by nodes with nonterminal A  that is a =Ej aj.</S><S sid ="23" ssid = "20">Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.</S><S sid ="104" ssid = "7">Next  the rank of each (shortest) derivation is computed as the sum of the ranks of the subtrees involved.</S><S sid ="61" ssid = "13">Let aj represent the number of subtrees headed by the node A@j.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'62'", "'23'", "'104'", "'61'"]
'5'
'62'
'23'
'104'
'61'
['5', '62', '23', '104', '61']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "3">To appreciate these innovations  it should be noted that the model was radically different from all other statistical parsing models at the time.</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="106" ssid = "9">What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.</S><S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S><S sid ="76" ssid = "28">Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'79'", "'106'", "'4'", "'76'"]
'6'
'79'
'106'
'4'
'76'
['6', '79', '106', '4', '76']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="128" ssid = "9">Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.</S><S sid ="23" ssid = "20">Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="14" ssid = "11">This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'128'", "'23'", "'15'", "'14'"]
'7'
'128'
'23'
'15'
'14'
['7', '128', '23', '15', '14']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">in Fujisaki et al. 1989; Black et al.</S><S sid ="10" ssid = "7">Waegner 1992; Pereira and Schabes 1992).</S><S sid ="98" ssid = "1">Most DOP models  such as in Bod (1993)  Goodman (1996)  Bonnema et al. (1997)  Sima'an (2000) and Collins & Duffy (2002)  use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.</S><S sid ="18" ssid = "15">Collins 1999; Charniak 2000; Goodman 1998).</S><S sid ="120" ssid = "1">For our experiments we used the standard division of the WSJ (Marcus et al. 1993)  with sections 2 through 21 for training (approx.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'10'", "'98'", "'18'", "'120'"]
'8'
'10'
'98'
'18'
'120'
['8', '10', '98', '18', '120']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "6">1992  1993; Briscoe and I Thanks to Ivan Sag for this pun.</S><S sid ="134" ssid = "15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.</S><S sid ="29" ssid = "26">Johnson (1998b  2002) showed that DOP1's subtree estimation method is statistically biased and inconsistent.</S><S sid ="35" ssid = "32">Bonnema et al. (1999) observed that another problem with DOP1's subtree-estimation method is that it provides more probability to nodes with more subtrees  and therefore more probability to larger subtrees.</S><S sid ="23" ssid = "20">Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'134'", "'29'", "'35'", "'23'"]
'9'
'134'
'29'
'35'
'23'
['9', '134', '29', '35', '23']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">Waegner 1992; Pereira and Schabes 1992).</S><S sid ="18" ssid = "15">Collins 1999; Charniak 2000; Goodman 1998).</S><S sid ="129" ssid = "10">Collins 1996  Charniak 1997  Collins 1999 and Charniak 2000).</S><S sid ="8" ssid = "5">in Fujisaki et al. 1989; Black et al.</S><S sid ="98" ssid = "1">Most DOP models  such as in Bod (1993)  Goodman (1996)  Bonnema et al. (1997)  Sima'an (2000) and Collins & Duffy (2002)  use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'18'", "'129'", "'8'", "'98'"]
'10'
'18'
'129'
'8'
'98'
['10', '18', '129', '8', '98']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "8">The DOP model  on the other hand  was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus  but to directly use corpus fragments as a grammar.</S><S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="143" ssid = "8">While SL-DOP and LS-DOP have been compared before in Bod (2002)  especially in the context of musical parsing  this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules  and (2) the use of arbitrarily large fragments rather than restricted ones.</S><S sid ="110" ssid = "13">We will refer to the first combination (which selects the simplest among the n likeliest trees) as Simplicity-Likelihood-DOP or SL-DOP  and to the second combination (which selects the likeliest among the n simplest trees) as Likelihood-Simplicity-DOP or LS-DOP.</S><S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'13'", "'143'", "'110'", "'4'"]
'11'
'13'
'143'
'110'
'4'
['11', '13', '143', '110', '4']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "9">This approach has now gained wide usage  as exemplified by the work of Collins (1996  1999)  Charniak (1996  1997)  Johnson (1998)  Chiang (2000)  and many others.</S><S sid ="98" ssid = "1">Most DOP models  such as in Bod (1993)  Goodman (1996)  Bonnema et al. (1997)  Sima'an (2000) and Collins & Duffy (2002)  use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.</S><S sid ="143" ssid = "8">While SL-DOP and LS-DOP have been compared before in Bod (2002)  especially in the context of musical parsing  this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules  and (2) the use of arbitrarily large fragments rather than restricted ones.</S><S sid ="16" ssid = "13">While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words  later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a).</S><S sid ="41" ssid = "38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'98'", "'143'", "'16'", "'41'"]
'12'
'98'
'143'
'16'
'41'
['12', '98', '143', '16', '41']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="54" ssid = "6">A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account.</S><S sid ="20" ssid = "17">Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage  and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).</S><S sid ="110" ssid = "13">We will refer to the first combination (which selects the simplest among the n likeliest trees) as Simplicity-Likelihood-DOP or SL-DOP  and to the second combination (which selects the likeliest among the n simplest trees) as Likelihood-Simplicity-DOP or LS-DOP.</S><S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'54'", "'20'", "'110'", "'4'"]
'13'
'54'
'20'
'110'
'4'
['13', '54', '20', '110', '4']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "11">This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="36" ssid = "33">As an alternative  Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'15'", "'7'", "'36'", "'82'"]
'14'
'15'
'7'
'36'
'82'
['14', '15', '7', '36', '82']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="14" ssid = "11">This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.</S><S sid ="134" ssid = "15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'45'", "'82'", "'14'", "'134'"]
'15'
'45'
'82'
'14'
'134'
['15', '45', '82', '14', '134']
parsed_discourse_facet ['method_citation']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
["Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.", 'What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.', 'The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.', "While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.", 'To appreciate these innovations  it should be noted that the model was radically different from all other statistical parsing models at the time.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:78', 'F:0']
['The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.']
['Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.', "Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.", 'The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.', "While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.", "Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation  which in some cases is a reasonable approximation of the most probable parse."]
['system', 'ROUGE-S*', 'Average_R:', '0.00631', '(95%-conf.int.', '0.00631', '-', '0.00631)']
['system', 'ROUGE-S*', 'Average_P:', '0.07333', '(95%-conf.int.', '0.07333', '-', '0.07333)']
['system', 'ROUGE-S*', 'Average_F:', '0.01162', '(95%-conf.int.', '0.01162', '-', '0.01162)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:300', 'F:22']
['Bod reports state-of-the-art results with this method, and observes no decrease in parse accuracy when larger subtrees are included (using subtrees up to depth 14).', 'For example, Bod (2001) samples a fixed number of subtrees of each depth, which has the effect of assigning roughly equal weight to each node in the training data, and roughly exponentially less probability for larger trees (see Goodman 2002: 12).']
['A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account.', 'The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.', 'Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage  and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).', 'We will refer to the first combination (which selects the simplest among the n likeliest trees) as Simplicity-Likelihood-DOP or SL-DOP  and to the second combination (which selects the likeliest among the n simplest trees) as Likelihood-Simplicity-DOP or LS-DOP.', 'The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.']
['system', 'ROUGE-S*', 'Average_R:', '0.00281', '(95%-conf.int.', '0.00281', '-', '0.00281)']
['system', 'ROUGE-S*', 'Average_P:', '0.01080', '(95%-conf.int.', '0.01080', '-', '0.01080)']
['system', 'ROUGE-S*', 'Average_F:', '0.00446', '(95%-conf.int.', '0.00446', '-', '0.00446)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:741', 'F:8']
['While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.']
['Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.', 'The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.', 'The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.', 'While SL-DOP and LS-DOP have been compared before in Bod (2002)  especially in the context of musical parsing  this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules  and (2) the use of arbitrarily large fragments rather than restricted ones.', 'To appreciate these innovations  it should be noted that the model was radically different from all other statistical parsing models at the time.']
['system', 'ROUGE-S*', 'Average_R:', '0.08963', '(95%-conf.int.', '0.08963', '-', '0.08963)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.16452', '(95%-conf.int.', '0.16452', '-', '0.16452)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3916', 'P:351', 'F:351']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.', 'However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.', 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.', 'As an alternative  Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:78', 'F:0']
['The only thing that needs to be changed for Simplicity-DOP is that all subtrees should be assigned equal probabilities.']
['Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.', 'In the second part of this paper  we extend our experiments with a new notion of the best parse tree.', 'However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.', 'This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:28', 'F:0']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['Waegner 1992; Pereira and Schabes 1992).', "Most DOP models  such as in Bod (1993)  Goodman (1996)  Bonnema et al. (1997)  Sima'an (2000) and Collins & Duffy (2002)  use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.", 'in Fujisaki et al. 1989; Black et al.', 'Collins 1996  Charniak 1997  Collins 1999 and Charniak 2000).', 'Collins 1999; Charniak 2000; Goodman 1998).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:78', 'F:0']
['DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).']
["While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).", 'Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models  and an average processing time of 3.6 seconds per WSJ sentence.', "Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task  the parsing time was reported to be over 200 seconds per sentence (Bod 2003).", "This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.", "Tested on the OVIS corpus  Bonnema et al. 's proposal obtains results that are comparable to Sima'an (1999) -- see Bonnema et al."]
['system', 'ROUGE-S*', 'Average_R:', '0.00140', '(95%-conf.int.', '0.00140', '-', '0.00140)']
['system', 'ROUGE-S*', 'Average_P:', '0.13333', '(95%-conf.int.', '0.13333', '-', '0.13333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00278', '(95%-conf.int.', '0.00278', '-', '0.00278)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4278', 'P:45', 'F:6']
['In case the shortest derivation is not unique, Bod (2000b) proposes to back off to a frequency ordering of the subtrees.', 'That is, all subtrees of each root label are assigned a rank according to their frequency in the treebank: the most frequent subtree (or subtrees) of each root label gets rank 1, the second most frequent subtree gets rank 2, etc.']
['Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', 'However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.', 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.', 'Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.']
['system', 'ROUGE-S*', 'Average_R:', '0.00102', '(95%-conf.int.', '0.00102', '-', '0.00102)']
['system', 'ROUGE-S*', 'Average_P:', '0.00493', '(95%-conf.int.', '0.00493', '-', '0.00493)']
['system', 'ROUGE-S*', 'Average_F:', '0.00170', '(95%-conf.int.', '0.00170', '-', '0.00170)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:406', 'F:2']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['1992  1993; Briscoe and I Thanks to Ivan Sag for this pun.', 'Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.', "Johnson (1998b  2002) showed that DOP1's subtree estimation method is statistically biased and inconsistent.", "Bonnema et al. (1999) observed that another problem with DOP1's subtree-estimation method is that it provides more probability to nodes with more subtrees  and therefore more probability to larger subtrees.", 'This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00169', '(95%-conf.int.', '0.00169', '-', '0.00169)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00325', '(95%-conf.int.', '0.00325', '-', '0.00325)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:78', 'F:3']
0.126084998739 0.0102859998971 0.0188329998117





input/ref/Task1/E03-1005_sweta.csv
input/res/Task1/E03-1005.csv
parsing: input/ref/Task1/E03-1005_sweta.csv
<S sid="20" ssid="17">Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).</S>
original cit marker offset is 0
new cit marker offset is 0



["20'"]
20'
['20']
parsed_discourse_facet ['method_citation']
 <S sid="74" ssid="26">Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.</S>
original cit marker offset is 0
new cit marker offset is 0



["74'"]
74'
['74']
parsed_discourse_facet ['method_citation']
<S sid="44" ssid="41">But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["44'"]
44'
['44']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["145'"]
145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S>
original cit marker offset is 0
new cit marker offset is 0



["22'"]
22'
['22']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="14">It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).</S>
original cit marker offset is 0
new cit marker offset is 0



["133'"]
133'
['133']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier &amp; Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="35">Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
 <S sid="100" ssid="3">In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="29">However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
 <S sid="130" ssid="11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["130'"]
130'
['130']
parsed_discourse_facet ['method_citation']
 <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["140'"]
140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="24">Goodman (1996, 1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'"]
27'
['27']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S sid ="1" ssid = "1">Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.</S><S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S><S sid ="76" ssid = "28">Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.</S><S sid ="26" ssid = "23">Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation  which in some cases is a reasonable approximation of the most probable parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'108'", "'28'", "'76'", "'26'"]
'1'
'108'
'28'
'76'
'26'
['1', '108', '28', '76', '26']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">This paper proposes an integration of the two models which outperforms each of them separately.</S><S sid ="107" ssid = "10">This suggests that a model which combines these two notions of best parse may boost the accuracy.</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S><S sid ="94" ssid = "46">This paper presents the first published results with this estimator on the WSJ.</S><S sid ="56" ssid = "8">Here we will only sketch this PCFG-reduction  which is heavily based on Goodman (2002).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'107'", "'45'", "'94'", "'56'"]
'2'
'107'
'45'
'94'
'56'
['2', '107', '45', '94', '56']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models  and an average processing time of 3.6 seconds per WSJ sentence.</S><S sid ="38" ssid = "35">Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task  the parsing time was reported to be over 200 seconds per sentence (Bod 2003).</S><S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="92" ssid = "44">Tested on the OVIS corpus  Bonnema et al. 's proposal obtains results that are comparable to Sima'an (1999) -- see Bonnema et al.</S><S sid ="41" ssid = "38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'38'", "'130'", "'92'", "'41'"]
'3'
'38'
'130'
'92'
'41'
['3', '38', '130', '92', '41']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S><S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="6" ssid = "3">To appreciate these innovations  it should be noted that the model was radically different from all other statistical parsing models at the time.</S><S sid ="143" ssid = "8">While SL-DOP and LS-DOP have been compared before in Bod (2002)  especially in the context of musical parsing  this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules  and (2) the use of arbitrarily large fragments rather than restricted ones.</S><S sid ="1" ssid = "1">Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'108'", "'6'", "'143'", "'1'"]
'4'
'108'
'6'
'143'
'1'
['4', '108', '6', '143', '1']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "2">Fragments include  for instance  subtrees of depth 1 (corresponding to context-free rules) as well as entire trees.</S><S sid ="62" ssid = "14">Let a represent the number of subtrees headed by nodes with nonterminal A  that is a =Ej aj.</S><S sid ="23" ssid = "20">Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.</S><S sid ="104" ssid = "7">Next  the rank of each (shortest) derivation is computed as the sum of the ranks of the subtrees involved.</S><S sid ="61" ssid = "13">Let aj represent the number of subtrees headed by the node A@j.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'62'", "'23'", "'104'", "'61'"]
'5'
'62'
'23'
'104'
'61'
['5', '62', '23', '104', '61']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "3">To appreciate these innovations  it should be noted that the model was radically different from all other statistical parsing models at the time.</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="106" ssid = "9">What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.</S><S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S><S sid ="76" ssid = "28">Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'79'", "'106'", "'4'", "'76'"]
'6'
'79'
'106'
'4'
'76'
['6', '79', '106', '4', '76']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="128" ssid = "9">Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.</S><S sid ="23" ssid = "20">Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="14" ssid = "11">This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'128'", "'23'", "'15'", "'14'"]
'7'
'128'
'23'
'15'
'14'
['7', '128', '23', '15', '14']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">in Fujisaki et al. 1989; Black et al.</S><S sid ="10" ssid = "7">Waegner 1992; Pereira and Schabes 1992).</S><S sid ="98" ssid = "1">Most DOP models  such as in Bod (1993)  Goodman (1996)  Bonnema et al. (1997)  Sima'an (2000) and Collins & Duffy (2002)  use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.</S><S sid ="18" ssid = "15">Collins 1999; Charniak 2000; Goodman 1998).</S><S sid ="120" ssid = "1">For our experiments we used the standard division of the WSJ (Marcus et al. 1993)  with sections 2 through 21 for training (approx.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'10'", "'98'", "'18'", "'120'"]
'8'
'10'
'98'
'18'
'120'
['8', '10', '98', '18', '120']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "6">1992  1993; Briscoe and I Thanks to Ivan Sag for this pun.</S><S sid ="134" ssid = "15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.</S><S sid ="29" ssid = "26">Johnson (1998b  2002) showed that DOP1's subtree estimation method is statistically biased and inconsistent.</S><S sid ="35" ssid = "32">Bonnema et al. (1999) observed that another problem with DOP1's subtree-estimation method is that it provides more probability to nodes with more subtrees  and therefore more probability to larger subtrees.</S><S sid ="23" ssid = "20">Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'134'", "'29'", "'35'", "'23'"]
'9'
'134'
'29'
'35'
'23'
['9', '134', '29', '35', '23']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">Waegner 1992; Pereira and Schabes 1992).</S><S sid ="18" ssid = "15">Collins 1999; Charniak 2000; Goodman 1998).</S><S sid ="129" ssid = "10">Collins 1996  Charniak 1997  Collins 1999 and Charniak 2000).</S><S sid ="8" ssid = "5">in Fujisaki et al. 1989; Black et al.</S><S sid ="98" ssid = "1">Most DOP models  such as in Bod (1993)  Goodman (1996)  Bonnema et al. (1997)  Sima'an (2000) and Collins & Duffy (2002)  use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'18'", "'129'", "'8'", "'98'"]
'10'
'18'
'129'
'8'
'98'
['10', '18', '129', '8', '98']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "8">The DOP model  on the other hand  was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus  but to directly use corpus fragments as a grammar.</S><S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="143" ssid = "8">While SL-DOP and LS-DOP have been compared before in Bod (2002)  especially in the context of musical parsing  this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules  and (2) the use of arbitrarily large fragments rather than restricted ones.</S><S sid ="110" ssid = "13">We will refer to the first combination (which selects the simplest among the n likeliest trees) as Simplicity-Likelihood-DOP or SL-DOP  and to the second combination (which selects the likeliest among the n simplest trees) as Likelihood-Simplicity-DOP or LS-DOP.</S><S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'13'", "'143'", "'110'", "'4'"]
'11'
'13'
'143'
'110'
'4'
['11', '13', '143', '110', '4']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "9">This approach has now gained wide usage  as exemplified by the work of Collins (1996  1999)  Charniak (1996  1997)  Johnson (1998)  Chiang (2000)  and many others.</S><S sid ="98" ssid = "1">Most DOP models  such as in Bod (1993)  Goodman (1996)  Bonnema et al. (1997)  Sima'an (2000) and Collins & Duffy (2002)  use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.</S><S sid ="143" ssid = "8">While SL-DOP and LS-DOP have been compared before in Bod (2002)  especially in the context of musical parsing  this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules  and (2) the use of arbitrarily large fragments rather than restricted ones.</S><S sid ="16" ssid = "13">While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words  later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a).</S><S sid ="41" ssid = "38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'98'", "'143'", "'16'", "'41'"]
'12'
'98'
'143'
'16'
'41'
['12', '98', '143', '16', '41']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="54" ssid = "6">A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account.</S><S sid ="20" ssid = "17">Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage  and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).</S><S sid ="110" ssid = "13">We will refer to the first combination (which selects the simplest among the n likeliest trees) as Simplicity-Likelihood-DOP or SL-DOP  and to the second combination (which selects the likeliest among the n simplest trees) as Likelihood-Simplicity-DOP or LS-DOP.</S><S sid ="4" ssid = "1">The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'54'", "'20'", "'110'", "'4'"]
'13'
'54'
'20'
'110'
'4'
['13', '54', '20', '110', '4']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "11">This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="36" ssid = "33">As an alternative  Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'15'", "'7'", "'36'", "'82'"]
'14'
'15'
'7'
'36'
'82'
['14', '15', '7', '36', '82']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="14" ssid = "11">This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.</S><S sid ="134" ssid = "15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'45'", "'82'", "'14'", "'134'"]
'15'
'45'
'82'
'14'
'134'
['15', '45', '82', '14', '134']
parsed_discourse_facet ['method_citation']
['This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.']
["Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.", 'What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.', 'The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.', "While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.", 'To appreciate these innovations  it should be noted that the model was radically different from all other statistical parsing models at the time.']
['system', 'ROUGE-S*', 'Average_R:', '0.00144', '(95%-conf.int.', '0.00144', '-', '0.00144)']
['system', 'ROUGE-S*', 'Average_P:', '0.02206', '(95%-conf.int.', '0.02206', '-', '0.02206)']
['system', 'ROUGE-S*', 'Average_F:', '0.00271', '(95%-conf.int.', '0.00271', '-', '0.00271)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:136', 'F:3']
['Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).']
['Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.', "Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.", 'The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.', "While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.", "Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation  which in some cases is a reasonable approximation of the most probable parse."]
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:153', 'F:0']
["While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996)."]
['A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account.', 'The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.', 'Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage  and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).', 'We will refer to the first combination (which selects the simplest among the n likeliest trees) as Simplicity-Likelihood-DOP or SL-DOP  and to the second combination (which selects the likeliest among the n simplest trees) as Likelihood-Simplicity-DOP or LS-DOP.', 'The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.']
['system', 'ROUGE-S*', 'Average_R:', '0.00035', '(95%-conf.int.', '0.00035', '-', '0.00035)']
['system', 'ROUGE-S*', 'Average_P:', '0.00526', '(95%-conf.int.', '0.00526', '-', '0.00526)']
['system', 'ROUGE-S*', 'Average_F:', '0.00066', '(95%-conf.int.', '0.00066', '-', '0.00066)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:190', 'F:1']
['While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.']
['Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.', 'The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.', 'The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments  without imposing any constraints on the size of these fragments.', 'While SL-DOP and LS-DOP have been compared before in Bod (2002)  especially in the context of musical parsing  this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules  and (2) the use of arbitrarily large fragments rather than restricted ones.', 'To appreciate these innovations  it should be noted that the model was radically different from all other statistical parsing models at the time.']
['system', 'ROUGE-S*', 'Average_R:', '0.08963', '(95%-conf.int.', '0.08963', '-', '0.08963)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.16452', '(95%-conf.int.', '0.16452', '-', '0.16452)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3916', 'P:351', 'F:351']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.', 'However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.', 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.', 'As an alternative  Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:78', 'F:0']
['Goodman (1996, 1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.']
['Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.', 'In the second part of this paper  we extend our experiments with a new notion of the best parse tree.', 'However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.', 'This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00144', '(95%-conf.int.', '0.00144', '-', '0.00144)']
['system', 'ROUGE-S*', 'Average_P:', '0.02105', '(95%-conf.int.', '0.02105', '-', '0.02105)']
['system', 'ROUGE-S*', 'Average_F:', '0.00270', '(95%-conf.int.', '0.00270', '-', '0.00270)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:190', 'F:4']
["Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003)."]
['Waegner 1992; Pereira and Schabes 1992).', "Most DOP models  such as in Bod (1993)  Goodman (1996)  Bonnema et al. (1997)  Sima'an (2000) and Collins & Duffy (2002)  use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.", 'in Fujisaki et al. 1989; Black et al.', 'Collins 1996  Charniak 1997  Collins 1999 and Charniak 2000).', 'Collins 1999; Charniak 2000; Goodman 1998).']
['system', 'ROUGE-S*', 'Average_R:', '0.00255', '(95%-conf.int.', '0.00255', '-', '0.00255)']
['system', 'ROUGE-S*', 'Average_P:', '0.01961', '(95%-conf.int.', '0.01961', '-', '0.01961)']
['system', 'ROUGE-S*', 'Average_F:', '0.00451', '(95%-conf.int.', '0.00451', '-', '0.00451)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:153', 'F:3']
["But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996)."]
["While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).", 'Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models  and an average processing time of 3.6 seconds per WSJ sentence.', "Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task  the parsing time was reported to be over 200 seconds per sentence (Bod 2003).", "This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.", "Tested on the OVIS corpus  Bonnema et al. 's proposal obtains results that are comparable to Sima'an (1999) -- see Bonnema et al."]
['system', 'ROUGE-S*', 'Average_R:', '0.03576', '(95%-conf.int.', '0.03576', '-', '0.03576)']
['system', 'ROUGE-S*', 'Average_P:', '0.80526', '(95%-conf.int.', '0.80526', '-', '0.80526)']
['system', 'ROUGE-S*', 'Average_F:', '0.06849', '(95%-conf.int.', '0.06849', '-', '0.06849)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4278', 'P:190', 'F:153']
['DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).']
['Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', 'However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees  i.e. single level rules with limited means of information percolation  or with restricted fragments  as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.', 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.', 'Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.']
['system', 'ROUGE-S*', 'Average_R:', '0.00307', '(95%-conf.int.', '0.00307', '-', '0.00307)']
['system', 'ROUGE-S*', 'Average_P:', '0.04412', '(95%-conf.int.', '0.04412', '-', '0.04412)']
['system', 'ROUGE-S*', 'Average_F:', '0.00574', '(95%-conf.int.', '0.00574', '-', '0.00574)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:136', 'F:6']
["Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002)."]
['1992  1993; Briscoe and I Thanks to Ivan Sag for this pun.', 'Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.', "Johnson (1998b  2002) showed that DOP1's subtree estimation method is statistically biased and inconsistent.", "Bonnema et al. (1999) observed that another problem with DOP1's subtree-estimation method is that it provides more probability to nodes with more subtrees  and therefore more probability to larger subtrees.", 'This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.']
['system', 'ROUGE-S*', 'Average_R:', '0.03051', '(95%-conf.int.', '0.03051', '-', '0.03051)']
['system', 'ROUGE-S*', 'Average_P:', '0.18000', '(95%-conf.int.', '0.18000', '-', '0.18000)']
['system', 'ROUGE-S*', 'Average_F:', '0.05217', '(95%-conf.int.', '0.05217', '-', '0.05217)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:300', 'F:54']
0.209735997903 0.0164749998353 0.0301499996985





input/ref/Task1/A00-2018_sweta.csv
input/res/Task1/A00-2018.csv
parsing: input/ref/Task1/A00-2018_sweta.csv
 <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["17'"]
17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
 <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["17'"]
17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="11">Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &amp;quot;correct&amp;quot;, and statistics were collected on the resulting parses.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="10">(It is &amp;quot;soft&amp;quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.)</S>
original cit marker offset is 0
new cit marker offset is 0



["119'"]
119'
['119']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.</S>
original cit marker offset is 0
new cit marker offset is 0



["95'"]
95'
['95']
parsed_discourse_facet ['method_citation']
 <S sid="175" ssid="2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["175'"]
175'
['175']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
original cit marker offset is 0
new cit marker offset is 0



["92'"]
92'
['92']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="17">This is indicated in Figure 2, where the model labeled &amp;quot;Best&amp;quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["126'"]
126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="1">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c, t(c) (t for &amp;quot;tag&amp;quot;), then the lexical head of c, h(c), and then the expansion of c into further constituents e(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["174'"]
174'
['174']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="32">As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &amp;quot;before&amp;quot;), and the label of the grandparent of c (la).</S>
original cit marker offset is 0
new cit marker offset is 0



["63'"]
63'
['63']
parsed_discourse_facet ['method_citation']
<S sid="78" ssid="47">With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.</S>
original cit marker offset is 0
new cit marker offset is 0



["78'"]
78'
['78']
parsed_discourse_facet ['method_citation']
 <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="7">From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.</S>
original cit marker offset is 0
new cit marker offset is 0



["180'"]
180'
['180']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2018.csv
<S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'5'", "'2'", "'6'", "'125'"]
'1'
'5'
'2'
'6'
'125'
['1', '5', '2', '6', '125']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="175" ssid = "2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'1'", "'125'", "'175'"]
'2'
'6'
'1'
'125'
'175'
['2', '6', '1', '125', '175']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.</S><S sid ="182" ssid = "9">As noted above  the main methodological innovation presented here is our &quot;maximumentropy-inspired&quot; model for conditioning and smoothing.</S><S sid ="128" ssid = "19">The model labeled &quot;Old&quot; attempts to recreate the Char97 system using the current program.</S><S sid ="118" ssid = "9">First  it uses a clustering scheme on words to give the system a &quot;soft&quot; clustering of heads and sub-heads.</S><S sid ="116" ssid = "7">This is as opposed to the &quot;Markovgrammar&quot; approach used in the current parser.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'182'", "'128'", "'118'", "'116'"]
'3'
'182'
'128'
'118'
'116'
['3', '182', '128', '118', '116']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid ="139" ssid = "30">In Figure 2 we show that this one factor improves performance by nearly 2%.</S><S sid ="164" ssid = "55">When we do so using our maximum-entropy-inspired conditioning  we get another 0.45% improvement in average precision/recall  as indicated in Figure 2 on the line labeled &quot;MaxEnt-Inspired'.</S><S sid ="17" ssid = "6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S><S sid ="94" ssid = "5">We use the gathered statistics for all observed words  even those with very low counts  though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'139'", "'164'", "'17'", "'94'"]
'4'
'139'
'164'
'17'
'94'
['4', '139', '164', '17', '94']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'2'", "'6'", "'7'"]
'5'
'1'
'2'
'6'
'7'
['5', '1', '2', '6', '7']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="6" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="175" ssid = "2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'1'", "'125'", "'175'"]
'2'
'6'
'1'
'125'
'175'
['2', '6', '1', '125', '175']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid ="171" ssid = "62">As already noted  our best model uses a Markov-grammar approach.</S><S sid ="90" ssid = "1">We created a parser based upon the maximumentropy-inspired model of the last section  smoothed using standard deleted interpolation.</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'171'", "'90'", "'1'", "'5'"]
'7'
'171'
'90'
'1'
'5'
['7', '171', '90', '1', '5']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">That is  for all sentences s and all parses 7r  the parser assigns a probability p(s   7r) = p(r)  the equality holding when we restrict consideration to 7r whose yield * This research was supported in part by NSF grant LIS SBR 9720368.</S><S sid ="12" ssid = "1">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).</S><S sid ="10" ssid = "6">That is  the parser implements the function arg maxrp(7r s) = arg maxirp(7r  s) = arg maxrp(w).</S><S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S><S sid ="147" ssid = "38">For example  when we first guess the lexical head we can move from computing p(r I 1 1p  h) to p(r I /  t  /p  h).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'12'", "'10'", "'41'", "'147'"]
'8'
'12'
'10'
'41'
'147'
['8', '12', '10', '41', '147']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.</S><S sid ="185" ssid = "12">We expect that as we experiment with other  more semantic conditioning information  the importance of this aspect of the model will increase.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="171" ssid = "62">As already noted  our best model uses a Markov-grammar approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'185'", "'5'", "'1'", "'171'"]
'9'
'185'
'5'
'1'
'171'
['9', '185', '5', '1', '171']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">That is  the parser implements the function arg maxrp(7r s) = arg maxirp(7r  s) = arg maxrp(w).</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid ="83" ssid = "52">In the simple (content-free) form (Equation 6)  it is clear that Z(H) = 1.</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="56" ssid = "25">In max-entropy models one can simply include features for all three events f1 (a  b  c)  f2 (a  b)  and f3(a  c) and combine them in the model according to Equation 3  or equivalently  Equation 4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'13'", "'83'", "'54'", "'56'"]
'10'
'13'
'83'
'54'
'56'
['10', '13', '83', '54', '56']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">What fundamentally distinguishes probabilistic generative parsers is how they compute p(r)  and it is to that topic we turn next.</S><S sid ="53" ssid = "22">That is  suppose we want to compute a conditional probability p(a b  c)  but we are not sure that we have enough examples of the conditioning event b  c in the training corpus to ensure that the empirically obtained probability P (a I b  c) is accurate.</S><S sid ="82" ssid = "51">The major advantage of using Equation 7 is that one can generally get away without computing the partition function Z(H).</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="188" ssid = "15">Ultimately it is this flexibility that let us try the various conditioning events  to move on to a Markov grammar approach  and to try several Markov grammars of different orders  without significant programming.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'53'", "'82'", "'54'", "'188'"]
'11'
'53'
'82'
'54'
'188'
['11', '53', '82', '54', '188']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "1">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).</S><S sid ="18" ssid = "7">The method that gives the best results  however  uses a Markov grammar — a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6 10 15].</S><S sid ="134" ssid = "25">As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.</S><S sid ="147" ssid = "38">For example  when we first guess the lexical head we can move from computing p(r I 1 1p  h) to p(r I /  t  /p  h).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'18'", "'134'", "'147'", "'33'"]
'12'
'18'
'134'
'147'
'33'
['12', '18', '134', '147', '33']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid ="16" ssid = "5">In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.</S><S sid ="45" ssid = "14">The function Z(H)  called the partition function  is a normalizing constant (for fixed H)  so the probabilities over all a sum to one.</S><S sid ="83" ssid = "52">In the simple (content-free) form (Equation 6)  it is clear that Z(H) = 1.</S><S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'16'", "'45'", "'83'", "'41'"]
'13'
'16'
'45'
'83'
'41'
['13', '16', '45', '83', '41']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "3">Much of the interesting work is determining what goes into H (c).</S><S sid ="73" ssid = "42">We make one more point on the connection of Equation 7 to a maximum entropy formulation.</S><S sid ="60" ssid = "29">But let us look at how it works for a particular case in our parsing scheme.</S><S sid ="159" ssid = "50">Something very much like this is done in [15].</S><S sid ="151" ssid = "42">We have already noted the importance of conditioning on the parent label /p.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'73'", "'60'", "'159'", "'151'"]
'14'
'73'
'60'
'159'
'151'
['14', '73', '60', '159', '151']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "4">Whenever it is clear to which constituent we are referring we omit the (c) in  e.g.  h(c).</S><S sid ="67" ssid = "36">For example  it does not seem to make much sense to condition on  say  hp without first conditioning on ti .</S><S sid ="151" ssid = "42">We have already noted the importance of conditioning on the parent label /p.</S><S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="140" ssid = "31">It may not be obvious why this should make so great a difference  since most words are effectively unambiguous.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'67'", "'151'", "'157'", "'140'"]
'15'
'67'
'151'
'157'
'140'
['15', '67', '151', '157', '140']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "5">In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.</S><S sid ="65" ssid = "34">We can now rewrite this in the form of Equation 5 as follows: Here we have sequentially conditioned on steadily increasing portions of c's history.</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid ="58" ssid = "27">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid ="46" ssid = "15">Now for our purposes it is useful to rewrite this as a sequence of multiplicative functions gi(a  H) for 0 < i < j: Here go(a  H) = 11Z (H) and gi(a  H) = eAi(a H) fi(° 11).</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'65'", "'13'", "'58'", "'46'"]
'16'
'65'
'13'
'58'
'46'
['16', '65', '13', '58', '46']
parsed_discourse_facet ['method_citation']
['In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.']
['We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.', 'We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.00839', '(95%-conf.int.', '0.00839', '-', '0.00839)']
['system', 'ROUGE-S*', 'Average_P:', '0.11429', '(95%-conf.int.', '0.11429', '-', '0.11429)']
['system', 'ROUGE-S*', 'Average_F:', '0.01563', '(95%-conf.int.', '0.01563', '-', '0.01563)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:105', 'F:12']
['We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank.']
['The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.', 'We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length ', 'We expect that as we experiment with other  more semantic conditioning information  the importance of this aspect of the model will increase.']
['system', 'ROUGE-S*', 'Average_R:', '0.15796', '(95%-conf.int.', '0.15796', '-', '0.15796)']
['system', 'ROUGE-S*', 'Average_P:', '0.21587', '(95%-conf.int.', '0.21587', '-', '0.21587)']
['system', 'ROUGE-S*', 'Average_F:', '0.18243', '(95%-conf.int.', '0.18243', '-', '0.18243)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:630', 'F:136']
['Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &quot;correct&quot;, and statistics were collected on the resulting parses.']
["When we do so using our maximum-entropy-inspired conditioning  we get another 0.45% improvement in average precision/recall  as indicated in Figure 2 on the line labeled &quot;MaxEnt-Inspired'.", 'In Figure 2 we show that this one factor improves performance by nearly 2%.', "We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.", 'In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.', 'We use the gathered statistics for all observed words  even those with very low counts  though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.']
['system', 'ROUGE-S*', 'Average_R:', '0.00114', '(95%-conf.int.', '0.00114', '-', '0.00114)']
['system', 'ROUGE-S*', 'Average_P:', '0.01754', '(95%-conf.int.', '0.01754', '-', '0.01754)']
['system', 'ROUGE-S*', 'Average_F:', '0.00214', '(95%-conf.int.', '0.00214', '-', '0.00214)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:171', 'F:3']
['This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].']
['We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.', 'We created a parser based upon the maximumentropy-inspired model of the last section  smoothed using standard deleted interpolation.', 'We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length ', 'As already noted  our best model uses a Markov-grammar approach.', 'Following [5 10]  our parser is based upon a probabilistic generative model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00167', '(95%-conf.int.', '0.00167', '-', '0.00167)']
['system', 'ROUGE-S*', 'Average_P:', '0.06410', '(95%-conf.int.', '0.06410', '-', '0.06410)']
['system', 'ROUGE-S*', 'Average_F:', '0.00325', '(95%-conf.int.', '0.00325', '-', '0.00325)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:78', 'F:5']
['As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.']
['We have already noted the importance of conditioning on the parent label /p.', 'Whenever it is clear to which constituent we are referring we omit the (c) in  e.g.  h(c).', 'It may not be obvious why this should make so great a difference  since most words are effectively unambiguous.', 'Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.', 'For example  it does not seem to make much sense to condition on  say  hp without first conditioning on ti .']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:231', 'F:0']
['We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.']
['We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.', 'This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].', 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].', 'For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.', 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].']
['system', 'ROUGE-S*', 'Average_R:', '0.00027', '(95%-conf.int.', '0.00027', '-', '0.00027)']
['system', 'ROUGE-S*', 'Average_P:', '0.00526', '(95%-conf.int.', '0.00526', '-', '0.00526)']
['system', 'ROUGE-S*', 'Average_F:', '0.00051', '(95%-conf.int.', '0.00051', '-', '0.00051)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3741', 'P:190', 'F:1']
['In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.']
['As noted above  the main methodological innovation presented here is our &quot;maximumentropy-inspired&quot; model for conditioning and smoothing.', 'The model labeled &quot;Old&quot; attempts to recreate the Char97 system using the current program.', 'The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.', 'This is as opposed to the &quot;Markovgrammar&quot; approach used in the current parser.', 'First  it uses a clustering scheme on words to give the system a &quot;soft&quot; clustering of heads and sub-heads.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:105', 'F:0']
['(It is &quot;soft&quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.)']
['We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:153', 'P:120', 'F:0']
['We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.']
['We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.', 'This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].', 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].', 'For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.', 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].']
['system', 'ROUGE-S*', 'Average_R:', '0.16653', '(95%-conf.int.', '0.16653', '-', '0.16653)']
['system', 'ROUGE-S*', 'Average_P:', '0.65856', '(95%-conf.int.', '0.65856', '-', '0.65856)']
['system', 'ROUGE-S*', 'Average_F:', '0.26584', '(95%-conf.int.', '0.26584', '-', '0.26584)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3741', 'P:946', 'F:623']
["From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head."]
['Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.', 'Now for our purposes it is useful to rewrite this as a sequence of multiplicative functions gi(a  H) for 0 ', u'Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c \u2014 information outside c that our probability model deems important in determining the probability in question.', "We can now rewrite this in the form of Equation 5 as follows: Here we have sequentially conditioned on steadily increasing portions of c's history.", 'In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.']
['system', 'ROUGE-S*', 'Average_R:', '0.00036', '(95%-conf.int.', '0.00036', '-', '0.00036)']
['system', 'ROUGE-S*', 'Average_P:', '0.00362', '(95%-conf.int.', '0.00362', '-', '0.00362)']
['system', 'ROUGE-S*', 'Average_F:', '0.00066', '(95%-conf.int.', '0.00066', '-', '0.00066)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:276', 'F:1']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.']
['For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).', u'The method that gives the best results  however  uses a Markov grammar \xe2\u20ac\u201d a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6 10 15].', 'For example  when we first guess the lexical head we can move from computing p(r I 1 1p  h) to p(r I /  t  /p  h).', 'The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).', 'As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.']
['system', 'ROUGE-S*', 'Average_R:', '0.00606', '(95%-conf.int.', '0.00606', '-', '0.00606)']
['system', 'ROUGE-S*', 'Average_P:', '0.03202', '(95%-conf.int.', '0.03202', '-', '0.03202)']
['system', 'ROUGE-S*', 'Average_F:', '0.01019', '(95%-conf.int.', '0.01019', '-', '0.01019)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:406', 'F:13']
['With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.']
['But let us look at how it works for a particular case in our parsing scheme.', 'Something very much like this is done in [15].', 'We have already noted the importance of conditioning on the parent label /p.', 'We make one more point on the connection of Equation 7 to a maximum entropy formulation.', 'Much of the interesting work is determining what goes into H (c).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:45', 'F:0']
0.0926049992283 0.0285316664289 0.0400541663329





input/ref/Task1/W11-2123_swastika.csv
input/res/Task1/W11-2123.csv
parsing: input/ref/Task1/W11-2123_swastika.csv
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



['52']
52
['52']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="177" ssid="49">However, TRIE partitions storage by n-gram length, so walking the trie reads N disjoint pages.</S>
original cit marker offset is 0
new cit marker offset is 0



['177']
177
['177']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="278" ssid="5">We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns.</S>
original cit marker offset is 0
new cit marker offset is 0



['278']
278
['278']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/W11-2123.csv
<S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid ="7" ssid = "2">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'274'", "'7'", "'259'", "'45'"]
'1'
'274'
'7'
'259'
'45'
['1', '274', '7', '259', '45']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">The structure uses linear probing hash tables and is designed for speed.</S><S sid ="194" ssid = "13">Further  the probing hash table does only one random lookup per query  explaining why it is faster on large data.</S><S sid ="23" ssid = "1">We implement two data structures: PROBING  designed for speed  and TRIE  optimized for memory.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S><S sid ="120" ssid = "24">The compressed variant uses block compression and is rather slow as a result.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'194'", "'23'", "'45'", "'120'"]
'2'
'194'
'23'
'45'
'120'
['2', '194', '23', '45', '120']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S><S sid ="128" ssid = "32">These models generally outperform our memory consumption but are much slower  even when cached.</S><S sid ="161" ssid = "33">In both cases  SRILM walks its trie an additional time to minimize context as mentioned in Section 4.1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'275'", "'276'", "'128'", "'161'"]
'3'
'275'
'276'
'128'
'161'
['3', '275', '276', '128', '161']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Our code is thread-safe  and integrated into the Moses  cdec  and Joshua translation systems.</S><S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="169" ssid = "41">In our case multi-threading is trivial because our data structures are read-only and uncached.</S><S sid ="282" ssid = "3">Adam Pauls provided a pre-release comparison to BerkeleyLM and an initial Java interface.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'279'", "'169'", "'282'", "'266'"]
'4'
'279'
'169'
'282'
'266'
['4', '279', '169', '282', '266']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid ="26" ssid = "4">We use two common techniques  hash tables and sorted arrays  describing each before the model that uses the technique.</S><S sid ="129" ssid = "1">In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'274'", "'26'", "'129'"]
'5'
'1'
'274'
'26'
'129'
['5', '1', '274', '26', '129']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "1">Language models are widely applied in natural language processing  and applications such as machine translation make very frequent queries.</S><S sid ="130" ssid = "2">Applications such as machine translation use language model probability as a feature to assist in choosing between hypotheses.</S><S sid ="184" ssid = "3">Sparse lookup is a key subproblem of language model queries.</S><S sid ="7" ssid = "2">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'130'", "'184'", "'7'", "'259'"]
'6'
'130'
'184'
'7'
'259'
['6', '130', '184', '7', '259']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "2">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'1'", "'274'", "'259'", "'45'"]
'7'
'1'
'274'
'259'
'45'
['7', '1', '274', '259', '45']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "3">Queries take the form p(wn|wn−1 1 ) where wn1 is an n-gram.</S><S sid ="84" ssid = "62">In a model we built with default settings  1.2% of n + 1-grams were missing their ngram suffix.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="72" ssid = "50">We maintain a separate array for each length n containing all n-gram entries sorted in suffix order.</S><S sid ="46" ssid = "24">Unigram lookup is dense so we use an array of probability and backoff values.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'84'", "'88'", "'72'", "'46'"]
'8'
'84'
'88'
'72'
'46'
['8', '84', '88', '72', '46']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "4">Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf   returning where the probability p(wn|wn−1 f ) and backoff penalties b(wn−1 i ) are given by an already-estimated model.</S><S sid ="127" ssid = "31">Minimal perfect hashing is used to find the index at which a quantized probability and possibly backoff are stored.</S><S sid ="136" ssid = "8">We offer a state function s(wn1) = wn� where substring wn� is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.</S><S sid ="78" ssid = "56">Unigram records store probability  backoff  and an index in the bigram table.</S><S sid ="154" ssid = "26">To optimize left-to-right queries  we extend state to store backoff information: where m is the minimal context from Section 4.1 and b is the backoff penalty.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'127'", "'136'", "'78'", "'154'"]
'9'
'127'
'136'
'78'
'154'
['9', '127', '136', '78', '154']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "5">The problem is to store these two values for a large and sparse set of n-grams in a way that makes queries efficient.</S><S sid ="25" ssid = "3">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S><S sid ="24" ssid = "2">The set of n-grams appearing in a model is sparse  and we want to efficiently find their associated probabilities and backoff penalties.</S><S sid ="93" ssid = "71">The cost of storing these averages  in bits  is Because there are comparatively few unigrams  we elected to store them byte-aligned and unquantized  making every query faster.</S><S sid ="85" ssid = "63">This causes a problem for reverse trie implementations  including SRILM itself  because it leaves n+1-grams without an n-gram node pointing to them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'25'", "'24'", "'93'", "'85'"]
'10'
'25'
'24'
'93'
'85'
['10', '25', '24', '93', '85']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "6">Many packages perform language model queries.</S><S sid ="68" ssid = "46">The trie data structure is commonly used for language modeling.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'68'", "'266'", "'259'", "'45'"]
'11'
'68'
'266'
'259'
'45'
['11', '68', '266', '259', '45']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke  2002) is a popular toolkit based on tries used in several decoders.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="97" ssid = "1">SRILM (Stolcke  2002) is widely used within academia.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'13'", "'14'", "'103'", "'97'"]
'12'
'13'
'14'
'103'
'97'
['12', '13', '14', '103', '97']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'103'", "'15'", "'14'", "'262'"]
'13'
'103'
'15'
'14'
'262'
['13', '103', '15', '14', '262']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="97" ssid = "1">SRILM (Stolcke  2002) is widely used within academia.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'15'", "'103'", "'97'", "'266'"]
'14'
'15'
'103'
'97'
'266'
['14', '15', '103', '97', '266']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'14'", "'103'", "'13'", "'262'"]
'15'
'14'
'103'
'13'
'262'
['15', '14', '103', '13', '262']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "11">BerkeleyLM revision 152 (Pauls and Klein  2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.</S><S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="51" ssid = "29">This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'15'", "'51'", "'262'", "'13'"]
'16'
'15'
'51'
'262'
'13'
['16', '15', '51', '262', '13']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "12">Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.</S><S sid ="18" ssid = "13">TPT Germann et al. (2009) describe tries with better locality properties  but did not release code.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="215" ssid = "34">Another option is the closedsource data structures from Sheffield (Guthrie and Hepple  2010).</S><S sid ="114" ssid = "18">Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'18'", "'262'", "'215'", "'114'"]
'17'
'18'
'262'
'215'
'114'
['17', '18', '262', '215', '114']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "13">TPT Germann et al. (2009) describe tries with better locality properties  but did not release code.</S><S sid ="17" ssid = "12">Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.</S><S sid ="232" ssid = "51">Memory usage is likely much lower than ours. fThe original paper (Germann et al.  2009) provided only 2s of query timing and compared with SRI when it exceeded available RAM.</S><S sid ="114" ssid = "18">Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'17'", "'232'", "'114'", "'262'"]
'18'
'17'
'232'
'114'
'262'
['18', '17', '232', '114', '262']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "14">These packages are further described in Section 3.</S><S sid ="129" ssid = "1">In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="5" ssid = "5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S sid ="150" ssid = "22">Section 4.1 explained that state s is stored by applications with partial hypotheses to determine when they can be recombined.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'129'", "'266'", "'5'", "'150'"]
'19'
'129'
'266'
'5'
'150'
['19', '129', '266', '5', '150']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "15">We substantially outperform all of them on query speed and offer lower memory consumption than lossless alternatives.</S><S sid ="128" ssid = "32">These models generally outperform our memory consumption but are much slower  even when cached.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S><S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'128'", "'276'", "'1'", "'275'"]
'20'
'128'
'276'
'1'
'275'
['20', '128', '276', '1', '275']
parsed_discourse_facet ['aim_citation']
['The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.']
['IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.', 'IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.', 'RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.', 'MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:120', 'F:0']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.', 'IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.', 'Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke  2002) is a popular toolkit based on tries used in several decoders.', 'MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.', 'SRILM (Stolcke  2002) is widely used within academia.']
['system', 'ROUGE-S*', 'Average_R:', '0.00135', '(95%-conf.int.', '0.00135', '-', '0.00135)']
['system', 'ROUGE-S*', 'Average_P:', '0.02564', '(95%-conf.int.', '0.02564', '-', '0.02564)']
['system', 'ROUGE-S*', 'Average_F:', '0.00256', '(95%-conf.int.', '0.00256', '-', '0.00256)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:78', 'F:2']
['We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns.']
['Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', 'TPT Germann et al. (2009) describe tries with better locality properties  but did not release code.', 'Memory usage is likely much lower than ours. fThe original paper (Germann et al.  2009) provided only 2s of query timing and compared with SRI when it exceeded available RAM.', 'Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.', 'Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:91', 'F:0']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.', 'We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.', 'We use two common techniques  hash tables and sorted arrays  describing each before the model that uses the technique.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.', 'In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.']
['system', 'ROUGE-S*', 'Average_R:', '0.06118', '(95%-conf.int.', '0.06118', '-', '0.06118)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.11530', '(95%-conf.int.', '0.11530', '-', '0.11530)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:78', 'F:78']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['These models generally outperform our memory consumption but are much slower  even when cached.', 'Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', 'The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.', 'The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.', 'In both cases  SRILM walks its trie an additional time to minimize context as mentioned in Section 4.1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00088', '(95%-conf.int.', '0.00088', '-', '0.00088)']
['system', 'ROUGE-S*', 'Average_P:', '0.03636', '(95%-conf.int.', '0.03636', '-', '0.03636)']
['system', 'ROUGE-S*', 'Average_F:', '0.00171', '(95%-conf.int.', '0.00171', '-', '0.00171)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:55', 'F:2']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.', 'Adam Pauls provided a pre-release comparison to BerkeleyLM and an initial Java interface.', 'Our code is thread-safe  and integrated into the Moses  cdec  and Joshua translation systems.', 'Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.', 'In our case multi-threading is trivial because our data structures are read-only and uncached.']
['system', 'ROUGE-S*', 'Average_R:', '0.00606', '(95%-conf.int.', '0.00606', '-', '0.00606)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.01124', '(95%-conf.int.', '0.01124', '-', '0.01124)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:78', 'F:6']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['Many packages perform language model queries.', 'The trie data structure is commonly used for language modeling.', 'The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.', 'There any many techniques for improving language model speed and reducing memory consumption.', 'Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.']
['system', 'ROUGE-S*', 'Average_R:', '0.00385', '(95%-conf.int.', '0.00385', '-', '0.00385)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00719', '(95%-conf.int.', '0.00719', '-', '0.00719)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:55', 'F:3']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.', 'TPT Germann et al. (2009) describe tries with better locality properties  but did not release code.', 'Another option is the closedsource data structures from Sheffield (Guthrie and Hepple  2010).', 'Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:55', 'F:0']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['The compressed variant uses block compression and is rather slow as a result.', 'Further  the probing hash table does only one random lookup per query  explaining why it is faster on large data.', 'The structure uses linear probing hash tables and is designed for speed.', 'The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.', 'We implement two data structures: PROBING  designed for speed  and TRIE  optimized for memory.']
['system', 'ROUGE-S*', 'Average_R:', '0.00886', '(95%-conf.int.', '0.00886', '-', '0.00886)']
['system', 'ROUGE-S*', 'Average_P:', '0.10256', '(95%-conf.int.', '0.10256', '-', '0.10256)']
['system', 'ROUGE-S*', 'Average_F:', '0.01631', '(95%-conf.int.', '0.01631', '-', '0.01631)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:78', 'F:8']
['The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.']
['This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.', 'IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.', 'RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', 'BerkeleyLM revision 152 (Pauls and Klein  2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:120', 'F:0']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.', 'There any many techniques for improving language model speed and reducing memory consumption.', 'We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.', 'The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.', 'This paper presents methods to query N-gram language models  minimizing time and space costs.']
['system', 'ROUGE-S*', 'Average_R:', '0.05660', '(95%-conf.int.', '0.05660', '-', '0.05660)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.10714', '(95%-conf.int.', '0.10714', '-', '0.10714)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:78', 'F:78']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.', 'There any many techniques for improving language model speed and reducing memory consumption.', 'We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.', 'The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.', 'This paper presents methods to query N-gram language models  minimizing time and space costs.']
['system', 'ROUGE-S*', 'Average_R:', '0.05660', '(95%-conf.int.', '0.05660', '-', '0.05660)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.10714', '(95%-conf.int.', '0.10714', '-', '0.10714)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:78', 'F:78']
['Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.']
['There any many techniques for improving language model speed and reducing memory consumption.', 'Applications such as machine translation use language model probability as a feature to assist in choosing between hypotheses.', 'This paper presents methods to query N-gram language models  minimizing time and space costs.', 'Sparse lookup is a key subproblem of language model queries.', 'Language models are widely applied in natural language processing  and applications such as machine translation make very frequent queries.']
['system', 'ROUGE-S*', 'Average_R:', '0.00510', '(95%-conf.int.', '0.00510', '-', '0.00510)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.00966', '(95%-conf.int.', '0.00966', '-', '0.00966)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:66', 'F:6']
['The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.']
['IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.', 'IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.', 'RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.', 'MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:120', 'F:0']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['These models generally outperform our memory consumption but are much slower  even when cached.', 'We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.', 'We substantially outperform all of them on query speed and offer lower memory consumption than lossless alternatives.', 'The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.', 'The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.']
['system', 'ROUGE-S*', 'Average_R:', '0.01241', '(95%-conf.int.', '0.01241', '-', '0.01241)']
['system', 'ROUGE-S*', 'Average_P:', '0.25455', '(95%-conf.int.', '0.25455', '-', '0.25455)']
['system', 'ROUGE-S*', 'Average_F:', '0.02367', '(95%-conf.int.', '0.02367', '-', '0.02367)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:55', 'F:14']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['Unigram lookup is dense so we use an array of probability and backoff values.', u'Queries take the form p(wn|wn\u22121 1 ) where wn1 is an n-gram.', u'By contrast  BerkeleyLM\u2019s hash and compressed variants will return incorrect results based on an n \u22121-gram.', 'We maintain a separate array for each length n containing all n-gram entries sorted in suffix order.', 'In a model we built with default settings  1.2% of n + 1-grams were missing their ngram suffix.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:78', 'F:0']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.', 'RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.', 'MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.', 'SRILM (Stolcke  2002) is widely used within academia.', 'Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.']
['system', 'ROUGE-S*', 'Average_R:', '0.00226', '(95%-conf.int.', '0.00226', '-', '0.00226)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00434', '(95%-conf.int.', '0.00434', '-', '0.00434)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:55', 'F:3']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['These packages are further described in Section 3.', 'In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.', 'Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.', 'Section 4.1 explained that state s is stored by applications with partial hypotheses to determine when they can be recombined.']
['system', 'ROUGE-S*', 'Average_R:', '0.00664', '(95%-conf.int.', '0.00664', '-', '0.00664)']
['system', 'ROUGE-S*', 'Average_P:', '0.10909', '(95%-conf.int.', '0.10909', '-', '0.10909)']
['system', 'ROUGE-S*', 'Average_F:', '0.01253', '(95%-conf.int.', '0.01253', '-', '0.01253)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:55', 'F:6']
0.211396109937 0.0123216665982 0.0232661109819





input/ref/Task1/P08-1043_sweta.csv
input/res/Task1/P08-1043.csv
parsing: input/ref/Task1/P08-1043_sweta.csv
<S sid="156" ssid="34">To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["4'"]
4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="2">Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.</S>
original cit marker offset is 0
new cit marker offset is 0



["70'"]
70'
['70']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="3">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="9">Tsarfaty (2006) used a morphological analyzer (Segal, 2000), a PoS tagger (Bar-Haim et al., 2005), and a general purpose parser (Schmid, 2000) in an integrated framework in which morphological and syntactic components interact to share information, leading to improved performance on the joint task.</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="107" ssid="39">Firstly, Hebrew unknown tokens are doubly unknown: each unknown token may correspond to several segmentation possibilities, and each segment in such sequences may be able to admit multiple PoS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["107'"]
107'
['107']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="10">The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["14'"]
14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="12">The current work treats both segmental and super-segmental phenomena, yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg, 2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="53" ssid="11">Both (Tsarfaty, 2006; Cohen and Smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has shown a single generative model which handles both tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["53'"]
53'
['53']
parsed_discourse_facet ['method_citation']
 <S sid="48" ssid="6">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["48'"]
48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="141" ssid="19">This analyzer setting is similar to that of (Cohen and Smith, 2007), and models using it are denoted nohsp, Parser and Grammar We used BitPar (Schmid, 2004), an efficient general purpose parser,10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["141'"]
141'
['141']
parsed_discourse_facet ['method_citation']
<S sid="188" ssid="2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



["188'"]
188'
['188']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["94'"]
94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="12">Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training data similar to (Cohen and Smith, 2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
    <S sid="156" ssid="34">To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth, &#8216;tokens&#8217;) that constitute the unanalyzed surface forms (utterances).</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S sid ="1" ssid = "1">Morphological processes in Semitic languages deliver space-delimited words which introduce multiple  distinct  syntactic units into the structure of the input sentence.</S><S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="39" ssid = "18">Such ambiguities cause discrepancies between token boundaries (indexed as white spaces) and constituent boundaries (imposed by syntactic categories) with respect to a surface form.</S><S sid ="34" ssid = "13">Lexical and Morphological Ambiguity The rich morphological processes for deriving Hebrew stems give rise to a high degree of ambiguity for Hebrew space-delimited tokens.</S><S sid ="24" ssid = "3">Co-occurrences among the particles themselves are subject to further syntactic and lexical constraints relative to the stem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'14'", "'39'", "'34'", "'24'"]
'1'
'14'
'39'
'34'
'24'
['1', '14', '39', '34', '24']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.</S><S sid ="16" ssid = "12">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S><S sid ="94" ssid = "26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S><S sid ="20" ssid = "16">We claim that no particular morphological segmentation is a-priory more likely for surface forms before exploring the compositional nature of syntactic structures  including manifestations of various long-distance dependencies.</S><S sid ="191" ssid = "5">In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'16'", "'94'", "'20'", "'191'"]
'2'
'16'
'94'
'20'
'191'
['2', '16', '94', '20', '191']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S><S sid ="19" ssid = "15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S sid ="187" ssid = "1">Employing a PCFG-based generative framework to make both syntactic and morphological disambiguation decisions is not only theoretically clean and linguistically justified and but also probabilistically apropriate and empirically sound.</S><S sid ="189" ssid = "3">Better grammars are shown here to improve performance on both morphological and syntactic tasks  providing support for the advantage of a joint framework over pipelined or factorized ones.</S><S sid ="21" ssid = "17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'19'", "'187'", "'189'", "'21'"]
'3'
'19'
'187'
'189'
'21'
['3', '19', '187', '189', '21']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Using a treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined  integrated or factorized systems for Hebrew morphological and syntactic processing  yielding an error reduction of 12% over the best published results so far.</S><S sid ="21" ssid = "17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S><S sid ="185" ssid = "23">In contrast  our morphological probabilities are based on a unigram  lexeme-based model  and all other (local and non-local) contextual considerations are delegated to the PCFG.</S><S sid ="188" ssid = "2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S><S sid ="191" ssid = "5">In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'21'", "'185'", "'188'", "'191'"]
'4'
'21'
'185'
'188'
'191'
['4', '21', '185', '188', '191']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  ‘tokens’) that constitute the unanalyzed surface forms (utterances).</S><S sid ="12" ssid = "8">This leads to word- and constituent-boundaries discrepancy  which breaks the assumptions underlying current state-of-the-art statistical parsers.</S><S sid ="94" ssid = "26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S><S sid ="21" ssid = "17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S><S sid ="16" ssid = "12">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'12'", "'94'", "'21'", "'16'"]
'5'
'12'
'94'
'21'
'16'
['5', '12', '94', '21', '16']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">In Semitic languages the situation is very different.</S><S sid ="78" ssid = "10">A similar structure is used in speech recognition.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="99" ssid = "31">This is done using a simple PCFG which is lexemebased.</S><S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'78'", "'92'", "'99'", "'180'"]
'6'
'78'
'92'
'99'
'180'
['6', '78', '92', '99', '180']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).</S><S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="17" ssid = "13">Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task.</S><S sid ="8" ssid = "4">The Hebrew token ‘bcl’1  for example  stands for the complete prepositional phrase 'We adopt here the transliteration of (Sima’an et al.  2001).</S><S sid ="29" ssid = "8">A less canonical representation of segmental morphology is triggered by a morpho-phonological process of omitting the definite article h when occurring after the particles b or l. This process triggers ambiguity as for the definiteness status of Nouns following these particles.We refer to such cases in which the concatenation of elements does not strictly correspond to the original surface form as super-segmental morphology.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'14'", "'17'", "'8'", "'29'"]
'7'
'14'
'17'
'8'
'29'
['7', '14', '17', '8', '29']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">The Hebrew token ‘bcl’1  for example  stands for the complete prepositional phrase 'We adopt here the transliteration of (Sima’an et al.  2001).</S><S sid ="35" ssid = "14">The form fmnh  for example  can be understood as the verb “lubricated”  the possessed noun “her oil”  the adjective “fat” or the verb “got fat”.</S><S sid ="62" ssid = "9">When a token fmnh is to be interpreted as the lexeme sequence f/REL mnh/VB  the analysis introduces two distinct entities  the relativizer f (“that”) and the verb mnh (“counted”)  and not as the complex entity “that counted”.</S><S sid ="7" ssid = "3">In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).</S><S sid ="15" ssid = "11">The aforementioned surface form bcl  for example  may also stand for the lexical item “onion”  a Noun.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'35'", "'62'", "'7'", "'15'"]
'8'
'35'
'62'
'7'
'15'
['8', '35', '62', '7', '15']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">“in the shadow”.</S><S sid ="184" ssid = "22">Oracle results).</S><S sid ="109" ssid = "41">The latter arcs correspond to OOV words in English.</S><S sid ="113" ssid = "45">The remaining arcs are marked OOV.</S><S sid ="78" ssid = "10">A similar structure is used in speech recognition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'184'", "'109'", "'113'", "'78'"]
'9'
'184'
'109'
'113'
'78'
['9', '184', '109', '113', '78']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">This token may further embed into a larger utterance  e.g.  ‘bcl hneim’ (literally “in-the-shadow the-pleasant”  meaning roughly “in the pleasant shadow”) in which the dominated Noun is modified by a proceeding space-delimited adjective.</S><S sid ="15" ssid = "11">The aforementioned surface form bcl  for example  may also stand for the lexical item “onion”  a Noun.</S><S sid ="76" ssid = "8">Furthermore  some of the arcs represent lexemes not present in the input tokens (e.g. h/DT  fl/POS)  however these are parts of valid analyses of the token (cf. super-segmental morphology section 2).</S><S sid ="58" ssid = "5">Such tag sequences are often treated as “complex tags” (e.g.</S><S sid ="108" ssid = "40">Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p —* (s  p)) > 0  while other segments have never been observed as a lexical event before.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'15'", "'76'", "'58'", "'108'"]
'10'
'15'
'76'
'58'
'108'
['10', '15', '76', '58', '108']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">It should be clear from the onset that the particle b (“in”) in ‘bcl’ may then attach higher than the bare noun cl (“shadow”).</S><S sid ="37" ssid = "16">The same form fmnh can be segmented as f-mnh  f (“that”) functioning as a reletivizer with the form mnh.</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="38" ssid = "17">The form mnh itself can be read as at least three different verbs (“counted”  “appointed”  “was appointed”)  a noun (“a portion”)  and a possessed noun (“her kind”).</S><S sid ="26" ssid = "5">The relativizer f(“that”) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'37'", "'57'", "'38'", "'26'"]
'11'
'37'
'57'
'38'
'26'
['11', '37', '57', '38', '26']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">This leads to word- and constituent-boundaries discrepancy  which breaks the assumptions underlying current state-of-the-art statistical parsers.</S><S sid ="5" ssid = "1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  ‘tokens’) that constitute the unanalyzed surface forms (utterances).</S><S sid ="13" ssid = "9">One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).</S><S sid ="2" ssid = "2">These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.</S><S sid ="19" ssid = "15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'5'", "'13'", "'2'", "'19'"]
'12'
'5'
'13'
'2'
'19'
['12', '5', '13', '2', '19']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).</S><S sid ="33" ssid = "12">The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).</S><S sid ="140" ssid = "18">For these models we limit the options provided for OOV words by not considering the entire token as a valid segmentation in case at least some prefix segmentation exists.</S><S sid ="139" ssid = "17">To control for the effect of the HSPELL-based pruning  we also experimented with a morphological analyzer that does not perform this pruning.</S><S sid ="3" ssid = "3">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'33'", "'140'", "'139'", "'3'"]
'13'
'33'
'140'
'139'
'3'
['13', '33', '140', '139', '3']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="7" ssid = "3">In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).</S><S sid ="140" ssid = "18">For these models we limit the options provided for OOV words by not considering the entire token as a valid segmentation in case at least some prefix segmentation exists.</S><S sid ="17" ssid = "13">Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task.</S><S sid ="121" ssid = "53">In such cases we use the non-pruned lattice including all (possibly ungrammatical) segmentation  and let the statistics (including OOV) decide.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'7'", "'140'", "'17'", "'121'"]
'14'
'7'
'140'
'17'
'121'
['14', '7', '140', '17', '121']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">The aforementioned surface form bcl  for example  may also stand for the lexical item “onion”  a Noun.</S><S sid ="32" ssid = "11">The additional morphological material in such cases appears after the stem and realizes the extended meaning.</S><S sid ="28" ssid = "7">The same argument holds for resolving PP attachment of a prefixed preposition or marking conjunction of elements of any kind.</S><S sid ="10" ssid = "6">This token may further embed into a larger utterance  e.g.  ‘bcl hneim’ (literally “in-the-shadow the-pleasant”  meaning roughly “in the pleasant shadow”) in which the dominated Noun is modified by a proceeding space-delimited adjective.</S><S sid ="30" ssid = "9">An additional case of super-segmental morphology is the case of Pronominal Clitics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'32'", "'28'", "'10'", "'30'"]
'15'
'32'
'28'
'10'
'30'
['15', '32', '28', '10', '30']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S><S sid ="94" ssid = "26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S><S sid ="188" ssid = "2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S><S sid ="13" ssid = "9">One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).</S><S sid ="2" ssid = "2">These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'94'", "'188'", "'13'", "'2'"]
'16'
'94'
'188'
'13'
'2'
['16', '94', '188', '13', '2']
parsed_discourse_facet ['method_citation']
['The current work treats both segmental and super-segmental phenomena, yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg, 2008).']
[u"The Hebrew token \u2018bcl\u20191  for example  stands for the complete prepositional phrase 'We adopt here the transliteration of (Sima\u2019an et al.  2001).", u'When a token fmnh is to be interpreted as the lexeme sequence f/REL mnh/VB  the analysis introduces two distinct entities  the relativizer f (\u201cthat\u201d) and the verb mnh (\u201ccounted\u201d)  and not as the complex entity \u201cthat counted\u201d.', u'The aforementioned surface form bcl  for example  may also stand for the lexical item \u201conion\u201d  a Noun.', 'In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).', u'The form fmnh  for example  can be understood as the verb \u201clubricated\u201d  the possessed noun \u201cher oil\u201d  the adjective \u201cfat\u201d or the verb \u201cgot fat\u201d.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:210', 'F:0']
['To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).']
['Morphological processes in Semitic languages deliver space-delimited words which introduce multiple  distinct  syntactic units into the structure of the input sentence.', 'Co-occurrences among the particles themselves are subject to further syntactic and lexical constraints relative to the stem.', 'Such ambiguities cause discrepancies between token boundaries (indexed as white spaces) and constituent boundaries (imposed by syntactic categories) with respect to a surface form.', 'The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).', 'Lexical and Morphological Ambiguity The rich morphological processes for deriving Hebrew stems give rise to a high degree of ambiguity for Hebrew space-delimited tokens.']
['system', 'ROUGE-S*', 'Average_R:', '0.00667', '(95%-conf.int.', '0.00667', '-', '0.00667)']
['system', 'ROUGE-S*', 'Average_P:', '0.03016', '(95%-conf.int.', '0.03016', '-', '0.03016)']
['system', 'ROUGE-S*', 'Average_F:', '0.01092', '(95%-conf.int.', '0.01092', '-', '0.01092)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:630', 'F:19']
['Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.']
[u'This token may further embed into a larger utterance  e.g.  \u2018bcl hneim\u2019 (literally \u201cin-the-shadow the-pleasant\u201d  meaning roughly \u201cin the pleasant shadow\u201d) in which the dominated Noun is modified by a proceeding space-delimited adjective.', u'Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p \u2014* (s  p)) > 0  while other segments have never been observed as a lexical event before.', u'The aforementioned surface form bcl  for example  may also stand for the lexical item \u201conion\u201d  a Noun.', u'Such tag sequences are often treated as \u201ccomplex tags\u201d (e.g.', 'Furthermore  some of the arcs represent lexemes not present in the input tokens (e.g. h/DT  fl/POS)  however these are parts of valid analyses of the token (cf. super-segmental morphology section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:78', 'F:0']
['Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.']
['The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.', 'In contrast  our morphological probabilities are based on a unigram  lexeme-based model  and all other (local and non-local) contextual considerations are delegated to the PCFG.', 'In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.', 'Using a treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined  integrated or factorized systems for Hebrew morphological and syntactic processing  yielding an error reduction of 12% over the best published results so far.', 'Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2']
['system', 'ROUGE-S*', 'Average_R:', '0.00288', '(95%-conf.int.', '0.00288', '-', '0.00288)']
['system', 'ROUGE-S*', 'Average_P:', '0.40000', '(95%-conf.int.', '0.40000', '-', '0.40000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00573', '(95%-conf.int.', '0.00573', '-', '0.00573)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:7626', 'P:55', 'F:22']
['Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.', 'Better grammars are shown here to improve performance on both morphological and syntactic tasks  providing support for the advantage of a joint framework over pipelined or factorized ones.', 'Employing a PCFG-based generative framework to make both syntactic and morphological disambiguation decisions is not only theoretically clean and linguistically justified and but also probabilistically apropriate and empirically sound.', 'Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2']
['system', 'ROUGE-S*', 'Average_R:', '0.00063', '(95%-conf.int.', '0.00063', '-', '0.00063)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00124', '(95%-conf.int.', '0.00124', '-', '0.00124)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4753', 'P:78', 'F:3']
['Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.', 'These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.', 'We claim that no particular morphological segmentation is a-priory more likely for surface forms before exploring the compositional nature of syntactic structures  including manifestations of various long-distance dependencies.', 'Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.', 'The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.']
['system', 'ROUGE-S*', 'Average_R:', '0.00430', '(95%-conf.int.', '0.00430', '-', '0.00430)']
['system', 'ROUGE-S*', 'Average_P:', '0.02910', '(95%-conf.int.', '0.02910', '-', '0.02910)']
['system', 'ROUGE-S*', 'Average_F:', '0.00750', '(95%-conf.int.', '0.00750', '-', '0.00750)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:378', 'F:11']
['The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006).']
[u"The Hebrew token \u2018bcl\u20191  for example  stands for the complete prepositional phrase 'We adopt here the transliteration of (Sima\u2019an et al.  2001).", 'Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task.', 'The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).', 'In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).', 'A less canonical representation of segmental morphology is triggered by a morpho-phonological process of omitting the definite article h when occurring after the particles b or l. This process triggers ambiguity as for the definiteness status of Nouns following these particles.We refer to such cases in which the concatenation of elements does not strictly correspond to the original surface form as super-segmental morphology.']
['system', 'ROUGE-S*', 'Average_R:', '0.03015', '(95%-conf.int.', '0.03015', '-', '0.03015)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.05854', '(95%-conf.int.', '0.05854', '-', '0.05854)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5671', 'P:171', 'F:171']
['Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth, &#8216;tokens&#8217;) that constitute the unanalyzed surface forms (utterances).']
['The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.', 'These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.', 'One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).', 'Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.', 'The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.']
['system', 'ROUGE-S*', 'Average_R:', '0.01941', '(95%-conf.int.', '0.01941', '-', '0.01941)']
['system', 'ROUGE-S*', 'Average_P:', '0.22529', '(95%-conf.int.', '0.22529', '-', '0.22529)']
['system', 'ROUGE-S*', 'Average_F:', '0.03573', '(95%-conf.int.', '0.03573', '-', '0.03573)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5050', 'P:435', 'F:98']
['Tsarfaty (2006) used a morphological analyzer (Segal, 2000), a PoS tagger (Bar-Haim et al., 2005), and a general purpose parser (Schmid, 2000) in an integrated framework in which morphological and syntactic components interact to share information, leading to improved performance on the joint task.']
['The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.', 'This leads to word- and constituent-boundaries discrepancy  which breaks the assumptions underlying current state-of-the-art statistical parsers.', u'Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  \u2018tokens\u2019) that constitute the unanalyzed surface forms (utterances).', 'Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.', 'Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2']
['system', 'ROUGE-S*', 'Average_R:', '0.00279', '(95%-conf.int.', '0.00279', '-', '0.00279)']
['system', 'ROUGE-S*', 'Average_P:', '0.04138', '(95%-conf.int.', '0.04138', '-', '0.04138)']
['system', 'ROUGE-S*', 'Average_F:', '0.00524', '(95%-conf.int.', '0.00524', '-', '0.00524)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6441', 'P:435', 'F:18']
['Both (Tsarfaty, 2006; Cohen and Smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has shown a single generative model which handles both tasks.']
[u'\u201cin the shadow\u201d.', 'The latter arcs correspond to OOV words in English.', 'The remaining arcs are marked OOV.', 'Oracle results).', 'A similar structure is used in speech recognition.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:120', 'P:171', 'F:0']
0.176438998236 0.00668299993317 0.0124899998751





input/ref/Task1/P04-1036_sweta.csv
input/res/Task1/P04-1036.csv
parsing: input/ref/Task1/P04-1036_sweta.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="82" ssid="11">We also calculate the WSD accuracy that would be obtained on SemCor, when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["82'"]
82'
['82']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="20">We briefly summarise the two measures here; for a more detailed summary see (Patwardhan et al., 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="172" ssid="20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="1">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="165" ssid="13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus, whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S>
original cit marker offset is 0
new cit marker offset is 0



["165'"]
165'
['165']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["171'"]
171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["89'"]
89'
['89']
parsed_discourse_facet ['method_citation']
<S sid="172" ssid="20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="12">Additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["189'"]
189'
['189']
parsed_discourse_facet ['method_citation']
<S sid="87" ssid="16">Again, the automatic ranking outperforms this by a large margin.</S>
original cit marker offset is 0
new cit marker offset is 0



["87'"]
87'
['87']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["89'"]
89'
['89']
parsed_discourse_facet ['method_citation']
 <S sid="137" ssid="14">Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["137'"]
137'
['137']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="19">We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline, but because of space limitations give results for the two which perform the best.</S>
original cit marker offset is 0
new cit marker offset is 0



["63'"]
63'
['63']
parsed_discourse_facet ['method_citation']
<S sid="159" ssid="7">Magnini and Cavagli`a (2000) have identified WordNet word senses with particular domains, and this has proven useful for high precision WSD (Magnini et al., 2001); indeed in section 5 we used these domain labels for evaluation.</S>
original cit marker offset is 0
new cit marker offset is 0



["159'"]
159'
['159']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="48" ssid = "4">To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.</S><S sid ="105" ssid = "3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents  rather than a lexical sample task  where the target words are manually determined and the results will depend on the skew of the words in the sample.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'153'", "'154'", "'48'", "'105'"]
'1'
'153'
'154'
'48'
'105'
['1', '153', '154', '48', '105']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">The problem with using the predominant  or first sense heuristic  aside from the fact that it does not take surrounding context into account  is that it assumes some quantity of handtagged data.</S><S sid ="17" ssid = "10">There are words where the first sense in WordNet is counter-intuitive  because of the size of the corpus  and because where the frequency data does not indicate a first sense  the ordering is arbitrary.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'17'", "'91'", "'154'", "'123'"]
'2'
'17'
'91'
'154'
'123'
['2', '17', '91', '154', '123']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.</S><S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="37" ssid = "30">From inspection  one can see that the ordered neighbours of such a thesaurus relate to the different senses of the target word.</S><S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'15'", "'37'", "'1'", "'170'"]
'3'
'15'
'37'
'1'
'170'
['3', '15', '37', '1', '170']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S sid ="179" ssid = "2">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S><S sid ="171" ssid = "19">In contrast  we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'179'", "'178'", "'45'", "'171'"]
'4'
'179'
'178'
'45'
'171'
['4', '179', '178', '45', '171']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="84" ssid = "13">The random baseline for choosing the predominant sense over all these words ( ) is 32%.</S><S sid ="88" ssid = "17">The first sense in SemCor provides an upperbound for this task of 67%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'100'", "'116'", "'84'", "'88'"]
'5'
'100'
'116'
'84'
'88'
['5', '100', '116', '84', '88']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="121" ssid = "19">For these one would need to obtain more sense-tagged text in order to use this heuristic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'126'", "'178'", "'13'", "'121'"]
'6'
'126'
'178'
'13'
'121'
['6', '126', '178', '13', '121']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S><S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S><S sid ="158" ssid = "6">We have evaluated our method using publically available resources  both for balanced and domain specific text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'184'", "'178'", "'126'", "'158'"]
'7'
'184'
'178'
'126'
'158'
['7', '184', '178', '126', '158']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="172" ssid = "20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet  for example those that use it for lexical acquisition or WSD.</S><S sid ="115" ssid = "13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor  which is very encouraging given that our method only uses raw text  with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'154'", "'14'", "'172'", "'115'"]
'8'
'154'
'14'
'172'
'115'
['8', '154', '14', '172', '115']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "2">This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).</S><S sid ="102" ssid = "31">We test this below on the SENSEVAL-2 English all-words data.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'102'", "'116'", "'100'", "'5'"]
'9'
'102'
'116'
'100'
'5'
['9', '102', '116', '100', '5']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="180" ssid = "3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S><S sid ="109" ssid = "7">We generated a thesaurus entry for all polysemous nouns in WordNet as described in section 2.1 above.</S><S sid ="112" ssid = "10">We compare results using the first sense listed in SemCor  and the first sense according to the SENSEVAL-2 English all-words test data itself.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'180'", "'142'", "'109'", "'112'"]
'10'
'180'
'142'
'109'
'112'
['10', '180', '142', '109', '112']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "4">Senses that have not occurred in SemCor are ordered arbitrarily and after those senses of the word that have occurred.</S><S sid ="120" ssid = "18">There were a similar number of words that were not covered by a predominant sense in SemCor.</S><S sid ="42" ssid = "35">The neighbours for a word in a thesaurus are words themselves  rather than senses.</S><S sid ="189" ssid = "12">Additionally  we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity  and what can be done to combat this problem using relation specific thesauruses.</S><S sid ="117" ssid = "15">The items that were not covered by our method were those with insufficient grammatical relations for the tuples employed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'120'", "'42'", "'189'", "'117'"]
'11'
'120'
'42'
'189'
'117'
['11', '120', '42', '189', '117']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "5">The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor  from those that do not (without HTD).</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'123'", "'153'", "'165'", "'178'"]
'12'
'123'
'153'
'165'
'178'
['12', '123', '153', '165', '178']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="84" ssid = "13">The random baseline for choosing the predominant sense over all these words ( ) is 32%.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'116'", "'147'", "'84'", "'126'"]
'13'
'116'
'147'
'84'
'126'
['13', '116', '147', '84', '126']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="182" ssid = "5">In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.</S><S sid ="21" ssid = "14">We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off (Wilks and Stevenson  1998; Hoste et al.  2001) and for systems that use it in lexical acquisition (McCarthy  1997; Merlo and Leybold  2001; Korhonen  2002) because of the limited size of hand-tagged resources.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'8'", "'154'", "'182'", "'21'"]
'14'
'8'
'154'
'182'
'21'
['14', '8', '154', '182', '21']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="3" ssid = "3">Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'154'", "'3'", "'8'", "'170'"]
'15'
'154'
'3'
'8'
'170'
['15', '154', '3', '8', '170']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">SemCor comprises a relatively small sample of 250 000 words.</S><S sid ="131" ssid = "8">The SPORTS corpus consists of 35317 documents (about 9.1 million words).</S><S sid ="140" ssid = "17">The resulting set consisted of 38 words.</S><S sid ="132" ssid = "9">The FINANCE corpus consists of 117734 documents (about 32.5 million words).</S><S sid ="76" ssid = "5">The jcn measure uses corpus data for the calculation of IC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'131'", "'140'", "'132'", "'76'"]
'16'
'131'
'140'
'132'
'76'
['16', '131', '140', '132', '76']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "10">There are words where the first sense in WordNet is counter-intuitive  because of the size of the corpus  and because where the frequency data does not indicate a first sense  the ordering is arbitrary.</S><S sid ="182" ssid = "5">In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.</S><S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="99" ssid = "28">Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'182'", "'13'", "'99'", "'170'"]
'17'
'182'
'13'
'99'
'170'
['17', '182', '13', '99', '170']
parsed_discourse_facet ['method_citation']
['We briefly summarise the two measures here; for a more detailed summary see (Patwardhan et al., 2003).']
['word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.', 'From inspection  one can see that the ordered neighbours of such a thesaurus relate to the different senses of the target word.', 'Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.', 'Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.', 'If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:28', 'F:0']
['We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline, but because of space limitations give results for the two which perform the best.']
['The FINANCE corpus consists of 117734 documents (about 32.5 million words).', 'The SPORTS corpus consists of 35317 documents (about 9.1 million words).', 'The jcn measure uses corpus data for the calculation of IC.', 'The resulting set consisted of 38 words.', 'SemCor comprises a relatively small sample of 250 000 words.']
['system', 'ROUGE-S*', 'Average_R:', '0.00159', '(95%-conf.int.', '0.00159', '-', '0.00159)']
['system', 'ROUGE-S*', 'Average_P:', '0.01515', '(95%-conf.int.', '0.01515', '-', '0.01515)']
['system', 'ROUGE-S*', 'Average_F:', '0.00287', '(95%-conf.int.', '0.00287', '-', '0.00287)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:66', 'F:1']
['In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.']
['Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor  which is very encouraging given that our method only uses raw text  with no manual labelling.', 'The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.', 'We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet  for example those that use it for lexical acquisition or WSD.', 'In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.', 'Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).']
['system', 'ROUGE-S*', 'Average_R:', '0.00264', '(95%-conf.int.', '0.00264', '-', '0.00264)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.00514', '(95%-conf.int.', '0.00514', '-', '0.00514)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:55', 'F:5']
['Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.']
['In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.', 'The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.', 'Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).', 'We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off (Wilks and Stevenson  1998; Hoste et al.  2001) and for systems that use it in lexical acquisition (McCarthy  1997; Merlo and Leybold  2001; Korhonen  2002) because of the limited size of hand-tagged resources.', 'In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.']
['system', 'ROUGE-S*', 'Average_R:', '0.00100', '(95%-conf.int.', '0.00100', '-', '0.00100)']
['system', 'ROUGE-S*', 'Average_P:', '0.01754', '(95%-conf.int.', '0.01754', '-', '0.01754)']
['system', 'ROUGE-S*', 'Average_F:', '0.00189', '(95%-conf.int.', '0.00189', '-', '0.00189)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:171', 'F:3']
['Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.', 'Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.', 'Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.', 'In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.', 'If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:136', 'F:0']
['word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.']
['For these one would need to obtain more sense-tagged text in order to use this heuristic.', 'The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.', 'We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.', 'We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.', 'This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.']
['system', 'ROUGE-S*', 'Average_R:', '0.02537', '(95%-conf.int.', '0.02537', '-', '0.02537)']
['system', 'ROUGE-S*', 'Average_P:', '0.30769', '(95%-conf.int.', '0.30769', '-', '0.30769)']
['system', 'ROUGE-S*', 'Average_F:', '0.04688', '(95%-conf.int.', '0.04688', '-', '0.04688)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:78', 'F:24']
['Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.']
['This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).', 'We test this below on the SENSEVAL-2 English all-words data.', 'In the English all-words SENSEVAL-2  25% of the noun data was monosemous.', 'The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.', 'The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.']
['system', 'ROUGE-S*', 'Average_R:', '0.00313', '(95%-conf.int.', '0.00313', '-', '0.00313)']
['system', 'ROUGE-S*', 'Average_P:', '0.02924', '(95%-conf.int.', '0.02924', '-', '0.02924)']
['system', 'ROUGE-S*', 'Average_F:', '0.00566', '(95%-conf.int.', '0.00566', '-', '0.00566)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:171', 'F:5']
['Magnini and Cavagli`a (2000) have identified WordNet word senses with particular domains, and this has proven useful for high precision WSD (Magnini et al., 2001); indeed in section 5 we used these domain labels for evaluation.']
['In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.', 'Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.', 'The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.', 'There are words where the first sense in WordNet is counter-intuitive  because of the size of the corpus  and because where the frequency data does not indicate a first sense  the ordering is arbitrary.', 'If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.']
['system', 'ROUGE-S*', 'Average_R:', '0.00149', '(95%-conf.int.', '0.00149', '-', '0.00149)']
['system', 'ROUGE-S*', 'Average_P:', '0.01579', '(95%-conf.int.', '0.01579', '-', '0.01579)']
['system', 'ROUGE-S*', 'Average_F:', '0.00272', '(95%-conf.int.', '0.00272', '-', '0.00272)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:190', 'F:3']
['word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.']
['The word share is among the words whose predominant sense remained the same for all three corpora.', 'The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.', 'The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.', 'We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.', 'The random baseline for choosing the predominant sense over all these words ( ) is 32%.']
['system', 'ROUGE-S*', 'Average_R:', '0.02537', '(95%-conf.int.', '0.02537', '-', '0.02537)']
['system', 'ROUGE-S*', 'Average_P:', '0.30769', '(95%-conf.int.', '0.30769', '-', '0.30769)']
['system', 'ROUGE-S*', 'Average_F:', '0.04688', '(95%-conf.int.', '0.04688', '-', '0.04688)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:78', 'F:24']
['Again, the automatic ranking outperforms this by a large margin.']
['Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.', 'This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.', 'The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor  from those that do not (without HTD).', 'Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.', 'We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2701', 'P:10', 'F:0']
0.078400999216 0.00605899993941 0.011203999888





input/ref/Task1/P87-1015_sweta.csv
input/res/Task1/P87-1015.csv
parsing: input/ref/Task1/P87-1015_sweta.csv
<S sid="205" ssid="11">Independence of paths at this level reflects context freeness of rewriting and suggests why they can be recognized efficiently.</S>
original cit marker offset is 0
new cit marker offset is 0



["205'"]
205'
['205']
parsed_discourse_facet ['method_citation']
<S sid="229" ssid="35">In addition, the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars, and the hierarchy defined by Weir).</S>
original cit marker offset is 0
new cit marker offset is 0



["229'"]
229'
['229']
parsed_discourse_facet ['method_citation']
<S sid="146" ssid="31">Since every CFL is known to be semilinear (Parikh, 1966), in order to show semilinearity of some language, we need only show the existence of a letter equivalent CFL Our definition of LCFRS's insists that the composition operations are linear and nonerasing.</S>
original cit marker offset is 0
new cit marker offset is 0



["146'"]
146'
['146']
parsed_discourse_facet ['method_citation']
<S sid="201" ssid="7">It is interesting to note, however, that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG, FUG and IG's.</S>
original cit marker offset is 0
new cit marker offset is 0



["201'"]
201'
['201']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
 <S sid="222" ssid="28">However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



["222'"]
222'
['222']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="7">Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.</S>
    <S sid="23" ssid="8">He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["22'", "'23'"]
22'
'23'
['22', '23']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="221" ssid="27">Members of LCFRS whose operations have this property can be translated into the ILFP notation (Rounds, 1985).</S>
original cit marker offset is 0
new cit marker offset is 0



["221'"]
221'
['221']
parsed_discourse_facet ['method_citation']
<S sid="54" ssid="39">An IG can be viewed as a CFG in which each nonterminal is associated with a stack.</S>
original cit marker offset is 0
new cit marker offset is 0



["54'"]
54'
['54']
parsed_discourse_facet ['method_citation']
<S sid="128" ssid="13">As in the case of the derivation trees of CFG's, nodes are labeled by a member of some finite set of symbols (perhaps only implicit in the grammar as in TAG's) used to denote derived structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["128'"]
128'
['128']
parsed_discourse_facet ['method_citation']
<S sid="217" ssid="23">In considering the recognition of these languages, we were forced to be more specific regarding the relationship between the structures derived by these formalisms and the substrings they span.</S>
original cit marker offset is 0
new cit marker offset is 0



[";217'"]
;217'
[';217']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="1">From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="1">From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="214" ssid="20">LCFRS's share several properties possessed by the class of mildly context-sensitive formalisms discussed by Joshi (1983/85).</S>
original cit marker offset is 0
new cit marker offset is 0



["214'"]
214'
['214']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="20">TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish, for cross serial dependencies in Dutch subordinate clauses, and for the nestings of paired English complementizers.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P87-1015.csv
<S sid ="1" ssid = "1">We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.</S><S sid ="9" ssid = "7">We examine both the complexity of the paths of trees in the tree sets  and the kinds of dependencies that the formalisms can impose between paths.</S><S sid ="232" ssid = "38">In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="110" ssid = "16">The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'232'", "'195'", "'110'"]
'1'
'9'
'232'
'195'
'110'
['1', '9', '232', '195', '110']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">In considering the relationship between formalisms  we show that it is useful to abstract away from the details of the formalism  and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation  we describe a class of formalisms which we call Linear Context- Free Rewriting Systems  and show they are recognizable in polynomial time and generate only semilinear languages.</S><S sid ="203" ssid = "9">In order to observe the similarity between these constrained systems  it is crucial to abstract away from the details of the structures and operations used by the system.</S><S sid ="232" ssid = "38">In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.</S><S sid ="11" ssid = "9">By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'203'", "'232'", "'11'", "'116'"]
'2'
'203'
'232'
'11'
'116'
['2', '203', '232', '11', '116']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S><S sid ="48" ssid = "33">There has been recent interest in the application of Indexed Grammars (IG's) to natural languages.</S><S sid ="23" ssid = "8">He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.</S><S sid ="56" ssid = "41">Gazdar (1985) argues that sharing of stacks can be used to give analyses for coordination.</S><S sid ="65" ssid = "50">While the generative power of CG's is greater that of CFG's  it appears to be highly constrained.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'48'", "'23'", "'56'", "'65'"]
'3'
'48'
'23'
'56'
'65'
['3', '48', '23', '56', '65']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "2">Little attention  however  has been paid to the structural descriptions that these formalisms can assign to strings  i.e. their strong generative capacity.</S><S sid ="3" ssid = "1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S><S sid ="66" ssid = "51">Hence  their relationship to formalisms such as HG's and TAG's is of interest.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="48" ssid = "33">There has been recent interest in the application of Indexed Grammars (IG's) to natural languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'3'", "'66'", "'195'", "'48'"]
'4'
'3'
'66'
'195'
'48'
['4', '3', '66', '195', '48']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "3">This aspect of the formalism is both linguistically and computationally important.</S><S sid ="198" ssid = "4">This property reflects an important aspect of the underlying linguistic theory associated with the formalism.</S><S sid ="10" ssid = "8">These two properties of the tree sets are not only linguistically relevant  but also have computational importance.</S><S sid ="123" ssid = "8">A second restriction on the formalisms is that choices during the derivation are independent of the context in the derivation.</S><S sid ="3" ssid = "1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'198'", "'10'", "'123'", "'3'"]
'5'
'198'
'10'
'123'
'3'
['5', '198', '10', '123', '3']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">For example  Gazdar (1985) discusses the applicability of Indexed Grammars (IG's) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar (LFG) and Government and Bindings grammars (GB).</S><S sid ="24" ssid = "9">Head Grammars (HG's)  introduced by Pollard (1984)  is a formalism that manipulates headed strings: i.e.  strings  one of whose symbols is distinguished as the head.</S><S sid ="35" ssid = "20">TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish  for cross serial dependencies in Dutch subordinate clauses  and for the nestings of paired English complementizers.</S><S sid ="64" ssid = "49">Steedman (1986) considers Categorial Grammars in which both the operations of function application and composition may be used  and in which function can specify whether they take their arguments from their right or left.</S><S sid ="22" ssid = "7">Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'24'", "'35'", "'64'", "'22'"]
'6'
'24'
'35'
'64'
'22'
['6', '24', '35', '64', '22']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "5">The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG's and IG's.</S><S sid ="50" ssid = "35">The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.</S><S sid ="16" ssid = "1">From Thatcher's (1973) work  it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S><S sid ="98" ssid = "4">Thatcher (1973) describes a tee pumping lemma for recognizable sets related to the string pumping lemma for regular sets.</S><S sid ="24" ssid = "9">Head Grammars (HG's)  introduced by Pollard (1984)  is a formalism that manipulates headed strings: i.e.  strings  one of whose symbols is distinguished as the head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'50'", "'16'", "'98'", "'24'"]
'7'
'50'
'16'
'98'
'24'
['7', '50', '16', '98', '24']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "6">We consider properties of the tree sets generated by CFG's  Tree Adjoining Grammars (TAG's)  Head Grammars (HG's)  Categorial Grammars (CG's)  and IG's.</S><S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="27" ssid = "12">Thus  the tree sets generated by HG's are similar to those of CFG's  with each node annotated by the operation (concatenation or wrapping) used to combine the headed strings derived by the daughters of Tree Adjoining Grammars  a tree rewriting formalism  was introduced by Joshi  Levy and Takahashi (1975) and Joshi (1983/85).</S><S sid ="94" ssid = "79">The semilinearity of Tree Adjoining Languages (TAL's)  MCTAL's  and Head Languages (HL's) can be proved using this property  with suitable restrictions on the composition operations.</S><S sid ="208" ssid = "14">This family represents an attempt to generalize the properties shared by CFG's  HG's  TAG's  and MCTAG's.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'151'", "'27'", "'94'", "'208'"]
'8'
'151'
'27'
'94'
'208'
['8', '151', '27', '94', '208']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "7">We examine both the complexity of the paths of trees in the tree sets  and the kinds of dependencies that the formalisms can impose between paths.</S><S sid ="1" ssid = "1">We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.</S><S sid ="8" ssid = "6">We consider properties of the tree sets generated by CFG's  Tree Adjoining Grammars (TAG's)  Head Grammars (HG's)  Categorial Grammars (CG's)  and IG's.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="110" ssid = "16">The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'1'", "'8'", "'195'", "'110'"]
'9'
'1'
'8'
'195'
'110'
['9', '1', '8', '195', '110']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "8">These two properties of the tree sets are not only linguistically relevant  but also have computational importance.</S><S sid ="5" ssid = "3">This aspect of the formalism is both linguistically and computationally important.</S><S sid ="198" ssid = "4">This property reflects an important aspect of the underlying linguistic theory associated with the formalism.</S><S sid ="68" ssid = "53">This kind of dependency arises from the use of the composition operation to compose two arbitrarily large categories.</S><S sid ="23" ssid = "8">He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'5'", "'198'", "'68'", "'23'"]
'10'
'5'
'198'
'68'
'23'
['10', '5', '198', '68', '23']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "9">By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.</S><S sid ="2" ssid = "2">In considering the relationship between formalisms  we show that it is useful to abstract away from the details of the formalism  and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation  we describe a class of formalisms which we call Linear Context- Free Rewriting Systems  and show they are recognizable in polynomial time and generate only semilinear languages.</S><S sid ="232" ssid = "38">In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.</S><S sid ="14" ssid = "12">This suggests that by generalizing the notion of context-freeness in CFG's  we can define a class of grammatical formalisms that manipulate more complex structures.</S><S sid ="203" ssid = "9">In order to observe the similarity between these constrained systems  it is crucial to abstract away from the details of the structures and operations used by the system.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'2'", "'232'", "'14'", "'203'"]
'11'
'2'
'232'
'14'
'203'
['11', '2', '232', '14', '203']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "10">We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.</S><S sid ="137" ssid = "22">In considering recognition of LCFRS's  we make further assumption concerning the contribution of each structure to the input string  and how the composition operations combine structures in this respect.</S><S sid ="200" ssid = "6">The importance of this property becomes clear in contrasting theories underlying GPSG (Gazdar  Klein  Pulluna  and Sag  1985)  and GB (as described by Berwick  1984) with those underlying LFG and FUG.</S><S sid ="22" ssid = "7">Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.</S><S sid ="60" ssid = "45">Gazdar (1985) considers a restriction of IG's in which no more than one nonterminal on the right-hand-side of a production can inherit the stack from the left-hand-side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'137'", "'200'", "'22'", "'60'"]
'12'
'137'
'200'
'22'
'60'
['12', '137', '200', '22', '60']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "11">It is striking that from this point of view many formalisms can be grouped together as having identically structured derivation tree sets.</S><S sid ="15" ssid = "13">In this paper  we outline how such family of formalisms can be defined  and show that like CFG's  each member possesses a number of desirable linguistic and computational properties: in particular  the constant growth property and polynomial recognizability.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S><S sid ="202" ssid = "8">As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.</S><S sid ="78" ssid = "63">Each member of a set of trees can be adjoined into distinct nodes of trees in a single elementary tree set  i.e  derivations always involve the adjunction of a derived auxiliary tree set into an elementary tree set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'15'", "'116'", "'202'", "'78'"]
'13'
'15'
'116'
'202'
'78'
['13', '15', '116', '202', '78']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "12">This suggests that by generalizing the notion of context-freeness in CFG's  we can define a class of grammatical formalisms that manipulate more complex structures.</S><S sid ="119" ssid = "4">In defining LCFRS's  we hope to generalize the definition of CFG's to formalisms manipulating any structure  e.g. strings  trees  or graphs.</S><S sid ="53" ssid = "38">We focus on this difference between the tree sets of CFG's and IG's  and formalize the notion of dependence between paths in a tree set in Section 3.</S><S sid ="142" ssid = "27">Although this property is not structural  it depends on the structural property that sentences can be built from a finite set of clauses of bounded structure as noted by Joshi (1983/85).</S><S sid ="159" ssid = "44">Similarly  for all the LCFRS's  discussed in Section 2  we can define the relationship between a structure and the sequence of substrings it spans  and the effect of the composition operations on sequences of substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'119'", "'53'", "'142'", "'159'"]
'14'
'119'
'53'
'142'
'159'
['14', '119', '53', '142', '159']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "13">In this paper  we outline how such family of formalisms can be defined  and show that like CFG's  each member possesses a number of desirable linguistic and computational properties: in particular  the constant growth property and polynomial recognizability.</S><S sid ="202" ssid = "8">As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.</S><S sid ="117" ssid = "2">Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S><S sid ="13" ssid = "11">It is striking that from this point of view many formalisms can be grouped together as having identically structured derivation tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'202'", "'117'", "'116'", "'13'"]
'15'
'202'
'117'
'116'
'13'
['15', '202', '117', '116', '13']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "1">From Thatcher's (1973) work  it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S><S sid ="50" ssid = "35">The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.</S><S sid ="19" ssid = "4">It can be easily shown from Thatcher's result that the path set of every local set is a regular set.</S><S sid ="229" ssid = "35">In addition  the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars  and the hierarchy defined by Weir).</S><S sid ="166" ssid = "51">With the additional assumptions  inspired by Rounds (1985)  we can show that members of this class can be recognized in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'50'", "'19'", "'229'", "'166'"]
'16'
'50'
'19'
'229'
'166'
['16', '50', '19', '229', '166']
parsed_discourse_facet ['method_citation']
['Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.', 'He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.']
["Head Grammars (HG's)  introduced by Pollard (1984)  is a formalism that manipulates headed strings: i.e.  strings  one of whose symbols is distinguished as the head.", 'Thatcher (1973) describes a tee pumping lemma for recognizable sets related to the string pumping lemma for regular sets.', "The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.", "The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG's and IG's.", "From Thatcher's (1973) work  it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'."]
['system', 'ROUGE-S*', 'Average_R:', '0.00121', '(95%-conf.int.', '0.00121', '-', '0.00121)']
['system', 'ROUGE-S*', 'Average_P:', '0.01429', '(95%-conf.int.', '0.01429', '-', '0.01429)']
['system', 'ROUGE-S*', 'Average_F:', '0.00223', '(95%-conf.int.', '0.00223', '-', '0.00223)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:210', 'F:3']
['Independence of paths at this level reflects context freeness of rewriting and suggests why they can be recognized efficiently.']
['We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.', 'We examine both the complexity of the paths of trees in the tree sets  and the kinds of dependencies that the formalisms can impose between paths.', 'We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.', 'The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .', 'In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.']
['system', 'ROUGE-S*', 'Average_R:', '0.00040', '(95%-conf.int.', '0.00040', '-', '0.00040)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00079', '(95%-conf.int.', '0.00079', '-', '0.00079)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:45', 'F:1']
["In addition, the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars, and the hierarchy defined by Weir)."]
["By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.", 'In order to observe the similarity between these constrained systems  it is crucial to abstract away from the details of the structures and operations used by the system.', 'In considering the relationship between formalisms  we show that it is useful to abstract away from the details of the formalism  and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation  we describe a class of formalisms which we call Linear Context- Free Rewriting Systems  and show they are recognizable in polynomial time and generate only semilinear languages.', 'From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).', 'In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.']
['system', 'ROUGE-S*', 'Average_R:', '0.00579', '(95%-conf.int.', '0.00579', '-', '0.00579)']
['system', 'ROUGE-S*', 'Average_P:', '0.09195', '(95%-conf.int.', '0.09195', '-', '0.09195)']
['system', 'ROUGE-S*', 'Average_F:', '0.01090', '(95%-conf.int.', '0.01090', '-', '0.01090)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6903', 'P:435', 'F:40']
["TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish, for cross serial dependencies in Dutch subordinate clauses, and for the nestings of paired English complementizers."]
["It can be easily shown from Thatcher's result that the path set of every local set is a regular set.", 'With the additional assumptions  inspired by Rounds (1985)  we can show that members of this class can be recognized in polynomial time.', "The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.", "In addition  the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars  and the hierarchy defined by Weir).", "From Thatcher's (1973) work  it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'."]
['system', 'ROUGE-S*', 'Average_R:', '0.00032', '(95%-conf.int.', '0.00032', '-', '0.00032)']
['system', 'ROUGE-S*', 'Average_P:', '0.00476', '(95%-conf.int.', '0.00476', '-', '0.00476)']
['system', 'ROUGE-S*', 'Average_F:', '0.00059', '(95%-conf.int.', '0.00059', '-', '0.00059)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:210', 'F:1']
["LCFRS's share several properties possessed by the class of mildly context-sensitive formalisms discussed by Joshi (1983/85)."]
['From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).', 'Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.', "As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.", "In this paper  we outline how such family of formalisms can be defined  and show that like CFG's  each member possesses a number of desirable linguistic and computational properties: in particular  the constant growth property and polynomial recognizability.", 'It is striking that from this point of view many formalisms can be grouped together as having identically structured derivation tree sets.']
['system', 'ROUGE-S*', 'Average_R:', '0.00341', '(95%-conf.int.', '0.00341', '-', '0.00341)']
['system', 'ROUGE-S*', 'Average_P:', '0.10256', '(95%-conf.int.', '0.10256', '-', '0.10256)']
['system', 'ROUGE-S*', 'Average_F:', '0.00660', '(95%-conf.int.', '0.00660', '-', '0.00660)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:78', 'F:8']
["From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'."]
["Similarly  for all the LCFRS's  discussed in Section 2  we can define the relationship between a structure and the sequence of substrings it spans  and the effect of the composition operations on sequences of substrings.", 'Although this property is not structural  it depends on the structural property that sentences can be built from a finite set of clauses of bounded structure as noted by Joshi (1983/85).', "In defining LCFRS's  we hope to generalize the definition of CFG's to formalisms manipulating any structure  e.g. strings  trees  or graphs.", "This suggests that by generalizing the notion of context-freeness in CFG's  we can define a class of grammatical formalisms that manipulate more complex structures.", "We focus on this difference between the tree sets of CFG's and IG's  and formalize the notion of dependence between paths in a tree set in Section 3."]
['system', 'ROUGE-S*', 'Average_R:', '0.00966', '(95%-conf.int.', '0.00966', '-', '0.00966)']
['system', 'ROUGE-S*', 'Average_P:', '0.20000', '(95%-conf.int.', '0.20000', '-', '0.20000)']
['system', 'ROUGE-S*', 'Average_F:', '0.01843', '(95%-conf.int.', '0.01843', '-', '0.01843)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:120', 'F:24']
['However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.']
["Head Grammars (HG's)  introduced by Pollard (1984)  is a formalism that manipulates headed strings: i.e.  strings  one of whose symbols is distinguished as the head.", 'Steedman (1986) considers Categorial Grammars in which both the operations of function application and composition may be used  and in which function can specify whether they take their arguments from their right or left.', 'Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.', "TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish  for cross serial dependencies in Dutch subordinate clauses  and for the nestings of paired English complementizers.", "For example  Gazdar (1985) discusses the applicability of Indexed Grammars (IG's) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar (LFG) and Government and Bindings grammars (GB)."]
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:190', 'F:0']
["As in the case of the derivation trees of CFG's, nodes are labeled by a member of some finite set of symbols (perhaps only implicit in the grammar as in TAG's) used to denote derived structures."]
["By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.", 'In order to observe the similarity between these constrained systems  it is crucial to abstract away from the details of the structures and operations used by the system.', 'In considering the relationship between formalisms  we show that it is useful to abstract away from the details of the formalism  and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation  we describe a class of formalisms which we call Linear Context- Free Rewriting Systems  and show they are recognizable in polynomial time and generate only semilinear languages.', "This suggests that by generalizing the notion of context-freeness in CFG's  we can define a class of grammatical formalisms that manipulate more complex structures.", 'In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.']
['system', 'ROUGE-S*', 'Average_R:', '0.00246', '(95%-conf.int.', '0.00246', '-', '0.00246)']
['system', 'ROUGE-S*', 'Average_P:', '0.14167', '(95%-conf.int.', '0.14167', '-', '0.14167)']
['system', 'ROUGE-S*', 'Average_F:', '0.00484', '(95%-conf.int.', '0.00484', '-', '0.00484)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6903', 'P:120', 'F:17']
["Since every CFL is known to be semilinear (Parikh, 1966), in order to show semilinearity of some language, we need only show the existence of a letter equivalent CFL Our definition of LCFRS's insists that the composition operations are linear and nonerasing."]
['Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.', 'Gazdar (1985) argues that sharing of stacks can be used to give analyses for coordination.', "While the generative power of CG's is greater that of CFG's  it appears to be highly constrained.", "There has been recent interest in the application of Indexed Grammars (IG's) to natural languages.", 'He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:190', 'F:0']
["It is interesting to note, however, that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG, FUG and IG's."]
['We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.', 'Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.', 'Little attention  however  has been paid to the structural descriptions that these formalisms can assign to strings  i.e. their strong generative capacity.', "Hence  their relationship to formalisms such as HG's and TAG's is of interest.", "There has been recent interest in the application of Indexed Grammars (IG's) to natural languages."]
['system', 'ROUGE-S*', 'Average_R:', '0.00327', '(95%-conf.int.', '0.00327', '-', '0.00327)']
['system', 'ROUGE-S*', 'Average_P:', '0.02105', '(95%-conf.int.', '0.02105', '-', '0.02105)']
['system', 'ROUGE-S*', 'Average_F:', '0.00565', '(95%-conf.int.', '0.00565', '-', '0.00565)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:190', 'F:4']
['Members of LCFRS whose operations have this property can be translated into the ILFP notation (Rounds, 1985).']
['We examine both the complexity of the paths of trees in the tree sets  and the kinds of dependencies that the formalisms can impose between paths.', 'We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.', 'We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.', 'The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .', "We consider properties of the tree sets generated by CFG's  Tree Adjoining Grammars (TAG's)  Head Grammars (HG's)  Categorial Grammars (CG's)  and IG's."]
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:36', 'F:0']
0.0544090904145 0.00241090906899 0.00454818177683





input/ref/Task1/D10-1044_sweta.csv
input/res/Task1/D10-1044.csv
parsing: input/ref/Task1/D10-1044_sweta.csv
<S sid="4" ssid="1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S>
original cit marker offset is 0
new cit marker offset is 0



["4'"]
4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="1">We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="4">For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components (word-alignment model, language model, translation model, etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["7'"]
7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="14">Linear weights are difficult to incorporate into the standard MERT procedure because they are &#8220;hidden&#8221; within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn, 2007), we circumvent this problem by choosing weights to optimize corpus loglikelihood, which is roughly speaking the training criterion used by the LM and TM themselves.</S>
original cit marker offset is 0
new cit marker offset is 0



["50'"]
50'
['50']
parsed_discourse_facet ['method_citation']
<S sid="152" ssid="9">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["152'"]
152'
['152']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
 <S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="97" ssid="1">We carried out translation experiments in two different settings.</S>
original cit marker offset is 0
new cit marker offset is 0



["97'"]
97'
['97']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="12">Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="10">Finally, we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="141" ssid="10">Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L&#168;u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L&#168;u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["141'"]
141'
['141']
parsed_discourse_facet ['method_citation']
 <S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="37" ssid="1">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S>
original cit marker offset is 0
new cit marker offset is 0



["37'"]
37'
['37']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D10-1044.csv
<S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="145" ssid = "2">Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="62" ssid = "26">To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'65'", "'145'", "'64'", "'62'"]
'1'
'65'
'145'
'64'
'62'
['1', '65', '145', '64', '62']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S><S sid ="23" ssid = "20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="153" ssid = "10">Finally  we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.</S><S sid ="68" ssid = "5">First  we learn weights on individual phrase pairs rather than sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'23'", "'20'", "'153'", "'68'"]
'2'
'23'
'20'
'153'
'68'
['2', '23', '20', '153', '68']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">We incorporate instance weighting into a mixture-model framework  and find that it yields consistent improvements over a wide range of baselines.</S><S sid ="150" ssid = "7">In both cases  the instanceweighting approach improved over a wide range of baselines  giving gains of over 2 BLEU points over the best non-adapted baseline  and gains of between 0.6 and 1.8 over an equivalent mixture model (with an identical training procedure but without instance weighting).</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'150'", "'65'", "'38'", "'1'"]
'3'
'150'
'65'
'38'
'1'
['3', '150', '65', '38', '1']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'10'", "'6'", "'24'", "'17'"]
'4'
'10'
'6'
'24'
'17'
['4', '10', '6', '24', '17']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "2">Even when there is training data available in the domain of interest  there is often additional data from other domains that could in principle be used to improve performance.</S><S sid ="138" ssid = "7">However  for multinomial models like our LMs and TMs  there is a one to one correspondence between instances and features  eg the correspondence between a phrase pair (s  t) and its conditional multinomial probability p(s1t).</S><S sid ="9" ssid = "6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="39" ssid = "3">Thus  provided at least this amount of IN data is available—as it is in our setting—adapting these weights is straightforward.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'138'", "'9'", "'6'", "'39'"]
'5'
'138'
'9'
'6'
'39'
['5', '138', '9', '6', '39']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'17'", "'24'", "'18'", "'4'"]
'6'
'17'
'24'
'18'
'4'
['6', '17', '24', '18', '4']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="9" ssid = "6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'24'", "'21'", "'4'", "'9'"]
'7'
'24'
'21'
'4'
'9'
['7', '24', '21', '4', '9']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'10'", "'18'", "'6'", "'20'"]
'8'
'10'
'18'
'6'
'20'
['8', '10', '18', '6', '20']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S><S sid ="144" ssid = "1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S><S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="42" ssid = "6">The natural baseline approach is to concatenate data from IN and OUT.</S><S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'144'", "'1'", "'42'", "'2'"]
'9'
'144'
'1'
'42'
'2'
['9', '144', '1', '42', '2']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="29" ssid = "26">This is a simple and effective alternative to setting weights discriminatively to maximize a metric such as BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'18'", "'6'", "'29'"]
'10'
'4'
'18'
'6'
'29'
['10', '4', '18', '6', '29']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="44" ssid = "8">When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.</S><S sid ="92" ssid = "29">6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'18'", "'6'", "'44'", "'92'"]
'11'
'18'
'6'
'44'
'92'
['11', '18', '6', '44', '92']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "9">For simplicity  we assume that OUT is homogeneous.</S><S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="92" ssid = "29">6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.</S><S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'18'", "'11'", "'92'", "'16'"]
'12'
'18'
'11'
'92'
'16'
['12', '18', '11', '92', '16']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "10">The techniques we develop can be extended in a relatively straightforward manner to the more general case when OUT consists of multiple sub-domains.</S><S sid ="139" ssid = "8">As mentioned above  it is not obvious how to apply Daum´e’s approach to multinomials  which do not have a mechanism for combining split features.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="82" ssid = "19">This is not unreasonable given the application to phrase pairs from OUT  but it suggests that an interesting alternative might be to use a plain log-linear weighting function exp(Ei Aifi(s  t))  with outputs in [0  oo].</S><S sid ="32" ssid = "29">This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'139'", "'21'", "'82'", "'32'"]
'13'
'139'
'21'
'82'
'32'
['13', '139', '21', '82', '32']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "11">There is a fairly large body of work on SMT adaptation.</S><S sid ="141" ssid = "10">Moving beyond directly related work  major themes in SMT adaptation include the IR (Hildebrand et al.  2005; L¨u et al.  2007; Zhao et al.  2004) and mixture (Finch and Sumita  2008; Foster and Kuhn  2007; Koehn and Schroeder  2007; L¨u et al.  2007) approaches for LMs and TMs described above  as well as methods for exploiting monolingual in-domain text  typically by translating it automatically and then performing self training (Bertoldi and Federico  2009; Ueffing et al.  2007; Schwenk and Senellart  2009).</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="34" ssid = "31">Section 2 describes our baseline techniques for SMT adaptation  and section 3 describes the instance-weighting approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'141'", "'4'", "'10'", "'34'"]
'14'
'141'
'4'
'10'
'34'
['14', '141', '4', '10', '34']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "12">We introduce several new ideas.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="27" ssid = "24">Finally  we make some improvements to baseline approaches.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="88" ssid = "25">We have not explored this strategy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'67'", "'27'", "'20'", "'88'"]
'15'
'67'
'27'
'20'
'88'
['15', '67', '27', '20', '88']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="42" ssid = "6">The natural baseline approach is to concatenate data from IN and OUT.</S><S sid ="68" ssid = "5">First  we learn weights on individual phrase pairs rather than sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'17'", "'24'", "'42'", "'68'"]
'16'
'17'
'24'
'42'
'68'
['16', '17', '24', '42', '68']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S><S sid ="100" ssid = "4">Figure 1 shows sample sentences from these domains  which are widely divergent.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'6'", "'24'", "'16'", "'100'"]
'17'
'6'
'24'
'16'
'100'
['17', '6', '24', '16', '100']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "15">This is less effective in our setting  where IN and OUT are disparate.</S><S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="44" ssid = "8">When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.</S><S sid ="6" ssid = "3">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'11'", "'44'", "'6'", "'10'"]
'18'
'11'
'44'
'6'
'10'
['18', '11', '44', '6', '10']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "16">The idea of distinguishing between general and domain-specific examples is due to Daum´e and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daum´e’s (2007) work that splits each feature into domain-specific and general copies.</S><S sid ="73" ssid = "10">Jiang and Zhai (2007) suggest the following derivation  making use of the true OUT distribution po(s  t): where each fi(s  t) is a feature intended to charac- !0ˆ = argmax pf(s  t) log pθ(s|t) (8) terize the usefulness of (s  t)  weighted by Ai. θ s t pf(s  t)po(s  t) log pθ(s|t) The mixing parameters and feature weights (col- != argmax po (s  t) lectively 0) are optimized simultaneously using dev- θ s t pf(s  t)co(s  t) log pθ(s|t)  set maximum likelihood as before: !�argmax po (s  t) ! θ s t �ˆ = argmax ˜p(s  t) log p(s|t; 0).</S><S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'136'", "'73'", "'140'", "'64'"]
'19'
'136'
'73'
'140'
'64'
['19', '136', '73', '140', '64']
parsed_discourse_facet ['method_citation']
['In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.']
[u'Daum\xc2\xb4e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.', 'We have not explored this strategy.', 'Finally  we make some improvements to baseline approaches.', 'We introduce several new ideas.', 'We extend the Matsoukas et al approach in several ways.']
['system', 'ROUGE-S*', 'Average_R:', '0.00308', '(95%-conf.int.', '0.00308', '-', '0.00308)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00496', '(95%-conf.int.', '0.00496', '-', '0.00496)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:325', 'P:78', 'F:1']
['To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.']
['Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.', 'This is a standard adaptation problem for SMT.', 'Domain adaptation is a common concern when optimizing empirical NLP applications.', 'Previous approaches have tried to find examples that are similar to the target domain.', 'Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.']
['system', 'ROUGE-S*', 'Average_R:', '0.01080', '(95%-conf.int.', '0.01080', '-', '0.01080)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01858', '(95%-conf.int.', '0.01858', '-', '0.01858)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:120', 'F:8']
['We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.']
['First  we learn weights on individual phrase pairs rather than sentences.', 'Finally  we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.', 'This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.', u'Daum\xc2\xb4e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.', 'Our second contribution is to apply instance weighting at the level of phrase pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.01659', '(95%-conf.int.', '0.01659', '-', '0.01659)']
['system', 'ROUGE-S*', 'Average_P:', '0.18333', '(95%-conf.int.', '0.18333', '-', '0.18333)']
['system', 'ROUGE-S*', 'Average_F:', '0.03043', '(95%-conf.int.', '0.03043', '-', '0.03043)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:120', 'F:22']
['Domain adaptation is a common concern when optimizing empirical NLP applications.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.', 'To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.', 'Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.', 'The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.', 'Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:28', 'F:0']
['Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.']
[u'The idea of distinguishing between general and domain-specific examples is due to Daum\xb4e and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.', 'The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.', u'Jiang and Zhai (2007) suggest the following derivation  making use of the true OUT distribution po(s  t): where each fi(s  t) is a feature intended to charac- !0\u02c6 = argmax pf(s  t) log p\u03b8(s|t) (8) terize the usefulness of (s  t)  weighted by Ai. \u03b8 s t pf(s  t)po(s  t) log p\u03b8(s|t) The mixing parameters and feature weights (col- != argmax po (s  t) lectively 0) are optimized simultaneously using dev- \u03b8 s t pf(s  t)co(s  t) log p\u03b8(s|t)  set maximum likelihood as before: !\ufffdargmax po (s  t) ! \u03b8 s t \ufffd\u02c6 = argmax \u02dcp(s  t) log p(s|t; 0).', u'It is also worth pointing out a connection with Daum\xb4e\u2019s (2007) work that splits each feature into domain-specific and general copies.', u'Recent work by Finkel and Manning (2009) which re-casts Daum\xb4e\u2019s approach in a hierarchical MAP framework may be applicable to this problem.']
['system', 'ROUGE-S*', 'Average_R:', '0.00400', '(95%-conf.int.', '0.00400', '-', '0.00400)']
['system', 'ROUGE-S*', 'Average_P:', '0.08225', '(95%-conf.int.', '0.08225', '-', '0.08225)']
['system', 'ROUGE-S*', 'Average_F:', '0.00762', '(95%-conf.int.', '0.00762', '-', '0.00762)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4753', 'P:231', 'F:19']
['However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.']
[u'In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material\u2014though adequate for reasonable performance\u2014is also available.', 'We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.', 'In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.', 'This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.', 'The natural baseline approach is to concatenate data from IN and OUT.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:136', 'F:0']
['Finally, we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.']
['This is a standard adaptation problem for SMT.', 'Domain adaptation is a common concern when optimizing empirical NLP applications.', 'Section 2 describes our baseline techniques for SMT adaptation  and section 3 describes the instance-weighting approach.', 'There is a fairly large body of work on SMT adaptation.', u'Moving beyond directly related work  major themes in SMT adaptation include the IR (Hildebrand et al.  2005; L\xa8u et al.  2007; Zhao et al.  2004) and mixture (Finch and Sumita  2008; Foster and Kuhn  2007; Koehn and Schroeder  2007; L\xa8u et al.  2007) approaches for LMs and TMs described above  as well as methods for exploiting monolingual in-domain text  typically by translating it automatically and then performing self training (Bertoldi and Federico  2009; Ueffing et al.  2007; Schwenk and Senellart  2009).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3403', 'P:55', 'F:0']
['We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.']
['It is difficult when IN and OUT are dissimilar  as they are in the cases we study.', 'This is less effective in our setting  where IN and OUT are disparate.', 'When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.', 'This is a standard adaptation problem for SMT.', 'Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00246', '(95%-conf.int.', '0.00246', '-', '0.00246)']
['system', 'ROUGE-S*', 'Average_P:', '0.00654', '(95%-conf.int.', '0.00654', '-', '0.00654)']
['system', 'ROUGE-S*', 'Average_F:', '0.00358', '(95%-conf.int.', '0.00358', '-', '0.00358)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:406', 'P:153', 'F:1']
['For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components (word-alignment model, language model, translation model, etc.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.', 'In both cases  the instanceweighting approach improved over a wide range of baselines  giving gains of over 2 BLEU points over the best non-adapted baseline  and gains of between 0.6 and 1.8 over an equivalent mixture model (with an identical training procedure but without instance weighting).', 'We incorporate instance weighting into a mixture-model framework  and find that it yields consistent improvements over a wide range of baselines.', 'The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.', 'Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.']
['system', 'ROUGE-S*', 'Average_R:', '0.00176', '(95%-conf.int.', '0.00176', '-', '0.00176)']
['system', 'ROUGE-S*', 'Average_P:', '0.03509', '(95%-conf.int.', '0.03509', '-', '0.03509)']
['system', 'ROUGE-S*', 'Average_F:', '0.00336', '(95%-conf.int.', '0.00336', '-', '0.00336)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3403', 'P:171', 'F:6']
0.0429666661893 0.00429888884112 0.00761444435984





input/ref/Task1/P08-1028_swastika.csv
input/res/Task1/P08-1028.csv
parsing: input/ref/Task1/P08-1028_swastika.csv
<S sid="21" ssid="17">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="23">Our results show that the multiplicative models are superior and correlate significantly with behavioral data.</S>
original cit marker offset is 0
new cit marker offset is 0



['27']
27
['27']
parsed_discourse_facet ['result_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['189']
189
['189']
parsed_discourse_facet ['aim_citation']
<S sid="53" ssid="1">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.</S>
original cit marker offset is 0
new cit marker offset is 0



['53']
53
['53']
parsed_discourse_facet ['method_citation']
<S sid="76" ssid="24">The models considered so far assume that components do not &#8216;interfere&#8217; with each other, i.e., that It is also possible to re-introduce the dependence on K into the model of vector composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['76']
76
['76']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['57']
57
['57']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['57']
57
['57']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="60" ssid="8">To derive specific models from this general framework requires the identification of appropriate constraints to narrow the space of functions being considered.</S>
original cit marker offset is 0
new cit marker offset is 0



['60']
60
['60']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="73" ssid="21">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware, since semantically important constituents can participate more actively in the composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['73']
73
['73']
parsed_discourse_facet ['result_citation']
<S sid="99" ssid="12">In order to establish an independent measure of sentence similarity, we assembled a set of experimental materials and elicited similarity ratings from human subjects.</S>
original cit marker offset is 0
new cit marker offset is 0



['99']
99
['99']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['189']
189
['189']
parsed_discourse_facet ['aim_citation']
<S sid="191" ssid="3">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S>
original cit marker offset is 0
new cit marker offset is 0



['191']
191
['191']
parsed_discourse_facet ['result_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P08-1028.csv
<S sid ="1" ssid = "1">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid ="189" ssid = "1">In this paper we presented a general framework for vector-based semantic composition.</S><S sid ="43" ssid = "16">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid ="24" ssid = "20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'189'", "'43'", "'24'", "'34'"]
'1'
'189'
'43'
'24'
'34'
['1', '189', '43', '24', '34']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S><S sid ="190" ssid = "2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="83" ssid = "31">Although we have presented multiplicative and additive models separately  there is nothing inherent in our formulation that disallows their combination.</S><S sid ="26" ssid = "22">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'190'", "'25'", "'83'", "'26'"]
'2'
'190'
'25'
'83'
'26'
['2', '190', '25', '83', '26']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="26" ssid = "22">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S><S sid ="52" ssid = "25">Under this framework  we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="24" ssid = "20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'26'", "'52'", "'25'", "'24'"]
'3'
'26'
'52'
'25'
'24'
['3', '26', '52', '25', '24']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S><S sid ="27" ssid = "23">Our results show that the multiplicative models are superior and correlate significantly with behavioral data.</S><S sid ="176" ssid = "10">The multiplicative and combined models yield means closer to the human ratings.</S><S sid ="26" ssid = "22">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S><S sid ="174" ssid = "8">The simple additive model fails to distinguish between High and Low Similarity items.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'27'", "'176'", "'26'", "'174'"]
'4'
'27'
'176'
'26'
'174'
['4', '27', '176', '26', '174']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.</S><S sid ="9" ssid = "5">In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S><S sid ="19" ssid = "15">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid ="8" ssid = "4">Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch¨utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'9'", "'20'", "'19'", "'8'"]
'5'
'9'
'20'
'19'
'8'
['5', '9', '20', '19', '8']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).</S><S sid ="21" ssid = "17">Central in these models is the notion of compositionality — the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S><S sid ="138" ssid = "51">Model Parameters Irrespectively of their form  all composition models discussed here are based on a semantic space for representing the meanings of individual words.</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S><S sid ="29" ssid = "2">While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'21'", "'138'", "'20'", "'29'"]
'6'
'21'
'138'
'20'
'29'
['6', '21', '138', '20', '29']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">A variety of NLP tasks have made good use of vector-based models.</S><S sid ="43" ssid = "16">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid ="200" ssid = "12">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid ="1" ssid = "1">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'43'", "'200'", "'1'", "'42'"]
'7'
'43'
'200'
'1'
'42'
['7', '43', '200', '1', '42']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch¨utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).</S><S sid ="19" ssid = "15">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid ="5" ssid = "1">Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.</S><S sid ="193" ssid = "5">Previous applications of vector addition to document indexing (Deerwester et al.  1990) or essay grading (Landauer et al.  1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'19'", "'5'", "'193'", "'20'"]
'8'
'19'
'5'
'193'
'20'
['8', '19', '5', '193', '20']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).</S><S sid ="5" ssid = "1">Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.</S><S sid ="19" ssid = "15">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid ="148" ssid = "61">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid ="193" ssid = "5">Previous applications of vector addition to document indexing (Deerwester et al.  1990) or essay grading (Landauer et al.  1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'5'", "'19'", "'148'", "'193'"]
'9'
'5'
'19'
'148'
'193'
['9', '5', '19', '148', '193']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S><S sid ="141" ssid = "54">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S><S sid ="163" ssid = "76">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S><S sid ="191" ssid = "3">Despite the popularity of additive models  our experimental results showed the superiority of models utilizing multiplicative combinations  at least for the sentence similarity task attempted here.</S><S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'141'", "'163'", "'191'", "'108'"]
'10'
'141'
'163'
'191'
'108'
['10', '141', '163', '191', '108']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">Despite their widespread use  vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature.</S><S sid ="33" ssid = "6">In contrast  symbolic representations can naturally handle the binding of constituents to their roles  in a systematic manner that avoids both these problems.</S><S sid ="6" ssid = "2">The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).</S><S sid ="29" ssid = "2">While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S><S sid ="95" ssid = "8">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'33'", "'6'", "'29'", "'95'"]
'11'
'33'
'6'
'29'
'95'
['11', '33', '6', '29', '95']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">In fact  the commonest method for combining the vectors is to average them.</S><S sid ="77" ssid = "25">For additive models  a natural way to achieve this is to include further vectors into the summation.</S><S sid ="43" ssid = "16">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'77'", "'43'", "'34'", "'39'"]
'12'
'77'
'43'
'34'
'39'
['12', '77', '43', '34', '39']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">Vector averaging is unfortunately insensitive to word order  and more generally syntactic structure  giving the same representation to any constructions that happen to share the same vocabulary.</S><S sid ="6" ssid = "2">The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).</S><S sid ="46" ssid = "19">However  since it is order independent  it cannot capture meaning differences that are modulated by differences in syntactic structure.</S><S sid ="93" ssid = "6">The idea here is that an appropriate composition model when applied to horse and ran will yield a vector closer to the landmark gallop than dissolve.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'6'", "'46'", "'93'", "'194'"]
'13'
'6'
'46'
'93'
'194'
['13', '6', '46', '93', '194']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">This is illustrated in the example below taken from Landauer et al. (1997).</S><S sid ="175" ssid = "9">We observe a similar pattern for the non compositional baseline model  the weighted additive model and Kintsch (2001).</S><S sid ="179" ssid = "13">The results of our correlation analysis are also given in Table 2.</S><S sid ="55" ssid = "3">A hypothetical semantic space is illustrated in Figure 1.</S><S sid ="88" ssid = "1">We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'175'", "'179'", "'55'", "'88'"]
'14'
'175'
'179'
'55'
'88'
['14', '175', '179', '55', '88']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">Sentences (1-a) and (1-b) contain exactly the same set of words but their meaning is entirely different.</S><S sid ="22" ssid = "18">Here  semantic analysis is guided by syntactic structure  and therefore sentences (1-a) and (1-b) receive distinct representations.</S><S sid ="56" ssid = "4">Here  the space has only five dimensions  and the matrix cells denote the co-occurrence of the target words (horse and run) with the context words animal  stable  and so on.</S><S sid ="79" ssid = "27">When modeling predicate-argument structures  Kintsch (2001) proposes including one or more distributional neighbors  n  of the predicate: Note that considerable latitude is allowed in selecting the appropriate neighbors.</S><S sid ="20" ssid = "16">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'22'", "'56'", "'79'", "'20'"]
'15'
'22'
'56'
'79'
'20'
['15', '22', '56', '79', '20']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">(1) a.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="15" ssid = "11">Sentences (1-a) and (1-b) contain exactly the same set of words but their meaning is entirely different.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="22" ssid = "18">Here  semantic analysis is guided by syntactic structure  and therefore sentences (1-a) and (1-b) receive distinct representations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'150'", "'15'", "'34'", "'22'"]
'16'
'150'
'15'
'34'
'22'
['16', '150', '15', '34', '22']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">It was not the sales manager who hit the bottle that day  but the office worker with the serious drinking problem. b.</S><S sid ="18" ssid = "14">That day the office manager  who was drinking  hit the problem sales worker with a bottle  but it was not serious.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S><S sid ="174" ssid = "8">The simple additive model fails to distinguish between High and Low Similarity items.</S><S sid ="122" ssid = "35">The opposite is the case for the reference The face glowed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'18'", "'164'", "'174'", "'122'"]
'17'
'18'
'164'
'174'
'122'
['17', '18', '164', '174', '122']
parsed_discourse_facet ['method_citation']
['Our results show that the multiplicative models are superior and correlate significantly with behavioral data.']
['This paper proposes a framework for representing the meaning of phrases and sentences in vector space.', 'In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.', 'In this paper we presented a general framework for vector-based semantic composition.', 'Vector addition is by far the most common method for representing the meaning of linguistic sequences.', 'Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:36', 'F:0']
['We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.']
['The simple additive model fails to distinguish between High and Low Similarity items.', 'The multiplicative and combined models yield means closer to the human ratings.', 'Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.', 'Our results show that the multiplicative models are superior and correlate significantly with behavioral data.', 'Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:325', 'F:0']
['In order to establish an independent measure of sentence similarity, we assembled a set of experimental materials and elicited similarity ratings from human subjects.']
['A hypothetical semantic space is illustrated in Figure 1.', 'We observe a similar pattern for the non compositional baseline model  the weighted additive model and Kintsch (2001).', 'We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).', 'This is illustrated in the example below taken from Landauer et al. (1997).', 'The results of our correlation analysis are also given in Table 2.']
['system', 'ROUGE-S*', 'Average_R:', '0.00284', '(95%-conf.int.', '0.00284', '-', '0.00284)']
['system', 'ROUGE-S*', 'Average_P:', '0.01905', '(95%-conf.int.', '0.01905', '-', '0.01905)']
['system', 'ROUGE-S*', 'Average_F:', '0.00495', '(95%-conf.int.', '0.00495', '-', '0.00495)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:105', 'F:2']
['The models considered so far assume that components do not &#8216;interfere&#8217; with each other, i.e., that It is also possible to re-introduce the dependence on K into the model of vector composition.']
['In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).', 'Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.', 'Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.', u'Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch\xc2\xa8utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).', 'While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).']
['system', 'ROUGE-S*', 'Average_R:', '0.00035', '(95%-conf.int.', '0.00035', '-', '0.00035)']
['system', 'ROUGE-S*', 'Average_P:', '0.04545', '(95%-conf.int.', '0.04545', '-', '0.04545)']
['system', 'ROUGE-S*', 'Average_F:', '0.00070', '(95%-conf.int.', '0.00070', '-', '0.00070)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:8515', 'P:66', 'F:3']
['We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).', 'While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).', 'Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.', 'Previous applications of vector addition to document indexing (Deerwester et al.  1990) or essay grading (Landauer et al.  1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.', 'In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).']
['system', 'ROUGE-S*', 'Average_R:', '0.00083', '(95%-conf.int.', '0.00083', '-', '0.00083)']
['system', 'ROUGE-S*', 'Average_P:', '0.16667', '(95%-conf.int.', '0.16667', '-', '0.16667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00164', '(95%-conf.int.', '0.00164', '-', '0.00164)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:7260', 'P:36', 'F:6']
['We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
[u'A more scrupulous evaluation requires directly correlating all the individual participants\u2019 similarity judgments with those of the models.6 We used Spearman\u2019s p for our correlation analyses.', 'That day the office manager  who was drinking  hit the problem sales worker with a bottle  but it was not serious.', 'The opposite is the case for the reference The face glowed.', 'The simple additive model fails to distinguish between High and Low Similarity items.', 'It was not the sales manager who hit the bottle that day  but the office worker with the serious drinking problem. b.']
['system', 'ROUGE-S*', 'Average_R:', '0.00097', '(95%-conf.int.', '0.00097', '-', '0.00097)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00187', '(95%-conf.int.', '0.00187', '-', '0.00187)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:36', 'F:1']
['We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['Despite their widespread use  vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature.', 'In contrast  symbolic representations can naturally handle the binding of constituents to their roles  in a systematic manner that avoids both these problems.', 'Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.', 'While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.', 'The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).']
['system', 'ROUGE-S*', 'Average_R:', '0.00273', '(95%-conf.int.', '0.00273', '-', '0.00273)']
['system', 'ROUGE-S*', 'Average_P:', '0.22222', '(95%-conf.int.', '0.22222', '-', '0.22222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00540', '(95%-conf.int.', '0.00540', '-', '0.00540)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:36', 'F:8']
['In this paper we presented a general framework for vector-based semantic composition.']
['We present a general framework for vector-based composition which allows us to consider different classes of models.', 'Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.', 'Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.', 'We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.', 'Although we have presented multiplicative and additive models separately  there is nothing inherent in our formulation that disallows their combination.']
['system', 'ROUGE-S*', 'Average_R:', '0.01330', '(95%-conf.int.', '0.01330', '-', '0.01330)']
['system', 'ROUGE-S*', 'Average_P:', '0.53571', '(95%-conf.int.', '0.53571', '-', '0.53571)']
['system', 'ROUGE-S*', 'Average_F:', '0.02595', '(95%-conf.int.', '0.02595', '-', '0.02595)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:28', 'F:15']
['Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.']
['This paper proposes a framework for representing the meaning of phrases and sentences in vector space.', 'The applications of the framework discussed here are many and varied both for cognitive science and NLP.', 'This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.', 'Vector addition is by far the most common method for representing the meaning of linguistic sequences.', 'A variety of NLP tasks have made good use of vector-based models.']
['system', 'ROUGE-S*', 'Average_R:', '0.00348', '(95%-conf.int.', '0.00348', '-', '0.00348)']
['system', 'ROUGE-S*', 'Average_P:', '0.02206', '(95%-conf.int.', '0.02206', '-', '0.02206)']
['system', 'ROUGE-S*', 'Average_F:', '0.00602', '(95%-conf.int.', '0.00602', '-', '0.00602)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:136', 'F:3']
0.115437776495 0.00272222219198 0.00516999994256





input/ref/Task1/P11-1060_aakansha.csv
input/res/Task1/P11-1060.csv
parsing: input/ref/Task1/P11-1060_aakansha.csv
<S sid="11" ssid="7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'"]
'2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="5">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'"]
'112'
['112']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="11">Although a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">However, we still model the logical form (now as a latent variable) to capture the complexities of language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="12">It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="21">The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="88" ssid="64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets, a restrictor A and a nuclear scope B.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'"]
'88'
['88']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S sid ="1" ssid = "1">Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="2" ssid = "2">In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'141'", "'15'", "'6'", "'2'"]
'1'
'141'
'15'
'6'
'2'
['1', '141', '15', '6', '2']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="117" ssid = "2">In each dataset  each sentence x is annotated with a Prolog logical form  which we use only to evaluate and get an answer y.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="149" ssid = "34">Think of DCS as a higher-level Our system learns lexical associations between programming language tailored to natural language  words and predicates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'3'", "'117'", "'100'", "'149'"]
'2'
'3'
'117'
'100'
'149'
['2', '3', '117', '100', '149']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="48" ssid = "24">In addition  trees enable efficient computation  thereby establishing a new connection between dependency syntax and efficient semantic evaluation.</S><S sid ="149" ssid = "34">Think of DCS as a higher-level Our system learns lexical associations between programming language tailored to natural language  words and predicates.</S><S sid ="2" ssid = "2">In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.</S><S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parent’s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'48'", "'149'", "'2'", "'51'"]
'3'
'48'
'149'
'2'
'51'
['3', '48', '149', '2', '51']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">On two stansemantic parsing benchmarks our system obtains the highest published accuracies  despite requiring no annotated logical forms.</S><S sid ="24" ssid = "20">Our system outperforms all existing systems despite using no annotated logical forms.</S><S sid ="136" ssid = "21">In fact  DCS performs comparably to even the version of SEMRESP trained using logical forms.</S><S sid ="139" ssid = "24">All other systems require logical forms as training data  whereas ours does not.</S><S sid ="135" ssid = "20">SEMRESP requires a lexicon of 1.42 words per non-value predicate  WordNet features  and syntactic parse trees; DCS requires only words for the domain-independent predicates (overall  around 0.5 words per non-value predicate)  POS tags  and very simple indicator features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'24'", "'136'", "'139'", "'135'"]
'4'
'24'
'136'
'139'
'135'
['4', '24', '136', '139', '135']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">What is the total population of the ten largest capitals in the US?</S><S sid ="52" ssid = "28">The denotation of the middle node is {s}  where s is all major cities.</S><S sid ="105" ssid = "81">The feature vector φ(x  z) is defined by sums of five simple indicator feature templates: (F1) a word triggers a predicate (e.g.  [city  city]); (F2) a word is under a relation (e.g.  [that  11]); (F3) a word is under a trace predicate (e.g.  [in  loc]); (F4) two predicates are linked via a relation in the left or right direction (e.g.  [city 11  loc  RIGHT]); and (F5) a predicate has a child relation (e.g.  [city  11]).</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S><S sid ="50" ssid = "26">For example  consider the phrase number of major cities  and suppose that number corresponds to the count predicate.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'52'", "'105'", "'62'", "'50'"]
'5'
'52'
'105'
'62'
'50'
['5', '52', '105', '62', '50']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="48" ssid = "24">In addition  trees enable efficient computation  thereby establishing a new connection between dependency syntax and efficient semantic evaluation.</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'169'", "'58'", "'48'", "'165'"]
'6'
'169'
'58'
'48'
'165'
['6', '169', '58', '48', '165']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Supervised semantic parsers (Zelle and Mooney  1996; Tang and Mooney  2001; Ge and Mooney  2005; Zettlemoyer and Collins  2005; Kate and Mooney  2007; Zettlemoyer and Collins  2007; Wong and Mooney  2007; Kwiatkowski et al.  2010) rely on manual annotation of logical forms  which is expensive.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="166" ssid = "51">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid ="125" ssid = "10">We used the same training-test splits as Zettlemoyer and Collins (2005) (600+280 for GEO and 500+140 for JOBS).</S><S sid ="161" ssid = "46">CCG (Steedman  2000)  in which semantic pars- The integration of natural language with denotaing is driven from the lexicon.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'8'", "'166'", "'125'", "'161'"]
'7'
'8'
'166'
'125'
'161'
['7', '8', '166', '125', '161']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="138" ssid = "23">Next  we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S><S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid ="18" ssid = "14">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'138'", "'9'", "'165'", "'18'"]
'8'
'138'
'9'
'165'
'18'
['8', '138', '9', '165', '18']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="20" ssid = "16">At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'58'", "'165'", "'26'", "'20'"]
'9'
'58'
'165'
'26'
'20'
['9', '58', '165', '26', '20']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="37" ssid = "13">The basic version of DCS restricts R to join and aggregate relations (see Table 1).</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'58'", "'169'", "'37'", "'26'"]
'10'
'58'
'169'
'37'
'26'
['10', '58', '169', '37', '26']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts)  producing an answer y.</S><S sid ="118" ssid = "3">This evaluation is done with respect to a world w. Recall that a world w maps each predicate p ∈ P to a set of tuples w(p).</S><S sid ="31" ssid = "7">Conceptually  a world is a relational database where each predicate is a relation (possibly infinite).</S><S sid ="38" ssid = "14">Let us start by considering a DCS tree z with only join relations.</S><S sid ="29" ssid = "5">Let P be a set of predicates (e.g.  state  count ∈ P)  which are just symbols.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'118'", "'31'", "'38'", "'29'"]
'11'
'118'
'31'
'38'
'29'
['11', '118', '31', '38', '29']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="45" ssid = "21">The logical forms in DCS are called DCS trees  where nodes are labeled with predicates  and edges are labeled with relations.</S><S sid ="32" ssid = "8">Define a special predicate ø with w(ø) = V. We represent functions by a set of inputoutput pairs  e.g.  w(count) = {(S  n) : n = |S|}.</S><S sid ="85" ssid = "61">Extraction allows us to return the set of consistent values of a marked non-root node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'169'", "'45'", "'32'", "'85'"]
'12'
'169'
'45'
'32'
'85'
['12', '169', '45', '32', '85']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x  y)  which is much cheaper to obtain than (x  z) pairs.</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S><S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="103" ssid = "79">To define the features  we technically need to augment each tree z ∈ ZL(x) with alignment information—namely  for each predicate in z  the span in x (if any) that triggered it.</S><S sid ="108" ssid = "84">However  in order to learn  we need to sum over {z ∈ ZL(x) : JzKw = y}  and unfortunately  the additional constraint JzKw = y does not factorize.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'62'", "'12'", "'103'", "'108'"]
'13'
'62'
'12'
'103'
'108'
['13', '62', '12', '103', '108']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.</S><S sid ="43" ssid = "19">Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter  2003).</S><S sid ="107" ssid = "83">Our model is arc-factored  so we can sum over all DCS trees in ZL(x) using dynamic programming.</S><S sid ="47" ssid = "23">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.</S><S sid ="59" ssid = "35">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'43'", "'107'", "'47'", "'59'"]
'14'
'43'
'107'
'47'
'59'
['14', '43', '107', '47', '59']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="142" ssid = "27">This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.</S><S sid ="163" ssid = "48">Feedback from the context; for example  the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x  y)  which means (Schuler  2003) and semantic parsing (Popescu et borders looks right for the first argument and left al.  2003; Clarke et al.  2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'100'", "'141'", "'142'", "'163'"]
'15'
'100'
'141'
'142'
'163'
['15', '100', '141', '142', '163']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Which one should we use?</S><S sid ="144" ssid = "29">Intuitions How is our system learning?</S><S sid ="142" ssid = "27">This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.</S><S sid ="95" ssid = "71">Our first question is: given notation of the DCS tree before execution is an utterance x  what trees z ∈ Z are permissible?</S><S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'144'", "'142'", "'95'", "'15'"]
'16'
'144'
'142'
'95'
'15'
['16', '144', '142', '95', '15']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "13">The dominant paradigm in compositional semantics is Montague semantics  which constructs lambda calculus forms in a bottom-up manner.</S><S sid ="36" ssid = "12">It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.</S><S sid ="37" ssid = "13">The basic version of DCS restricts R to join and aggregate relations (see Table 1).</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid ="25" ssid = "1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS)  which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'36'", "'37'", "'58'", "'25'"]
'17'
'36'
'37'
'58'
'25'
['17', '36', '37', '58', '25']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S><S sid ="165" ssid = "50">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="20" ssid = "16">At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.</S><S sid ="167" ssid = "52">In DCS  we start with lexical triggers  which are 6 Conclusion more basic than CCG lexical entries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'165'", "'26'", "'20'", "'167'"]
'18'
'165'
'26'
'20'
'167'
['18', '165', '26', '20', '167']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "15">However  the logical forms there can become quite complex  and in the context of program induction  this would lead to an unwieldy search space.</S><S sid ="122" ssid = "7">For JOBS  if we use the standard Jobs database  close to half the y’s are empty  which makes it uninteresting.</S><S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parent’s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid ="14" ssid = "10">The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.</S><S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'122'", "'51'", "'14'", "'9'"]
'19'
'122'
'51'
'14'
'9'
['19', '122', '51', '14', '9']
parsed_discourse_facet ['method_citation']
['The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).']
['This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.', 'Our model is arc-factored  so we can sum over all DCS trees in ZL(x) using dynamic programming.', 'The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).', 'The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.', 'Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter  2003).']
['system', 'ROUGE-S*', 'Average_R:', '0.00121', '(95%-conf.int.', '0.00121', '-', '0.00121)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00237', '(95%-conf.int.', '0.00237', '-', '0.00237)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:45', 'F:3']
['The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.', u'The feature vector \xcf\u2020(x  z) is defined by sums of five simple indicator feature templates: (F1) a word triggers a predicate (e.g.  [city  city]); (F2) a word is under a relation (e.g.  [that  11]); (F3) a word is under a trace predicate (e.g.  [in  loc]); (F4) two predicates are linked via a relation in the left or right direction (e.g.  [city 11  loc  RIGHT]); and (F5) a predicate has a child relation (e.g.  [city  11]).', 'What is the total population of the ten largest capitals in the US?', 'For example  consider the phrase number of major cities  and suppose that number corresponds to the count predicate.', 'The denotation of the middle node is {s}  where s is all major cities.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:105', 'F:0']
['The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['However  we still model the logical form (now as a latent variable) to capture the complexities of language.', 'We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.', 'The basic version of DCS restricts R to join and aggregate relations (see Table 1).', 'We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.', 'The combination rules are encoded in the tems  despite using no annotated logical forms.']
['system', 'ROUGE-S*', 'Average_R:', '0.00355', '(95%-conf.int.', '0.00355', '-', '0.00355)']
['system', 'ROUGE-S*', 'Average_P:', '0.03810', '(95%-conf.int.', '0.03810', '-', '0.03810)']
['system', 'ROUGE-S*', 'Average_F:', '0.00649', '(95%-conf.int.', '0.00649', '-', '0.00649)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:105', 'F:4']
['Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.']
['As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.', 'On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.', 'Next  we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).', 'CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).', 'These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:300', 'F:0']
['This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.']
['However  the logical forms there can become quite complex  and in the context of program induction  this would lead to an unwieldy search space.', 'As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.', u'For JOBS  if we use the standard Jobs database  close to half the y\u2019s are empty  which makes it uninteresting.', u'It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parent\u2019s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.', 'The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.']
['system', 'ROUGE-S*', 'Average_R:', '0.00070', '(95%-conf.int.', '0.00070', '-', '0.00070)']
['system', 'ROUGE-S*', 'Average_P:', '0.01053', '(95%-conf.int.', '0.01053', '-', '0.01053)']
['system', 'ROUGE-S*', 'Average_F:', '0.00132', '(95%-conf.int.', '0.00132', '-', '0.00132)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:190', 'F:2']
['Although a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words.']
['As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.', 'These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.', 'We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.', 'At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.', 'We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.']
['system', 'ROUGE-S*', 'Average_R:', '0.00195', '(95%-conf.int.', '0.00195', '-', '0.00195)']
['system', 'ROUGE-S*', 'Average_P:', '0.10909', '(95%-conf.int.', '0.10909', '-', '0.10909)']
['system', 'ROUGE-S*', 'Average_F:', '0.00383', '(95%-conf.int.', '0.00383', '-', '0.00383)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:55', 'F:6']
['The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.']
['Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.', u'Our first question is: given notation of the DCS tree before execution is an utterance x  what trees z \xe2\u02c6\u02c6 Z are permissible?', 'Intuitions How is our system learning?', 'This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.', 'Which one should we use?']
['system', 'ROUGE-S*', 'Average_R:', '0.00568', '(95%-conf.int.', '0.00568', '-', '0.00568)']
['system', 'ROUGE-S*', 'Average_P:', '0.04545', '(95%-conf.int.', '0.04545', '-', '0.04545)']
['system', 'ROUGE-S*', 'Average_F:', '0.01010', '(95%-conf.int.', '0.01010', '-', '0.01010)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:528', 'P:66', 'F:3']
['Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.']
['Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.', 'In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.', 'Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).', 'Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.', 'Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.']
['system', 'ROUGE-S*', 'Average_R:', '0.00043', '(95%-conf.int.', '0.00043', '-', '0.00043)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00083', '(95%-conf.int.', '0.00083', '-', '0.00083)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:55', 'F:1']
['Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets, a restrictor A and a nuclear scope B.']
['We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS)  which captures the core idea of using trees to represent formal semantics.', 'The basic version of DCS restricts R to join and aggregate relations (see Table 1).', 'It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.', 'The dominant paradigm in compositional semantics is Montague semantics  which constructs lambda calculus forms in a bottom-up manner.', 'We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:45', 'F:0']
0.0320022218666 0.00150222220553 0.00277111108032





input/ref/Task1/W06-3114_swastika.csv
input/res/Task1/W06-3114.csv
parsing: input/ref/Task1/W06-3114_swastika.csv
<S sid="47" ssid="13">Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data.</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
    <S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="37">Our suspicion is that BLEU is very sensitive to jargon, to selecting exactly the right words, and not synonyms that human judges may appreciate as equally good.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
<S sid="145" ssid="38">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S>
original cit marker offset is 0
new cit marker offset is 0



['145']
145
['145']
parsed_discourse_facet ['result_citation']
    <S sid="103" ssid="19">Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['103']
103
['103']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="16">Following this method, we repeatedly &#8212; say, 1000 times &#8212; sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.</S>
original cit marker offset is 0
new cit marker offset is 0



['50']
50
['50']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="7">We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.</S>
original cit marker offset is 0
new cit marker offset is 0



['68']
68
['68']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="8">The way judgements are collected, human judges tend to use the scores to rank systems against each other.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['result_citation']
<S sid="145" ssid="38">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S>
original cit marker offset is 0
new cit marker offset is 0



['145']
145
['145']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="143" ssid = "36">For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.</S><S sid ="129" ssid = "22">All systems (except for Systran  which was not tuned to Europarl) did considerably worse on outof-domain training data.</S><S sid ="121" ssid = "14">For the automatic scoring method BLEU  we can distinguish three quarters of the systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'136'", "'143'", "'129'", "'121'"]
'1'
'136'
'143'
'129'
'121'
['1', '136', '143', '129', '121']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S sid ="86" ssid = "2">The average fluency judgement per judge ranged from 2.33 to 3.67  the average adequacy judgement ranged from 2.56 to 4.13.</S><S sid ="150" ssid = "43">For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French)  at least as measured by BLEU.</S><S sid ="95" ssid = "11">The judgement of 4 in the first case will go to a vastly better system output than in the second case.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'86'", "'150'", "'95'", "'136'"]
'2'
'86'
'150'
'95'
'136'
['2', '86', '150', '95', '136']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "1">was done by the participants.</S><S sid ="67" ssid = "6">Participants and other volunteers contributed about 180 hours of labor in the manual evaluation.</S><S sid ="29" ssid = "22">About half of the participants of last year’s shared task participated again.</S><S sid ="78" ssid = "17">Judges where excluded from assessing the quality of MT systems that were submitted by their institution.</S><S sid ="168" ssid = "61">Annotators argued for the importance of having correct and even multiple references.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'67'", "'29'", "'78'", "'168'"]
'3'
'67'
'29'
'78'
'168'
['3', '67', '29', '78', '168']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "2">This revealed interesting clues about the properties of automatic and manual scoring.</S><S sid ="146" ssid = "39">This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.</S><S sid ="168" ssid = "61">Annotators argued for the importance of having correct and even multiple references.</S><S sid ="176" ssid = "7">Human judges also pointed out difficulties with the evaluation of long sentences.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'146'", "'168'", "'176'", "'134'"]
'4'
'146'
'168'
'176'
'134'
['4', '146', '168', '176', '134']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "3">• We evaluated translation from English  in addition to into English.</S><S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S><S sid ="152" ssid = "45">The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'6'", "'152'", "'170'", "'162'"]
'5'
'6'
'152'
'170'
'162'
['5', '6', '152', '170', '162']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S><S sid ="5" ssid = "3">• We evaluated translation from English  in addition to into English.</S><S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="79" ssid = "18">Sentences and systems were randomly selected and randomly shuffled for presentation.</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'5'", "'163'", "'79'", "'162'"]
'6'
'5'
'163'
'79'
'162'
['6', '5', '163', '79', '162']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "5">We dropped  however  one of the languages  Finnish  partly to keep the number of tracks manageable  partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation.</S><S sid ="165" ssid = "58">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid ="66" ssid = "5">In this shared task  we were also confronted with this problem  and since we had no funding for paying human judgements  we asked participants in the evaluation to share the burden.</S><S sid ="47" ssid = "13">Because of this  we retokenized and lowercased submitted output with our own tokenizer  which was also used to prepare the training and test data.</S><S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'165'", "'66'", "'47'", "'125'"]
'7'
'165'
'66'
'47'
'125'
['7', '165', '66', '47', '125']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "1">The evaluation framework for the shared task is similar to the one used in last year’s shared task.</S><S sid ="57" ssid = "23">We are therefore applying a different method  which has been used at the 2005 DARPA/NIST evaluation.</S><S sid ="29" ssid = "22">About half of the participants of last year’s shared task participated again.</S><S sid ="31" ssid = "24">Compared to last year’s shared task  the participants represent more long-term research efforts.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'57'", "'29'", "'31'", "'28'"]
'8'
'57'
'29'
'31'
'28'
['8', '57', '29', '31', '28']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "2">Training and testing is based on the Europarl corpus.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="143" ssid = "36">For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.</S><S sid ="21" ssid = "14">The out-of-domain test set differs from the Europarl data in various ways.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'28'", "'170'", "'143'", "'21'"]
'9'
'28'
'170'
'143'
'21'
['9', '28', '170', '143', '21']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "3">Figure 1 provides some statistics about this corpus.</S><S sid ="20" ssid = "13">For statistics on this test set  refer to Figure 1.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="4" ssid = "2">This revealed interesting clues about the properties of automatic and manual scoring.</S><S sid ="120" ssid = "13">In Figure 4  we displayed the number of system comparisons  for which we concluded statistical significance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'20'", "'134'", "'4'", "'120'"]
'10'
'20'
'134'
'4'
'120'
['10', '20', '134', '4', '120']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "4">To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="161" ssid = "54">Since all long sentence translation are somewhat muddled  even a contrastive evaluation between systems was difficult.</S><S sid ="152" ssid = "45">The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.</S><S sid ="31" ssid = "24">Compared to last year’s shared task  the participants represent more long-term research efforts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'28'", "'161'", "'152'", "'31'"]
'11'
'28'
'161'
'152'
'31'
['11', '28', '161', '152', '31']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "5">To summarize  we provided: The performance of the baseline system is similar to the best submissions in last year’s shared task.</S><S sid ="141" ssid = "34">In-domain Systran scores on this metric are lower than all statistical systems  even the ones that have much worse human scores.</S><S sid ="8" ssid = "1">The evaluation framework for the shared task is similar to the one used in last year’s shared task.</S><S sid ="81" ssid = "20">This is less than the 694 judgements 2004 DARPA/NIST evaluation  or the 532 judgements in the 2005 DARPA/NIST evaluation.</S><S sid ="151" ssid = "44">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages  as demonstrated by the results for English-German and English-French.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'141'", "'8'", "'81'", "'151'"]
'12'
'141'
'8'
'81'
'151'
['12', '141', '8', '81', '151']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "6">We are currently working on a complete open source implementation of a training and decoding system  which should become available over the summer. pus  from which also the in-domain test set is taken.</S><S sid ="33" ssid = "26">While building a machine translation system is a serious undertaking  in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.</S><S sid ="43" ssid = "9">At the very least  we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.</S><S sid ="74" ssid = "13">While we had up to 11 submissions for a translation direction  we did decide against presenting all 11 system outputs to the human judge.</S><S sid ="42" ssid = "8">It was our hope that this competition  which included the manual and automatic evaluation of statistical systems and one rulebased commercial system  will give further insight into the relation between automatic and manual evaluation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'33'", "'43'", "'74'", "'42'"]
'13'
'33'
'43'
'74'
'42'
['13', '33', '43', '74', '42']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "7">There is twice as much language modelling data  since training data for the machine translation system is filtered against sentences of length larger than 40 words.</S><S sid ="165" ssid = "58">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid ="58" ssid = "24">We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set  53 blocks for the out-of-domain test set)  check for each block  if one system has a higher BLEU score than the other  and then use the sign test.</S><S sid ="151" ssid = "44">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages  as demonstrated by the results for English-German and English-French.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'165'", "'58'", "'151'", "'155'"]
'14'
'165'
'58'
'151'
'155'
['14', '165', '58', '151', '155']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "8">Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.</S><S sid ="21" ssid = "14">The out-of-domain test set differs from the Europarl data in various ways.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="142" ssid = "35">Surprisingly  this effect is much less obvious for out-of-domain test data.</S><S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'21'", "'28'", "'142'", "'18'"]
'15'
'21'
'28'
'142'
'18'
['15', '21', '28', '142', '18']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="165" ssid = "58">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid ="14" ssid = "7">There is twice as much language modelling data  since training data for the machine translation system is filtered against sentences of length larger than 40 words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'126'", "'18'", "'165'", "'14'"]
'16'
'126'
'18'
'165'
'14'
['16', '126', '18', '165', '14']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="69" ssid = "8">We settled on contrastive evaluations of 5 system outputs for a single test sentence.</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="19" ssid = "12">We aligned the texts at a sentence level across all four languages  resulting in 1064 sentence per language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'69'", "'162'", "'170'", "'19'"]
'17'
'69'
'162'
'170'
'19'
['17', '69', '162', '170', '19']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="15" ssid = "8">Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'126'", "'16'", "'15'", "'155'"]
'18'
'126'
'16'
'15'
'155'
['18', '126', '16', '15', '155']
parsed_discourse_facet ['method_citation']
['This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.']
['In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.', 'For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.', 'Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00164', '(95%-conf.int.', '0.00164', '-', '0.00164)']
['system', 'ROUGE-S*', 'Average_P:', '0.02206', '(95%-conf.int.', '0.02206', '-', '0.02206)']
['system', 'ROUGE-S*', 'Average_F:', '0.00305', '(95%-conf.int.', '0.00305', '-', '0.00305)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:136', 'F:3']
['Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'All systems (except for Systran  which was not tuned to Europarl) did considerably worse on outof-domain training data.', 'The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).', 'For the automatic scoring method BLEU  we can distinguish three quarters of the systems.', 'For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.05556', '(95%-conf.int.', '0.05556', '-', '0.05556)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:973710', 'P:36', 'F:2']
['Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.']
['Training and testing is based on the Europarl corpus.', 'Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.', 'For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.', 'The out-of-domain test set differs from the Europarl data in various ways.', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_P:', '0.01587', '(95%-conf.int.', '0.01587', '-', '0.01587)']
['system', 'ROUGE-S*', 'Average_F:', '0.01418', '(95%-conf.int.', '0.01418', '-', '0.01418)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:630', 'F:10']
['In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.']
['was done by the participants.', 'Judges where excluded from assessing the quality of MT systems that were submitted by their institution.', 'Annotators argued for the importance of having correct and even multiple references.', u'About half of the participants of last year\xe2\u20ac\u2122s shared task participated again.', 'Participants and other volunteers contributed about 180 hours of labor in the manual evaluation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00230', '(95%-conf.int.', '0.00230', '-', '0.00230)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00380', '(95%-conf.int.', '0.00380', '-', '0.00380)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:91', 'F:1']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.', 'Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.', 'The out-of-domain test set differs from the Europarl data in various ways.', 'In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.', 'Surprisingly  this effect is much less obvious for out-of-domain test data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:55', 'F:0']
['In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.', 'The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.', u'\xe2\u20ac\xa2 We evaluated translation from English  in addition to into English.', 'A few annotators suggested to break up long sentences into clauses and evaluate these separately.', 'English was again paired with German  French  and Spanish.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:91', 'F:0']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages  as demonstrated by the results for English-German and English-French.', u'The evaluation framework for the shared task is similar to the one used in last year\u2019s shared task.', u'To summarize  we provided: The performance of the baseline system is similar to the best submissions in last year\u2019s shared task.', 'This is less than the 694 judgements 2004 DARPA/NIST evaluation  or the 532 judgements in the 2005 DARPA/NIST evaluation.', 'In-domain Systran scores on this metric are lower than all statistical systems  even the ones that have much worse human scores.']
['system', 'ROUGE-S*', 'Average_R:', '0.00175', '(95%-conf.int.', '0.00175', '-', '0.00175)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00340', '(95%-conf.int.', '0.00340', '-', '0.00340)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:55', 'F:3']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
[u'However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate \xe2\u20ac\u201d due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).', 'In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.', 'There is twice as much language modelling data  since training data for the machine translation system is filtered against sentences of length larger than 40 words.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00167', '(95%-conf.int.', '0.00167', '-', '0.00167)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.00327', '(95%-conf.int.', '0.00327', '-', '0.00327)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:55', 'F:5']
['We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.']
['To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.', u'Compared to last year\xe2\u20ac\u2122s shared task  the participants represent more long-term research efforts.', 'The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.', 'Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.', 'Since all long sentence translation are somewhat muddled  even a contrastive evaluation between systems was difficult.']
['system', 'ROUGE-S*', 'Average_R:', '0.00349', '(95%-conf.int.', '0.00349', '-', '0.00349)']
['system', 'ROUGE-S*', 'Average_P:', '0.05495', '(95%-conf.int.', '0.05495', '-', '0.05495)']
['system', 'ROUGE-S*', 'Average_F:', '0.00657', '(95%-conf.int.', '0.00657', '-', '0.00657)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:91', 'F:5']
['The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.']
['The judgement of 4 in the first case will go to a vastly better system output than in the second case.', 'For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French)  at least as measured by BLEU.', u'0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 \u2022systran \u2022 ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency \u2022systran \u2022nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p \ufffd 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain \u2022upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv 23 24 25 26 27 28 29 30 31 32 \u2022upc-mr \u2022utd \u2022upc-jmc \u2022uedin-birch \u2022ntt \u2022rali \u2022uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy \u2022upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency \u2022ntt \u2022nrc \u2022rali \u2022uedin-birch -0.2 -0.3 -0.5 \u2022upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr \u2022 rali Fluency -0.4 \u2022upc-mr utd \u2022upc-jmc -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 \u2022upv 0.5 0.4 \u2022systran \u2022upc-mr \u2022 \u2022rali 0.3 \u2022ntt 0.2 0.1 -0.0 -0.1 \u2022systran \u2022upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022upv \u2022systran \u2022upc-mr \u2022 Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022systran \u2022ntt', 'The average fluency judgement per judge ranged from 2.33 to 3.67  the average adequacy judgement ranged from 2.56 to 4.13.', 'The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:116403', 'P:45', 'F:0']
['The way judgements are collected, human judges tend to use the scores to rank systems against each other.']
['We aligned the texts at a sentence level across all four languages  resulting in 1064 sentence per language.', 'We settled on contrastive evaluations of 5 system outputs for a single test sentence.', 'A few annotators suggested to break up long sentences into clauses and evaluate these separately.', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:28', 'F:0']
0.0277172724753 0.00215181816226 0.00311545451713





input/ref/Task1/W99-0623_sweta.csv
input/res/Task1/W99-0623.csv
parsing: input/ref/Task1/W99-0623_sweta.csv
 <S sid="144" ssid="6">Combining multiple highly-accurate independent parsers yields promising results.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
 <S sid="125" ssid="54">The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["125'"]
125'
['125']
parsed_discourse_facet ['method_citation']
<S sid="125" ssid="54">The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["125'"]
125'
['125']
parsed_discourse_facet ['method_citation']
    <S sid="48" ssid="34">&#8226; Similarly, when the na&#239;ve Bayes classifier is configured such that the constituents require estimated probabilities strictly larger than 0.5 to be accepted, there is not enough probability mass remaining on crossing brackets for them to be included in the hypothesis.</S>
original cit marker offset is 0
new cit marker offset is 0



["48'"]
48'
['48']
parsed_discourse_facet ['method_citation']
 <S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="63">As seen by the drop in average individual parser performance baseline, the introduced parser does not perform very well.</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
    <S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="13" ssid="9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993).</S>
original cit marker offset is 0
new cit marker offset is 0



["13'"]
13'
['13']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="37">From this we see that a finer-grained model for parser combination, at least for the features we have examined, will not give us any additional power.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="98" ssid="27">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["98'"]
98'
['98']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="13">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'"]
27'
['27']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="9">For our experiments we also report the mean of precision and recall, which we denote by (P + R)I2 and F-measure.</S>
    <S sid="81" ssid="10">F-measure is the harmonic mean of precision and recall, 2PR/(P + R).</S>
    <S sid="82" ssid="11">It is closer to the smaller value of precision and recall when there is a large skew in their values.</S>
original cit marker offset is 0
new cit marker offset is 0



["80'", "'81'", "'82'"]
80'
'81'
'82'
['80', '81', '82']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="35">In general, the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["49'"]
49'
['49']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="7">Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["11'"]
11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="98" ssid="27">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["98'"]
98'
['98']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0623.csv
<S sid ="1" ssid = "1">Three state-of-the-art statistical parsers are combined to produce more accurate parses  as well as new bounds on achievable Treebank parsing accuracy.</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="129" ssid = "58">In the interest of testing the robustness of these combining techniques  we added a fourth  simple nonlexicalized PCFG parser.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'72'", "'38'", "'129'", "'130'"]
'1'
'72'
'38'
'129'
'130'
['1', '72', '38', '129', '130']
Error in Discourse Facet
<S sid ="2" ssid = "2">Two general approaches are presented and two combination techniques are described for each approach.</S><S sid ="59" ssid = "45">Once again we present both a non-parametric and a parametric technique for this task.</S><S sid ="3" ssid = "3">Both parametric and non-parametric models are explored.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'59'", "'3'", "'85'", "'139'"]
'2'
'59'
'3'
'85'
'139'
['2', '59', '3', '85', '139']
Error in Discourse Facet
<S sid ="3" ssid = "3">Both parametric and non-parametric models are explored.</S><S sid ="59" ssid = "45">Once again we present both a non-parametric and a parametric technique for this task.</S><S sid ="2" ssid = "2">Two general approaches are presented and two combination techniques are described for each approach.</S><S sid ="138" ssid = "67">Surprisingly  the non-parametric switching technique also exhibited robust behaviour in this situation.</S><S sid ="136" ssid = "65">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'59'", "'2'", "'138'", "'136'"]
'3'
'59'
'2'
'138'
'136'
['3', '59', '2', '138', '136']
Error in Discourse Facet
<S sid ="4" ssid = "4">The resulting parsers surpass the best previously published performance results for the Penn Treebank.</S><S sid ="76" ssid = "5">The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="134" ssid = "63">As seen by the drop in average individual parser performance baseline  the introduced parser does not perform very well.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'76'", "'85'", "'130'", "'134'"]
'4'
'76'
'85'
'130'
'134'
['4', '76', '85', '130', '134']
Error in Discourse Facet
<S sid ="5" ssid = "1">The natural language processing community is in the strong position of having many available approaches to solving some of its most fundamental problems.</S><S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid ="26" ssid = "12">This technique has the advantage of requiring no training  but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'6'", "'12'", "'13'", "'26'"]
'5'
'6'
'12'
'13'
'26'
['5', '6', '12', '13', '26']
Error in Discourse Facet
<S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="108" ssid = "37">From this we see that a finer-grained model for parser combination  at least for the features we have examined  will not give us any additional power.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'11'", "'9'", "'12'", "'108'"]
'6'
'11'
'9'
'12'
'108'
['6', '11', '9', '12', '108']
Error in Discourse Facet
<S sid ="7" ssid = "3">Their theoretical finding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent  accurate classifiers.</S><S sid ="30" ssid = "16">This is equivalent to the assumption used in probability estimation for naïve Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.</S><S sid ="116" ssid = "45">The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="41" ssid = "27">IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'30'", "'116'", "'38'", "'41'"]
'7'
'30'
'116'
'38'
'41'
['7', '30', '116', '38', '41']
Error in Discourse Facet
<S sid ="8" ssid = "4">The theory has also been validated empirically.</S><S sid ="36" ssid = "22">The estimation of the probabilities in the model is carried out as shown in Equation 4.</S><S sid ="83" ssid = "12">We performed three experiments to evaluate our techniques.</S><S sid ="121" ssid = "50">Table 3 contains the results for evaluating our systems on the test set (section 22).</S><S sid ="109" ssid = "38">The results in Table 2 were achieved on the development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'36'", "'83'", "'121'", "'109'"]
'8'
'36'
'83'
'121'
'109'
['8', '36', '83', '121', '109']
Error in Discourse Facet
<S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="88" ssid = "17">For example  one parser could be more accurate at predicting noun phrases than the other parsers.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'55'", "'88'", "'21'", "'139'"]
'9'
'55'
'88'
'21'
'139'
['9', '55', '88', '21', '139']
Error in Discourse Facet
<S sid ="10" ssid = "6">In both cases the investigators were able to achieve significant improvements over the previous best tagging results.</S><S sid ="136" ssid = "65">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="141" ssid = "3">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid ="134" ssid = "63">As seen by the drop in average individual parser performance baseline  the introduced parser does not perform very well.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'136'", "'85'", "'141'", "'134'"]
'10'
'136'
'85'
'141'
'134'
['10', '136', '85', '141', '134']
Error in Discourse Facet
<S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'9'", "'6'", "'12'", "'13'"]
'11'
'9'
'6'
'12'
'13'
['11', '9', '6', '12', '13']
Error in Discourse Facet
<S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'13'", "'11'", "'6'", "'72'"]
'12'
'13'
'11'
'6'
'72'
['12', '13', '11', '6', '72']
Error in Discourse Facet
<S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="4" ssid = "4">The resulting parsers surpass the best previously published performance results for the Penn Treebank.</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'12'", "'4'", "'72'", "'130'"]
'13'
'12'
'4'
'72'
'130'
['13', '12', '4', '72', '130']
Error in Discourse Facet
<S sid ="14" ssid = "10">We used these three parsers to explore parser combination techniques.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="126" ssid = "55">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S><S sid ="140" ssid = "2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'139'", "'85'", "'126'", "'140'"]
'14'
'139'
'85'
'126'
'140'
['14', '139', '85', '126', '140']
Error in Discourse Facet
<S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="88" ssid = "17">For example  one parser could be more accurate at predicting noun phrases than the other parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'55'", "'27'", "'85'", "'88'"]
'15'
'55'
'27'
'85'
'88'
['15', '55', '27', '85', '88']
Error in Discourse Facet
<S sid ="16" ssid = "2">We call this approach parse hybridization.</S><S sid ="58" ssid = "44">We call this approach parser switching.</S><S sid ="23" ssid = "9">We call this technique constituent voting.</S><S sid ="96" ssid = "25">We call such a constituent an isolated constituent.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'58'", "'23'", "'96'", "'139'"]
'16'
'58'
'23'
'96'
'139'
['16', '58', '23', '96', '139']
Error in Discourse Facet
<S sid ="17" ssid = "3">The substructures that are unanimously hypothesized by the parsers should be preserved after combination  and the combination technique should not foolishly create substructures for which there is no supporting evidence.</S><S sid ="49" ssid = "35">In general  the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.</S><S sid ="117" ssid = "46">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S><S sid ="125" ssid = "54">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S><S sid ="95" ssid = "24">One side of the decision making process is when we choose to believe a constituent should be in the parse  even though only one parser suggests it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'49'", "'117'", "'125'", "'95'"]
'17'
'49'
'117'
'125'
'95'
['17', '49', '117', '125', '95']
Error in Discourse Facet
['Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.']
['Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.', 'One side of the decision making process is when we choose to believe a constituent should be in the parse  even though only one parser suggests it.', u'The constituent voting and na\xc3\xafve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.', 'The substructures that are unanimously hypothesized by the parsers should be preserved after combination  and the combination technique should not foolishly create substructures for which there is no supporting evidence.', 'In general  the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.00063', '(95%-conf.int.', '0.00063', '-', '0.00063)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00118', '(95%-conf.int.', '0.00118', '-', '0.00118)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:105', 'F:1']
['It is closer to the smaller value of precision and recall when there is a large skew in their values.', 'For our experiments we also report the mean of precision and recall, which we denote by (P + R)I2 and F-measure.', 'F-measure is the harmonic mean of precision and recall, 2PR/(P + R).']
['We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.', 'For example  one parser could be more accurate at predicting noun phrases than the other parsers.', u'Another technique for parse hybridization is to use a na\xc3\xafve Bayes classifier to determine which constituents to include in the parse.', 'We are interested in combining the substructures of the input parses to produce a better parse.', 'We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:171', 'F:0']
['We have presented two general approaches to studying parser combination: parser switching and parse hybridization.']
['Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).', 'These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).', 'The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).', 'The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.', 'The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).']
['system', 'ROUGE-S*', 'Average_R:', '0.00154', '(95%-conf.int.', '0.00154', '-', '0.00154)']
['system', 'ROUGE-S*', 'Average_P:', '0.11111', '(95%-conf.int.', '0.11111', '-', '0.11111)']
['system', 'ROUGE-S*', 'Average_F:', '0.00304', '(95%-conf.int.', '0.00304', '-', '0.00304)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3240', 'P:45', 'F:5']
['In general, the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.']
['We have presented two general approaches to studying parser combination: parser switching and parse hybridization.', 'We call such a constituent an isolated constituent.', 'We call this technique constituent voting.', 'We call this approach parser switching.', 'We call this approach parse hybridization.']
['system', 'ROUGE-S*', 'Average_R:', '0.01846', '(95%-conf.int.', '0.01846', '-', '0.01846)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.03069', '(95%-conf.int.', '0.03069', '-', '0.03069)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:325', 'P:66', 'F:6']
['The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.']
['Both parametric and non-parametric models are explored.', 'Two general approaches are presented and two combination techniques are described for each approach.', 'Once again we present both a non-parametric and a parametric technique for this task.', 'We have presented two general approaches to studying parser combination: parser switching and parse hybridization.', 'We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.00178', '(95%-conf.int.', '0.00178', '-', '0.00178)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00300', '(95%-conf.int.', '0.00300', '-', '0.00300)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:105', 'F:1']
['Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.']
[u'This is equivalent to the assumption used in probability estimation for na\xefve Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.', 'The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.', 'IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.', 'Their theoretical finding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent  accurate classifiers.', u'Under certain conditions the constituent voting and na\xefve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.']
['system', 'ROUGE-S*', 'Average_R:', '0.04749', '(95%-conf.int.', '0.04749', '-', '0.04749)']
['system', 'ROUGE-S*', 'Average_P:', '0.87500', '(95%-conf.int.', '0.87500', '-', '0.87500)']
['system', 'ROUGE-S*', 'Average_F:', '0.09009', '(95%-conf.int.', '0.09009', '-', '0.09009)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:120', 'F:105']
['The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.']
['The theory has also been validated empirically.', 'The results in Table 2 were achieved on the development set.', 'The estimation of the probabilities in the model is carried out as shown in Equation 4.', 'Table 3 contains the results for evaluating our systems on the test set (section 22).', 'We performed three experiments to evaluate our techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:406', 'P:300', 'F:0']
['The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.']
['Surprisingly  the non-parametric switching technique also exhibited robust behaviour in this situation.', 'Both parametric and non-parametric models are explored.', 'Two general approaches are presented and two combination techniques are described for each approach.', 'Once again we present both a non-parametric and a parametric technique for this task.', 'The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:105', 'F:0']
['From this we see that a finer-grained model for parser combination, at least for the features we have examined, will not give us any additional power.']
['Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).', 'Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).', 'These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).', 'The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).', 'The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).']
['system', 'ROUGE-S*', 'Average_R:', '0.00032', '(95%-conf.int.', '0.00032', '-', '0.00032)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00062', '(95%-conf.int.', '0.00062', '-', '0.00062)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:45', 'F:1']
0.124253331953 0.00780222213553 0.0142911109523





input/ref/Task1/P87-1015_swastika.csv
input/res/Task1/P87-1015.csv
parsing: input/ref/Task1/P87-1015_swastika.csv
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="149" ssid="34">We can obtain a letter equivalent CFL defined by a CFG in which the for each rule as above, we have the production A &#8212;* A1 Anup where tk (up) = cp.</S>
original cit marker offset is 0
new cit marker offset is 0



['149']
149
['149']
parsed_discourse_facet ['method_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
    <S sid="2" ssid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['2']
2
['2']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="222" ssid="28">However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



['222']
222
['222']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['???']
???
['???']
parsed_discourse_facet ['aim_citation']
<S sid="207" ssid="13">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['207']
207
['207']
parsed_discourse_facet ['result_citation']
<S sid="207" ssid="13">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['207']
207
['207']
parsed_discourse_facet ['result_citation']
<S sid="19" ssid="4">It can be easily shown from Thatcher's result that the path set of every local set is a regular set.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/P87-1015.csv
<S sid ="1" ssid = "1">We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.</S><S sid ="9" ssid = "7">We examine both the complexity of the paths of trees in the tree sets  and the kinds of dependencies that the formalisms can impose between paths.</S><S sid ="232" ssid = "38">In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="110" ssid = "16">The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'232'", "'195'", "'110'"]
'1'
'9'
'232'
'195'
'110'
['1', '9', '232', '195', '110']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">In considering the relationship between formalisms  we show that it is useful to abstract away from the details of the formalism  and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation  we describe a class of formalisms which we call Linear Context- Free Rewriting Systems  and show they are recognizable in polynomial time and generate only semilinear languages.</S><S sid ="203" ssid = "9">In order to observe the similarity between these constrained systems  it is crucial to abstract away from the details of the structures and operations used by the system.</S><S sid ="232" ssid = "38">In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.</S><S sid ="11" ssid = "9">By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'203'", "'232'", "'11'", "'116'"]
'2'
'203'
'232'
'11'
'116'
['2', '203', '232', '11', '116']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S><S sid ="48" ssid = "33">There has been recent interest in the application of Indexed Grammars (IG's) to natural languages.</S><S sid ="23" ssid = "8">He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.</S><S sid ="56" ssid = "41">Gazdar (1985) argues that sharing of stacks can be used to give analyses for coordination.</S><S sid ="65" ssid = "50">While the generative power of CG's is greater that of CFG's  it appears to be highly constrained.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'48'", "'23'", "'56'", "'65'"]
'3'
'48'
'23'
'56'
'65'
['3', '48', '23', '56', '65']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "2">Little attention  however  has been paid to the structural descriptions that these formalisms can assign to strings  i.e. their strong generative capacity.</S><S sid ="3" ssid = "1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S><S sid ="66" ssid = "51">Hence  their relationship to formalisms such as HG's and TAG's is of interest.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="48" ssid = "33">There has been recent interest in the application of Indexed Grammars (IG's) to natural languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'3'", "'66'", "'195'", "'48'"]
'4'
'3'
'66'
'195'
'48'
['4', '3', '66', '195', '48']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "3">This aspect of the formalism is both linguistically and computationally important.</S><S sid ="198" ssid = "4">This property reflects an important aspect of the underlying linguistic theory associated with the formalism.</S><S sid ="10" ssid = "8">These two properties of the tree sets are not only linguistically relevant  but also have computational importance.</S><S sid ="123" ssid = "8">A second restriction on the formalisms is that choices during the derivation are independent of the context in the derivation.</S><S sid ="3" ssid = "1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'198'", "'10'", "'123'", "'3'"]
'5'
'198'
'10'
'123'
'3'
['5', '198', '10', '123', '3']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">For example  Gazdar (1985) discusses the applicability of Indexed Grammars (IG's) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar (LFG) and Government and Bindings grammars (GB).</S><S sid ="24" ssid = "9">Head Grammars (HG's)  introduced by Pollard (1984)  is a formalism that manipulates headed strings: i.e.  strings  one of whose symbols is distinguished as the head.</S><S sid ="35" ssid = "20">TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish  for cross serial dependencies in Dutch subordinate clauses  and for the nestings of paired English complementizers.</S><S sid ="64" ssid = "49">Steedman (1986) considers Categorial Grammars in which both the operations of function application and composition may be used  and in which function can specify whether they take their arguments from their right or left.</S><S sid ="22" ssid = "7">Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'24'", "'35'", "'64'", "'22'"]
'6'
'24'
'35'
'64'
'22'
['6', '24', '35', '64', '22']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "5">The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG's and IG's.</S><S sid ="50" ssid = "35">The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.</S><S sid ="16" ssid = "1">From Thatcher's (1973) work  it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S><S sid ="98" ssid = "4">Thatcher (1973) describes a tee pumping lemma for recognizable sets related to the string pumping lemma for regular sets.</S><S sid ="24" ssid = "9">Head Grammars (HG's)  introduced by Pollard (1984)  is a formalism that manipulates headed strings: i.e.  strings  one of whose symbols is distinguished as the head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'50'", "'16'", "'98'", "'24'"]
'7'
'50'
'16'
'98'
'24'
['7', '50', '16', '98', '24']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "6">We consider properties of the tree sets generated by CFG's  Tree Adjoining Grammars (TAG's)  Head Grammars (HG's)  Categorial Grammars (CG's)  and IG's.</S><S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="27" ssid = "12">Thus  the tree sets generated by HG's are similar to those of CFG's  with each node annotated by the operation (concatenation or wrapping) used to combine the headed strings derived by the daughters of Tree Adjoining Grammars  a tree rewriting formalism  was introduced by Joshi  Levy and Takahashi (1975) and Joshi (1983/85).</S><S sid ="94" ssid = "79">The semilinearity of Tree Adjoining Languages (TAL's)  MCTAL's  and Head Languages (HL's) can be proved using this property  with suitable restrictions on the composition operations.</S><S sid ="208" ssid = "14">This family represents an attempt to generalize the properties shared by CFG's  HG's  TAG's  and MCTAG's.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'151'", "'27'", "'94'", "'208'"]
'8'
'151'
'27'
'94'
'208'
['8', '151', '27', '94', '208']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "7">We examine both the complexity of the paths of trees in the tree sets  and the kinds of dependencies that the formalisms can impose between paths.</S><S sid ="1" ssid = "1">We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.</S><S sid ="8" ssid = "6">We consider properties of the tree sets generated by CFG's  Tree Adjoining Grammars (TAG's)  Head Grammars (HG's)  Categorial Grammars (CG's)  and IG's.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="110" ssid = "16">The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'1'", "'8'", "'195'", "'110'"]
'9'
'1'
'8'
'195'
'110'
['9', '1', '8', '195', '110']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "8">These two properties of the tree sets are not only linguistically relevant  but also have computational importance.</S><S sid ="5" ssid = "3">This aspect of the formalism is both linguistically and computationally important.</S><S sid ="198" ssid = "4">This property reflects an important aspect of the underlying linguistic theory associated with the formalism.</S><S sid ="68" ssid = "53">This kind of dependency arises from the use of the composition operation to compose two arbitrarily large categories.</S><S sid ="23" ssid = "8">He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'5'", "'198'", "'68'", "'23'"]
'10'
'5'
'198'
'68'
'23'
['10', '5', '198', '68', '23']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "9">By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.</S><S sid ="2" ssid = "2">In considering the relationship between formalisms  we show that it is useful to abstract away from the details of the formalism  and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation  we describe a class of formalisms which we call Linear Context- Free Rewriting Systems  and show they are recognizable in polynomial time and generate only semilinear languages.</S><S sid ="232" ssid = "38">In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.</S><S sid ="14" ssid = "12">This suggests that by generalizing the notion of context-freeness in CFG's  we can define a class of grammatical formalisms that manipulate more complex structures.</S><S sid ="203" ssid = "9">In order to observe the similarity between these constrained systems  it is crucial to abstract away from the details of the structures and operations used by the system.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'2'", "'232'", "'14'", "'203'"]
'11'
'2'
'232'
'14'
'203'
['11', '2', '232', '14', '203']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "10">We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.</S><S sid ="137" ssid = "22">In considering recognition of LCFRS's  we make further assumption concerning the contribution of each structure to the input string  and how the composition operations combine structures in this respect.</S><S sid ="200" ssid = "6">The importance of this property becomes clear in contrasting theories underlying GPSG (Gazdar  Klein  Pulluna  and Sag  1985)  and GB (as described by Berwick  1984) with those underlying LFG and FUG.</S><S sid ="22" ssid = "7">Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.</S><S sid ="60" ssid = "45">Gazdar (1985) considers a restriction of IG's in which no more than one nonterminal on the right-hand-side of a production can inherit the stack from the left-hand-side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'137'", "'200'", "'22'", "'60'"]
'12'
'137'
'200'
'22'
'60'
['12', '137', '200', '22', '60']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "11">It is striking that from this point of view many formalisms can be grouped together as having identically structured derivation tree sets.</S><S sid ="15" ssid = "13">In this paper  we outline how such family of formalisms can be defined  and show that like CFG's  each member possesses a number of desirable linguistic and computational properties: in particular  the constant growth property and polynomial recognizability.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S><S sid ="202" ssid = "8">As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.</S><S sid ="78" ssid = "63">Each member of a set of trees can be adjoined into distinct nodes of trees in a single elementary tree set  i.e  derivations always involve the adjunction of a derived auxiliary tree set into an elementary tree set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'15'", "'116'", "'202'", "'78'"]
'13'
'15'
'116'
'202'
'78'
['13', '15', '116', '202', '78']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "12">This suggests that by generalizing the notion of context-freeness in CFG's  we can define a class of grammatical formalisms that manipulate more complex structures.</S><S sid ="119" ssid = "4">In defining LCFRS's  we hope to generalize the definition of CFG's to formalisms manipulating any structure  e.g. strings  trees  or graphs.</S><S sid ="53" ssid = "38">We focus on this difference between the tree sets of CFG's and IG's  and formalize the notion of dependence between paths in a tree set in Section 3.</S><S sid ="142" ssid = "27">Although this property is not structural  it depends on the structural property that sentences can be built from a finite set of clauses of bounded structure as noted by Joshi (1983/85).</S><S sid ="159" ssid = "44">Similarly  for all the LCFRS's  discussed in Section 2  we can define the relationship between a structure and the sequence of substrings it spans  and the effect of the composition operations on sequences of substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'119'", "'53'", "'142'", "'159'"]
'14'
'119'
'53'
'142'
'159'
['14', '119', '53', '142', '159']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "13">In this paper  we outline how such family of formalisms can be defined  and show that like CFG's  each member possesses a number of desirable linguistic and computational properties: in particular  the constant growth property and polynomial recognizability.</S><S sid ="202" ssid = "8">As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.</S><S sid ="117" ssid = "2">Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S><S sid ="13" ssid = "11">It is striking that from this point of view many formalisms can be grouped together as having identically structured derivation tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'202'", "'117'", "'116'", "'13'"]
'15'
'202'
'117'
'116'
'13'
['15', '202', '117', '116', '13']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "1">From Thatcher's (1973) work  it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S><S sid ="50" ssid = "35">The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.</S><S sid ="19" ssid = "4">It can be easily shown from Thatcher's result that the path set of every local set is a regular set.</S><S sid ="229" ssid = "35">In addition  the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars  and the hierarchy defined by Weir).</S><S sid ="166" ssid = "51">With the additional assumptions  inspired by Rounds (1985)  we can show that members of this class can be recognized in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'50'", "'19'", "'229'", "'166'"]
'16'
'50'
'19'
'229'
'166'
['16', '50', '19', '229', '166']
parsed_discourse_facet ['method_citation']
['In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.']
["Head Grammars (HG's)  introduced by Pollard (1984)  is a formalism that manipulates headed strings: i.e.  strings  one of whose symbols is distinguished as the head.", 'Thatcher (1973) describes a tee pumping lemma for recognizable sets related to the string pumping lemma for regular sets.', "The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.", "The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG's and IG's.", "From Thatcher's (1973) work  it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'."]
['system', 'ROUGE-S*', 'Average_R:', '0.04225', '(95%-conf.int.', '0.04225', '-', '0.04225)']
['system', 'ROUGE-S*', 'Average_P:', '0.11099', '(95%-conf.int.', '0.11099', '-', '0.11099)']
['system', 'ROUGE-S*', 'Average_F:', '0.06121', '(95%-conf.int.', '0.06121', '-', '0.06121)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:946', 'F:105']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
['We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.', 'We examine both the complexity of the paths of trees in the tree sets  and the kinds of dependencies that the formalisms can impose between paths.', 'We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.', 'The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .', 'In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.']
['system', 'ROUGE-S*', 'Average_R:', '0.00161', '(95%-conf.int.', '0.00161', '-', '0.00161)']
['system', 'ROUGE-S*', 'Average_P:', '0.03333', '(95%-conf.int.', '0.03333', '-', '0.03333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00307', '(95%-conf.int.', '0.00307', '-', '0.00307)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:120', 'F:4']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
["By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.", 'In order to observe the similarity between these constrained systems  it is crucial to abstract away from the details of the structures and operations used by the system.', 'In considering the relationship between formalisms  we show that it is useful to abstract away from the details of the formalism  and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation  we describe a class of formalisms which we call Linear Context- Free Rewriting Systems  and show they are recognizable in polynomial time and generate only semilinear languages.', 'From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).', 'In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.']
['system', 'ROUGE-S*', 'Average_R:', '0.00449', '(95%-conf.int.', '0.00449', '-', '0.00449)']
['system', 'ROUGE-S*', 'Average_P:', '0.25833', '(95%-conf.int.', '0.25833', '-', '0.25833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00883', '(95%-conf.int.', '0.00883', '-', '0.00883)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6903', 'P:120', 'F:31']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
["It can be easily shown from Thatcher's result that the path set of every local set is a regular set.", 'With the additional assumptions  inspired by Rounds (1985)  we can show that members of this class can be recognized in polynomial time.', "The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.", "In addition  the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars  and the hierarchy defined by Weir).", "From Thatcher's (1973) work  it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'."]
['system', 'ROUGE-S*', 'Average_R:', '0.00348', '(95%-conf.int.', '0.00348', '-', '0.00348)']
['system', 'ROUGE-S*', 'Average_P:', '0.09167', '(95%-conf.int.', '0.09167', '-', '0.09167)']
['system', 'ROUGE-S*', 'Average_F:', '0.00671', '(95%-conf.int.', '0.00671', '-', '0.00671)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:120', 'F:11']
["In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs."]
['From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).', 'Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.', "As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.", "In this paper  we outline how such family of formalisms can be defined  and show that like CFG's  each member possesses a number of desirable linguistic and computational properties: in particular  the constant growth property and polynomial recognizability.", 'It is striking that from this point of view many formalisms can be grouped together as having identically structured derivation tree sets.']
['system', 'ROUGE-S*', 'Average_R:', '0.00426', '(95%-conf.int.', '0.00426', '-', '0.00426)']
['system', 'ROUGE-S*', 'Average_P:', '0.15152', '(95%-conf.int.', '0.15152', '-', '0.15152)']
['system', 'ROUGE-S*', 'Average_F:', '0.00829', '(95%-conf.int.', '0.00829', '-', '0.00829)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:66', 'F:10']
["It can be easily shown from Thatcher's result that the path set of every local set is a regular set."]
["Similarly  for all the LCFRS's  discussed in Section 2  we can define the relationship between a structure and the sequence of substrings it spans  and the effect of the composition operations on sequences of substrings.", 'Although this property is not structural  it depends on the structural property that sentences can be built from a finite set of clauses of bounded structure as noted by Joshi (1983/85).', "In defining LCFRS's  we hope to generalize the definition of CFG's to formalisms manipulating any structure  e.g. strings  trees  or graphs.", "This suggests that by generalizing the notion of context-freeness in CFG's  we can define a class of grammatical formalisms that manipulate more complex structures.", "We focus on this difference between the tree sets of CFG's and IG's  and formalize the notion of dependence between paths in a tree set in Section 3."]
['system', 'ROUGE-S*', 'Average_R:', '0.00161', '(95%-conf.int.', '0.00161', '-', '0.00161)']
['system', 'ROUGE-S*', 'Average_P:', '0.08889', '(95%-conf.int.', '0.08889', '-', '0.08889)']
['system', 'ROUGE-S*', 'Average_F:', '0.00316', '(95%-conf.int.', '0.00316', '-', '0.00316)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:45', 'F:4']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
["Head Grammars (HG's)  introduced by Pollard (1984)  is a formalism that manipulates headed strings: i.e.  strings  one of whose symbols is distinguished as the head.", 'Steedman (1986) considers Categorial Grammars in which both the operations of function application and composition may be used  and in which function can specify whether they take their arguments from their right or left.', 'Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.', "TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish  for cross serial dependencies in Dutch subordinate clauses  and for the nestings of paired English complementizers.", "For example  Gazdar (1985) discusses the applicability of Indexed Grammars (IG's) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar (LFG) and Government and Bindings grammars (GB)."]
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:120', 'F:0']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
["By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.", 'In order to observe the similarity between these constrained systems  it is crucial to abstract away from the details of the structures and operations used by the system.', 'In considering the relationship between formalisms  we show that it is useful to abstract away from the details of the formalism  and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation  we describe a class of formalisms which we call Linear Context- Free Rewriting Systems  and show they are recognizable in polynomial time and generate only semilinear languages.', "This suggests that by generalizing the notion of context-freeness in CFG's  we can define a class of grammatical formalisms that manipulate more complex structures.", 'In considering this aspect of a formalism  we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism  and the properties of semilinearity and polynomial recognizability.']
['system', 'ROUGE-S*', 'Average_R:', '0.00565', '(95%-conf.int.', '0.00565', '-', '0.00565)']
['system', 'ROUGE-S*', 'Average_P:', '0.32500', '(95%-conf.int.', '0.32500', '-', '0.32500)']
['system', 'ROUGE-S*', 'Average_F:', '0.01111', '(95%-conf.int.', '0.01111', '-', '0.01111)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6903', 'P:120', 'F:39']
['We can obtain a letter equivalent CFL defined by a CFG in which the for each rule as above, we have the production A &#8212;* A1 Anup where tk (up) = cp.']
['Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.', 'Gazdar (1985) argues that sharing of stacks can be used to give analyses for coordination.', "While the generative power of CG's is greater that of CFG's  it appears to be highly constrained.", "There has been recent interest in the application of Indexed Grammars (IG's) to natural languages.", 'He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:78', 'F:0']
['However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.']
['We examine both the complexity of the paths of trees in the tree sets  and the kinds of dependencies that the formalisms can impose between paths.', 'We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.', 'We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.', 'The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .', "We consider properties of the tree sets generated by CFG's  Tree Adjoining Grammars (TAG's)  Head Grammars (HG's)  Categorial Grammars (CG's)  and IG's."]
['system', 'ROUGE-S*', 'Average_R:', '0.00152', '(95%-conf.int.', '0.00152', '-', '0.00152)']
['system', 'ROUGE-S*', 'Average_P:', '0.02105', '(95%-conf.int.', '0.02105', '-', '0.02105)']
['system', 'ROUGE-S*', 'Average_F:', '0.00284', '(95%-conf.int.', '0.00284', '-', '0.00284)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:190', 'F:4']
0.108077998919 0.00648699993513 0.0105219998948





input/ref/Task1/P04-1036_swastika.csv
input/res/Task1/P04-1036.csv
parsing: input/ref/Task1/P04-1036_swastika.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="24">We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.</S>
original cit marker offset is 0
new cit marker offset is 0



['68']
68
['68']
parsed_discourse_facet ['result_citation']
<S sid="83" ssid="12">The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.</S>
original cit marker offset is 0
new cit marker offset is 0



['83']
83
['83']
parsed_discourse_facet ['result_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
<S sid="101" ssid="30">Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.</S>
original cit marker offset is 0
new cit marker offset is 0



['101']
101
['101']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="83" ssid="12">The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.</S>
original cit marker offset is 0
new cit marker offset is 0



['83']
83
['83']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="46" ssid="2">This provides the nearest neighbours to each target word, along with the distributional similarity score between the target word and its neighbour.</S>
original cit marker offset is 0
new cit marker offset is 0



['46']
46
['46']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents, rather than a lexical sample task, where the target words are manually determined and the results will depend on the skew of the words in the sample.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
<S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['method_citation']
<S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="48" ssid = "4">To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.</S><S sid ="105" ssid = "3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents  rather than a lexical sample task  where the target words are manually determined and the results will depend on the skew of the words in the sample.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'153'", "'154'", "'48'", "'105'"]
'1'
'153'
'154'
'48'
'105'
['1', '153', '154', '48', '105']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">The problem with using the predominant  or first sense heuristic  aside from the fact that it does not take surrounding context into account  is that it assumes some quantity of handtagged data.</S><S sid ="17" ssid = "10">There are words where the first sense in WordNet is counter-intuitive  because of the size of the corpus  and because where the frequency data does not indicate a first sense  the ordering is arbitrary.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'17'", "'91'", "'154'", "'123'"]
'2'
'17'
'91'
'154'
'123'
['2', '17', '91', '154', '123']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.</S><S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="37" ssid = "30">From inspection  one can see that the ordered neighbours of such a thesaurus relate to the different senses of the target word.</S><S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'15'", "'37'", "'1'", "'170'"]
'3'
'15'
'37'
'1'
'170'
['3', '15', '37', '1', '170']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S sid ="179" ssid = "2">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S><S sid ="171" ssid = "19">In contrast  we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'179'", "'178'", "'45'", "'171'"]
'4'
'179'
'178'
'45'
'171'
['4', '179', '178', '45', '171']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="84" ssid = "13">The random baseline for choosing the predominant sense over all these words ( ) is 32%.</S><S sid ="88" ssid = "17">The first sense in SemCor provides an upperbound for this task of 67%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'100'", "'116'", "'84'", "'88'"]
'5'
'100'
'116'
'84'
'88'
['5', '100', '116', '84', '88']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="121" ssid = "19">For these one would need to obtain more sense-tagged text in order to use this heuristic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'126'", "'178'", "'13'", "'121'"]
'6'
'126'
'178'
'13'
'121'
['6', '126', '178', '13', '121']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S><S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S><S sid ="158" ssid = "6">We have evaluated our method using publically available resources  both for balanced and domain specific text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'184'", "'178'", "'126'", "'158'"]
'7'
'184'
'178'
'126'
'158'
['7', '184', '178', '126', '158']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="172" ssid = "20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet  for example those that use it for lexical acquisition or WSD.</S><S sid ="115" ssid = "13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor  which is very encouraging given that our method only uses raw text  with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'154'", "'14'", "'172'", "'115'"]
'8'
'154'
'14'
'172'
'115'
['8', '154', '14', '172', '115']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "2">This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).</S><S sid ="102" ssid = "31">We test this below on the SENSEVAL-2 English all-words data.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'102'", "'116'", "'100'", "'5'"]
'9'
'102'
'116'
'100'
'5'
['9', '102', '116', '100', '5']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="180" ssid = "3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S><S sid ="109" ssid = "7">We generated a thesaurus entry for all polysemous nouns in WordNet as described in section 2.1 above.</S><S sid ="112" ssid = "10">We compare results using the first sense listed in SemCor  and the first sense according to the SENSEVAL-2 English all-words test data itself.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'180'", "'142'", "'109'", "'112'"]
'10'
'180'
'142'
'109'
'112'
['10', '180', '142', '109', '112']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "4">Senses that have not occurred in SemCor are ordered arbitrarily and after those senses of the word that have occurred.</S><S sid ="120" ssid = "18">There were a similar number of words that were not covered by a predominant sense in SemCor.</S><S sid ="42" ssid = "35">The neighbours for a word in a thesaurus are words themselves  rather than senses.</S><S sid ="189" ssid = "12">Additionally  we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity  and what can be done to combat this problem using relation specific thesauruses.</S><S sid ="117" ssid = "15">The items that were not covered by our method were those with insufficient grammatical relations for the tuples employed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'120'", "'42'", "'189'", "'117'"]
'11'
'120'
'42'
'189'
'117'
['11', '120', '42', '189', '117']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "5">The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor  from those that do not (without HTD).</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'123'", "'153'", "'165'", "'178'"]
'12'
'123'
'153'
'165'
'178'
['12', '123', '153', '165', '178']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="84" ssid = "13">The random baseline for choosing the predominant sense over all these words ( ) is 32%.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'116'", "'147'", "'84'", "'126'"]
'13'
'116'
'147'
'84'
'126'
['13', '116', '147', '84', '126']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="182" ssid = "5">In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.</S><S sid ="21" ssid = "14">We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off (Wilks and Stevenson  1998; Hoste et al.  2001) and for systems that use it in lexical acquisition (McCarthy  1997; Merlo and Leybold  2001; Korhonen  2002) because of the limited size of hand-tagged resources.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'8'", "'154'", "'182'", "'21'"]
'14'
'8'
'154'
'182'
'21'
['14', '8', '154', '182', '21']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="3" ssid = "3">Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'154'", "'3'", "'8'", "'170'"]
'15'
'154'
'3'
'8'
'170'
['15', '154', '3', '8', '170']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">SemCor comprises a relatively small sample of 250 000 words.</S><S sid ="131" ssid = "8">The SPORTS corpus consists of 35317 documents (about 9.1 million words).</S><S sid ="140" ssid = "17">The resulting set consisted of 38 words.</S><S sid ="132" ssid = "9">The FINANCE corpus consists of 117734 documents (about 32.5 million words).</S><S sid ="76" ssid = "5">The jcn measure uses corpus data for the calculation of IC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'131'", "'140'", "'132'", "'76'"]
'16'
'131'
'140'
'132'
'76'
['16', '131', '140', '132', '76']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "10">There are words where the first sense in WordNet is counter-intuitive  because of the size of the corpus  and because where the frequency data does not indicate a first sense  the ordering is arbitrary.</S><S sid ="182" ssid = "5">In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.</S><S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="99" ssid = "28">Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'182'", "'13'", "'99'", "'170'"]
'17'
'182'
'13'
'99'
'170'
['17', '182', '13', '99', '170']
parsed_discourse_facet ['method_citation']
['We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.']
['word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.', 'From inspection  one can see that the ordered neighbours of such a thesaurus relate to the different senses of the target word.', 'Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.', 'Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.', 'If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.']
['system', 'ROUGE-S*', 'Average_R:', '0.00253', '(95%-conf.int.', '0.00253', '-', '0.00253)']
['system', 'ROUGE-S*', 'Average_P:', '0.07619', '(95%-conf.int.', '0.07619', '-', '0.07619)']
['system', 'ROUGE-S*', 'Average_F:', '0.00490', '(95%-conf.int.', '0.00490', '-', '0.00490)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:105', 'F:8']
['The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.']
['The FINANCE corpus consists of 117734 documents (about 32.5 million words).', 'The SPORTS corpus consists of 35317 documents (about 9.1 million words).', 'The jcn measure uses corpus data for the calculation of IC.', 'The resulting set consisted of 38 words.', 'SemCor comprises a relatively small sample of 250 000 words.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:55', 'F:0']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor  which is very encouraging given that our method only uses raw text  with no manual labelling.', 'The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.', 'We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet  for example those that use it for lexical acquisition or WSD.', 'In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.', 'Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).']
['system', 'ROUGE-S*', 'Average_R:', '0.03490', '(95%-conf.int.', '0.03490', '-', '0.03490)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.06745', '(95%-conf.int.', '0.06745', '-', '0.06745)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:66', 'F:66']
['We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents, rather than a lexical sample task, where the target words are manually determined and the results will depend on the skew of the words in the sample.']
['In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.', 'The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.', 'Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).', 'We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off (Wilks and Stevenson  1998; Hoste et al.  2001) and for systems that use it in lexical acquisition (McCarthy  1997; Merlo and Leybold  2001; Korhonen  2002) because of the limited size of hand-tagged resources.', 'In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.']
['system', 'ROUGE-S*', 'Average_R:', '0.00200', '(95%-conf.int.', '0.00200', '-', '0.00200)']
['system', 'ROUGE-S*', 'Average_P:', '0.02857', '(95%-conf.int.', '0.02857', '-', '0.02857)']
['system', 'ROUGE-S*', 'Average_F:', '0.00373', '(95%-conf.int.', '0.00373', '-', '0.00373)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:210', 'F:6']
['The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.', 'Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.', 'Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.', 'In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.', 'If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.']
['system', 'ROUGE-S*', 'Average_R:', '0.00602', '(95%-conf.int.', '0.00602', '-', '0.00602)']
['system', 'ROUGE-S*', 'Average_P:', '0.38182', '(95%-conf.int.', '0.38182', '-', '0.38182)']
['system', 'ROUGE-S*', 'Average_F:', '0.01186', '(95%-conf.int.', '0.01186', '-', '0.01186)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:55', 'F:21']
['Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.']
['For these one would need to obtain more sense-tagged text in order to use this heuristic.', 'The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.', 'We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.', 'We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.', 'This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.']
['system', 'ROUGE-S*', 'Average_R:', '0.00423', '(95%-conf.int.', '0.00423', '-', '0.00423)']
['system', 'ROUGE-S*', 'Average_P:', '0.07273', '(95%-conf.int.', '0.07273', '-', '0.07273)']
['system', 'ROUGE-S*', 'Average_F:', '0.00799', '(95%-conf.int.', '0.00799', '-', '0.00799)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:55', 'F:4']
['The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.']
['This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).', 'We test this below on the SENSEVAL-2 English all-words data.', 'In the English all-words SENSEVAL-2  25% of the noun data was monosemous.', 'The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.', 'The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.']
['system', 'ROUGE-S*', 'Average_R:', '0.00313', '(95%-conf.int.', '0.00313', '-', '0.00313)']
['system', 'ROUGE-S*', 'Average_P:', '0.02924', '(95%-conf.int.', '0.02924', '-', '0.02924)']
['system', 'ROUGE-S*', 'Average_F:', '0.00566', '(95%-conf.int.', '0.00566', '-', '0.00566)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:171', 'F:5']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.', 'Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.', 'The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.', 'There are words where the first sense in WordNet is counter-intuitive  because of the size of the corpus  and because where the frequency data does not indicate a first sense  the ordering is arbitrary.', 'If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.']
['system', 'ROUGE-S*', 'Average_R:', '0.00149', '(95%-conf.int.', '0.00149', '-', '0.00149)']
['system', 'ROUGE-S*', 'Average_P:', '0.04545', '(95%-conf.int.', '0.04545', '-', '0.04545)']
['system', 'ROUGE-S*', 'Average_F:', '0.00288', '(95%-conf.int.', '0.00288', '-', '0.00288)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:66', 'F:3']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['The word share is among the words whose predominant sense remained the same for all three corpora.', 'The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.', 'The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.', 'We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.', 'The random baseline for choosing the predominant sense over all these words ( ) is 32%.']
['system', 'ROUGE-S*', 'Average_R:', '0.00317', '(95%-conf.int.', '0.00317', '-', '0.00317)']
['system', 'ROUGE-S*', 'Average_P:', '0.04545', '(95%-conf.int.', '0.04545', '-', '0.04545)']
['system', 'ROUGE-S*', 'Average_F:', '0.00593', '(95%-conf.int.', '0.00593', '-', '0.00593)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:66', 'F:3']
['We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.']
['Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.', 'This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.', 'The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor  from those that do not (without HTD).', 'Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.', 'We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.']
['system', 'ROUGE-S*', 'Average_R:', '0.00148', '(95%-conf.int.', '0.00148', '-', '0.00148)']
['system', 'ROUGE-S*', 'Average_P:', '0.14286', '(95%-conf.int.', '0.14286', '-', '0.14286)']
['system', 'ROUGE-S*', 'Average_F:', '0.00293', '(95%-conf.int.', '0.00293', '-', '0.00293)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2701', 'P:28', 'F:4']
0.182230998178 0.00589499994105 0.0113329998867





input/ref/Task1/W06-2932_vardha.csv
input/res/Task1/W06-2932.csv
parsing: input/ref/Task1/W06-2932_vardha.csv
    <S sid="19" ssid="1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="5">However, in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="76" ssid="14">For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'"]
'76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="54" ssid="2">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'"]
'54'
['54']
parsed_discourse_facet ['method_citation']
    <S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="58" ssid="6">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'"]
'58'
['58']
parsed_discourse_facet ['method_citation']
    <S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'"]
'22'
['22']
parsed_discourse_facet ['method_citation']
    <S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
    <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'"]
'64'
['64']
parsed_discourse_facet ['method_citation']
    <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'"]
'43'
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S sid ="1" ssid = "1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S><S sid ="9" ssid = "5">Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S><S sid ="23" ssid = "5">That system uses MIRA  an online large-margin learning algorithm  to compute model parameters.</S><S sid ="27" ssid = "9">We augmented this model to incorporate morphological features derived from each token.</S>
original cit marker offset is nan
new cit marker offset is 0



["'1'", "'9'", "'107'", "'23'", "'27'"]
'1'
'9'
'107'
'23'
'27'
['1', '9', '107', '23', '27']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="14" ssid = "10">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S><S sid ="58" ssid = "6">These results show that the discriminative spanning tree parsing framework (McDonald et al.  2005b; McDonald and Pereira  2006) is easily adapted across all these languages.</S><S sid ="25" ssid = "7">For instance  the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'20'", "'14'", "'58'", "'25'"]
'2'
'20'
'14'
'58'
'25'
['2', '20', '14', '58', '25']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S><S sid ="19" ssid = "1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S><S sid ="109" ssid = "6">The current system simply includes all morphological bi-gram features.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="51" ssid = "20">Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'19'", "'109'", "'10'", "'51'"]
'3'
'19'
'109'
'10'
'51'
['3', '19', '109', '10', '51']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.</S><S sid ="53" ssid = "1">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).</S><S sid ="44" ssid = "13">We use the MIRA online learner to set the weights (Crammer and Singer  2003; McDonald et al.  2005a) since we found it trained quickly and provide good performance.</S><S sid ="13" ssid = "9">We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajiˇc et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B¨ohmov´a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dˇzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).</S><S sid ="56" ssid = "4">Results on the test set are given in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'53'", "'44'", "'13'", "'56'"]
'4'
'53'
'44'
'13'
'56'
['4', '53', '44', '13', '56']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="24" ssid = "6">Its power lies in the ability to define a rich set of features over parsing decisions  as well as surface level features relative to these decisions.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'10'", "'88'", "'24'", "'107'"]
'5'
'10'
'88'
'24'
'107'
['5', '10', '88', '24', '107']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.</S><S sid ="14" ssid = "10">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'14'", "'7'", "'36'", "'8'"]
'6'
'14'
'7'
'36'
'8'
['6', '14', '7', '36', '8']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="110" ssid = "7">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="26" ssid = "8">These features are highly important to overall accuracy since they eliminate unlikely scenarios such as a preposition modifying a noun not directly to its left  or a noun modifying a verb with another verb occurring between them.</S><S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'110'", "'11'", "'26'", "'8'"]
'7'
'110'
'11'
'26'
'8'
['7', '110', '11', '26', '8']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="30" ssid = "12">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="6" ssid = "2">With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'7'", "'30'", "'11'", "'6'"]
'8'
'7'
'30'
'11'
'6'
['8', '7', '30', '11', '6']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="5" ssid = "1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S><S sid ="25" ssid = "7">For instance  the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'20'", "'88'", "'5'", "'25'"]
'9'
'20'
'88'
'5'
'25'
['9', '20', '88', '5', '25']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="5" ssid = "1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S><S sid ="24" ssid = "6">Its power lies in the ability to define a rich set of features over parsing decisions  as well as surface level features relative to these decisions.</S><S sid ="34" ssid = "3">However  the parser is fundamentally limited by the scope of local factorizations that make inference tractable.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'5'", "'24'", "'34'", "'107'"]
'10'
'5'
'24'
'34'
'107'
['10', '5', '24', '34', '107']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="110" ssid = "7">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S><S sid ="45" ssid = "14">Furthermore  it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira  2006).</S><S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'7'", "'110'", "'45'", "'8'"]
'11'
'7'
'110'
'45'
'8'
['11', '7', '110', '45', '8']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S sid ="95" ssid = "17">Note the difference in error between the unlabeled parser and the edge labeler: the former makes mistakes on edges into prepositions  conjunctions and verbs  and the latter makes mistakes on edges into nouns (subject/objects).</S><S sid ="110" ssid = "7">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S><S sid ="69" ssid = "7">However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).</S><S sid ="106" ssid = "3">First  we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'95'", "'110'", "'69'", "'106'"]
'12'
'95'
'110'
'69'
'106'
['12', '95', '110', '69', '106']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajiˇc et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B¨ohmov´a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dˇzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).</S><S sid ="54" ssid = "2">Based on performance from a held-out section of the training data  we used non-projective parsing algorithms for Czech  Danish  Dutch  German  Japanese  Portuguese and Slovene  and projective parsing algorithms for Arabic  Bulgarian  Chinese  Spanish  Swedish and Turkish.</S><S sid ="4" ssid = "4">We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.</S><S sid ="53" ssid = "1">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).</S><S sid ="14" ssid = "10">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'54'", "'4'", "'53'", "'14'"]
'13'
'54'
'4'
'53'
'14'
['13', '54', '4', '53', '14']
parsed_discourse_facet ['method_citation']
['Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.']
['Its power lies in the ability to define a rich set of features over parsing decisions  as well as surface level features relative to these decisions.', 'Dependency graphs also encode much of the deep syntactic information needed for further processing.', 'Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.', 'Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.', 'It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.']
['system', 'ROUGE-S*', 'Average_R:', '0.00073', '(95%-conf.int.', '0.00073', '-', '0.00073)']
['system', 'ROUGE-S*', 'Average_P:', '0.00333', '(95%-conf.int.', '0.00333', '-', '0.00333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00119', '(95%-conf.int.', '0.00119', '-', '0.00119)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:300', 'F:1']
['An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.']
['This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).', 'This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.', 'These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.', u"However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'\xcb\u2021cuk  1988).", 'With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.']
['system', 'ROUGE-S*', 'Average_R:', '0.00444', '(95%-conf.int.', '0.00444', '-', '0.00444)']
['system', 'ROUGE-S*', 'Average_P:', '0.12500', '(95%-conf.int.', '0.12500', '-', '0.12500)']
['system', 'ROUGE-S*', 'Average_F:', '0.00858', '(95%-conf.int.', '0.00858', '-', '0.00858)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3828', 'P:136', 'F:17']
['Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.']
['The first stage of our system creates an unlabeled parse y for an input sentence x.', 'Dependency graphs also encode much of the deep syntactic information needed for further processing.', 'The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.', 'The current system simply includes all morphological bi-gram features.', 'Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.']
['system', 'ROUGE-S*', 'Average_R:', '0.02039', '(95%-conf.int.', '0.02039', '-', '0.02039)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.03331', '(95%-conf.int.', '0.03331', '-', '0.03331)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:253', 'F:23']
['N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.']
['Its power lies in the ability to define a rich set of features over parsing decisions  as well as surface level features relative to these decisions.', 'Dependency graphs also encode much of the deep syntactic information needed for further processing.', 'Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.', 'It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.', 'However  the parser is fundamentally limited by the scope of local factorizations that make inference tractable.']
['system', 'ROUGE-S*', 'Average_R:', '0.00679', '(95%-conf.int.', '0.00679', '-', '0.00679)']
['system', 'ROUGE-S*', 'Average_P:', '0.03557', '(95%-conf.int.', '0.03557', '-', '0.03557)']
['system', 'ROUGE-S*', 'Average_F:', '0.01140', '(95%-conf.int.', '0.01140', '-', '0.01140)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:253', 'F:9']
['For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.']
['Based on performance from a held-out section of the training data  we used non-projective parsing algorithms for Czech  Danish  Dutch  German  Japanese  Portuguese and Slovene  and projective parsing algorithms for Arabic  Bulgarian  Chinese  Spanish  Swedish and Turkish.', u'We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Haji\u02c7c et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B\xa8ohmov\xb4a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; D\u02c7zeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).', 'We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.', 'We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).', 'The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:7381', 'P:231', 'F:0']
['In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).', 'This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.', 'These features are highly important to overall accuracy since they eliminate unlikely scenarios such as a preposition modifying a noun not directly to its left  or a noun modifying a verb with another verb occurring between them.', 'It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.', u"However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'\xcb\u2021cuk  1988)."]
['system', 'ROUGE-S*', 'Average_R:', '0.00112', '(95%-conf.int.', '0.00112', '-', '0.00112)']
['system', 'ROUGE-S*', 'Average_P:', '0.06061', '(95%-conf.int.', '0.06061', '-', '0.06061)']
['system', 'ROUGE-S*', 'Average_F:', '0.00220', '(95%-conf.int.', '0.00220', '-', '0.00220)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3570', 'P:66', 'F:4']
['We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.']
['This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.', 'However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.', 'The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.', u"However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'\xcb\u2021cuk  1988).", 'With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.']
['system', 'ROUGE-S*', 'Average_R:', '0.00370', '(95%-conf.int.', '0.00370', '-', '0.00370)']
['system', 'ROUGE-S*', 'Average_P:', '0.05848', '(95%-conf.int.', '0.05848', '-', '0.05848)']
['system', 'ROUGE-S*', 'Average_F:', '0.00696', '(95%-conf.int.', '0.00696', '-', '0.00696)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2701', 'P:171', 'F:10']
['To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.']
['Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.', 'Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.', 'Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.', 'This system is primarily based on the parsing models described by McDonald and Pereira (2006).', 'For instance  the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:378', 'F:0']
['For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.']
['We use the MIRA online learner to set the weights (Crammer and Singer  2003; McDonald et al.  2005a) since we found it trained quickly and provide good performance.', 'Results on the test set are given in Table 1.', u'We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Haji\u02c7c et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B\xa8ohmov\xb4a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; D\u02c7zeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).', 'We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.', 'We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5671', 'P:630', 'F:0']
0.0415444439828 0.00412999995411 0.00707111103254





input/ref/Task1/W06-2932_sweta.csv
input/res/Task1/W06-2932.csv
parsing: input/ref/Task1/W06-2932_sweta.csv
 <S sid="5" ssid="1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="5">However, in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["36'"]
36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="9">In fact, for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="76" ssid="14">For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["76'"]
76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="45" ssid="14">Furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["45'"]
45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="3">First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["106'"]
106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="58" ssid="6">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["58'"]
58'
['58']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
 <S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="2">Ideally one would like to make all parsing and labeling decisions jointly so that the shared knowledge of both decisions will help resolve any ambiguities.</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
 <S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["43'"]
43'
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S sid ="1" ssid = "1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S><S sid ="9" ssid = "5">Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S><S sid ="23" ssid = "5">That system uses MIRA  an online large-margin learning algorithm  to compute model parameters.</S><S sid ="27" ssid = "9">We augmented this model to incorporate morphological features derived from each token.</S>
original cit marker offset is nan
new cit marker offset is 0



["'1'", "'9'", "'107'", "'23'", "'27'"]
'1'
'9'
'107'
'23'
'27'
['1', '9', '107', '23', '27']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="14" ssid = "10">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S><S sid ="58" ssid = "6">These results show that the discriminative spanning tree parsing framework (McDonald et al.  2005b; McDonald and Pereira  2006) is easily adapted across all these languages.</S><S sid ="25" ssid = "7">For instance  the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'20'", "'14'", "'58'", "'25'"]
'2'
'20'
'14'
'58'
'25'
['2', '20', '14', '58', '25']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S><S sid ="19" ssid = "1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S><S sid ="109" ssid = "6">The current system simply includes all morphological bi-gram features.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="51" ssid = "20">Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'19'", "'109'", "'10'", "'51'"]
'3'
'19'
'109'
'10'
'51'
['3', '19', '109', '10', '51']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.</S><S sid ="53" ssid = "1">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).</S><S sid ="44" ssid = "13">We use the MIRA online learner to set the weights (Crammer and Singer  2003; McDonald et al.  2005a) since we found it trained quickly and provide good performance.</S><S sid ="13" ssid = "9">We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajiˇc et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B¨ohmov´a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dˇzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).</S><S sid ="56" ssid = "4">Results on the test set are given in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'53'", "'44'", "'13'", "'56'"]
'4'
'53'
'44'
'13'
'56'
['4', '53', '44', '13', '56']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="24" ssid = "6">Its power lies in the ability to define a rich set of features over parsing decisions  as well as surface level features relative to these decisions.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'10'", "'88'", "'24'", "'107'"]
'5'
'10'
'88'
'24'
'107'
['5', '10', '88', '24', '107']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.</S><S sid ="14" ssid = "10">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'14'", "'7'", "'36'", "'8'"]
'6'
'14'
'7'
'36'
'8'
['6', '14', '7', '36', '8']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="110" ssid = "7">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="26" ssid = "8">These features are highly important to overall accuracy since they eliminate unlikely scenarios such as a preposition modifying a noun not directly to its left  or a noun modifying a verb with another verb occurring between them.</S><S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'110'", "'11'", "'26'", "'8'"]
'7'
'110'
'11'
'26'
'8'
['7', '110', '11', '26', '8']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="30" ssid = "12">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="6" ssid = "2">With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'7'", "'30'", "'11'", "'6'"]
'8'
'7'
'30'
'11'
'6'
['8', '7', '30', '11', '6']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="5" ssid = "1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S><S sid ="25" ssid = "7">For instance  the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'20'", "'88'", "'5'", "'25'"]
'9'
'20'
'88'
'5'
'25'
['9', '20', '88', '5', '25']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="5" ssid = "1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S><S sid ="24" ssid = "6">Its power lies in the ability to define a rich set of features over parsing decisions  as well as surface level features relative to these decisions.</S><S sid ="34" ssid = "3">However  the parser is fundamentally limited by the scope of local factorizations that make inference tractable.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'5'", "'24'", "'34'", "'107'"]
'10'
'5'
'24'
'34'
'107'
['10', '5', '24', '34', '107']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="110" ssid = "7">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S><S sid ="45" ssid = "14">Furthermore  it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira  2006).</S><S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'7'", "'110'", "'45'", "'8'"]
'11'
'7'
'110'
'45'
'8'
['11', '7', '110', '45', '8']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S sid ="95" ssid = "17">Note the difference in error between the unlabeled parser and the edge labeler: the former makes mistakes on edges into prepositions  conjunctions and verbs  and the latter makes mistakes on edges into nouns (subject/objects).</S><S sid ="110" ssid = "7">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S><S sid ="69" ssid = "7">However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).</S><S sid ="106" ssid = "3">First  we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'95'", "'110'", "'69'", "'106'"]
'12'
'95'
'110'
'69'
'106'
['12', '95', '110', '69', '106']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajiˇc et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B¨ohmov´a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dˇzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).</S><S sid ="54" ssid = "2">Based on performance from a held-out section of the training data  we used non-projective parsing algorithms for Czech  Danish  Dutch  German  Japanese  Portuguese and Slovene  and projective parsing algorithms for Arabic  Bulgarian  Chinese  Spanish  Swedish and Turkish.</S><S sid ="4" ssid = "4">We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.</S><S sid ="53" ssid = "1">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).</S><S sid ="14" ssid = "10">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'54'", "'4'", "'53'", "'14'"]
'13'
'54'
'4'
'53'
'14'
['13', '54', '4', '53', '14']
parsed_discourse_facet ['method_citation']
['Furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira, 2006).']
['Its power lies in the ability to define a rich set of features over parsing decisions  as well as surface level features relative to these decisions.', 'Dependency graphs also encode much of the deep syntactic information needed for further processing.', 'Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.', 'Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.', 'It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:66', 'F:0']
['N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.']
['This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).', 'This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.', 'These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.', u"However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'\xcb\u2021cuk  1988).", 'With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.']
['system', 'ROUGE-S*', 'Average_R:', '0.00131', '(95%-conf.int.', '0.00131', '-', '0.00131)']
['system', 'ROUGE-S*', 'Average_P:', '0.01976', '(95%-conf.int.', '0.01976', '-', '0.01976)']
['system', 'ROUGE-S*', 'Average_F:', '0.00245', '(95%-conf.int.', '0.00245', '-', '0.00245)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3828', 'P:253', 'F:5']
['In fact, for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).']
['The first stage of our system creates an unlabeled parse y for an input sentence x.', 'Dependency graphs also encode much of the deep syntactic information needed for further processing.', 'The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.', 'The current system simply includes all morphological bi-gram features.', 'Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:78', 'F:0']
['N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.']
['Its power lies in the ability to define a rich set of features over parsing decisions  as well as surface level features relative to these decisions.', 'Dependency graphs also encode much of the deep syntactic information needed for further processing.', 'Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.', 'It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.', 'However  the parser is fundamentally limited by the scope of local factorizations that make inference tractable.']
['system', 'ROUGE-S*', 'Average_R:', '0.00679', '(95%-conf.int.', '0.00679', '-', '0.00679)']
['system', 'ROUGE-S*', 'Average_P:', '0.03557', '(95%-conf.int.', '0.03557', '-', '0.03557)']
['system', 'ROUGE-S*', 'Average_F:', '0.01140', '(95%-conf.int.', '0.01140', '-', '0.01140)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:253', 'F:9']
['For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.']
['Based on performance from a held-out section of the training data  we used non-projective parsing algorithms for Czech  Danish  Dutch  German  Japanese  Portuguese and Slovene  and projective parsing algorithms for Arabic  Bulgarian  Chinese  Spanish  Swedish and Turkish.', u'We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Haji\u02c7c et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B\xa8ohmov\xb4a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; D\u02c7zeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).', 'We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.', 'We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).', 'The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:7381', 'P:231', 'F:0']
['In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).', 'This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.', 'These features are highly important to overall accuracy since they eliminate unlikely scenarios such as a preposition modifying a noun not directly to its left  or a noun modifying a verb with another verb occurring between them.', 'It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.', u"However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'\xcb\u2021cuk  1988)."]
['system', 'ROUGE-S*', 'Average_R:', '0.00112', '(95%-conf.int.', '0.00112', '-', '0.00112)']
['system', 'ROUGE-S*', 'Average_P:', '0.06061', '(95%-conf.int.', '0.06061', '-', '0.06061)']
['system', 'ROUGE-S*', 'Average_F:', '0.00220', '(95%-conf.int.', '0.00220', '-', '0.00220)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3570', 'P:66', 'F:4']
['First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.']
['This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.', 'However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.', 'The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.', u"However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'\xcb\u2021cuk  1988).", 'With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.']
['system', 'ROUGE-S*', 'Average_R:', '0.00296', '(95%-conf.int.', '0.00296', '-', '0.00296)']
['system', 'ROUGE-S*', 'Average_P:', '0.12121', '(95%-conf.int.', '0.12121', '-', '0.12121)']
['system', 'ROUGE-S*', 'Average_F:', '0.00578', '(95%-conf.int.', '0.00578', '-', '0.00578)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2701', 'P:66', 'F:8']
['To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.']
['Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.', 'Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.', 'Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.', 'This system is primarily based on the parsing models described by McDonald and Pereira (2006).', 'For instance  the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:378', 'F:0']
['For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.']
['We use the MIRA online learner to set the weights (Crammer and Singer  2003; McDonald et al.  2005a) since we found it trained quickly and provide good performance.', 'Results on the test set are given in Table 1.', u'We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Haji\u02c7c et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B\xa8ohmov\xb4a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; D\u02c7zeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).', 'We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.', 'We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5671', 'P:630', 'F:0']
['Ideally one would like to make all parsing and labeling decisions jointly so that the shared knowledge of both decisions will help resolve any ambiguities.']
['This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).', 'This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.', 'Furthermore  it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira  2006).', 'It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.', u"However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'\xcb\u2021cuk  1988)."]
['system', 'ROUGE-S*', 'Average_R:', '0.00031', '(95%-conf.int.', '0.00031', '-', '0.00031)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00061', '(95%-conf.int.', '0.00061', '-', '0.00061)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3240', 'P:55', 'F:1']
0.0255329997447 0.00124899998751 0.00224399997756





input/ref/Task1/P04-1036_aakansha.csv
input/res/Task1/P04-1036.csv
parsing: input/ref/Task1/P04-1036_aakansha.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="34">In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
    <S sid="181" ssid="4">This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'181'"]
'180'
'181'
['180', '181']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="4">To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'"]
'48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="169" ssid="17">This method obtains precision of 61% and recall 51%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'"]
'169'
['169']
parsed_discourse_facet ['result_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="115" ssid="13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
    <S sid="181" ssid="4">This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'181'"]
'180'
'181'
['180', '181']
parsed_discourse_facet ['result_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'"]
'180'
['180']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="34">In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="48" ssid = "4">To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.</S><S sid ="105" ssid = "3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents  rather than a lexical sample task  where the target words are manually determined and the results will depend on the skew of the words in the sample.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'153'", "'154'", "'48'", "'105'"]
'1'
'153'
'154'
'48'
'105'
['1', '153', '154', '48', '105']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">The problem with using the predominant  or first sense heuristic  aside from the fact that it does not take surrounding context into account  is that it assumes some quantity of handtagged data.</S><S sid ="17" ssid = "10">There are words where the first sense in WordNet is counter-intuitive  because of the size of the corpus  and because where the frequency data does not indicate a first sense  the ordering is arbitrary.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'17'", "'91'", "'154'", "'123'"]
'2'
'17'
'91'
'154'
'123'
['2', '17', '91', '154', '123']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.</S><S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="37" ssid = "30">From inspection  one can see that the ordered neighbours of such a thesaurus relate to the different senses of the target word.</S><S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'15'", "'37'", "'1'", "'170'"]
'3'
'15'
'37'
'1'
'170'
['3', '15', '37', '1', '170']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S sid ="179" ssid = "2">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S><S sid ="171" ssid = "19">In contrast  we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'179'", "'178'", "'45'", "'171'"]
'4'
'179'
'178'
'45'
'171'
['4', '179', '178', '45', '171']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="84" ssid = "13">The random baseline for choosing the predominant sense over all these words ( ) is 32%.</S><S sid ="88" ssid = "17">The first sense in SemCor provides an upperbound for this task of 67%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'100'", "'116'", "'84'", "'88'"]
'5'
'100'
'116'
'84'
'88'
['5', '100', '116', '84', '88']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="121" ssid = "19">For these one would need to obtain more sense-tagged text in order to use this heuristic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'126'", "'178'", "'13'", "'121'"]
'6'
'126'
'178'
'13'
'121'
['6', '126', '178', '13', '121']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S><S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S><S sid ="158" ssid = "6">We have evaluated our method using publically available resources  both for balanced and domain specific text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'184'", "'178'", "'126'", "'158'"]
'7'
'184'
'178'
'126'
'158'
['7', '184', '178', '126', '158']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="172" ssid = "20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet  for example those that use it for lexical acquisition or WSD.</S><S sid ="115" ssid = "13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor  which is very encouraging given that our method only uses raw text  with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'154'", "'14'", "'172'", "'115'"]
'8'
'154'
'14'
'172'
'115'
['8', '154', '14', '172', '115']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "2">This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).</S><S sid ="102" ssid = "31">We test this below on the SENSEVAL-2 English all-words data.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'102'", "'116'", "'100'", "'5'"]
'9'
'102'
'116'
'100'
'5'
['9', '102', '116', '100', '5']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="180" ssid = "3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S><S sid ="109" ssid = "7">We generated a thesaurus entry for all polysemous nouns in WordNet as described in section 2.1 above.</S><S sid ="112" ssid = "10">We compare results using the first sense listed in SemCor  and the first sense according to the SENSEVAL-2 English all-words test data itself.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'180'", "'142'", "'109'", "'112'"]
'10'
'180'
'142'
'109'
'112'
['10', '180', '142', '109', '112']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "4">Senses that have not occurred in SemCor are ordered arbitrarily and after those senses of the word that have occurred.</S><S sid ="120" ssid = "18">There were a similar number of words that were not covered by a predominant sense in SemCor.</S><S sid ="42" ssid = "35">The neighbours for a word in a thesaurus are words themselves  rather than senses.</S><S sid ="189" ssid = "12">Additionally  we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity  and what can be done to combat this problem using relation specific thesauruses.</S><S sid ="117" ssid = "15">The items that were not covered by our method were those with insufficient grammatical relations for the tuples employed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'120'", "'42'", "'189'", "'117'"]
'11'
'120'
'42'
'189'
'117'
['11', '120', '42', '189', '117']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "5">The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor  from those that do not (without HTD).</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'123'", "'153'", "'165'", "'178'"]
'12'
'123'
'153'
'165'
'178'
['12', '123', '153', '165', '178']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="84" ssid = "13">The random baseline for choosing the predominant sense over all these words ( ) is 32%.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'116'", "'147'", "'84'", "'126'"]
'13'
'116'
'147'
'84'
'126'
['13', '116', '147', '84', '126']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="182" ssid = "5">In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.</S><S sid ="21" ssid = "14">We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off (Wilks and Stevenson  1998; Hoste et al.  2001) and for systems that use it in lexical acquisition (McCarthy  1997; Merlo and Leybold  2001; Korhonen  2002) because of the limited size of hand-tagged resources.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'8'", "'154'", "'182'", "'21'"]
'14'
'8'
'154'
'182'
'21'
['14', '8', '154', '182', '21']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="3" ssid = "3">Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'154'", "'3'", "'8'", "'170'"]
'15'
'154'
'3'
'8'
'170'
['15', '154', '3', '8', '170']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">SemCor comprises a relatively small sample of 250 000 words.</S><S sid ="131" ssid = "8">The SPORTS corpus consists of 35317 documents (about 9.1 million words).</S><S sid ="140" ssid = "17">The resulting set consisted of 38 words.</S><S sid ="132" ssid = "9">The FINANCE corpus consists of 117734 documents (about 32.5 million words).</S><S sid ="76" ssid = "5">The jcn measure uses corpus data for the calculation of IC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'131'", "'140'", "'132'", "'76'"]
'16'
'131'
'140'
'132'
'76'
['16', '131', '140', '132', '76']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "10">There are words where the first sense in WordNet is counter-intuitive  because of the size of the corpus  and because where the frequency data does not indicate a first sense  the ordering is arbitrary.</S><S sid ="182" ssid = "5">In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.</S><S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="99" ssid = "28">Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'182'", "'13'", "'99'", "'170'"]
'17'
'182'
'13'
'99'
'170'
['17', '182', '13', '99', '170']
parsed_discourse_facet ['method_citation']
['In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.']
['word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.', 'From inspection  one can see that the ordered neighbours of such a thesaurus relate to the different senses of the target word.', 'Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.', 'Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.', 'If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.']
['system', 'ROUGE-S*', 'Average_R:', '0.00411', '(95%-conf.int.', '0.00411', '-', '0.00411)']
['system', 'ROUGE-S*', 'Average_P:', '0.12381', '(95%-conf.int.', '0.12381', '-', '0.12381)']
['system', 'ROUGE-S*', 'Average_F:', '0.00796', '(95%-conf.int.', '0.00796', '-', '0.00796)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:105', 'F:13']
['The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.']
['The FINANCE corpus consists of 117734 documents (about 32.5 million words).', 'The SPORTS corpus consists of 35317 documents (about 9.1 million words).', 'The jcn measure uses corpus data for the calculation of IC.', 'The resulting set consisted of 38 words.', 'SemCor comprises a relatively small sample of 250 000 words.']
['system', 'ROUGE-S*', 'Average_R:', '0.00159', '(95%-conf.int.', '0.00159', '-', '0.00159)']
['system', 'ROUGE-S*', 'Average_P:', '0.00526', '(95%-conf.int.', '0.00526', '-', '0.00526)']
['system', 'ROUGE-S*', 'Average_F:', '0.00244', '(95%-conf.int.', '0.00244', '-', '0.00244)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:190', 'F:1']
['To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.']
['Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor  which is very encouraging given that our method only uses raw text  with no manual labelling.', 'The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.', 'We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet  for example those that use it for lexical acquisition or WSD.', 'In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.', 'Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).']
['system', 'ROUGE-S*', 'Average_R:', '0.00317', '(95%-conf.int.', '0.00317', '-', '0.00317)']
['system', 'ROUGE-S*', 'Average_P:', '0.10909', '(95%-conf.int.', '0.10909', '-', '0.10909)']
['system', 'ROUGE-S*', 'Average_F:', '0.00617', '(95%-conf.int.', '0.00617', '-', '0.00617)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:55', 'F:6']
['We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.']
['In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.', 'The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.', 'Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).', 'We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off (Wilks and Stevenson  1998; Hoste et al.  2001) and for systems that use it in lexical acquisition (McCarthy  1997; Merlo and Leybold  2001; Korhonen  2002) because of the limited size of hand-tagged resources.', 'In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.']
['system', 'ROUGE-S*', 'Average_R:', '0.00067', '(95%-conf.int.', '0.00067', '-', '0.00067)']
['system', 'ROUGE-S*', 'Average_P:', '0.07143', '(95%-conf.int.', '0.07143', '-', '0.07143)']
['system', 'ROUGE-S*', 'Average_F:', '0.00132', '(95%-conf.int.', '0.00132', '-', '0.00132)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:28', 'F:2']
['The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.', 'This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.', 'Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.', 'Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.', 'In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.', 'If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.']
['system', 'ROUGE-S*', 'Average_R:', '0.01406', '(95%-conf.int.', '0.01406', '-', '0.01406)']
['system', 'ROUGE-S*', 'Average_P:', '0.08734', '(95%-conf.int.', '0.08734', '-', '0.08734)']
['system', 'ROUGE-S*', 'Average_F:', '0.02422', '(95%-conf.int.', '0.02422', '-', '0.02422)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:561', 'F:49']
['The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.', 'This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.']
['For these one would need to obtain more sense-tagged text in order to use this heuristic.', 'The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.', 'We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.', 'We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.', 'This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.']
['system', 'ROUGE-S*', 'Average_R:', '0.05391', '(95%-conf.int.', '0.05391', '-', '0.05391)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.06768', '(95%-conf.int.', '0.06768', '-', '0.06768)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:561', 'F:51']
['This method obtains precision of 61% and recall 51%.']
['This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).', 'We test this below on the SENSEVAL-2 English all-words data.', 'In the English all-words SENSEVAL-2  25% of the noun data was monosemous.', 'The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.', 'The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:15', 'F:0']
['In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.']
['In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.', 'Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.', 'The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.', 'There are words where the first sense in WordNet is counter-intuitive  because of the size of the corpus  and because where the frequency data does not indicate a first sense  the ordering is arbitrary.', 'If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.']
['system', 'ROUGE-S*', 'Average_R:', '0.01141', '(95%-conf.int.', '0.01141', '-', '0.01141)']
['system', 'ROUGE-S*', 'Average_P:', '0.21905', '(95%-conf.int.', '0.21905', '-', '0.21905)']
['system', 'ROUGE-S*', 'Average_F:', '0.02169', '(95%-conf.int.', '0.02169', '-', '0.02169)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:105', 'F:23']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['The word share is among the words whose predominant sense remained the same for all three corpora.', 'The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.', 'The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.', 'We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.', 'The random baseline for choosing the predominant sense over all these words ( ) is 32%.']
['system', 'ROUGE-S*', 'Average_R:', '0.00317', '(95%-conf.int.', '0.00317', '-', '0.00317)']
['system', 'ROUGE-S*', 'Average_P:', '0.04545', '(95%-conf.int.', '0.04545', '-', '0.04545)']
['system', 'ROUGE-S*', 'Average_F:', '0.00593', '(95%-conf.int.', '0.00593', '-', '0.00593)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:66', 'F:3']
['Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.']
['Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.', 'This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.', 'The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor  from those that do not (without HTD).', 'Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.', 'We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.']
['system', 'ROUGE-S*', 'Average_R:', '0.01074', '(95%-conf.int.', '0.01074', '-', '0.01074)']
['system', 'ROUGE-S*', 'Average_P:', '0.27619', '(95%-conf.int.', '0.27619', '-', '0.27619)']
['system', 'ROUGE-S*', 'Average_F:', '0.02067', '(95%-conf.int.', '0.02067', '-', '0.02067)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2701', 'P:105', 'F:29']
0.102852998971 0.0102829998972 0.0158079998419





input/ref/Task1/P04-1036_vardha.csv
input/res/Task1/P04-1036.csv
parsing: input/ref/Task1/P04-1036_vardha.csv
    <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
    <S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="66" ssid="22">It uses the glosses of semantically related (according to WordNet) senses too. jcn (Jiang and Conrath, 1997) This score uses corpus data to populate classes (synsets) in the WordNet hierarchy with frequency counts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'66'"]
'66'
['66']
parsed_discourse_facet ['method_citation']
 <S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="101" ssid="30">Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'"]
'101'
['101']
parsed_discourse_facet ['method_citation']
 <S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
 <S sid="115" ssid="13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
  <S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'"]
'89'
['89']
parsed_discourse_facet ['method_citation']
 <S sid="137" ssid="14">Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'"]
'137'
['137']
parsed_discourse_facet ['method_citation']
75 ssid="4">We generated a thesaurus entry for all polysemous nouns which occurred in SemCor with a frequency 2, and in the BNC with a frequency 10 in the grammatical relations listed in section 2.1 above.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'"]
'75'
['75']
Error in Reference Offset
 <S sid="155" ssid="3">A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
  <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
 <S sid="155" ssid="3">A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="24">We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'"]
'68'
['68']
parsed_discourse_facet ['method_citation']
    <S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'"]
'13'
['13']
parsed_discourse_facet ['method_citation']
  <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="48" ssid = "4">To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.</S><S sid ="105" ssid = "3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents  rather than a lexical sample task  where the target words are manually determined and the results will depend on the skew of the words in the sample.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'153'", "'154'", "'48'", "'105'"]
'1'
'153'
'154'
'48'
'105'
['1', '153', '154', '48', '105']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">The problem with using the predominant  or first sense heuristic  aside from the fact that it does not take surrounding context into account  is that it assumes some quantity of handtagged data.</S><S sid ="17" ssid = "10">There are words where the first sense in WordNet is counter-intuitive  because of the size of the corpus  and because where the frequency data does not indicate a first sense  the ordering is arbitrary.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'17'", "'91'", "'154'", "'123'"]
'2'
'17'
'91'
'154'
'123'
['2', '17', '91', '154', '123']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.</S><S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="37" ssid = "30">From inspection  one can see that the ordered neighbours of such a thesaurus relate to the different senses of the target word.</S><S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'15'", "'37'", "'1'", "'170'"]
'3'
'15'
'37'
'1'
'170'
['3', '15', '37', '1', '170']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S sid ="179" ssid = "2">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S><S sid ="171" ssid = "19">In contrast  we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'179'", "'178'", "'45'", "'171'"]
'4'
'179'
'178'
'45'
'171'
['4', '179', '178', '45', '171']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="84" ssid = "13">The random baseline for choosing the predominant sense over all these words ( ) is 32%.</S><S sid ="88" ssid = "17">The first sense in SemCor provides an upperbound for this task of 67%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'100'", "'116'", "'84'", "'88'"]
'5'
'100'
'116'
'84'
'88'
['5', '100', '116', '84', '88']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="121" ssid = "19">For these one would need to obtain more sense-tagged text in order to use this heuristic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'126'", "'178'", "'13'", "'121'"]
'6'
'126'
'178'
'13'
'121'
['6', '126', '178', '13', '121']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S><S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S><S sid ="158" ssid = "6">We have evaluated our method using publically available resources  both for balanced and domain specific text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'184'", "'178'", "'126'", "'158'"]
'7'
'184'
'178'
'126'
'158'
['7', '184', '178', '126', '158']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="172" ssid = "20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet  for example those that use it for lexical acquisition or WSD.</S><S sid ="115" ssid = "13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor  which is very encouraging given that our method only uses raw text  with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'154'", "'14'", "'172'", "'115'"]
'8'
'154'
'14'
'172'
'115'
['8', '154', '14', '172', '115']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "2">This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).</S><S sid ="102" ssid = "31">We test this below on the SENSEVAL-2 English all-words data.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'102'", "'116'", "'100'", "'5'"]
'9'
'102'
'116'
'100'
'5'
['9', '102', '116', '100', '5']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="180" ssid = "3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S><S sid ="109" ssid = "7">We generated a thesaurus entry for all polysemous nouns in WordNet as described in section 2.1 above.</S><S sid ="112" ssid = "10">We compare results using the first sense listed in SemCor  and the first sense according to the SENSEVAL-2 English all-words test data itself.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'180'", "'142'", "'109'", "'112'"]
'10'
'180'
'142'
'109'
'112'
['10', '180', '142', '109', '112']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "4">Senses that have not occurred in SemCor are ordered arbitrarily and after those senses of the word that have occurred.</S><S sid ="120" ssid = "18">There were a similar number of words that were not covered by a predominant sense in SemCor.</S><S sid ="42" ssid = "35">The neighbours for a word in a thesaurus are words themselves  rather than senses.</S><S sid ="189" ssid = "12">Additionally  we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity  and what can be done to combat this problem using relation specific thesauruses.</S><S sid ="117" ssid = "15">The items that were not covered by our method were those with insufficient grammatical relations for the tuples employed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'120'", "'42'", "'189'", "'117'"]
'11'
'120'
'42'
'189'
'117'
['11', '120', '42', '189', '117']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "5">The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor  from those that do not (without HTD).</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="178" ssid = "1">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'123'", "'153'", "'165'", "'178'"]
'12'
'123'
'153'
'165'
'178'
['12', '123', '153', '165', '178']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="116" ssid = "14">The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="84" ssid = "13">The random baseline for choosing the predominant sense over all these words ( ) is 32%.</S><S sid ="126" ssid = "3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'116'", "'147'", "'84'", "'126'"]
'13'
'116'
'147'
'84'
'126'
['13', '116', '147', '84', '126']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="182" ssid = "5">In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.</S><S sid ="21" ssid = "14">We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off (Wilks and Stevenson  1998; Hoste et al.  2001) and for systems that use it in lexical acquisition (McCarthy  1997; Merlo and Leybold  2001; Korhonen  2002) because of the limited size of hand-tagged resources.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'8'", "'154'", "'182'", "'21'"]
'14'
'8'
'154'
'182'
'21'
['14', '8', '154', '182', '21']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="154" ssid = "2">In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.</S><S sid ="3" ssid = "3">Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'154'", "'3'", "'8'", "'170'"]
'15'
'154'
'3'
'8'
'170'
['15', '154', '3', '8', '170']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">SemCor comprises a relatively small sample of 250 000 words.</S><S sid ="131" ssid = "8">The SPORTS corpus consists of 35317 documents (about 9.1 million words).</S><S sid ="140" ssid = "17">The resulting set consisted of 38 words.</S><S sid ="132" ssid = "9">The FINANCE corpus consists of 117734 documents (about 32.5 million words).</S><S sid ="76" ssid = "5">The jcn measure uses corpus data for the calculation of IC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'131'", "'140'", "'132'", "'76'"]
'16'
'131'
'140'
'132'
'76'
['16', '131', '140', '132', '76']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "10">There are words where the first sense in WordNet is counter-intuitive  because of the size of the corpus  and because where the frequency data does not indicate a first sense  the ordering is arbitrary.</S><S sid ="182" ssid = "5">In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.</S><S sid ="13" ssid = "6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid ="99" ssid = "28">Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.</S><S sid ="170" ssid = "18">If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'182'", "'13'", "'99'", "'170'"]
'17'
'182'
'13'
'99'
'170'
['17', '182', '13', '99', '170']
parsed_discourse_facet ['method_citation']
['In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).']
['word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.', 'From inspection  one can see that the ordered neighbours of such a thesaurus relate to the different senses of the target word.', 'Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.', 'Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.', 'If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.']
['system', 'ROUGE-S*', 'Average_R:', '0.00696', '(95%-conf.int.', '0.00696', '-', '0.00696)']
['system', 'ROUGE-S*', 'Average_P:', '0.20952', '(95%-conf.int.', '0.20952', '-', '0.20952)']
['system', 'ROUGE-S*', 'Average_F:', '0.01348', '(95%-conf.int.', '0.01348', '-', '0.01348)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:105', 'F:22']
['The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.']
['The FINANCE corpus consists of 117734 documents (about 32.5 million words).', 'The SPORTS corpus consists of 35317 documents (about 9.1 million words).', 'The jcn measure uses corpus data for the calculation of IC.', 'The resulting set consisted of 38 words.', 'SemCor comprises a relatively small sample of 250 000 words.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:55', 'F:0']
['Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.']
['Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor  which is very encouraging given that our method only uses raw text  with no manual labelling.', 'The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.', 'We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet  for example those that use it for lexical acquisition or WSD.', 'In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.', 'Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).']
['system', 'ROUGE-S*', 'Average_R:', '0.05553', '(95%-conf.int.', '0.05553', '-', '0.05553)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.10521', '(95%-conf.int.', '0.10521', '-', '0.10521)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:105', 'F:105']
['A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.']
['In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.', 'The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.', 'Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).', 'We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off (Wilks and Stevenson  1998; Hoste et al.  2001) and for systems that use it in lexical acquisition (McCarthy  1997; Merlo and Leybold  2001; Korhonen  2002) because of the limited size of hand-tagged resources.', 'In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.']
['system', 'ROUGE-S*', 'Average_R:', '0.00766', '(95%-conf.int.', '0.00766', '-', '0.00766)']
['system', 'ROUGE-S*', 'Average_P:', '0.15033', '(95%-conf.int.', '0.15033', '-', '0.15033)']
['system', 'ROUGE-S*', 'Average_F:', '0.01458', '(95%-conf.int.', '0.01458', '-', '0.01458)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:153', 'F:23']
['We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.', 'Whilst there are a few hand-tagged corpora available for some languages  one would expect the frequency distribution of the senses of words  particularly topical words  to depend on the genre and domain of the text under consideration.', 'Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.', 'In contrast  our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one  and because handtagged data is not always available.', 'If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.']
['system', 'ROUGE-S*', 'Average_R:', '0.00201', '(95%-conf.int.', '0.00201', '-', '0.00201)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00390', '(95%-conf.int.', '0.00390', '-', '0.00390)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:105', 'F:7']
['Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.']
['For these one would need to obtain more sense-tagged text in order to use this heuristic.', 'The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.', 'We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.', 'We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.', 'This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.']
['system', 'ROUGE-S*', 'Average_R:', '0.00423', '(95%-conf.int.', '0.00423', '-', '0.00423)']
['system', 'ROUGE-S*', 'Average_P:', '0.07273', '(95%-conf.int.', '0.07273', '-', '0.07273)']
['system', 'ROUGE-S*', 'Average_F:', '0.00799', '(95%-conf.int.', '0.00799', '-', '0.00799)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:55', 'F:4']
['Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.']
['This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).', 'We test this below on the SENSEVAL-2 English all-words data.', 'In the English all-words SENSEVAL-2  25% of the noun data was monosemous.', 'The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.', 'The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.']
['system', 'ROUGE-S*', 'Average_R:', '0.00313', '(95%-conf.int.', '0.00313', '-', '0.00313)']
['system', 'ROUGE-S*', 'Average_P:', '0.02924', '(95%-conf.int.', '0.02924', '-', '0.02924)']
['system', 'ROUGE-S*', 'Average_F:', '0.00566', '(95%-conf.int.', '0.00566', '-', '0.00566)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:171', 'F:5']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.', 'Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.', 'The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.', 'There are words where the first sense in WordNet is counter-intuitive  because of the size of the corpus  and because where the frequency data does not indicate a first sense  the ordering is arbitrary.', 'If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.']
['system', 'ROUGE-S*', 'Average_R:', '0.00149', '(95%-conf.int.', '0.00149', '-', '0.00149)']
['system', 'ROUGE-S*', 'Average_P:', '0.04545', '(95%-conf.int.', '0.04545', '-', '0.04545)']
['system', 'ROUGE-S*', 'Average_F:', '0.00288', '(95%-conf.int.', '0.00288', '-', '0.00288)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:66', 'F:3']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['The word share is among the words whose predominant sense remained the same for all three corpora.', 'The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.', 'The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.', 'We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.', 'The random baseline for choosing the predominant sense over all these words ( ) is 32%.']
['system', 'ROUGE-S*', 'Average_R:', '0.00317', '(95%-conf.int.', '0.00317', '-', '0.00317)']
['system', 'ROUGE-S*', 'Average_P:', '0.04545', '(95%-conf.int.', '0.04545', '-', '0.04545)']
['system', 'ROUGE-S*', 'Average_F:', '0.00593', '(95%-conf.int.', '0.00593', '-', '0.00593)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:66', 'F:3']
['A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.']
['Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.', 'This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.', 'The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor  from those that do not (without HTD).', 'Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.', 'We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.']
['system', 'ROUGE-S*', 'Average_R:', '0.00889', '(95%-conf.int.', '0.00889', '-', '0.00889)']
['system', 'ROUGE-S*', 'Average_P:', '0.15686', '(95%-conf.int.', '0.15686', '-', '0.15686)']
['system', 'ROUGE-S*', 'Average_F:', '0.01682', '(95%-conf.int.', '0.01682', '-', '0.01682)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2701', 'P:153', 'F:24']
0.177624998224 0.00930699990693 0.0176449998236





input/ref/Task1/W99-0623_swastika.csv
input/res/Task1/W99-0623.csv
parsing: input/ref/Task1/W99-0623_swastika.csv
<S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['aim_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['result_citation']
<S sid="91" ssid="20">Features and context were initially introduced into the models, but they refused to offer any gains in performance.</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



['139']
139
['139']
parsed_discourse_facet ['result_citation']
<S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['method_citation']
<S sid="103" ssid="32">The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.</S>
original cit marker offset is 0
new cit marker offset is 0



['103']
103
['103']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



['139']
139
['139']
parsed_discourse_facet ['result_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
<S sid="140" ssid="2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
<S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['result_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/W99-0623.csv
<S sid ="1" ssid = "1">Three state-of-the-art statistical parsers are combined to produce more accurate parses  as well as new bounds on achievable Treebank parsing accuracy.</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="129" ssid = "58">In the interest of testing the robustness of these combining techniques  we added a fourth  simple nonlexicalized PCFG parser.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'72'", "'38'", "'129'", "'130'"]
'1'
'72'
'38'
'129'
'130'
['1', '72', '38', '129', '130']
Error in Discourse Facet
<S sid ="2" ssid = "2">Two general approaches are presented and two combination techniques are described for each approach.</S><S sid ="59" ssid = "45">Once again we present both a non-parametric and a parametric technique for this task.</S><S sid ="3" ssid = "3">Both parametric and non-parametric models are explored.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'59'", "'3'", "'85'", "'139'"]
'2'
'59'
'3'
'85'
'139'
['2', '59', '3', '85', '139']
Error in Discourse Facet
<S sid ="3" ssid = "3">Both parametric and non-parametric models are explored.</S><S sid ="59" ssid = "45">Once again we present both a non-parametric and a parametric technique for this task.</S><S sid ="2" ssid = "2">Two general approaches are presented and two combination techniques are described for each approach.</S><S sid ="138" ssid = "67">Surprisingly  the non-parametric switching technique also exhibited robust behaviour in this situation.</S><S sid ="136" ssid = "65">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'59'", "'2'", "'138'", "'136'"]
'3'
'59'
'2'
'138'
'136'
['3', '59', '2', '138', '136']
Error in Discourse Facet
<S sid ="4" ssid = "4">The resulting parsers surpass the best previously published performance results for the Penn Treebank.</S><S sid ="76" ssid = "5">The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="134" ssid = "63">As seen by the drop in average individual parser performance baseline  the introduced parser does not perform very well.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'76'", "'85'", "'130'", "'134'"]
'4'
'76'
'85'
'130'
'134'
['4', '76', '85', '130', '134']
Error in Discourse Facet
<S sid ="5" ssid = "1">The natural language processing community is in the strong position of having many available approaches to solving some of its most fundamental problems.</S><S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid ="26" ssid = "12">This technique has the advantage of requiring no training  but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'6'", "'12'", "'13'", "'26'"]
'5'
'6'
'12'
'13'
'26'
['5', '6', '12', '13', '26']
Error in Discourse Facet
<S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="108" ssid = "37">From this we see that a finer-grained model for parser combination  at least for the features we have examined  will not give us any additional power.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'11'", "'9'", "'12'", "'108'"]
'6'
'11'
'9'
'12'
'108'
['6', '11', '9', '12', '108']
Error in Discourse Facet
<S sid ="7" ssid = "3">Their theoretical finding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent  accurate classifiers.</S><S sid ="30" ssid = "16">This is equivalent to the assumption used in probability estimation for naïve Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.</S><S sid ="116" ssid = "45">The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="41" ssid = "27">IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'30'", "'116'", "'38'", "'41'"]
'7'
'30'
'116'
'38'
'41'
['7', '30', '116', '38', '41']
Error in Discourse Facet
<S sid ="8" ssid = "4">The theory has also been validated empirically.</S><S sid ="36" ssid = "22">The estimation of the probabilities in the model is carried out as shown in Equation 4.</S><S sid ="83" ssid = "12">We performed three experiments to evaluate our techniques.</S><S sid ="121" ssid = "50">Table 3 contains the results for evaluating our systems on the test set (section 22).</S><S sid ="109" ssid = "38">The results in Table 2 were achieved on the development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'36'", "'83'", "'121'", "'109'"]
'8'
'36'
'83'
'121'
'109'
['8', '36', '83', '121', '109']
Error in Discourse Facet
<S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="88" ssid = "17">For example  one parser could be more accurate at predicting noun phrases than the other parsers.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'55'", "'88'", "'21'", "'139'"]
'9'
'55'
'88'
'21'
'139'
['9', '55', '88', '21', '139']
Error in Discourse Facet
<S sid ="10" ssid = "6">In both cases the investigators were able to achieve significant improvements over the previous best tagging results.</S><S sid ="136" ssid = "65">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="141" ssid = "3">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid ="134" ssid = "63">As seen by the drop in average individual parser performance baseline  the introduced parser does not perform very well.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'136'", "'85'", "'141'", "'134'"]
'10'
'136'
'85'
'141'
'134'
['10', '136', '85', '141', '134']
Error in Discourse Facet
<S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'9'", "'6'", "'12'", "'13'"]
'11'
'9'
'6'
'12'
'13'
['11', '9', '6', '12', '13']
Error in Discourse Facet
<S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="6" ssid = "2">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'13'", "'11'", "'6'", "'72'"]
'12'
'13'
'11'
'6'
'72'
['12', '13', '11', '6', '72']
Error in Discourse Facet
<S sid ="13" ssid = "9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="4" ssid = "4">The resulting parsers surpass the best previously published performance results for the Penn Treebank.</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'12'", "'4'", "'72'", "'130'"]
'13'
'12'
'4'
'72'
'130'
['13', '12', '4', '72', '130']
Error in Discourse Facet
<S sid ="14" ssid = "10">We used these three parsers to explore parser combination techniques.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="126" ssid = "55">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S><S sid ="140" ssid = "2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'139'", "'85'", "'126'", "'140'"]
'14'
'139'
'85'
'126'
'140'
['14', '139', '85', '126', '140']
Error in Discourse Facet
<S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid ="85" ssid = "14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid ="88" ssid = "17">For example  one parser could be more accurate at predicting noun phrases than the other parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'55'", "'27'", "'85'", "'88'"]
'15'
'55'
'27'
'85'
'88'
['15', '55', '27', '85', '88']
Error in Discourse Facet
<S sid ="16" ssid = "2">We call this approach parse hybridization.</S><S sid ="58" ssid = "44">We call this approach parser switching.</S><S sid ="23" ssid = "9">We call this technique constituent voting.</S><S sid ="96" ssid = "25">We call such a constituent an isolated constituent.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'58'", "'23'", "'96'", "'139'"]
'16'
'58'
'23'
'96'
'139'
['16', '58', '23', '96', '139']
Error in Discourse Facet
<S sid ="17" ssid = "3">The substructures that are unanimously hypothesized by the parsers should be preserved after combination  and the combination technique should not foolishly create substructures for which there is no supporting evidence.</S><S sid ="49" ssid = "35">In general  the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.</S><S sid ="117" ssid = "46">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S><S sid ="125" ssid = "54">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S><S sid ="95" ssid = "24">One side of the decision making process is when we choose to believe a constituent should be in the parse  even though only one parser suggests it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'49'", "'117'", "'125'", "'95'"]
'17'
'49'
'117'
'125'
'95'
['17', '49', '117', '125', '95']
Error in Discourse Facet
["In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers."]
['Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.', 'One side of the decision making process is when we choose to believe a constituent should be in the parse  even though only one parser suggests it.', u'The constituent voting and na\xc3\xafve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.', 'The substructures that are unanimously hypothesized by the parsers should be preserved after combination  and the combination technique should not foolishly create substructures for which there is no supporting evidence.', 'In general  the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.00940', '(95%-conf.int.', '0.00940', '-', '0.00940)']
['system', 'ROUGE-S*', 'Average_P:', '0.07143', '(95%-conf.int.', '0.07143', '-', '0.07143)']
['system', 'ROUGE-S*', 'Average_F:', '0.01661', '(95%-conf.int.', '0.01661', '-', '0.01661)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:210', 'F:15']
["In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers."]
['We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.', 'For example  one parser could be more accurate at predicting noun phrases than the other parsers.', u'Another technique for parse hybridization is to use a na\xc3\xafve Bayes classifier to determine which constituents to include in the parse.', 'We are interested in combining the substructures of the input parses to produce a better parse.', 'We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.02114', '(95%-conf.int.', '0.02114', '-', '0.02114)']
['system', 'ROUGE-S*', 'Average_P:', '0.09524', '(95%-conf.int.', '0.09524', '-', '0.09524)']
['system', 'ROUGE-S*', 'Average_F:', '0.03460', '(95%-conf.int.', '0.03460', '-', '0.03460)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:210', 'F:20']
['We have presented two general approaches to studying parser combination: parser switching and parse hybridization.']
['Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).', 'These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).', 'The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).', 'The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.', 'The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).']
['system', 'ROUGE-S*', 'Average_R:', '0.00154', '(95%-conf.int.', '0.00154', '-', '0.00154)']
['system', 'ROUGE-S*', 'Average_P:', '0.11111', '(95%-conf.int.', '0.11111', '-', '0.11111)']
['system', 'ROUGE-S*', 'Average_F:', '0.00304', '(95%-conf.int.', '0.00304', '-', '0.00304)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3240', 'P:45', 'F:5']
['We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.']
['We have presented two general approaches to studying parser combination: parser switching and parse hybridization.', 'We call such a constituent an isolated constituent.', 'We call this technique constituent voting.', 'We call this approach parser switching.', 'We call this approach parse hybridization.']
['system', 'ROUGE-S*', 'Average_R:', '0.02462', '(95%-conf.int.', '0.02462', '-', '0.02462)']
['system', 'ROUGE-S*', 'Average_P:', '0.22222', '(95%-conf.int.', '0.22222', '-', '0.22222)']
['system', 'ROUGE-S*', 'Average_F:', '0.04432', '(95%-conf.int.', '0.04432', '-', '0.04432)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:325', 'P:36', 'F:8']
['The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.']
['Both parametric and non-parametric models are explored.', 'Two general approaches are presented and two combination techniques are described for each approach.', 'Once again we present both a non-parametric and a parametric technique for this task.', 'We have presented two general approaches to studying parser combination: parser switching and parse hybridization.', 'We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.02317', '(95%-conf.int.', '0.02317', '-', '0.02317)']
['system', 'ROUGE-S*', 'Average_P:', '0.04333', '(95%-conf.int.', '0.04333', '-', '0.04333)']
['system', 'ROUGE-S*', 'Average_F:', '0.03020', '(95%-conf.int.', '0.03020', '-', '0.03020)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:300', 'F:13']
['The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.']
[u'This is equivalent to the assumption used in probability estimation for na\xefve Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.', 'The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.', 'IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.', 'Their theoretical finding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent  accurate classifiers.', u'Under certain conditions the constituent voting and na\xefve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.']
['system', 'ROUGE-S*', 'Average_R:', '0.01493', '(95%-conf.int.', '0.01493', '-', '0.01493)']
['system', 'ROUGE-S*', 'Average_P:', '0.11000', '(95%-conf.int.', '0.11000', '-', '0.11000)']
['system', 'ROUGE-S*', 'Average_F:', '0.02628', '(95%-conf.int.', '0.02628', '-', '0.02628)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:300', 'F:33']
['The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.']
['The theory has also been validated empirically.', 'The results in Table 2 were achieved on the development set.', 'The estimation of the probabilities in the model is carried out as shown in Equation 4.', 'Table 3 contains the results for evaluating our systems on the test set (section 22).', 'We performed three experiments to evaluate our techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:406', 'P:300', 'F:0']
['In our particular case the majority requires the agreement of only two parsers because we have only three.']
['Surprisingly  the non-parametric switching technique also exhibited robust behaviour in this situation.', 'Both parametric and non-parametric models are explored.', 'Two general approaches are presented and two combination techniques are described for each approach.', 'Once again we present both a non-parametric and a parametric technique for this task.', 'The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:10', 'F:0']
['The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.']
['Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).', 'Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).', 'These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).', 'The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).', 'The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:45', 'F:0']
0.0725922214156 0.0105333332163 0.0172277775864





input/ref/Task1/W06-2932_swastika.csv
input/res/Task1/W06-2932.csv
parsing: input/ref/Task1/W06-2932_swastika.csv
<S sid="86" ssid="8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S>
original cit marker offset is 
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="54" ssid="2">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



['54']
54
['54']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



['12']
12
['12']
parsed_discourse_facet ['method_citation']
    <S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



['41']
41
['41']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="6">Its power lies in the ability to define a rich set of features over parsing decisions, as well as surface level features relative to these decisions.</S>
original cit marker offset is 0
new cit marker offset is 0



['24']
24
['24']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



['12']
12
['12']
parsed_discourse_facet ['method_citation']
<S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



['43']
43
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S sid ="1" ssid = "1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S><S sid ="9" ssid = "5">Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S><S sid ="23" ssid = "5">That system uses MIRA  an online large-margin learning algorithm  to compute model parameters.</S><S sid ="27" ssid = "9">We augmented this model to incorporate morphological features derived from each token.</S>
original cit marker offset is nan
new cit marker offset is 0



["'1'", "'9'", "'107'", "'23'", "'27'"]
'1'
'9'
'107'
'23'
'27'
['1', '9', '107', '23', '27']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="14" ssid = "10">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S><S sid ="58" ssid = "6">These results show that the discriminative spanning tree parsing framework (McDonald et al.  2005b; McDonald and Pereira  2006) is easily adapted across all these languages.</S><S sid ="25" ssid = "7">For instance  the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'20'", "'14'", "'58'", "'25'"]
'2'
'20'
'14'
'58'
'25'
['2', '20', '14', '58', '25']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S><S sid ="19" ssid = "1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S><S sid ="109" ssid = "6">The current system simply includes all morphological bi-gram features.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="51" ssid = "20">Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'19'", "'109'", "'10'", "'51'"]
'3'
'19'
'109'
'10'
'51'
['3', '19', '109', '10', '51']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.</S><S sid ="53" ssid = "1">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).</S><S sid ="44" ssid = "13">We use the MIRA online learner to set the weights (Crammer and Singer  2003; McDonald et al.  2005a) since we found it trained quickly and provide good performance.</S><S sid ="13" ssid = "9">We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajiˇc et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B¨ohmov´a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dˇzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).</S><S sid ="56" ssid = "4">Results on the test set are given in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'53'", "'44'", "'13'", "'56'"]
'4'
'53'
'44'
'13'
'56'
['4', '53', '44', '13', '56']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="24" ssid = "6">Its power lies in the ability to define a rich set of features over parsing decisions  as well as surface level features relative to these decisions.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'10'", "'88'", "'24'", "'107'"]
'5'
'10'
'88'
'24'
'107'
['5', '10', '88', '24', '107']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.</S><S sid ="14" ssid = "10">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'14'", "'7'", "'36'", "'8'"]
'6'
'14'
'7'
'36'
'8'
['6', '14', '7', '36', '8']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="110" ssid = "7">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="26" ssid = "8">These features are highly important to overall accuracy since they eliminate unlikely scenarios such as a preposition modifying a noun not directly to its left  or a noun modifying a verb with another verb occurring between them.</S><S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'110'", "'11'", "'26'", "'8'"]
'7'
'110'
'11'
'26'
'8'
['7', '110', '11', '26', '8']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="30" ssid = "12">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="6" ssid = "2">With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'7'", "'30'", "'11'", "'6'"]
'8'
'7'
'30'
'11'
'6'
['8', '7', '30', '11', '6']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="5" ssid = "1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S><S sid ="25" ssid = "7">For instance  the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'20'", "'88'", "'5'", "'25'"]
'9'
'20'
'88'
'5'
'25'
['9', '20', '88', '5', '25']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="5" ssid = "1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S><S sid ="24" ssid = "6">Its power lies in the ability to define a rich set of features over parsing decisions  as well as surface level features relative to these decisions.</S><S sid ="34" ssid = "3">However  the parser is fundamentally limited by the scope of local factorizations that make inference tractable.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'5'", "'24'", "'34'", "'107'"]
'10'
'5'
'24'
'34'
'107'
['10', '5', '24', '34', '107']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid ="110" ssid = "7">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S><S sid ="45" ssid = "14">Furthermore  it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira  2006).</S><S sid ="8" ssid = "4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'7'", "'110'", "'45'", "'8'"]
'11'
'7'
'110'
'45'
'8'
['11', '7', '110', '45', '8']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S sid ="95" ssid = "17">Note the difference in error between the unlabeled parser and the edge labeler: the former makes mistakes on edges into prepositions  conjunctions and verbs  and the latter makes mistakes on edges into nouns (subject/objects).</S><S sid ="110" ssid = "7">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S><S sid ="69" ssid = "7">However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).</S><S sid ="106" ssid = "3">First  we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'95'", "'110'", "'69'", "'106'"]
'12'
'95'
'110'
'69'
'106'
['12', '95', '110', '69', '106']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajiˇc et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B¨ohmov´a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dˇzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).</S><S sid ="54" ssid = "2">Based on performance from a held-out section of the training data  we used non-projective parsing algorithms for Czech  Danish  Dutch  German  Japanese  Portuguese and Slovene  and projective parsing algorithms for Arabic  Bulgarian  Chinese  Spanish  Swedish and Turkish.</S><S sid ="4" ssid = "4">We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.</S><S sid ="53" ssid = "1">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).</S><S sid ="14" ssid = "10">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'54'", "'4'", "'53'", "'14'"]
'13'
'54'
'4'
'53'
'14'
['13', '54', '4', '53', '14']
parsed_discourse_facet ['method_citation']
['An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.']
['Its power lies in the ability to define a rich set of features over parsing decisions  as well as surface level features relative to these decisions.', 'Dependency graphs also encode much of the deep syntactic information needed for further processing.', 'Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.', 'Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.', 'It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.']
['system', 'ROUGE-S*', 'Average_R:', '0.00435', '(95%-conf.int.', '0.00435', '-', '0.00435)']
['system', 'ROUGE-S*', 'Average_P:', '0.04412', '(95%-conf.int.', '0.04412', '-', '0.04412)']
['system', 'ROUGE-S*', 'Average_F:', '0.00793', '(95%-conf.int.', '0.00793', '-', '0.00793)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:136', 'F:6']
['An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.']
['This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).', 'This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.', 'These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.', u"However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'\xcb\u2021cuk  1988).", 'With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.']
['system', 'ROUGE-S*', 'Average_R:', '0.00444', '(95%-conf.int.', '0.00444', '-', '0.00444)']
['system', 'ROUGE-S*', 'Average_P:', '0.12500', '(95%-conf.int.', '0.12500', '-', '0.12500)']
['system', 'ROUGE-S*', 'Average_F:', '0.00858', '(95%-conf.int.', '0.00858', '-', '0.00858)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3828', 'P:136', 'F:17']
['Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.']
['The first stage of our system creates an unlabeled parse y for an input sentence x.', 'Dependency graphs also encode much of the deep syntactic information needed for further processing.', 'The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.', 'The current system simply includes all morphological bi-gram features.', 'Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:171', 'F:0']
['Its power lies in the ability to define a rich set of features over parsing decisions, as well as surface level features relative to these decisions.']
['Its power lies in the ability to define a rich set of features over parsing decisions  as well as surface level features relative to these decisions.', 'Dependency graphs also encode much of the deep syntactic information needed for further processing.', 'Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.', 'It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.', 'However  the parser is fundamentally limited by the scope of local factorizations that make inference tractable.']
['system', 'ROUGE-S*', 'Average_R:', '0.06863', '(95%-conf.int.', '0.06863', '-', '0.06863)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.12844', '(95%-conf.int.', '0.12844', '-', '0.12844)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:91', 'F:91']
['For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.']
['Based on performance from a held-out section of the training data  we used non-projective parsing algorithms for Czech  Danish  Dutch  German  Japanese  Portuguese and Slovene  and projective parsing algorithms for Arabic  Bulgarian  Chinese  Spanish  Swedish and Turkish.', u'We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Haji\u02c7c et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B\xa8ohmov\xb4a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; D\u02c7zeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).', 'We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.', 'We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).', 'The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:7381', 'P:231', 'F:0']
['In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).', 'This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.', 'These features are highly important to overall accuracy since they eliminate unlikely scenarios such as a preposition modifying a noun not directly to its left  or a noun modifying a verb with another verb occurring between them.', 'It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.', u"However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'\xcb\u2021cuk  1988)."]
['system', 'ROUGE-S*', 'Average_R:', '0.00112', '(95%-conf.int.', '0.00112', '-', '0.00112)']
['system', 'ROUGE-S*', 'Average_P:', '0.06061', '(95%-conf.int.', '0.06061', '-', '0.06061)']
['system', 'ROUGE-S*', 'Average_F:', '0.00220', '(95%-conf.int.', '0.00220', '-', '0.00220)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3570', 'P:66', 'F:4']
['Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.']
['This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.', 'However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.', 'The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.', u"However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'\xcb\u2021cuk  1988).", 'With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.']
['system', 'ROUGE-S*', 'Average_R:', '0.00111', '(95%-conf.int.', '0.00111', '-', '0.00111)']
['system', 'ROUGE-S*', 'Average_P:', '0.01000', '(95%-conf.int.', '0.01000', '-', '0.01000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00200', '(95%-conf.int.', '0.00200', '-', '0.00200)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2701', 'P:300', 'F:3']
['To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.']
['Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.', 'Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.', 'Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.', 'This system is primarily based on the parsing models described by McDonald and Pereira (2006).', 'For instance  the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:378', 'F:0']
['Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.']
['We use the MIRA online learner to set the weights (Crammer and Singer  2003; McDonald et al.  2005a) since we found it trained quickly and provide good performance.', 'Results on the test set are given in Table 1.', u'We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Haji\u02c7c et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B\xa8ohmov\xb4a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; D\u02c7zeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).', 'We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.', 'We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5671', 'P:171', 'F:0']
['Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.']
['This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).', 'This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.', 'Furthermore  it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira  2006).', 'It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.', u"However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'\xcb\u2021cuk  1988)."]
['system', 'ROUGE-S*', 'Average_R:', '0.00031', '(95%-conf.int.', '0.00031', '-', '0.00031)']
['system', 'ROUGE-S*', 'Average_P:', '0.00585', '(95%-conf.int.', '0.00585', '-', '0.00585)']
['system', 'ROUGE-S*', 'Average_F:', '0.00059', '(95%-conf.int.', '0.00059', '-', '0.00059)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3240', 'P:171', 'F:1']
0.124557998754 0.00799599992004 0.0149739998503





input/ref/Task1/J01-2004_sweta.csv
input/res/Task1/J01-2004.csv
parsing: input/ref/Task1/J01-2004_sweta.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["372'"]
372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="28">The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.</S>
    <S sid="41" ssid="29">There will also be a brief review of previous work using syntactic information for language modeling, before we introduce our model in Section 4.</S>
    <S sid="42" ssid="30">Three parse trees: (a) a complete parse tree; (b) a complete parse tree with an explicit stop symbol; and (c) a partial parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["40'", "'41'", "'42'"]
40'
'41'
'42'
['40', '41', '42']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="13">A parser that is not left to right, but which has rooted derivations, e.g., a headfirst parser, will be able to calculate generative joint probabilities for entire strings; however, it will not be able to calculate probabilities for each word conditioned on previously generated words, unless each derivation generates the words in the string in exactly the same order.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="364" ssid="120">We follow Chelba (2000) in dealing with this problem: for parsing purposes, we use the Penn Treebank tokenization; for interpolation with the provided trigram model, and for evaluation, the lattice tokenization is used.</S>
original cit marker offset is 0
new cit marker offset is 0



["364'"]
364'
['364']
parsed_discourse_facet ['method_citation']
<S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



["302'"]
302'
['302']
parsed_discourse_facet ['method_citation']
 <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="231" ssid="135">Since we do not know the POS for the word, we must sum the LAP for all POS For a PCFG G, a stack S = Ao An$ (which we will write AN and a look-ahead terminal item wi, we define the look-ahead probability as follows: We recursively estimate this with two empirically observed conditional probabilities for every nonterminal A,: 13(A, w,a) and P(A, c).</S>
original cit marker offset is 0
new cit marker offset is 0



["231'"]
231'
['231']
parsed_discourse_facet ['method_citation']
<S sid="297" ssid="53">The differences between a k-best and a beam-search parser (not to mention the use of dynamic programming) make a running time difference unsurprising.</S>
original cit marker offset is 0
new cit marker offset is 0



["297'"]
297'
['297']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="37">Statistically based heuristic best-first or beam-search strategies (Caraballo and Charniak 1998; Charniak, Goldwater, and Johnson 1998; Goodman 1997) have yielded an enormous improvement in the quality and speed of parsers, even without any guarantee that the parse returned is, in fact, that with the maximum likelihood for the probability model.</S>
original cit marker offset is 0
new cit marker offset is 0



["133'"]
133'
['133']
parsed_discourse_facet ['method_citation']
<S sid="291" ssid="47">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["291'"]
291'
['291']
parsed_discourse_facet ['method_citation']
<S sid="355" ssid="111">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.</S>
original cit marker offset is 0
new cit marker offset is 0



["355'"]
355'
['355']
parsed_discourse_facet ['method_citation']
<S sid="59" ssid="17">A PCFG is a CFG with a probability assigned to each rule; specifically, each righthand side has a probability given the left-hand side of the rule.</S>
original cit marker offset is 0
new cit marker offset is 0



["59'"]
59'
['59']
parsed_discourse_facet ['method_citation']
<S sid="100" ssid="4">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="12">Another approach that uses syntactic structure for language modeling has been to use a shift-reduce parser to &amp;quot;surface&amp;quot; c-commanding phrasal headwords or part-of-speech (POS) tags from arbitrarily far back in the prefix string, for use in a trigram-like model.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S sid ="1" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="7" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="209" ssid = "113">This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).</S><S sid ="132" ssid = "36">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid ="277" ssid = "33">The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989 860 words  39 832 sentences) of the Penn Treebank (Marcus  Santorini  and Marcinkiewicz 1993) served as the training data  Section 24 (34 199 words  1 346 sentences) as the held-out data for parameter estimation  and Section 23 (59 100 words  2 416 sentences) as the test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'7'", "'209'", "'132'", "'277'"]
'1'
'7'
'209'
'132'
'277'
['1', '7', '209', '132', '277']
parsed_discourse_facet ['aim_citation']
<S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="41" ssid = "29">There will also be a brief review of previous work using syntactic information for language modeling  before we introduce our model in Section 4.</S><S sid ="18" ssid = "6">Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture long-distance dependencies for language modeling will be reviewed.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'2'", "'41'", "'18'", "'4'"]
'8'
'2'
'41'
'18'
'4'
['8', '2', '41', '18', '4']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="9" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'390'", "'10'", "'4'"]
'3'
'9'
'390'
'10'
'4'
['3', '9', '390', '10', '4']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="100" ssid = "4">The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'100'", "'2'", "'8'"]
'10'
'4'
'100'
'2'
'8'
['10', '4', '100', '2', '8']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="11" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="379" ssid = "135">While the improvements over the trigram model in these trials are modest  they do indicate that our model is robust enough to provide good information even in the face of noisy input.</S><S sid ="300" ssid = "56">The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'11'", "'379'", "'300'", "'404'"]
'5'
'11'
'379'
'300'
'404'
['5', '11', '379', '300', '404']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="6" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="39" ssid = "27">The top-down guidance that is provided makes this approach quite efficient in practice.</S><S sid ="388" ssid = "1">The empirical results presented above are quite encouraging  and the potential of this kind of approach both for parsing and language modeling seems very promising.</S><S sid ="114" ssid = "18">He was able to reduce both sentence and word error rates on the ATIS corpus using this method.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'6'", "'39'", "'388'", "'114'"]
'12'
'6'
'39'
'388'
'114'
['12', '6', '39', '388', '114']
parsed_discourse_facet ['method_citation']
<S sid ="1" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="7" ssid = "1">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid ="209" ssid = "113">This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).</S><S sid ="132" ssid = "36">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid ="277" ssid = "33">The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989 860 words  39 832 sentences) of the Penn Treebank (Marcus  Santorini  and Marcinkiewicz 1993) served as the training data  Section 24 (34 199 words  1 346 sentences) as the held-out data for parameter estimation  and Section 23 (59 100 words  2 416 sentences) as the test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'7'", "'209'", "'132'", "'277'"]
'1'
'7'
'209'
'132'
'277'
['1', '7', '209', '132', '277']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="41" ssid = "29">There will also be a brief review of previous work using syntactic information for language modeling  before we introduce our model in Section 4.</S><S sid ="18" ssid = "6">Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture long-distance dependencies for language modeling will be reviewed.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'2'", "'41'", "'18'", "'4'"]
'8'
'2'
'41'
'18'
'4'
['8', '2', '41', '18', '4']
parsed_discourse_facet ['results_citation']
<S sid ="3" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="9" ssid = "3">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'390'", "'10'", "'4'"]
'3'
'9'
'390'
'10'
'4'
['3', '9', '390', '10', '4']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="100" ssid = "4">The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S sid ="2" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid ="8" ssid = "2">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'4'", "'100'", "'2'", "'8'"]
'10'
'4'
'100'
'2'
'8'
['10', '4', '100', '2', '8']
parsed_discourse_facet ['method_citation']
<S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="11" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="379" ssid = "135">While the improvements over the trigram model in these trials are modest  they do indicate that our model is robust enough to provide good information even in the face of noisy input.</S><S sid ="300" ssid = "56">The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'11'", "'379'", "'300'", "'404'"]
'5'
'11'
'379'
'300'
'404'
['5', '11', '379', '300', '404']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="6" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S><S sid ="39" ssid = "27">The top-down guidance that is provided makes this approach quite efficient in practice.</S><S sid ="388" ssid = "1">The empirical results presented above are quite encouraging  and the potential of this kind of approach both for parsing and language modeling seems very promising.</S><S sid ="114" ssid = "18">He was able to reduce both sentence and word error rates on the ATIS corpus using this method.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'6'", "'39'", "'388'", "'114'"]
'12'
'6'
'39'
'388'
'114'
['12', '6', '39', '388', '114']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "1">With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.</S><S sid ="14" ssid = "2">Perhaps one reason for this is that  until relatively recently  few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.</S><S sid ="15" ssid = "3">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S><S sid ="383" ssid = "139">It was selected with the goal of high parser accuracy; but in this new domain  parser accuracy is a secondary measure of performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'14'", "'15'", "'404'", "'383'"]
'13'
'14'
'15'
'404'
'383'
['13', '14', '15', '404', '383']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "2">Perhaps one reason for this is that  until relatively recently  few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.</S><S sid ="13" ssid = "1">With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.</S><S sid ="300" ssid = "56">The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.</S><S sid ="15" ssid = "3">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid ="319" ssid = "75">The hope  however  is that the improved parsing model provided by our conditional probability model will cause the distribution over structures to be more peaked  thus enabling us to capture more of the total probability mass  and making this a fairly snug upper bound on the perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'13'", "'300'", "'15'", "'319'"]
'14'
'13'
'300'
'15'
'319'
['14', '13', '300', '15', '319']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "3">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid ="97" ssid = "1">There have been attempts to jump over adjacent words to words farther back in the left context  without the use of dependency links or syntactic structure  for example Saul and Pereira (1997) and Rosenfeld (1996  1997).</S><S sid ="13" ssid = "1">With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.</S><S sid ="38" ssid = "26">This has lead us to a formulation of the conditional probability model in terms of values returned from tree-walking functions that themselves are contextually sensitive.</S><S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'97'", "'13'", "'38'", "'5'"]
'15'
'97'
'13'
'38'
'5'
['15', '97', '13', '38', '5']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "4">While language models built around shallow local dependencies are still the standard in state-of-the-art speech recognition systems  there is reason to hope that better language models can and will be developed by computational linguists for this task.</S><S sid ="397" ssid = "10">A top-down parser  in contrast to a standard bottom-up chart parser  has enough information to predict empty categories only where they are likely to occur.</S><S sid ="139" ssid = "43">This on-line characteristic allows our language model to be interpolated on a word-by-word basis with other models  such as the trigram  yielding further improvements.</S><S sid ="4" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid ="10" ssid = "4">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'397'", "'139'", "'4'", "'10'"]
'16'
'397'
'139'
'4'
'10'
['16', '397', '139', '4', '10']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "5">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S sid ="132" ssid = "36">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid ="88" ssid = "46">This section will briefly introduce language modeling for statistical speech recognition.'</S><S sid ="40" ssid = "28">The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.</S><S sid ="247" ssid = "3">Perplexity is a standard measure within the speech recognition community for comparing language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'132'", "'88'", "'40'", "'247'"]
'17'
'132'
'88'
'40'
'247'
['17', '132', '88', '40', '247']
parsed_discourse_facet ['method_citation']
['Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).']
['Perplexity is a standard measure within the speech recognition community for comparing language models.', 'The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.', "This section will briefly introduce language modeling for statistical speech recognition.'", 'The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.', 'This paper will examine language modeling for speech recognition from a natural language processing point of view.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:300', 'F:0']
['The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.']
['With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.', 'Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.', 'In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.', 'Perhaps one reason for this is that  until relatively recently  few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.', 'It was selected with the goal of high parser accuracy; but in this new domain  parser accuracy is a secondary measure of performance.']
['system', 'ROUGE-S*', 'Average_R:', '0.00316', '(95%-conf.int.', '0.00316', '-', '0.00316)']
['system', 'ROUGE-S*', 'Average_P:', '0.02849', '(95%-conf.int.', '0.02849', '-', '0.02849)']
['system', 'ROUGE-S*', 'Average_F:', '0.00570', '(95%-conf.int.', '0.00570', '-', '0.00570)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:351', 'F:10']
['Since we do not know the POS for the word, we must sum the LAP for all POS For a PCFG G, a stack S = Ao An$ (which we will write AN and a look-ahead terminal item wi, we define the look-ahead probability as follows: We recursively estimate this with two empirically observed conditional probabilities for every nonterminal A,: 13(A, w,a) and P(A, c).']
['This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.', 'This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).', 'The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989 860 words  39 832 sentences) of the Penn Treebank (Marcus  Santorini  and Marcinkiewicz 1993) served as the training data  Section 24 (34 199 words  1 346 sentences) as the held-out data for parameter estimation  and Section 23 (59 100 words  2 416 sentences) as the test data.', 'This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.', 'The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.']
['system', 'ROUGE-S*', 'Average_R:', '0.00061', '(95%-conf.int.', '0.00061', '-', '0.00061)']
['system', 'ROUGE-S*', 'Average_P:', '0.01087', '(95%-conf.int.', '0.01087', '-', '0.01087)']
['system', 'ROUGE-S*', 'Average_F:', '0.00115', '(95%-conf.int.', '0.00115', '-', '0.00115)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4950', 'P:276', 'F:3']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.', 'This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).', 'The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989 860 words  39 832 sentences) of the Penn Treebank (Marcus  Santorini  and Marcinkiewicz 1993) served as the training data  Section 24 (34 199 words  1 346 sentences) as the held-out data for parameter estimation  and Section 23 (59 100 words  2 416 sentences) as the test data.', 'This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.', 'The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.']
['system', 'ROUGE-S*', 'Average_R:', '0.00061', '(95%-conf.int.', '0.00061', '-', '0.00061)']
['system', 'ROUGE-S*', 'Average_P:', '0.03297', '(95%-conf.int.', '0.03297', '-', '0.03297)']
['system', 'ROUGE-S*', 'Average_F:', '0.00119', '(95%-conf.int.', '0.00119', '-', '0.00119)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4950', 'P:91', 'F:3']
['In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.']
['Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.', 'While the improvements over the trigram model in these trials are modest  they do indicate that our model is robust enough to provide good information even in the face of noisy input.', 'The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.', 'Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.', 'Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00032', '(95%-conf.int.', '0.00032', '-', '0.00032)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00063', '(95%-conf.int.', '0.00063', '-', '0.00063)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:105', 'F:1']
['Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.']
['A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.', 'The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.', 'The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.', 'A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.', 'The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.']
['system', 'ROUGE-S*', 'Average_R:', '0.00327', '(95%-conf.int.', '0.00327', '-', '0.00327)']
['system', 'ROUGE-S*', 'Average_P:', '0.03218', '(95%-conf.int.', '0.03218', '-', '0.03218)']
['system', 'ROUGE-S*', 'Average_F:', '0.00594', '(95%-conf.int.', '0.00594', '-', '0.00594)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4278', 'P:435', 'F:14']
['A PCFG is a CFG with a probability assigned to each rule; specifically, each righthand side has a probability given the left-hand side of the rule.']
['The empirical results presented above are quite encouraging  and the potential of this kind of approach both for parsing and language modeling seems very promising.', 'He was able to reduce both sentence and word error rates on the ATIS corpus using this method.', 'The top-down guidance that is provided makes this approach quite efficient in practice.', 'A small recognition experiment also demonstrates the utility of the model.', 'A small recognition experiment also demonstrates the utility of the model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:78', 'F:0']
['Three parse trees: (a) a complete parse tree; (b) a complete parse tree with an explicit stop symbol; and (c) a partial parse tree.', 'The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.', 'There will also be a brief review of previous work using syntactic information for language modeling, before we introduce our model in Section 4.']
['The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.', 'The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.', 'A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.', 'Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture long-distance dependencies for language modeling will be reviewed.', 'There will also be a brief review of previous work using syntactic information for language modeling  before we introduce our model in Section 4.']
['system', 'ROUGE-S*', 'Average_R:', '0.05261', '(95%-conf.int.', '0.05261', '-', '0.05261)']
['system', 'ROUGE-S*', 'Average_P:', '0.23175', '(95%-conf.int.', '0.23175', '-', '0.23175)']
['system', 'ROUGE-S*', 'Average_F:', '0.08576', '(95%-conf.int.', '0.08576', '-', '0.08576)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:630', 'F:146']
0.0432224994597 0.00757249990534 0.0125462498432





input/ref/Task1/P05-1013_vardha.csv
input/res/Task1/P05-1013.csv
parsing: input/ref/Task1/P05-1013_vardha.csv
 <S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
  <S sid="9" ssid="5">This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
  <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
    <S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
 <S sid="62" ssid="1">In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al., 2004) and English (Nivre and Scholz, 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="23" ssid="19">By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
    <S sid="96" ssid="7">With respect to exact match, the improvement is even more noticeable, which shows quite clearly that even if non-projective dependencies are rare on the token level, they are nevertheless important for getting the global syntactic structure correct.</S>
original cit marker offset is 0
new cit marker offset is 0



["'96'"]
'96'
['96']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
  <S sid="40" ssid="11">Even this may be nondeterministic, in case the graph contains several non-projective arcs whose lifts interact, but we use the following algorithm to construct a minimal projective transformation D0 = (W, A0) of a (nonprojective) dependency graph D = (W, A): The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right).</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
  <S sid="7" ssid="3">From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="14" ssid="10">While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
 <S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'"]
'49'
['49']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P05-1013.csv
<S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="13" ssid = "9">The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="16" ssid = "12">Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.</S><S sid ="80" ssid = "7">As shown in Table 3  the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'13'", "'7'", "'16'", "'80'"]
'1'
'13'
'7'
'16'
'80'
['1', '13', '7', '16', '80']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="44" ssid = "15">Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="30" ssid = "1">We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.</S><S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'44'", "'36'", "'30'", "'8'"]
'2'
'44'
'36'
'30'
'8'
['2', '44', '36', '30', '8']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy.</S><S sid ="28" ssid = "24">First  in section 4  we evaluate the graph transformation techniques in themselves  with data from the Prague Dependency Treebank and the Danish Dependency Treebank.</S><S sid ="29" ssid = "25">In section 5  we then evaluate the entire parsing system by training and evaluating on data from the Prague Dependency Treebank.</S><S sid ="62" ssid = "1">In the experiments below  we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs 3 previously tested on Swedish (Nivre et al.  2004) and English (Nivre and Scholz  2004).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'28'", "'29'", "'62'", "'75'"]
'3'
'28'
'29'
'62'
'75'
['3', '28', '29', '62', '75']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'104'", "'106'", "'78'", "'8'"]
'4'
'104'
'106'
'78'
'8'
['4', '104', '106', '78', '8']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order  where discontinuous syntactic constructions are more common than in languages like English (Mel’ˇcuk  1988; Covington  1990).</S><S sid ="94" ssid = "5">The first thing to note is that projectivizing helps in itself  even if no encoding is used  as seen from the fact that the projective baseline outperforms the non-projective training condition by more than half a percentage point on attachment score  although the gain is much smaller with respect to exact match.</S><S sid ="99" ssid = "10">This may seem surprising  given the experiments reported in section 4  but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift  where the encoding of path information is often redundant.</S><S sid ="9" ssid = "5">This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).</S><S sid ="47" ssid = "18">In practice  we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'94'", "'99'", "'9'", "'47'"]
'5'
'94'
'99'
'9'
'47'
['5', '94', '99', '9', '47']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.</S><S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="44" ssid = "15">Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.</S><S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'2'", "'36'", "'44'", "'89'"]
'6'
'2'
'36'
'44'
'89'
['6', '2', '36', '44', '89']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="16" ssid = "12">Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.</S><S sid ="19" ssid = "15">Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).</S><S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="47" ssid = "18">In practice  we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'16'", "'19'", "'1'", "'47'"]
'7'
'16'
'19'
'1'
'47'
['7', '16', '19', '1', '47']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S><S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="30" ssid = "1">We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'2'", "'104'", "'4'", "'30'"]
'8'
'2'
'104'
'4'
'30'
['8', '2', '104', '4', '30']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).</S><S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="17" ssid = "13">There exist a few robust broad-coverage parsers that produce non-projective dependency structures  notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English  Foth et al. (2004) for German  and Holan (2004) for Czech.</S><S sid ="105" ssid = "16">Although the best published results for the Collins parser is 80% UAS (Collins  1999)  this parser reaches 82% when trained on the entire training data set  and an adapted version of Charniak’s parser (Charniak  2000) performs at 84% (Jan Hajiˇc  pers. comm.).</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'10'", "'17'", "'105'", "'24'"]
'9'
'10'
'17'
'105'
'24'
['9', '10', '17', '105', '24']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="42" ssid = "13">Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="12" ssid = "8">Prague Dependency Treebank (Hajiˇc et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'42'", "'36'", "'4'", "'12'"]
'10'
'42'
'36'
'4'
'12'
['10', '42', '36', '4', '12']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="6" ssid = "2">However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'6'", "'78'", "'10'", "'36'"]
'11'
'6'
'78'
'10'
'36'
['11', '6', '78', '10', '36']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Prague Dependency Treebank (Hajiˇc et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S><S sid ="10" ssid = "6">It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="42" ssid = "13">Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'10'", "'36'", "'42'", "'78'"]
'12'
'10'
'36'
'42'
'78'
['12', '10', '36', '42', '78']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.</S><S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'2'", "'95'", "'1'", "'104'"]
'13'
'2'
'95'
'1'
'104'
['13', '2', '95', '1', '104']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 15–25%  the total proportion of non-projective arcs is normally only 1–2%.</S><S sid ="80" ssid = "7">As shown in Table 3  the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S><S sid ="100" ssid = "11">It is likely that the more complex cases  where path information could make a difference  are beyond the reach of the parser in most cases.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'80'", "'106'", "'81'", "'100'"]
'14'
'80'
'106'
'81'
'100'
['14', '80', '106', '81', '100']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">As long as the main evaluation metric is dependency accuracy per word  with state-of-the-art accuracy mostly below 90%  the penalty for not handling non-projective constructions is almost negligible.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="101" ssid = "12">However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.</S><S sid ="110" ssid = "2">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="105" ssid = "16">Although the best published results for the Collins parser is 80% UAS (Collins  1999)  this parser reaches 82% when trained on the entire training data set  and an adapted version of Charniak’s parser (Charniak  2000) performs at 84% (Jan Hajiˇc  pers. comm.).</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'106'", "'101'", "'110'", "'105'"]
'15'
'106'
'101'
'110'
'105'
['15', '106', '101', '110', '105']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="1" ssid = "1">In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.</S><S sid ="110" ssid = "2">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="19" ssid = "15">Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'7'", "'1'", "'110'", "'19'"]
'16'
'7'
'1'
'110'
'19'
['16', '7', '1', '110', '19']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">This leads to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="8" ssid = "4">Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'104'", "'106'", "'78'", "'8'"]
'4'
'104'
'106'
'78'
'8'
['4', '104', '106', '78', '8']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">In addition  there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington  1990; Kahane et al.  1998; Duchier and Debusmann  2001; Holan et al.  2001; Hellwig  2003).</S><S sid ="17" ssid = "13">There exist a few robust broad-coverage parsers that produce non-projective dependency structures  notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English  Foth et al. (2004) for German  and Holan (2004) for Czech.</S><S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S><S sid ="100" ssid = "11">It is likely that the more complex cases  where path information could make a difference  are beyond the reach of the parser in most cases.</S><S sid ="109" ssid = "1">We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'17'", "'20'", "'100'", "'109'"]
'18'
'17'
'20'
'100'
'109'
['18', '17', '20', '100', '109']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "15">Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="13" ssid = "9">The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.</S><S sid ="107" ssid = "18">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'7'", "'95'", "'13'", "'107'"]
'19'
'7'
'95'
'13'
'107'
['19', '7', '95', '13', '107']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S><S sid ="109" ssid = "1">We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'109'", "'24'", "'60'", "'95'"]
'20'
'109'
'24'
'60'
'95'
['20', '109', '24', '60', '95']
parsed_discourse_facet ['aim_citation']
['The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
['It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).', u'Prague Dependency Treebank (Haji\u02c7c et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.', 'Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', 'The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.']
['system', 'ROUGE-S*', 'Average_R:', '0.00026', '(95%-conf.int.', '0.00026', '-', '0.00026)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00051', '(95%-conf.int.', '0.00051', '-', '0.00051)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3916', 'P:36', 'F:1']
['In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al., 2004) and English (Nivre and Scholz, 2004).']
['This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).', 'It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).', 'We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).', u'There exist a few robust broad-coverage parsers that produce non-projective dependency structures  notably Tapanainen and J\xa8arvinen (1997) and Wang and Harper (2004) for English  Foth et al. (2004) for German  and Holan (2004) for Czech.', u'Although the best published results for the Collins parser is 80% UAS (Collins  1999)  this parser reaches 82% when trained on the entire training data set  and an adapted version of Charniak\u2019s parser (Charniak  2000) performs at 84% (Jan Haji\u02c7c  pers. comm.).']
['system', 'ROUGE-S*', 'Average_R:', '0.01062', '(95%-conf.int.', '0.01062', '-', '0.01062)']
['system', 'ROUGE-S*', 'Average_P:', '0.22925', '(95%-conf.int.', '0.22925', '-', '0.22925)']
['system', 'ROUGE-S*', 'Average_F:', '0.02030', '(95%-conf.int.', '0.02030', '-', '0.02030)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5460', 'P:253', 'F:58']
['The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
['Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.', 'However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).', 'This leads to the best reported performance for robust non-projective parsing of Czech.', 'The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.', 'The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.']
['system', 'ROUGE-S*', 'Average_R:', '0.02424', '(95%-conf.int.', '0.02424', '-', '0.02424)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.04734', '(95%-conf.int.', '0.04734', '-', '0.04734)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:36', 'F:36']
['As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.']
['Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.', 'We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.', 'This leads to the best reported performance for robust non-projective parsing of Czech.', 'The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.', 'We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.01584', '(95%-conf.int.', '0.01584', '-', '0.01584)']
['system', 'ROUGE-S*', 'Average_P:', '0.05556', '(95%-conf.int.', '0.05556', '-', '0.05556)']
['system', 'ROUGE-S*', 'Average_F:', '0.02465', '(95%-conf.int.', '0.02465', '-', '0.02465)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:378', 'F:21']
['While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.']
['In addition  there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington  1990; Kahane et al.  1998; Duchier and Debusmann  2001; Holan et al.  2001; Hellwig  2003).', 'It is likely that the more complex cases  where path information could make a difference  are beyond the reach of the parser in most cases.', 'We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.', 'In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.', u'There exist a few robust broad-coverage parsers that produce non-projective dependency structures  notably Tapanainen and J\xc2\xa8arvinen (1997) and Wang and Harper (2004) for English  Foth et al. (2004) for German  and Holan (2004) for Czech.']
['system', 'ROUGE-S*', 'Average_R:', '0.00090', '(95%-conf.int.', '0.00090', '-', '0.00090)']
['system', 'ROUGE-S*', 'Average_P:', '0.03297', '(95%-conf.int.', '0.03297', '-', '0.03297)']
['system', 'ROUGE-S*', 'Average_F:', '0.00176', '(95%-conf.int.', '0.00176', '-', '0.00176)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3321', 'P:91', 'F:3']
['The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.']
['Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).', 'The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.', 'The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.', 'Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.', 'From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.']
['system', 'ROUGE-S*', 'Average_R:', '0.00820', '(95%-conf.int.', '0.00820', '-', '0.00820)']
['system', 'ROUGE-S*', 'Average_P:', '0.06883', '(95%-conf.int.', '0.06883', '-', '0.06883)']
['system', 'ROUGE-S*', 'Average_F:', '0.01466', '(95%-conf.int.', '0.01466', '-', '0.01466)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6216', 'P:741', 'F:51']
['The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
['It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).', u'Prague Dependency Treebank (Haji\u02c7c et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.', 'Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', 'This leads to the best reported performance for robust non-projective parsing of Czech.']
['system', 'ROUGE-S*', 'Average_R:', '0.00031', '(95%-conf.int.', '0.00031', '-', '0.00031)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00061', '(95%-conf.int.', '0.00061', '-', '0.00061)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3240', 'P:36', 'F:1']
['The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
['We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).', 'In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.', 'We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.', 'In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.', 'The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.']
['system', 'ROUGE-S*', 'Average_R:', '0.00538', '(95%-conf.int.', '0.00538', '-', '0.00538)']
['system', 'ROUGE-S*', 'Average_P:', '0.36111', '(95%-conf.int.', '0.36111', '-', '0.36111)']
['system', 'ROUGE-S*', 'Average_F:', '0.01061', '(95%-conf.int.', '0.01061', '-', '0.01061)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:36', 'F:13']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['This may seem surprising  given the experiments reported in section 4  but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift  where the encoding of path information is often redundant.', 'This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).', u'It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order  where discontinuous syntactic constructions are more common than in languages like English (Mel\u2019\u02c7cuk  1988; Covington  1990).', 'The first thing to note is that projectivizing helps in itself  even if no encoding is used  as seen from the fact that the projective baseline outperforms the non-projective training condition by more than half a percentage point on attachment score  although the gain is much smaller with respect to exact match.', 'In practice  we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00225', '(95%-conf.int.', '0.00225', '-', '0.00225)']
['system', 'ROUGE-S*', 'Average_P:', '0.13333', '(95%-conf.int.', '0.13333', '-', '0.13333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00443', '(95%-conf.int.', '0.00443', '-', '0.00443)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6216', 'P:105', 'F:14']
['The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\xcb\u2021c  1998).', 'Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy.', 'First  in section 4  we evaluate the graph transformation techniques in themselves  with data from the Prague Dependency Treebank and the Danish Dependency Treebank.', 'In section 5  we then evaluate the entire parsing system by training and evaluating on data from the Prague Dependency Treebank.', 'In the experiments below  we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs 3 previously tested on Swedish (Nivre et al.  2004) and English (Nivre and Scholz  2004).']
['system', 'ROUGE-S*', 'Average_R:', '0.00172', '(95%-conf.int.', '0.00172', '-', '0.00172)']
['system', 'ROUGE-S*', 'Average_P:', '0.16667', '(95%-conf.int.', '0.16667', '-', '0.16667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00341', '(95%-conf.int.', '0.00341', '-', '0.00341)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:36', 'F:6']
['This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004).']
['Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.', 'Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.', 'We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', 'We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.01364', '(95%-conf.int.', '0.01364', '-', '0.01364)']
['system', 'ROUGE-S*', 'Average_P:', '0.08466', '(95%-conf.int.', '0.08466', '-', '0.08466)']
['system', 'ROUGE-S*', 'Average_F:', '0.02349', '(95%-conf.int.', '0.02349', '-', '0.02349)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:378', 'F:32']
['Even this may be nondeterministic, in case the graph contains several non-projective arcs whose lifts interact, but we use the following algorithm to construct a minimal projective transformation D0 = (W, A0) of a (nonprojective) dependency graph D = (W, A): The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right).']
['In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.', 'Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).', 'The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.', 'From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.', 'Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.']
['system', 'ROUGE-S*', 'Average_R:', '0.00388', '(95%-conf.int.', '0.00388', '-', '0.00388)']
['system', 'ROUGE-S*', 'Average_P:', '0.04301', '(95%-conf.int.', '0.04301', '-', '0.04301)']
['system', 'ROUGE-S*', 'Average_F:', '0.00712', '(95%-conf.int.', '0.00712', '-', '0.00712)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5151', 'P:465', 'F:20']
['In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
['Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).', 'In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.', 'In practice  we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.', 'From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.', 'Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.']
['system', 'ROUGE-S*', 'Average_R:', '0.00307', '(95%-conf.int.', '0.00307', '-', '0.00307)']
['system', 'ROUGE-S*', 'Average_P:', '0.15385', '(95%-conf.int.', '0.15385', '-', '0.15385)']
['system', 'ROUGE-S*', 'Average_F:', '0.00602', '(95%-conf.int.', '0.00602', '-', '0.00602)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4560', 'P:91', 'F:14']
['In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
['In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.', 'The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.', 'As shown in Table 3  the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.', 'From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.', 'Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.']
['system', 'ROUGE-S*', 'Average_R:', '0.00439', '(95%-conf.int.', '0.00439', '-', '0.00439)']
['system', 'ROUGE-S*', 'Average_P:', '0.10989', '(95%-conf.int.', '0.10989', '-', '0.10989)']
['system', 'ROUGE-S*', 'Average_F:', '0.00844', '(95%-conf.int.', '0.00844', '-', '0.00844)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:91', 'F:10']
['From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.']
['Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.', 'However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).', 'This leads to the best reported performance for robust non-projective parsing of Czech.', 'The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.', 'The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.']
['system', 'ROUGE-S*', 'Average_R:', '0.00673', '(95%-conf.int.', '0.00673', '-', '0.00673)']
['system', 'ROUGE-S*', 'Average_P:', '0.07353', '(95%-conf.int.', '0.07353', '-', '0.07353)']
['system', 'ROUGE-S*', 'Average_F:', '0.01234', '(95%-conf.int.', '0.01234', '-', '0.01234)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:136', 'F:10']
['The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.']
['Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.', 'We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.', 'The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', 'However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.']
['system', 'ROUGE-S*', 'Average_R:', '0.00124', '(95%-conf.int.', '0.00124', '-', '0.00124)']
['system', 'ROUGE-S*', 'Average_P:', '0.02206', '(95%-conf.int.', '0.02206', '-', '0.02206)']
['system', 'ROUGE-S*', 'Average_F:', '0.00235', '(95%-conf.int.', '0.00235', '-', '0.00235)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:136', 'F:3']
['By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.']
['This is in contrast to dependency treebanks  e.g.', 'It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).', 'The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', 'However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.']
['system', 'ROUGE-S*', 'Average_R:', '0.02012', '(95%-conf.int.', '0.02012', '-', '0.02012)']
['system', 'ROUGE-S*', 'Average_P:', '0.10081', '(95%-conf.int.', '0.10081', '-', '0.10081)']
['system', 'ROUGE-S*', 'Average_F:', '0.03355', '(95%-conf.int.', '0.03355', '-', '0.03355)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:496', 'F:50']
0.158299410834 0.00722294113398 0.0130347058057





input/ref/Task1/W99-0613_aakansha.csv
input/res/Task1/W99-0613.csv
parsing: input/ref/Task1/W99-0613_aakansha.csv
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="30">Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'"]
'137'
['137']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="12">2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="4">The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="12">But we will show that the use of unlabeled data can drastically reduce the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
    <S sid="237" ssid="4">The numbers falling into the location, person, organization categories were 186, 289 and 402 respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'", "'237'"]
'236'
'237'
['236', '237']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
    <S sid="10" ssid="4">The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'10'"]
'9'
'10'
['9', '10']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S sid="139" ssid="6">This section describes AdaBoost, which is the basis for the CoBoost algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'39'"]
'137'
'39'
['137', '39']
parsed_discourse_facet ['method_citation']
<S sid="26" ssid="20">We present two algorithms.</S>
    <S sid="27" ssid="21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'27'"]
'26'
'27'
['26', '27']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="12">But we will show that the use of unlabeled data can drastically reduce the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="2">Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S>
    <S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'9'"]
'8'
'9'
['8', '9']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="1" ssid = "1">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="9" ssid = "3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="136" ssid = "3">We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'250'", "'136'", "'163'"]
'1'
'9'
'250'
'136'
'163'
['1', '9', '250', '136', '163']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.</S><S sid ="17" ssid = "11">At first glance  the problem seems quite complex: a large number of rules is needed to cover the domain  suggesting that a large number of labeled examples is required to train an accurate classifier.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="105" ssid = "38">Thus the method makes the fairly strong assumption that the features can be partitioned into two types such that each type alone is sufficient for classification.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'17'", "'250'", "'105'", "'248'"]
'2'
'17'
'250'
'105'
'248'
['2', '17', '250', '105', '248']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid ="92" ssid = "25">Thus an explicit assumption about the redundancy of the features — that either the spelling or context alone should be sufficient to build a classifier — has been built into the algorithm.</S><S sid ="121" ssid = "54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).</S><S sid ="58" ssid = "12">Having found (spelling  context) pairs in the parsed data  a number of features are extracted.</S><S sid ="232" ssid = "11">For example  the independence assumptions mean that the model fails to capture the dependence between specific and more general features (for example the fact that the feature full.-string=New_York is always seen with the features contains (New) and The baseline method tags all entities as the most frequent class type (organization). contains (York) and is never seen with a feature such as contains (Group) ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'92'", "'121'", "'58'", "'232'"]
'3'
'92'
'121'
'58'
'232'
['3', '92', '121', '58', '232']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "20">We present two algorithms.</S><S sid ="4" ssid = "4">We present two algorithms.</S><S sid ="68" ssid = "1">The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S><S sid ="88" ssid = "21">We can now compare this algorithm to that of (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'4'", "'68'", "'163'", "'88'"]
'26'
'4'
'68'
'163'
'88'
['26', '4', '68', '163', '88']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid ="6" ssid = "6">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="68" ssid = "1">The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'31'", "'6'", "'97'", "'68'"]
'5'
'31'
'6'
'97'
'68'
['5', '31', '6', '97', '68']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "6">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid ="5" ssid = "5">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="30" ssid = "24">(Blum and Mitchell 98) offer a promising formulation of redundancy  also prove some results about how the use of unlabeled examples can help classification  and suggest an objective function when training with unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'31'", "'5'", "'204'", "'30'"]
'6'
'31'
'5'
'204'
'30'
['6', '31', '5', '204', '30']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision  in the form of labeled training examples.</S><S sid ="2" ssid = "2">A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="3" ssid = "3">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid ="130" ssid = "63">So the success of the algorithm may well be due to its success in maximizing the number of unlabeled examples on which the two decision lists agree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'2'", "'250'", "'3'", "'130'"]
'7'
'2'
'250'
'3'
'130'
['7', '2', '250', '3', '130']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "2">Recent results (e.g.  (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S><S sid ="27" ssid = "21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S><S sid ="120" ssid = "53">(Blum and Mitchell 98) go on to give PAC results for learning in the cotraining case.</S><S sid ="40" ssid = "34">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid ="88" ssid = "21">We can now compare this algorithm to that of (Yarowsky 95).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'27'", "'120'", "'40'", "'88'"]
'8'
'27'
'120'
'40'
'88'
['8', '27', '120', '40', '88']
parsed_discourse_facet ['results_citation']
<S sid ="1" ssid = "1">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="9" ssid = "3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid ="250" ssid = "1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid ="136" ssid = "3">We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.</S><S sid ="163" ssid = "30">We now describe the CoBoost algorithm for the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'9'", "'250'", "'136'", "'163'"]
'1'
'9'
'250'
'136'
'163'
['1', '9', '250', '136', '163']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "4">The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.</S><S sid ="147" ssid = "14">The weak learner for two-class problems computes a weak hypothesis h from the input space into the reals (h : 2x -4 R)  where the sign4 of h(x) is interpreted as the predicted label and the magnitude I h(x)I is the confidence in the prediction: large numbers for I h(x)I indicate high confidence in the prediction  and numbers close to zero indicate low confidence.</S><S sid ="35" ssid = "29">AdaBoost finds a weighted combination of simple (weak) classifiers  where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.</S><S sid ="224" ssid = "3">The model was parameterized such that the joint probability of a (label  feature-set) pair P(yi  xi) is written as The model assumes that (y  x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).</S><S sid ="74" ssid = "7">Output of the learning algorithm: a function h:Xxy [0  1] where h(x  y) is an estimate of the conditional probability p(y1x) of seeing label y given that feature x is present.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'147'", "'35'", "'224'", "'74'"]
'10'
'147'
'35'
'224'
'74'
['10', '147', '35', '224', '74']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "5">For example  a good classifier would identify Mrs. Frank as a person  Steptoe & Johnson as a company  and Honduras as a location.</S><S sid ="40" ssid = "34">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid ="23" ssid = "17">For example  in ..  says Mr. Cooper  a vice president of.. both a spelling feature (that the string contains Mr.) and a contextual feature (that president modifies the string) are strong indications that Mr. Cooper is of type Person.</S><S sid ="48" ssid = "2">For example  take ...  says Maury Cooper  a vice president at S.&P.</S><S sid ="55" ssid = "9">In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'40'", "'23'", "'48'", "'55'"]
'11'
'40'
'23'
'48'
'55'
['11', '40', '23', '48', '55']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "6">The approach uses both spelling and contextual rules.</S><S sid ="58" ssid = "12">Having found (spelling  context) pairs in the parsed data  a number of features are extracted.</S><S sid ="22" ssid = "16">In many cases  inspection of either the spelling or context alone is sufficient to classify an example.</S><S sid ="165" ssid = "32">In the namedentity problem each example is a (spelling context) pair.</S><S sid ="55" ssid = "9">In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'58'", "'22'", "'165'", "'55'"]
'12'
'58'
'22'
'165'
'55'
['12', '58', '22', '165', '55']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "7">A spelling rule might be a simple look-up for the string (e.g.  a rule that Honduras is a location) or a rule that looks at words within a string (e.g.  a rule that any string containing Mr. is a person).</S><S sid ="14" ssid = "8">A contextual rule considers words surrounding the string in the sentence in which it appears (e.g.  a rule that any proper name modified by an appositive whose head is president is a person).</S><S sid ="61" ssid = "15">The following features were used: full-string=x The full string (e.g.  for Maury Cooper  full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word  this feature applies for any words that the string contains (e.g.  Maury Cooper contributes two such features  contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g.  IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods  and contains at least one period.</S><S sid ="22" ssid = "16">In many cases  inspection of either the spelling or context alone is sufficient to classify an example.</S><S sid ="165" ssid = "32">In the namedentity problem each example is a (spelling context) pair.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'14'", "'61'", "'22'", "'165'"]
'13'
'14'
'61'
'22'
'165'
['13', '14', '61', '22', '165']
parsed_discourse_facet ['method_citation']
['The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.']
['They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).', 'The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.', u'Thus an explicit assumption about the redundancy of the features \u2014 that either the spelling or context alone should be sufficient to build a classifier \u2014 has been built into the algorithm.', 'Having found (spelling  context) pairs in the parsed data  a number of features are extracted.', 'For example  the independence assumptions mean that the model fails to capture the dependence between specific and more general features (for example the fact that the feature full.-string=New_York is always seen with the features contains (New) and The baseline method tags all entities as the most frequent class type (organization). contains (York) and is never seen with a feature such as contains (Group) ).']
['system', 'ROUGE-S*', 'Average_R:', '0.00144', '(95%-conf.int.', '0.00144', '-', '0.00144)']
['system', 'ROUGE-S*', 'Average_P:', '0.11111', '(95%-conf.int.', '0.11111', '-', '0.11111)']
['system', 'ROUGE-S*', 'Average_F:', '0.00285', '(95%-conf.int.', '0.00285', '-', '0.00285)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:36', 'F:4']
['This section describes AdaBoost, which is the basis for the CoBoost algorithm.', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.']
['This paper discusses the use of unlabeled examples for the problem of named entity classification.', 'This paper discusses the use of unlabeled examples for the problem of named entity classification.', 'Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.', 'We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.', 'We now describe the CoBoost algorithm for the named entity problem.']
['system', 'ROUGE-S*', 'Average_R:', '0.00775', '(95%-conf.int.', '0.00775', '-', '0.00775)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01389', '(95%-conf.int.', '0.01389', '-', '0.01389)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:105', 'F:7']
['The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.']
['The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).', "Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).", 'The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', 'The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:78', 'F:0']
['This paper discusses the use of unlabeled examples for the problem of named entity classification.', 'Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.']
['The following features were used: full-string=x The full string (e.g.  for Maury Cooper  full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word  this feature applies for any words that the string contains (e.g.  Maury Cooper contributes two such features  contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g.  IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods  and contains at least one period.', 'A spelling rule might be a simple look-up for the string (e.g.  a rule that Honduras is a location) or a rule that looks at words within a string (e.g.  a rule that any string containing Mr. is a person).', 'A contextual rule considers words surrounding the string in the sentence in which it appears (e.g.  a rule that any proper name modified by an appositive whose head is president is a person).', 'In many cases  inspection of either the spelling or context alone is sufficient to classify an example.', 'In the namedentity problem each example is a (spelling context) pair.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:253', 'F:0']
['We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.', 'The numbers falling into the location, person, organization categories were 186, 289 and 402 respectively.']
['So the success of the algorithm may well be due to its success in maximizing the number of unlabeled examples on which the two decision lists agree.', 'Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.', 'A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.', 'The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.', 'Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision  in the form of labeled training examples.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:171', 'F:0']
['We present two algorithms.', 'The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).']
['The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.', 'Output of the learning algorithm: a function h:Xxy [0  1] where h(x  y) is an estimate of the conditional probability p(y1x) of seeing label y given that feature x is present.', 'The model was parameterized such that the joint probability of a (label  feature-set) pair P(yi  xi) is written as The model assumes that (y  x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).', 'AdaBoost finds a weighted combination of simple (weak) classifiers  where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.', 'The weak learner for two-class problems computes a weak hypothesis h from the input space into the reals (h : 2x -4 R)  where the sign4 of h(x) is interpreted as the predicted label and the magnitude I h(x)I is the confidence in the prediction: large numbers for I h(x)I indicate high confidence in the prediction  and numbers close to zero indicate low confidence.']
['system', 'ROUGE-S*', 'Average_R:', '0.00035', '(95%-conf.int.', '0.00035', '-', '0.00035)']
['system', 'ROUGE-S*', 'Average_P:', '0.03636', '(95%-conf.int.', '0.03636', '-', '0.03636)']
['system', 'ROUGE-S*', 'Average_F:', '0.00070', '(95%-conf.int.', '0.00070', '-', '0.00070)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5671', 'P:55', 'F:2']
['This paper discusses the use of unlabeled examples for the problem of named entity classification.', 'The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.']
['(Blum and Mitchell 98) go on to give PAC results for learning in the cotraining case.', 'Recent results (e.g.  (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.', 'We can now compare this algorithm to that of (Yarowsky 95).', 'The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).', '(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:210', 'F:0']
['Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.']
['Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.', 'A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.', 'With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.', 'At first glance  the problem seems quite complex: a large number of rules is needed to cover the domain  suggesting that a large number of labeled examples is required to train an accurate classifier.', 'Thus the method makes the fairly strong assumption that the features can be partitioned into two types such that each type alone is sufficient for classification.']
['system', 'ROUGE-S*', 'Average_R:', '0.00171', '(95%-conf.int.', '0.00171', '-', '0.00171)']
['system', 'ROUGE-S*', 'Average_P:', '0.04762', '(95%-conf.int.', '0.04762', '-', '0.04762)']
['system', 'ROUGE-S*', 'Average_F:', '0.00330', '(95%-conf.int.', '0.00330', '-', '0.00330)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:105', 'F:5']
['(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.']
['In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.', 'In the namedentity problem each example is a (spelling context) pair.', 'The approach uses both spelling and contextual rules.', 'Having found (spelling  context) pairs in the parsed data  a number of features are extracted.', 'In many cases  inspection of either the spelling or context alone is sufficient to classify an example.']
['system', 'ROUGE-S*', 'Average_R:', '0.00178', '(95%-conf.int.', '0.00178', '-', '0.00178)']
['system', 'ROUGE-S*', 'Average_P:', '0.00159', '(95%-conf.int.', '0.00159', '-', '0.00159)']
['system', 'ROUGE-S*', 'Average_F:', '0.00168', '(95%-conf.int.', '0.00168', '-', '0.00168)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:630', 'F:1']
['But we will show that the use of unlabeled data can drastically reduce the need for supervision.']
["Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).", '(Blum and Mitchell 98) offer a promising formulation of redundancy  also prove some results about how the use of unlabeled examples can help classification  and suggest an objective function when training with unlabeled examples.', 'The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).', 'In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.', 'The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).']
['system', 'ROUGE-S*', 'Average_R:', '0.00051', '(95%-conf.int.', '0.00051', '-', '0.00051)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00102', '(95%-conf.int.', '0.00102', '-', '0.00102)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:15', 'F:1']
0.03300199967 0.00135399998646 0.00234399997656





input/ref/Task1/P08-1043_swastika.csv
input/res/Task1/P08-1043.csv
parsing: input/ref/Task1/P08-1043_swastika.csv
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['result_citation']
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="37">3An English sentence with ambiguous PoS assignment can be trivially represented as a lattice similar to our own, where every pair of consecutive nodes correspond to a word, and every possible PoS assignment for this word is a connecting arc.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="163" ssid="1">The accuracy results for segmentation, tagging and parsing using our different models and our standard data split are summarized in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



['163']
163
['163']
parsed_discourse_facet ['result_citation']
    <S sid="100" ssid="32">This means that the rules in our grammar are of two kinds: (a) syntactic rules relating nonterminals to a sequence of non-terminals and/or PoS tags, and (b) lexical rules relating PoS tags to lattice arcs (lexemes).</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['method_citation']
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['result_citation']
<S sid="188" ssid="2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



['188']
188
['188']
parsed_discourse_facet ['result_citation']
<S sid="86" ssid="18">A morphological analyzer M : W&#8212;* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S>
original cit marker offset is 0
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['result_citation']
<S sid="97" ssid="29">Thus our proposed model is a proper model assigning probability mass to all (7r, L) pairs, where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S>
original cit marker offset is 0
new cit marker offset is 0



['97']
97
['97']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S sid ="1" ssid = "1">Morphological processes in Semitic languages deliver space-delimited words which introduce multiple  distinct  syntactic units into the structure of the input sentence.</S><S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="39" ssid = "18">Such ambiguities cause discrepancies between token boundaries (indexed as white spaces) and constituent boundaries (imposed by syntactic categories) with respect to a surface form.</S><S sid ="34" ssid = "13">Lexical and Morphological Ambiguity The rich morphological processes for deriving Hebrew stems give rise to a high degree of ambiguity for Hebrew space-delimited tokens.</S><S sid ="24" ssid = "3">Co-occurrences among the particles themselves are subject to further syntactic and lexical constraints relative to the stem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'14'", "'39'", "'34'", "'24'"]
'1'
'14'
'39'
'34'
'24'
['1', '14', '39', '34', '24']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.</S><S sid ="16" ssid = "12">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S><S sid ="94" ssid = "26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S><S sid ="20" ssid = "16">We claim that no particular morphological segmentation is a-priory more likely for surface forms before exploring the compositional nature of syntactic structures  including manifestations of various long-distance dependencies.</S><S sid ="191" ssid = "5">In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'16'", "'94'", "'20'", "'191'"]
'2'
'16'
'94'
'20'
'191'
['2', '16', '94', '20', '191']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S><S sid ="19" ssid = "15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S sid ="187" ssid = "1">Employing a PCFG-based generative framework to make both syntactic and morphological disambiguation decisions is not only theoretically clean and linguistically justified and but also probabilistically apropriate and empirically sound.</S><S sid ="189" ssid = "3">Better grammars are shown here to improve performance on both morphological and syntactic tasks  providing support for the advantage of a joint framework over pipelined or factorized ones.</S><S sid ="21" ssid = "17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'19'", "'187'", "'189'", "'21'"]
'3'
'19'
'187'
'189'
'21'
['3', '19', '187', '189', '21']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Using a treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined  integrated or factorized systems for Hebrew morphological and syntactic processing  yielding an error reduction of 12% over the best published results so far.</S><S sid ="21" ssid = "17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S><S sid ="185" ssid = "23">In contrast  our morphological probabilities are based on a unigram  lexeme-based model  and all other (local and non-local) contextual considerations are delegated to the PCFG.</S><S sid ="188" ssid = "2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S><S sid ="191" ssid = "5">In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'21'", "'185'", "'188'", "'191'"]
'4'
'21'
'185'
'188'
'191'
['4', '21', '185', '188', '191']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  ‘tokens’) that constitute the unanalyzed surface forms (utterances).</S><S sid ="12" ssid = "8">This leads to word- and constituent-boundaries discrepancy  which breaks the assumptions underlying current state-of-the-art statistical parsers.</S><S sid ="94" ssid = "26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S><S sid ="21" ssid = "17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S><S sid ="16" ssid = "12">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'12'", "'94'", "'21'", "'16'"]
'5'
'12'
'94'
'21'
'16'
['5', '12', '94', '21', '16']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "2">In Semitic languages the situation is very different.</S><S sid ="78" ssid = "10">A similar structure is used in speech recognition.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="99" ssid = "31">This is done using a simple PCFG which is lexemebased.</S><S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'78'", "'92'", "'99'", "'180'"]
'6'
'78'
'92'
'99'
'180'
['6', '78', '92', '99', '180']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).</S><S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="17" ssid = "13">Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task.</S><S sid ="8" ssid = "4">The Hebrew token ‘bcl’1  for example  stands for the complete prepositional phrase 'We adopt here the transliteration of (Sima’an et al.  2001).</S><S sid ="29" ssid = "8">A less canonical representation of segmental morphology is triggered by a morpho-phonological process of omitting the definite article h when occurring after the particles b or l. This process triggers ambiguity as for the definiteness status of Nouns following these particles.We refer to such cases in which the concatenation of elements does not strictly correspond to the original surface form as super-segmental morphology.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'14'", "'17'", "'8'", "'29'"]
'7'
'14'
'17'
'8'
'29'
['7', '14', '17', '8', '29']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "4">The Hebrew token ‘bcl’1  for example  stands for the complete prepositional phrase 'We adopt here the transliteration of (Sima’an et al.  2001).</S><S sid ="35" ssid = "14">The form fmnh  for example  can be understood as the verb “lubricated”  the possessed noun “her oil”  the adjective “fat” or the verb “got fat”.</S><S sid ="62" ssid = "9">When a token fmnh is to be interpreted as the lexeme sequence f/REL mnh/VB  the analysis introduces two distinct entities  the relativizer f (“that”) and the verb mnh (“counted”)  and not as the complex entity “that counted”.</S><S sid ="7" ssid = "3">In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).</S><S sid ="15" ssid = "11">The aforementioned surface form bcl  for example  may also stand for the lexical item “onion”  a Noun.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'35'", "'62'", "'7'", "'15'"]
'8'
'35'
'62'
'7'
'15'
['8', '35', '62', '7', '15']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "5">“in the shadow”.</S><S sid ="184" ssid = "22">Oracle results).</S><S sid ="109" ssid = "41">The latter arcs correspond to OOV words in English.</S><S sid ="113" ssid = "45">The remaining arcs are marked OOV.</S><S sid ="78" ssid = "10">A similar structure is used in speech recognition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'184'", "'109'", "'113'", "'78'"]
'9'
'184'
'109'
'113'
'78'
['9', '184', '109', '113', '78']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "6">This token may further embed into a larger utterance  e.g.  ‘bcl hneim’ (literally “in-the-shadow the-pleasant”  meaning roughly “in the pleasant shadow”) in which the dominated Noun is modified by a proceeding space-delimited adjective.</S><S sid ="15" ssid = "11">The aforementioned surface form bcl  for example  may also stand for the lexical item “onion”  a Noun.</S><S sid ="76" ssid = "8">Furthermore  some of the arcs represent lexemes not present in the input tokens (e.g. h/DT  fl/POS)  however these are parts of valid analyses of the token (cf. super-segmental morphology section 2).</S><S sid ="58" ssid = "5">Such tag sequences are often treated as “complex tags” (e.g.</S><S sid ="108" ssid = "40">Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p —* (s  p)) > 0  while other segments have never been observed as a lexical event before.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'15'", "'76'", "'58'", "'108'"]
'10'
'15'
'76'
'58'
'108'
['10', '15', '76', '58', '108']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">It should be clear from the onset that the particle b (“in”) in ‘bcl’ may then attach higher than the bare noun cl (“shadow”).</S><S sid ="37" ssid = "16">The same form fmnh can be segmented as f-mnh  f (“that”) functioning as a reletivizer with the form mnh.</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="38" ssid = "17">The form mnh itself can be read as at least three different verbs (“counted”  “appointed”  “was appointed”)  a noun (“a portion”)  and a possessed noun (“her kind”).</S><S sid ="26" ssid = "5">The relativizer f(“that”) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'37'", "'57'", "'38'", "'26'"]
'11'
'37'
'57'
'38'
'26'
['11', '37', '57', '38', '26']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">This leads to word- and constituent-boundaries discrepancy  which breaks the assumptions underlying current state-of-the-art statistical parsers.</S><S sid ="5" ssid = "1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  ‘tokens’) that constitute the unanalyzed surface forms (utterances).</S><S sid ="13" ssid = "9">One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).</S><S sid ="2" ssid = "2">These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.</S><S sid ="19" ssid = "15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'5'", "'13'", "'2'", "'19'"]
'12'
'5'
'13'
'2'
'19'
['12', '5', '13', '2', '19']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "9">One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).</S><S sid ="33" ssid = "12">The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).</S><S sid ="140" ssid = "18">For these models we limit the options provided for OOV words by not considering the entire token as a valid segmentation in case at least some prefix segmentation exists.</S><S sid ="139" ssid = "17">To control for the effect of the HSPELL-based pruning  we also experimented with a morphological analyzer that does not perform this pruning.</S><S sid ="3" ssid = "3">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'33'", "'140'", "'139'", "'3'"]
'13'
'33'
'140'
'139'
'3'
['13', '33', '140', '139', '3']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="7" ssid = "3">In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).</S><S sid ="140" ssid = "18">For these models we limit the options provided for OOV words by not considering the entire token as a valid segmentation in case at least some prefix segmentation exists.</S><S sid ="17" ssid = "13">Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task.</S><S sid ="121" ssid = "53">In such cases we use the non-pruned lattice including all (possibly ungrammatical) segmentation  and let the statistics (including OOV) decide.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'7'", "'140'", "'17'", "'121'"]
'14'
'7'
'140'
'17'
'121'
['14', '7', '140', '17', '121']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "11">The aforementioned surface form bcl  for example  may also stand for the lexical item “onion”  a Noun.</S><S sid ="32" ssid = "11">The additional morphological material in such cases appears after the stem and realizes the extended meaning.</S><S sid ="28" ssid = "7">The same argument holds for resolving PP attachment of a prefixed preposition or marking conjunction of elements of any kind.</S><S sid ="10" ssid = "6">This token may further embed into a larger utterance  e.g.  ‘bcl hneim’ (literally “in-the-shadow the-pleasant”  meaning roughly “in the pleasant shadow”) in which the dominated Noun is modified by a proceeding space-delimited adjective.</S><S sid ="30" ssid = "9">An additional case of super-segmental morphology is the case of Pronominal Clitics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'32'", "'28'", "'10'", "'30'"]
'15'
'32'
'28'
'10'
'30'
['15', '32', '28', '10', '30']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S><S sid ="94" ssid = "26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S><S sid ="188" ssid = "2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S><S sid ="13" ssid = "9">One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).</S><S sid ="2" ssid = "2">These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'94'", "'188'", "'13'", "'2'"]
'16'
'94'
'188'
'13'
'2'
['16', '94', '188', '13', '2']
parsed_discourse_facet ['method_citation']
['Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.']
['Morphological processes in Semitic languages deliver space-delimited words which introduce multiple  distinct  syntactic units into the structure of the input sentence.', 'Co-occurrences among the particles themselves are subject to further syntactic and lexical constraints relative to the stem.', 'Such ambiguities cause discrepancies between token boundaries (indexed as white spaces) and constituent boundaries (imposed by syntactic categories) with respect to a surface form.', 'The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).', 'Lexical and Morphological Ambiguity The rich morphological processes for deriving Hebrew stems give rise to a high degree of ambiguity for Hebrew space-delimited tokens.']
['system', 'ROUGE-S*', 'Average_R:', '0.00316', '(95%-conf.int.', '0.00316', '-', '0.00316)']
['system', 'ROUGE-S*', 'Average_P:', '0.05882', '(95%-conf.int.', '0.05882', '-', '0.05882)']
['system', 'ROUGE-S*', 'Average_F:', '0.00599', '(95%-conf.int.', '0.00599', '-', '0.00599)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:153', 'F:9']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
[u'This token may further embed into a larger utterance  e.g.  \u2018bcl hneim\u2019 (literally \u201cin-the-shadow the-pleasant\u201d  meaning roughly \u201cin the pleasant shadow\u201d) in which the dominated Noun is modified by a proceeding space-delimited adjective.', u'Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p \u2014* (s  p)) > 0  while other segments have never been observed as a lexical event before.', u'The aforementioned surface form bcl  for example  may also stand for the lexical item \u201conion\u201d  a Noun.', u'Such tag sequences are often treated as \u201ccomplex tags\u201d (e.g.', 'Furthermore  some of the arcs represent lexemes not present in the input tokens (e.g. h/DT  fl/POS)  however these are parts of valid analyses of the token (cf. super-segmental morphology section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00090', '(95%-conf.int.', '0.00090', '-', '0.00090)']
['system', 'ROUGE-S*', 'Average_P:', '0.01667', '(95%-conf.int.', '0.01667', '-', '0.01667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00172', '(95%-conf.int.', '0.00172', '-', '0.00172)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:120', 'F:2']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.', 'In contrast  our morphological probabilities are based on a unigram  lexeme-based model  and all other (local and non-local) contextual considerations are delegated to the PCFG.', 'In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.', 'Using a treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined  integrated or factorized systems for Hebrew morphological and syntactic processing  yielding an error reduction of 12% over the best published results so far.', 'Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2']
['system', 'ROUGE-S*', 'Average_R:', '0.00223', '(95%-conf.int.', '0.00223', '-', '0.00223)']
['system', 'ROUGE-S*', 'Average_P:', '0.14167', '(95%-conf.int.', '0.14167', '-', '0.14167)']
['system', 'ROUGE-S*', 'Average_F:', '0.00439', '(95%-conf.int.', '0.00439', '-', '0.00439)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:7626', 'P:120', 'F:17']
['Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.', 'Better grammars are shown here to improve performance on both morphological and syntactic tasks  providing support for the advantage of a joint framework over pipelined or factorized ones.', 'Employing a PCFG-based generative framework to make both syntactic and morphological disambiguation decisions is not only theoretically clean and linguistically justified and but also probabilistically apropriate and empirically sound.', 'Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2']
['system', 'ROUGE-S*', 'Average_R:', '0.00210', '(95%-conf.int.', '0.00210', '-', '0.00210)']
['system', 'ROUGE-S*', 'Average_P:', '0.06536', '(95%-conf.int.', '0.06536', '-', '0.06536)']
['system', 'ROUGE-S*', 'Average_F:', '0.00408', '(95%-conf.int.', '0.00408', '-', '0.00408)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4753', 'P:153', 'F:10']
['Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.', 'These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.', 'We claim that no particular morphological segmentation is a-priory more likely for surface forms before exploring the compositional nature of syntactic structures  including manifestations of various long-distance dependencies.', 'Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.', 'The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.']
['system', 'ROUGE-S*', 'Average_R:', '0.00430', '(95%-conf.int.', '0.00430', '-', '0.00430)']
['system', 'ROUGE-S*', 'Average_P:', '0.02910', '(95%-conf.int.', '0.02910', '-', '0.02910)']
['system', 'ROUGE-S*', 'Average_F:', '0.00750', '(95%-conf.int.', '0.00750', '-', '0.00750)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:378', 'F:11']
['3An English sentence with ambiguous PoS assignment can be trivially represented as a lattice similar to our own, where every pair of consecutive nodes correspond to a word, and every possible PoS assignment for this word is a connecting arc.']
[u"The Hebrew token \u2018bcl\u20191  for example  stands for the complete prepositional phrase 'We adopt here the transliteration of (Sima\u2019an et al.  2001).", 'Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task.', 'The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).', 'In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).', 'A less canonical representation of segmental morphology is triggered by a morpho-phonological process of omitting the definite article h when occurring after the particles b or l. This process triggers ambiguity as for the definiteness status of Nouns following these particles.We refer to such cases in which the concatenation of elements does not strictly correspond to the original surface form as super-segmental morphology.']
['system', 'ROUGE-S*', 'Average_R:', '0.00071', '(95%-conf.int.', '0.00071', '-', '0.00071)']
['system', 'ROUGE-S*', 'Average_P:', '0.02105', '(95%-conf.int.', '0.02105', '-', '0.02105)']
['system', 'ROUGE-S*', 'Average_F:', '0.00136', '(95%-conf.int.', '0.00136', '-', '0.00136)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5671', 'P:190', 'F:4']
['Thus our proposed model is a proper model assigning probability mass to all (7r, L) pairs, where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.']
['The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.', 'These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.', 'One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).', 'Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.', 'The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.']
['system', 'ROUGE-S*', 'Average_R:', '0.00218', '(95%-conf.int.', '0.00218', '-', '0.00218)']
['system', 'ROUGE-S*', 'Average_P:', '0.06433', '(95%-conf.int.', '0.06433', '-', '0.06433)']
['system', 'ROUGE-S*', 'Average_F:', '0.00421', '(95%-conf.int.', '0.00421', '-', '0.00421)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5050', 'P:171', 'F:11']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.', 'This leads to word- and constituent-boundaries discrepancy  which breaks the assumptions underlying current state-of-the-art statistical parsers.', u'Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  \u2018tokens\u2019) that constitute the unanalyzed surface forms (utterances).', 'Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.', 'Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2']
['system', 'ROUGE-S*', 'Average_R:', '0.00062', '(95%-conf.int.', '0.00062', '-', '0.00062)']
['system', 'ROUGE-S*', 'Average_P:', '0.03333', '(95%-conf.int.', '0.03333', '-', '0.03333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00122', '(95%-conf.int.', '0.00122', '-', '0.00122)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6441', 'P:120', 'F:4']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
[u'\u201cin the shadow\u201d.', 'The latter arcs correspond to OOV words in English.', 'The remaining arcs are marked OOV.', 'Oracle results).', 'A similar structure is used in speech recognition.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:120', 'P:120', 'F:0']
0.0478144439132 0.00179999998 0.00338555551794





input/ref/Task1/W11-2123_vardha.csv
input/res/Task1/W11-2123.csv
parsing: input/ref/Task1/W11-2123_vardha.csv
    <S sid="12" ssid="7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
 <S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
 <S sid="131" ssid="3">Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N &#8722; 1 preceding words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'"]
'131'
['131']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="16">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="16">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="12">Compared with SRILM, IRSTLM adds several features: lower memory consumption, a binary file format with memory mapping, caching to increase speed, and quantization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'"]
'108'
['108']
parsed_discourse_facet ['method_citation']
    <S sid="129" ssid="1">In addition to the optimizations specific to each datastructure described in Section 2, we implement several general optimizations for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'129'"]
'129'
['129']
parsed_discourse_facet ['method_citation']
    <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
    <S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
    <S sid="263" ssid="5">Quantization can be improved by jointly encoding probability and backoff.</S>
original cit marker offset is 0
new cit marker offset is 0



["'263'"]
'263'
['263']
parsed_discourse_facet ['method_citation']
    <S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
    <S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="17">If the context wnf will never extend to the right (i.e. wnf v is not present in the model for all words v) then no subsequent query will match the full context.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'"]
'145'
['145']
parsed_discourse_facet ['method_citation']
 <S sid="182" ssid="1">This section measures performance on shared tasks in order of increasing complexity: sparse lookups, evaluating perplexity of a large file, and translation with Moses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'182'"]
'182'
['182']
parsed_discourse_facet ['method_citation']
    <S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
    <S sid="12" ssid="7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
    <S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="12">We have modified Moses (Koehn et al., 2007) to keep our state with hypotheses; to conserve memory, phrases do not keep state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
 <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
 <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W11-2123.csv
<S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid ="7" ssid = "2">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'274'", "'7'", "'259'", "'45'"]
'1'
'274'
'7'
'259'
'45'
['1', '274', '7', '259', '45']
parsed_discourse_facet ['aim_citation']
<S sid ="2" ssid = "2">The structure uses linear probing hash tables and is designed for speed.</S><S sid ="194" ssid = "13">Further  the probing hash table does only one random lookup per query  explaining why it is faster on large data.</S><S sid ="23" ssid = "1">We implement two data structures: PROBING  designed for speed  and TRIE  optimized for memory.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S><S sid ="120" ssid = "24">The compressed variant uses block compression and is rather slow as a result.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'194'", "'23'", "'45'", "'120'"]
'2'
'194'
'23'
'45'
'120'
['2', '194', '23', '45', '120']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S><S sid ="128" ssid = "32">These models generally outperform our memory consumption but are much slower  even when cached.</S><S sid ="161" ssid = "33">In both cases  SRILM walks its trie an additional time to minimize context as mentioned in Section 4.1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'275'", "'276'", "'128'", "'161'"]
'3'
'275'
'276'
'128'
'161'
['3', '275', '276', '128', '161']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Our code is thread-safe  and integrated into the Moses  cdec  and Joshua translation systems.</S><S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="169" ssid = "41">In our case multi-threading is trivial because our data structures are read-only and uncached.</S><S sid ="282" ssid = "3">Adam Pauls provided a pre-release comparison to BerkeleyLM and an initial Java interface.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'279'", "'169'", "'282'", "'266'"]
'4'
'279'
'169'
'282'
'266'
['4', '279', '169', '282', '266']
parsed_discourse_facet ['aim_citation']
<S sid ="5" ssid = "5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid ="26" ssid = "4">We use two common techniques  hash tables and sorted arrays  describing each before the model that uses the technique.</S><S sid ="129" ssid = "1">In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'274'", "'26'", "'129'"]
'5'
'1'
'274'
'26'
'129'
['5', '1', '274', '26', '129']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "1">Language models are widely applied in natural language processing  and applications such as machine translation make very frequent queries.</S><S sid ="130" ssid = "2">Applications such as machine translation use language model probability as a feature to assist in choosing between hypotheses.</S><S sid ="184" ssid = "3">Sparse lookup is a key subproblem of language model queries.</S><S sid ="7" ssid = "2">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'130'", "'184'", "'7'", "'259'"]
'6'
'130'
'184'
'7'
'259'
['6', '130', '184', '7', '259']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "2">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'1'", "'274'", "'259'", "'45'"]
'7'
'1'
'274'
'259'
'45'
['7', '1', '274', '259', '45']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "3">Queries take the form p(wn|wn−1 1 ) where wn1 is an n-gram.</S><S sid ="84" ssid = "62">In a model we built with default settings  1.2% of n + 1-grams were missing their ngram suffix.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="72" ssid = "50">We maintain a separate array for each length n containing all n-gram entries sorted in suffix order.</S><S sid ="46" ssid = "24">Unigram lookup is dense so we use an array of probability and backoff values.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'84'", "'88'", "'72'", "'46'"]
'8'
'84'
'88'
'72'
'46'
['8', '84', '88', '72', '46']
parsed_discourse_facet ['results_citation']
<S sid ="9" ssid = "4">Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf   returning where the probability p(wn|wn−1 f ) and backoff penalties b(wn−1 i ) are given by an already-estimated model.</S><S sid ="127" ssid = "31">Minimal perfect hashing is used to find the index at which a quantized probability and possibly backoff are stored.</S><S sid ="136" ssid = "8">We offer a state function s(wn1) = wn� where substring wn� is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.</S><S sid ="78" ssid = "56">Unigram records store probability  backoff  and an index in the bigram table.</S><S sid ="154" ssid = "26">To optimize left-to-right queries  we extend state to store backoff information: where m is the minimal context from Section 4.1 and b is the backoff penalty.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'127'", "'136'", "'78'", "'154'"]
'9'
'127'
'136'
'78'
'154'
['9', '127', '136', '78', '154']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "5">The problem is to store these two values for a large and sparse set of n-grams in a way that makes queries efficient.</S><S sid ="25" ssid = "3">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S><S sid ="24" ssid = "2">The set of n-grams appearing in a model is sparse  and we want to efficiently find their associated probabilities and backoff penalties.</S><S sid ="93" ssid = "71">The cost of storing these averages  in bits  is Because there are comparatively few unigrams  we elected to store them byte-aligned and unquantized  making every query faster.</S><S sid ="85" ssid = "63">This causes a problem for reverse trie implementations  including SRILM itself  because it leaves n+1-grams without an n-gram node pointing to them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'25'", "'24'", "'93'", "'85'"]
'10'
'25'
'24'
'93'
'85'
['10', '25', '24', '93', '85']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "6">Many packages perform language model queries.</S><S sid ="68" ssid = "46">The trie data structure is commonly used for language modeling.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="259" ssid = "1">There any many techniques for improving language model speed and reducing memory consumption.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'68'", "'266'", "'259'", "'45'"]
'11'
'68'
'266'
'259'
'45'
['11', '68', '266', '259', '45']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke  2002) is a popular toolkit based on tries used in several decoders.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="97" ssid = "1">SRILM (Stolcke  2002) is widely used within academia.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'13'", "'14'", "'103'", "'97'"]
'12'
'13'
'14'
'103'
'97'
['12', '13', '14', '103', '97']
parsed_discourse_facet ['aim_citation']
<S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'103'", "'15'", "'14'", "'262'"]
'13'
'103'
'15'
'14'
'262'
['13', '103', '15', '14', '262']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="97" ssid = "1">SRILM (Stolcke  2002) is widely used within academia.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'15'", "'103'", "'97'", "'266'"]
'14'
'15'
'103'
'97'
'266'
['14', '15', '103', '97', '266']
parsed_discourse_facet ['method_citation']
<S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="103" ssid = "7">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'14'", "'103'", "'13'", "'262'"]
'15'
'14'
'103'
'13'
'262'
['15', '14', '103', '13', '262']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "11">BerkeleyLM revision 152 (Pauls and Klein  2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.</S><S sid ="15" ssid = "10">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid ="51" ssid = "29">This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'15'", "'51'", "'262'", "'13'"]
'16'
'15'
'51'
'262'
'13'
['16', '15', '51', '262', '13']
parsed_discourse_facet ['method_citation']
<S sid ="17" ssid = "12">Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.</S><S sid ="18" ssid = "13">TPT Germann et al. (2009) describe tries with better locality properties  but did not release code.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="215" ssid = "34">Another option is the closedsource data structures from Sheffield (Guthrie and Hepple  2010).</S><S sid ="114" ssid = "18">Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'18'", "'262'", "'215'", "'114'"]
'17'
'18'
'262'
'215'
'114'
['17', '18', '262', '215', '114']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "13">TPT Germann et al. (2009) describe tries with better locality properties  but did not release code.</S><S sid ="17" ssid = "12">Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.</S><S sid ="232" ssid = "51">Memory usage is likely much lower than ours. fThe original paper (Germann et al.  2009) provided only 2s of query timing and compared with SRI when it exceeded available RAM.</S><S sid ="114" ssid = "18">Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'17'", "'232'", "'114'", "'262'"]
'18'
'17'
'232'
'114'
'262'
['18', '17', '232', '114', '262']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "14">These packages are further described in Section 3.</S><S sid ="129" ssid = "1">In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="5" ssid = "5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S sid ="150" ssid = "22">Section 4.1 explained that state s is stored by applications with partial hypotheses to determine when they can be recombined.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'129'", "'266'", "'5'", "'150'"]
'19'
'129'
'266'
'5'
'150'
['19', '129', '266', '5', '150']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "15">We substantially outperform all of them on query speed and offer lower memory consumption than lossless alternatives.</S><S sid ="128" ssid = "32">These models generally outperform our memory consumption but are much slower  even when cached.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S><S sid ="1" ssid = "1">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'128'", "'276'", "'1'", "'275'"]
'20'
'128'
'276'
'1'
'275'
['20', '128', '276', '1', '275']
parsed_discourse_facet ['aim_citation']
['If the context wnf will never extend to the right (i.e. wnf v is not present in the model for all words v) then no subsequent query will match the full context.']
['IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.', 'IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.', 'RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.', 'MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:66', 'F:0']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.', 'IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.', 'Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke  2002) is a popular toolkit based on tries used in several decoders.', 'MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.', 'SRILM (Stolcke  2002) is widely used within academia.']
['system', 'ROUGE-S*', 'Average_R:', '0.00135', '(95%-conf.int.', '0.00135', '-', '0.00135)']
['system', 'ROUGE-S*', 'Average_P:', '0.02564', '(95%-conf.int.', '0.02564', '-', '0.02564)']
['system', 'ROUGE-S*', 'Average_F:', '0.00256', '(95%-conf.int.', '0.00256', '-', '0.00256)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:78', 'F:2']
['We have modified Moses (Koehn et al., 2007) to keep our state with hypotheses; to conserve memory, phrases do not keep state.']
['Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', 'TPT Germann et al. (2009) describe tries with better locality properties  but did not release code.', 'Memory usage is likely much lower than ours. fThe original paper (Germann et al.  2009) provided only 2s of query timing and compared with SRI when it exceeded available RAM.', 'Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.', 'Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.']
['system', 'ROUGE-S*', 'Average_R:', '0.00078', '(95%-conf.int.', '0.00078', '-', '0.00078)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00150', '(95%-conf.int.', '0.00150', '-', '0.00150)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:55', 'F:1']
['Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.']
['We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.', 'We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.', 'We use two common techniques  hash tables and sorted arrays  describing each before the model that uses the technique.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.', 'In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:171', 'F:0']
['Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N &#8722; 1 preceding words.']
['These models generally outperform our memory consumption but are much slower  even when cached.', 'Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', 'The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.', 'The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.', 'In both cases  SRILM walks its trie an additional time to minimize context as mentioned in Section 4.1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00044', '(95%-conf.int.', '0.00044', '-', '0.00044)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00084', '(95%-conf.int.', '0.00084', '-', '0.00084)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:105', 'F:1']
['Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.']
['The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.', 'Adam Pauls provided a pre-release comparison to BerkeleyLM and an initial Java interface.', 'Our code is thread-safe  and integrated into the Moses  cdec  and Joshua translation systems.', 'Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.', 'In our case multi-threading is trivial because our data structures are read-only and uncached.']
['system', 'ROUGE-S*', 'Average_R:', '0.01111', '(95%-conf.int.', '0.01111', '-', '0.01111)']
['system', 'ROUGE-S*', 'Average_P:', '0.06433', '(95%-conf.int.', '0.06433', '-', '0.06433)']
['system', 'ROUGE-S*', 'Average_F:', '0.01895', '(95%-conf.int.', '0.01895', '-', '0.01895)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:171', 'F:11']
['Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.']
['Many packages perform language model queries.', 'The trie data structure is commonly used for language modeling.', 'The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.', 'There any many techniques for improving language model speed and reducing memory consumption.', 'Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.']
['system', 'ROUGE-S*', 'Average_R:', '0.00641', '(95%-conf.int.', '0.00641', '-', '0.00641)']
['system', 'ROUGE-S*', 'Average_P:', '0.07576', '(95%-conf.int.', '0.07576', '-', '0.07576)']
['system', 'ROUGE-S*', 'Average_F:', '0.01182', '(95%-conf.int.', '0.01182', '-', '0.01182)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:66', 'F:5']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.', 'TPT Germann et al. (2009) describe tries with better locality properties  but did not release code.', 'Another option is the closedsource data structures from Sheffield (Guthrie and Hepple  2010).', 'Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:55', 'F:0']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['The compressed variant uses block compression and is rather slow as a result.', 'Further  the probing hash table does only one random lookup per query  explaining why it is faster on large data.', 'The structure uses linear probing hash tables and is designed for speed.', 'The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.', 'We implement two data structures: PROBING  designed for speed  and TRIE  optimized for memory.']
['system', 'ROUGE-S*', 'Average_R:', '0.00332', '(95%-conf.int.', '0.00332', '-', '0.00332)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00626', '(95%-conf.int.', '0.00626', '-', '0.00626)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:55', 'F:3']
['Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.']
['This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.', 'IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.', 'RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', 'BerkeleyLM revision 152 (Pauls and Klein  2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.']
['system', 'ROUGE-S*', 'Average_R:', '0.00192', '(95%-conf.int.', '0.00192', '-', '0.00192)']
['system', 'ROUGE-S*', 'Average_P:', '0.05128', '(95%-conf.int.', '0.05128', '-', '0.05128)']
['system', 'ROUGE-S*', 'Average_F:', '0.00371', '(95%-conf.int.', '0.00371', '-', '0.00371)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:78', 'F:4']
['Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.']
['We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.', 'There any many techniques for improving language model speed and reducing memory consumption.', 'We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.', 'The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.', 'This paper presents methods to query N-gram language models  minimizing time and space costs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:78', 'F:0']
['In addition to the optimizations specific to each datastructure described in Section 2, we implement several general optimizations for language modeling.']
['We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.', 'There any many techniques for improving language model speed and reducing memory consumption.', 'We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.', 'The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.', 'This paper presents methods to query N-gram language models  minimizing time and space costs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00218', '(95%-conf.int.', '0.00218', '-', '0.00218)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00419', '(95%-conf.int.', '0.00419', '-', '0.00419)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:55', 'F:3']
['Compared with SRILM, IRSTLM adds several features: lower memory consumption, a binary file format with memory mapping, caching to increase speed, and quantization.']
['There any many techniques for improving language model speed and reducing memory consumption.', 'Applications such as machine translation use language model probability as a feature to assist in choosing between hypotheses.', 'This paper presents methods to query N-gram language models  minimizing time and space costs.', 'Sparse lookup is a key subproblem of language model queries.', 'Language models are widely applied in natural language processing  and applications such as machine translation make very frequent queries.']
['system', 'ROUGE-S*', 'Average_R:', '0.00085', '(95%-conf.int.', '0.00085', '-', '0.00085)']
['system', 'ROUGE-S*', 'Average_P:', '0.00735', '(95%-conf.int.', '0.00735', '-', '0.00735)']
['system', 'ROUGE-S*', 'Average_F:', '0.00152', '(95%-conf.int.', '0.00152', '-', '0.00152)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:136', 'F:1']
['We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.']
['IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.', 'IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.', 'RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.', 'MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.']
['system', 'ROUGE-S*', 'Average_R:', '0.00565', '(95%-conf.int.', '0.00565', '-', '0.00565)']
['system', 'ROUGE-S*', 'Average_P:', '0.22222', '(95%-conf.int.', '0.22222', '-', '0.22222)']
['system', 'ROUGE-S*', 'Average_F:', '0.01102', '(95%-conf.int.', '0.01102', '-', '0.01102)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:45', 'F:10']
['For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.']
['These models generally outperform our memory consumption but are much slower  even when cached.', 'We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.', 'We substantially outperform all of them on query speed and offer lower memory consumption than lossless alternatives.', 'The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.', 'The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.']
['system', 'ROUGE-S*', 'Average_R:', '0.00177', '(95%-conf.int.', '0.00177', '-', '0.00177)']
['system', 'ROUGE-S*', 'Average_P:', '0.00615', '(95%-conf.int.', '0.00615', '-', '0.00615)']
['system', 'ROUGE-S*', 'Average_F:', '0.00275', '(95%-conf.int.', '0.00275', '-', '0.00275)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:325', 'F:2']
['For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.']
['Unigram lookup is dense so we use an array of probability and backoff values.', u'Queries take the form p(wn|wn\u22121 1 ) where wn1 is an n-gram.', u'By contrast  BerkeleyLM\u2019s hash and compressed variants will return incorrect results based on an n \u22121-gram.', 'We maintain a separate array for each length n containing all n-gram entries sorted in suffix order.', 'In a model we built with default settings  1.2% of n + 1-grams were missing their ngram suffix.']
['system', 'ROUGE-S*', 'Average_R:', '0.00097', '(95%-conf.int.', '0.00097', '-', '0.00097)']
['system', 'ROUGE-S*', 'Average_P:', '0.00308', '(95%-conf.int.', '0.00308', '-', '0.00308)']
['system', 'ROUGE-S*', 'Average_F:', '0.00147', '(95%-conf.int.', '0.00147', '-', '0.00147)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:325', 'F:1']
['This section measures performance on shared tasks in order of increasing complexity: sparse lookups, evaluating perplexity of a large file, and translation with Moses.']
['IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.', 'RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.', 'MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.', 'SRILM (Stolcke  2002) is widely used within academia.', 'Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.']
['system', 'ROUGE-S*', 'Average_R:', '0.00151', '(95%-conf.int.', '0.00151', '-', '0.00151)']
['system', 'ROUGE-S*', 'Average_P:', '0.01667', '(95%-conf.int.', '0.01667', '-', '0.01667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00277', '(95%-conf.int.', '0.00277', '-', '0.00277)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:120', 'F:2']
['For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.']
['These packages are further described in Section 3.', 'In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.', 'Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.', 'Section 4.1 explained that state s is stored by applications with partial hypotheses to determine when they can be recombined.']
['system', 'ROUGE-S*', 'Average_R:', '0.00111', '(95%-conf.int.', '0.00111', '-', '0.00111)']
['system', 'ROUGE-S*', 'Average_P:', '0.00308', '(95%-conf.int.', '0.00308', '-', '0.00308)']
['system', 'ROUGE-S*', 'Average_F:', '0.00163', '(95%-conf.int.', '0.00163', '-', '0.00163)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:325', 'F:1']
0.034019999811 0.00218722221007 0.00394388886698
creating setting: abstract
else part task 2
eval: abstract
['system', 'ROUGE-1', 'Average_R:', '0.99025', '(95%-conf.int.', '0.98333', '-', '0.99616)']
['system', 'ROUGE-1', 'Average_P:', '0.07940', '(95%-conf.int.', '0.05502', '-', '0.10114)']
['system', 'ROUGE-1', 'Average_F:', '0.14679', '(95%-conf.int.', '0.10424', '-', '0.18362)']
['system', 'ROUGE-1', 'Eval', 'A00-2018.system', 'R:82', 'P:630', 'F:81']
['system', 'ROUGE-1', 'Eval', 'A00-2030.system', 'R:31', 'P:639', 'F:31']
['system', 'ROUGE-1', 'Eval', 'A97-1014.system', 'R:36', 'P:354', 'F:35']
['system', 'ROUGE-1', 'Eval', 'D09-1092.system', 'R:50', 'P:569', 'F:50']
['system', 'ROUGE-1', 'Eval', 'D10-1044.system', 'R:44', 'P:606', 'F:44']
['system', 'ROUGE-1', 'Eval', 'E03-1005.system', 'R:53', 'P:632', 'F:53']
['system', 'ROUGE-1', 'Eval', 'J01-2004.system', 'R:90', 'P:595', 'F:90']
['system', 'ROUGE-1', 'Eval', 'P04-1036.system', 'R:87', 'P:673', 'F:87']
['system', 'ROUGE-1', 'Eval', 'P05-1013.system', 'R:52', 'P:738', 'F:52']
['system', 'ROUGE-1', 'Eval', 'P08-1028.system', 'R:40', 'P:733', 'F:40']
['system', 'ROUGE-1', 'Eval', 'P08-1043.system', 'R:67', 'P:748', 'F:67']
['system', 'ROUGE-1', 'Eval', 'P08-1102.system', 'R:62', 'P:799', 'F:60']
['system', 'ROUGE-1', 'Eval', 'P11-1060.system', 'R:63', 'P:810', 'F:63']
['system', 'ROUGE-1', 'Eval', 'P11-1061.system', 'R:70', 'P:853', 'F:70']
['system', 'ROUGE-1', 'Eval', 'P87-1015.system', 'R:60', 'P:748', 'F:57']
['system', 'ROUGE-1', 'Eval', 'W06-2932.system', 'R:53', 'P:533', 'F:53']
['system', 'ROUGE-1', 'Eval', 'W06-3114.system', 'R:24', 'P:2391', 'F:23']
['system', 'ROUGE-1', 'Eval', 'W11-2123.system', 'R:79', 'P:622', 'F:77']
['system', 'ROUGE-1', 'Eval', 'W99-0613.system', 'R:77', 'P:594', 'F:76']
['system', 'ROUGE-1', 'Eval', 'W99-0623.system', 'R:32', 'P:488', 'F:32']
['system', 'ROUGE-2', 'Average_R:', '0.90779', '(95%-conf.int.', '0.87394', '-', '0.93595)']
yay
['system', 'ROUGE-2', 'Average_P:', '0.07174', '(95%-conf.int.', '0.04852', '-', '0.09229)']
yay
['system', 'ROUGE-2', 'Average_F:', '0.13277', '(95%-conf.int.', '0.09203', '-', '0.16771)']
yay
['system', 'ROUGE-2', 'Eval', 'A00-2018.system', 'R:81', 'P:629', 'F:74']
yay
['system', 'ROUGE-2', 'Eval', 'A00-2030.system', 'R:30', 'P:638', 'F:29']
yay
['system', 'ROUGE-2', 'Eval', 'A97-1014.system', 'R:35', 'P:353', 'F:30']
yay
['system', 'ROUGE-2', 'Eval', 'D09-1092.system', 'R:49', 'P:568', 'F:47']
yay
['system', 'ROUGE-2', 'Eval', 'D10-1044.system', 'R:43', 'P:605', 'F:41']
yay
['system', 'ROUGE-2', 'Eval', 'E03-1005.system', 'R:52', 'P:631', 'F:50']
yay
['system', 'ROUGE-2', 'Eval', 'J01-2004.system', 'R:89', 'P:594', 'F:86']
yay
['system', 'ROUGE-2', 'Eval', 'P04-1036.system', 'R:86', 'P:672', 'F:79']
yay
['system', 'ROUGE-2', 'Eval', 'P05-1013.system', 'R:51', 'P:737', 'F:49']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1028.system', 'R:39', 'P:732', 'F:36']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1043.system', 'R:66', 'P:747', 'F:63']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1102.system', 'R:61', 'P:798', 'F:54']
yay
['system', 'ROUGE-2', 'Eval', 'P11-1060.system', 'R:62', 'P:809', 'F:58']
yay
['system', 'ROUGE-2', 'Eval', 'P11-1061.system', 'R:69', 'P:852', 'F:67']
yay
['system', 'ROUGE-2', 'Eval', 'P87-1015.system', 'R:59', 'P:747', 'F:52']
yay
['system', 'ROUGE-2', 'Eval', 'W06-2932.system', 'R:52', 'P:532', 'F:47']
yay
['system', 'ROUGE-2', 'Eval', 'W06-3114.system', 'R:23', 'P:2390', 'F:8']
yay
['system', 'ROUGE-2', 'Eval', 'W11-2123.system', 'R:78', 'P:621', 'F:63']
yay
['system', 'ROUGE-2', 'Eval', 'W99-0613.system', 'R:76', 'P:593', 'F:67']
yay
['system', 'ROUGE-2', 'Eval', 'W99-0623.system', 'R:31', 'P:487', 'F:28']
yay
['system', 'ROUGE-SU*', 'Average_R:', '0.97587', '(95%-conf.int.', '0.95806', '-', '0.99074)']
['system', 'ROUGE-SU*', 'Average_P:', '0.00610', '(95%-conf.int.', '0.00235', '-', '0.01080)']
['system', 'ROUGE-SU*', 'Average_F:', '0.01211', '(95%-conf.int.', '0.00470', '-', '0.02137)']
['system', 'ROUGE-SU*', 'Eval', 'A00-2018.system', 'R:3402', 'P:198764', 'F:3316']
['system', 'ROUGE-SU*', 'Eval', 'A00-2030.system', 'R:495', 'P:204479', 'F:495']
['system', 'ROUGE-SU*', 'Eval', 'A97-1014.system', 'R:665', 'P:62834', 'F:627']
['system', 'ROUGE-SU*', 'Eval', 'D09-1092.system', 'R:1274', 'P:162164', 'F:1274']
['system', 'ROUGE-SU*', 'Eval', 'D10-1044.system', 'R:989', 'P:183920', 'F:989']
['system', 'ROUGE-SU*', 'Eval', 'E03-1005.system', 'R:1430', 'P:200027', 'F:1430']
['system', 'ROUGE-SU*', 'Eval', 'J01-2004.system', 'R:4094', 'P:177309', 'F:4094']
['system', 'ROUGE-SU*', 'Eval', 'P04-1036.system', 'R:3827', 'P:226800', 'F:3818']
['system', 'ROUGE-SU*', 'Eval', 'P05-1013.system', 'R:1377', 'P:272690', 'F:1377']
['system', 'ROUGE-SU*', 'Eval', 'P08-1028.system', 'R:819', 'P:269010', 'F:819']
['system', 'ROUGE-SU*', 'Eval', 'P08-1043.system', 'R:2277', 'P:280125', 'F:2277']
['system', 'ROUGE-SU*', 'Eval', 'P08-1102.system', 'R:1952', 'P:319599', 'F:1718']
['system', 'ROUGE-SU*', 'Eval', 'P11-1060.system', 'R:2015', 'P:328454', 'F:2007']
['system', 'ROUGE-SU*', 'Eval', 'P11-1061.system', 'R:2484', 'P:364230', 'F:2438']
['system', 'ROUGE-SU*', 'Eval', 'P87-1015.system', 'R:1829', 'P:280125', 'F:1652']
['system', 'ROUGE-SU*', 'Eval', 'W06-2932.system', 'R:1430', 'P:142310', 'F:1430']
['system', 'ROUGE-SU*', 'Eval', 'W06-3114.system', 'R:299', 'P:2859635', 'F:266']
['system', 'ROUGE-SU*', 'Eval', 'W11-2123.system', 'R:3159', 'P:193752', 'F:2985']
['system', 'ROUGE-SU*', 'Eval', 'W99-0613.system', 'R:3002', 'P:176714', 'F:2922']
['system', 'ROUGE-SU*', 'Eval', 'W99-0623.system', 'R:527', 'P:119315', 'F:527']
creating setting: community
else part task 2
eval: community
['system', 'ROUGE-1', 'Average_R:', '0.58702', '(95%-conf.int.', '0.55718', '-', '0.61482)']
['system', 'ROUGE-1', 'Average_P:', '0.22698', '(95%-conf.int.', '0.16457', '-', '0.28598)']
['system', 'ROUGE-1', 'Average_F:', '0.32604', '(95%-conf.int.', '0.25579', '-', '0.38408)']
['system', 'ROUGE-1', 'Eval', 'A00-2018.system', 'R:1269', 'P:1890', 'F:783']
['system', 'ROUGE-1', 'Eval', 'A00-2030.system', 'R:1053', 'P:1917', 'F:645']
['system', 'ROUGE-1', 'Eval', 'A97-1014.system', 'R:603', 'P:1062', 'F:222']
['system', 'ROUGE-1', 'Eval', 'D09-1092.system', 'R:1296', 'P:1707', 'F:663']
['system', 'ROUGE-1', 'Eval', 'D10-1044.system', 'R:1125', 'P:1818', 'F:690']
['system', 'ROUGE-1', 'Eval', 'E03-1005.system', 'R:801', 'P:1896', 'F:450']
['system', 'ROUGE-1', 'Eval', 'J01-2004.system', 'R:558', 'P:1785', 'F:324']
['system', 'ROUGE-1', 'Eval', 'P04-1036.system', 'R:1004', 'P:2692', 'F:636']
['system', 'ROUGE-1', 'Eval', 'P05-1013.system', 'R:633', 'P:2214', 'F:429']
['system', 'ROUGE-1', 'Eval', 'P08-1028.system', 'R:612', 'P:2199', 'F:390']
['system', 'ROUGE-1', 'Eval', 'P08-1043.system', 'R:1323', 'P:2244', 'F:783']
['system', 'ROUGE-1', 'Eval', 'P08-1102.system', 'R:972', 'P:2397', 'F:600']
['system', 'ROUGE-1', 'Eval', 'P11-1060.system', 'R:762', 'P:2430', 'F:408']
['system', 'ROUGE-1', 'Eval', 'P11-1061.system', 'R:768', 'P:2559', 'F:579']
['system', 'ROUGE-1', 'Eval', 'P87-1015.system', 'R:897', 'P:2244', 'F:573']
['system', 'ROUGE-1', 'Eval', 'W06-2932.system', 'R:1014', 'P:1599', 'F:528']
['system', 'ROUGE-1', 'Eval', 'W06-3114.system', 'R:543', 'P:7173', 'F:324']
['system', 'ROUGE-1', 'Eval', 'W11-2123.system', 'R:300', 'P:1866', 'F:183']
['system', 'ROUGE-1', 'Eval', 'W99-0613.system', 'R:824', 'P:2376', 'F:416']
['system', 'ROUGE-1', 'Eval', 'W99-0623.system', 'R:852', 'P:1464', 'F:504']
['system', 'ROUGE-2', 'Average_R:', '0.34199', '(95%-conf.int.', '0.30908', '-', '0.37559)']
yay
['system', 'ROUGE-2', 'Average_P:', '0.13207', '(95%-conf.int.', '0.09264', '-', '0.17054)']
yay
['system', 'ROUGE-2', 'Average_F:', '0.18978', '(95%-conf.int.', '0.14273', '-', '0.22969)']
yay
['system', 'ROUGE-2', 'Eval', 'A00-2018.system', 'R:1266', 'P:1887', 'F:399']
yay
['system', 'ROUGE-2', 'Eval', 'A00-2030.system', 'R:1050', 'P:1914', 'F:411']
yay
['system', 'ROUGE-2', 'Eval', 'A97-1014.system', 'R:600', 'P:1059', 'F:99']
yay
['system', 'ROUGE-2', 'Eval', 'D09-1092.system', 'R:1293', 'P:1704', 'F:378']
yay
['system', 'ROUGE-2', 'Eval', 'D10-1044.system', 'R:1122', 'P:1815', 'F:447']
yay
['system', 'ROUGE-2', 'Eval', 'E03-1005.system', 'R:798', 'P:1893', 'F:210']
yay
['system', 'ROUGE-2', 'Eval', 'J01-2004.system', 'R:555', 'P:1782', 'F:174']
yay
['system', 'ROUGE-2', 'Eval', 'P04-1036.system', 'R:1000', 'P:2688', 'F:436']
yay
['system', 'ROUGE-2', 'Eval', 'P05-1013.system', 'R:630', 'P:2211', 'F:258']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1028.system', 'R:609', 'P:2196', 'F:186']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1043.system', 'R:1320', 'P:2241', 'F:522']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1102.system', 'R:969', 'P:2394', 'F:360']
yay
['system', 'ROUGE-2', 'Eval', 'P11-1060.system', 'R:759', 'P:2427', 'F:192']
yay
['system', 'ROUGE-2', 'Eval', 'P11-1061.system', 'R:765', 'P:2556', 'F:405']
yay
['system', 'ROUGE-2', 'Eval', 'P87-1015.system', 'R:894', 'P:2241', 'F:339']
yay
['system', 'ROUGE-2', 'Eval', 'W06-2932.system', 'R:1011', 'P:1596', 'F:285']
yay
['system', 'ROUGE-2', 'Eval', 'W06-3114.system', 'R:540', 'P:7170', 'F:165']
yay
['system', 'ROUGE-2', 'Eval', 'W11-2123.system', 'R:297', 'P:1863', 'F:84']
yay
['system', 'ROUGE-2', 'Eval', 'W99-0613.system', 'R:820', 'P:2372', 'F:204']
yay
['system', 'ROUGE-2', 'Eval', 'W99-0623.system', 'R:849', 'P:1461', 'F:330']
yay
['system', 'ROUGE-SU*', 'Average_R:', '0.32445', '(95%-conf.int.', '0.29306', '-', '0.35704)']
['system', 'ROUGE-SU*', 'Average_P:', '0.04643', '(95%-conf.int.', '0.01947', '-', '0.08047)']
['system', 'ROUGE-SU*', 'Average_F:', '0.07994', '(95%-conf.int.', '0.03664', '-', '0.12838)']
['system', 'ROUGE-SU*', 'Eval', 'A00-2018.system', 'R:269025', 'P:596292', 'F:91926']
['system', 'ROUGE-SU*', 'Eval', 'A00-2030.system', 'R:185325', 'P:613437', 'F:68802']
['system', 'ROUGE-SU*', 'Eval', 'A97-1014.system', 'R:60900', 'P:188502', 'F:7455']
['system', 'ROUGE-SU*', 'Eval', 'D09-1092.system', 'R:280581', 'P:486492', 'F:68397']
['system', 'ROUGE-SU*', 'Eval', 'D10-1044.system', 'R:211497', 'P:551760', 'F:77421']
['system', 'ROUGE-SU*', 'Eval', 'E03-1005.system', 'R:107331', 'P:600081', 'F:33360']
['system', 'ROUGE-SU*', 'Eval', 'J01-2004.system', 'R:52170', 'P:531927', 'F:15558']
['system', 'ROUGE-SU*', 'Eval', 'P04-1036.system', 'R:126500', 'P:907200', 'F:46892']
['system', 'ROUGE-SU*', 'Eval', 'P05-1013.system', 'R:67095', 'P:818070', 'F:28629']
['system', 'ROUGE-SU*', 'Eval', 'P08-1028.system', 'R:62727', 'P:807030', 'F:22389']
['system', 'ROUGE-SU*', 'Eval', 'P08-1043.system', 'R:292380', 'P:840375', 'F:92076']
['system', 'ROUGE-SU*', 'Eval', 'P08-1102.system', 'R:157947', 'P:958797', 'F:55836']
['system', 'ROUGE-SU*', 'Eval', 'P11-1060.system', 'R:97152', 'P:985362', 'F:26769']
['system', 'ROUGE-SU*', 'Eval', 'P11-1061.system', 'R:98685', 'P:1092690', 'F:57132']
['system', 'ROUGE-SU*', 'Eval', 'P87-1015.system', 'R:134547', 'P:840375', 'F:48978']
['system', 'ROUGE-SU*', 'Eval', 'W06-2932.system', 'R:171870', 'P:426930', 'F:41037']
['system', 'ROUGE-SU*', 'Eval', 'W06-3114.system', 'R:49410', 'P:8578905', 'F:15615']
['system', 'ROUGE-SU*', 'Eval', 'W11-2123.system', 'R:15147', 'P:581256', 'F:5418']
['system', 'ROUGE-SU*', 'Eval', 'W99-0613.system', 'R:85280', 'P:706856', 'F:19896']
['system', 'ROUGE-SU*', 'Eval', 'W99-0623.system', 'R:121407', 'P:357945', 'F:39660']
creating setting: human
else part task 2
eval: human
['system', 'ROUGE-1', 'Average_R:', '0.88445', '(95%-conf.int.', '0.85514', '-', '0.91228)']
['system', 'ROUGE-1', 'Average_P:', '0.10388', '(95%-conf.int.', '0.07748', '-', '0.12577)']
['system', 'ROUGE-1', 'Average_F:', '0.18566', '(95%-conf.int.', '0.14223', '-', '0.22034)']
['system', 'ROUGE-1', 'Eval', 'A00-2018.system', 'R:336', 'P:1890', 'F:294']
['system', 'ROUGE-1', 'Eval', 'A00-2030.system', 'R:252', 'P:1917', 'F:234']
['system', 'ROUGE-1', 'Eval', 'A97-1014.system', 'R:228', 'P:1062', 'F:174']
['system', 'ROUGE-1', 'Eval', 'D09-1092.system', 'R:201', 'P:1707', 'F:189']
['system', 'ROUGE-1', 'Eval', 'D10-1044.system', 'R:348', 'P:1818', 'F:315']
['system', 'ROUGE-1', 'Eval', 'E03-1005.system', 'R:309', 'P:1896', 'F:291']
['system', 'ROUGE-1', 'Eval', 'J01-2004.system', 'R:201', 'P:1785', 'F:171']
['system', 'ROUGE-1', 'Eval', 'P04-1036.system', 'R:344', 'P:2692', 'F:284']
['system', 'ROUGE-1', 'Eval', 'P05-1013.system', 'R:243', 'P:2214', 'F:234']
['system', 'ROUGE-1', 'Eval', 'P08-1028.system', 'R:333', 'P:2199', 'F:267']
['system', 'ROUGE-1', 'Eval', 'P08-1043.system', 'R:198', 'P:2244', 'F:174']
['system', 'ROUGE-1', 'Eval', 'P08-1102.system', 'R:318', 'P:2397', 'F:309']
['system', 'ROUGE-1', 'Eval', 'P11-1060.system', 'R:270', 'P:2430', 'F:225']
['system', 'ROUGE-1', 'Eval', 'P11-1061.system', 'R:306', 'P:2559', 'F:285']
['system', 'ROUGE-1', 'Eval', 'P87-1015.system', 'R:180', 'P:2244', 'F:174']
['system', 'ROUGE-1', 'Eval', 'W06-2932.system', 'R:219', 'P:1599', 'F:192']
['system', 'ROUGE-1', 'Eval', 'W06-3114.system', 'R:213', 'P:7173', 'F:165']
['system', 'ROUGE-1', 'Eval', 'W11-2123.system', 'R:252', 'P:1866', 'F:201']
['system', 'ROUGE-1', 'Eval', 'W99-0613.system', 'R:256', 'P:2376', 'F:252']
['system', 'ROUGE-1', 'Eval', 'W99-0623.system', 'R:261', 'P:1464', 'F:228']
['system', 'ROUGE-2', 'Average_R:', '0.67319', '(95%-conf.int.', '0.61375', '-', '0.73320)']
yay
['system', 'ROUGE-2', 'Average_P:', '0.07838', '(95%-conf.int.', '0.05485', '-', '0.09706)']
yay
['system', 'ROUGE-2', 'Average_F:', '0.14022', '(95%-conf.int.', '0.10108', '-', '0.17077)']
yay
['system', 'ROUGE-2', 'Eval', 'A00-2018.system', 'R:333', 'P:1887', 'F:225']
yay
['system', 'ROUGE-2', 'Eval', 'A00-2030.system', 'R:249', 'P:1914', 'F:165']
yay
['system', 'ROUGE-2', 'Eval', 'A97-1014.system', 'R:225', 'P:1059', 'F:132']
yay
['system', 'ROUGE-2', 'Eval', 'D09-1092.system', 'R:198', 'P:1704', 'F:180']
yay
['system', 'ROUGE-2', 'Eval', 'D10-1044.system', 'R:345', 'P:1815', 'F:240']
yay
['system', 'ROUGE-2', 'Eval', 'E03-1005.system', 'R:306', 'P:1893', 'F:222']
yay
['system', 'ROUGE-2', 'Eval', 'J01-2004.system', 'R:198', 'P:1782', 'F:144']
yay
['system', 'ROUGE-2', 'Eval', 'P04-1036.system', 'R:340', 'P:2688', 'F:188']
yay
['system', 'ROUGE-2', 'Eval', 'P05-1013.system', 'R:240', 'P:2211', 'F:213']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1028.system', 'R:330', 'P:2196', 'F:210']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1043.system', 'R:195', 'P:2241', 'F:108']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1102.system', 'R:315', 'P:2394', 'F:270']
yay
['system', 'ROUGE-2', 'Eval', 'P11-1060.system', 'R:267', 'P:2427', 'F:126']
yay
['system', 'ROUGE-2', 'Eval', 'P11-1061.system', 'R:303', 'P:2556', 'F:201']
yay
['system', 'ROUGE-2', 'Eval', 'P87-1015.system', 'R:177', 'P:2241', 'F:156']
yay
['system', 'ROUGE-2', 'Eval', 'W06-2932.system', 'R:216', 'P:1596', 'F:108']
yay
['system', 'ROUGE-2', 'Eval', 'W06-3114.system', 'R:210', 'P:7170', 'F:69']
yay
['system', 'ROUGE-2', 'Eval', 'W11-2123.system', 'R:249', 'P:1863', 'F:144']
yay
['system', 'ROUGE-2', 'Eval', 'W99-0613.system', 'R:252', 'P:2372', 'F:220']
yay
['system', 'ROUGE-2', 'Eval', 'W99-0623.system', 'R:258', 'P:1461', 'F:174']
yay
['system', 'ROUGE-SU*', 'Average_R:', '0.74664', '(95%-conf.int.', '0.70064', '-', '0.79125)']
['system', 'ROUGE-SU*', 'Average_P:', '0.00936', '(95%-conf.int.', '0.00411', '-', '0.01514)']
['system', 'ROUGE-SU*', 'Average_F:', '0.01846', '(95%-conf.int.', '0.00818', '-', '0.02967)']
['system', 'ROUGE-SU*', 'Eval', 'A00-2018.system', 'R:18981', 'P:596292', 'F:13464']
['system', 'ROUGE-SU*', 'Eval', 'A00-2030.system', 'R:10707', 'P:613437', 'F:8433']
['system', 'ROUGE-SU*', 'Eval', 'A97-1014.system', 'R:8775', 'P:188502', 'F:4647']
['system', 'ROUGE-SU*', 'Eval', 'D09-1092.system', 'R:6831', 'P:486492', 'F:5667']
['system', 'ROUGE-SU*', 'Eval', 'D10-1044.system', 'R:20355', 'P:551760', 'F:15903']
['system', 'ROUGE-SU*', 'Eval', 'E03-1005.system', 'R:16065', 'P:600081', 'F:12609']
['system', 'ROUGE-SU*', 'Eval', 'J01-2004.system', 'R:6831', 'P:531927', 'F:4938']
['system', 'ROUGE-SU*', 'Eval', 'P04-1036.system', 'R:14960', 'P:907200', 'F:9864']
['system', 'ROUGE-SU*', 'Eval', 'P05-1013.system', 'R:9960', 'P:818070', 'F:8892']
['system', 'ROUGE-SU*', 'Eval', 'P08-1028.system', 'R:18645', 'P:807030', 'F:12207']
['system', 'ROUGE-SU*', 'Eval', 'P08-1043.system', 'R:6630', 'P:840375', 'F:4857']
['system', 'ROUGE-SU*', 'Eval', 'P08-1102.system', 'R:17010', 'P:958797', 'F:15201']
['system', 'ROUGE-SU*', 'Eval', 'P11-1060.system', 'R:12282', 'P:985362', 'F:8094']
['system', 'ROUGE-SU*', 'Eval', 'P11-1061.system', 'R:15756', 'P:1092690', 'F:12972']
['system', 'ROUGE-SU*', 'Eval', 'P87-1015.system', 'R:5487', 'P:840375', 'F:5097']
['system', 'ROUGE-SU*', 'Eval', 'W06-2932.system', 'R:8100', 'P:426930', 'F:6108']
['system', 'ROUGE-SU*', 'Eval', 'W06-3114.system', 'R:7665', 'P:8578905', 'F:4329']
['system', 'ROUGE-SU*', 'Eval', 'W11-2123.system', 'R:10707', 'P:581256', 'F:6315']
['system', 'ROUGE-SU*', 'Eval', 'W99-0613.system', 'R:8316', 'P:706856', 'F:7900']
['system', 'ROUGE-SU*', 'Eval', 'W99-0623.system', 'R:11481', 'P:357945', 'F:8223']
