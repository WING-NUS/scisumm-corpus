parsing: input/ref/Task1/W99-0613_swastika.csv
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
<S sid="159" ssid="26">To prevent this we &amp;quot;smooth&amp;quot; the confidence by adding a small value, e, to both W+ and W_, giving at = Plugging the value of at from Equ.</S>
original cit marker offset is 0
new cit marker offset is 0



['159']
159
['159']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S>
original cit marker offset is 0
new cit marker offset is 0



['137']
137
['137']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['method_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
 <S sid="250" ssid="1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S>
original cit marker offset is 0
new cit marker offset is 0



['250']
250
['250']
parsed_discourse_facet ['result_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
    <S sid="36" ssid="30">Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.</S>
original cit marker offset is 0
new cit marker offset is 0



['36']
36
['36']
parsed_discourse_facet ['result_citation']
<S sid="29" ssid="23">Unfortunately, Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective function.</S>
original cit marker offset is 0
new cit marker offset is 0



['29']
29
['29']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
    <S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['method_citation']
    <S sid="95" ssid="28">(Specifically, the limit n starts at 5 and increases by 5 at each iteration.)</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['method_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="125" ssid = "58">It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'178'", "'125'", "'41'"]
'256'
'198'
'178'
'125'
'41'
['256', '198', '178', '125', '41']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'97'", "'70'", "'198'"]
'256'
'178'
'97'
'70'
'198'
['256', '178', '97', '70', '198']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="128" ssid = "61">At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'128'", "'198'", "'178'", "'70'"]
'256'
'128'
'198'
'178'
'70'
['256', '128', '198', '178', '70']
parsed_discourse_facet ['results_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'97'", "'70'", "'198'", "'178'"]
'256'
'97'
'70'
'198'
'178'
['256', '97', '70', '198', '178']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="70" ssid = "3">.</S><S sid ="220" ssid = "87">(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'198'", "'70'", "'220'"]
'256'
'178'
'198'
'70'
'220'
['256', '178', '198', '70', '220']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'178'", "'70'", "'236'"]
'256'
'198'
'178'
'70'
'236'
['256', '198', '178', '70', '236']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="70" ssid = "3">.</S><S sid ="128" ssid = "61">At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'70'", "'128'", "'97'"]
'256'
'198'
'70'
'128'
'97'
['256', '198', '70', '128', '97']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S><S sid ="220" ssid = "87">(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'70'", "'220'", "'198'"]
'256'
'178'
'70'
'220'
'198'
['256', '178', '70', '220', '198']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'236'", "'41'", "'97'"]
'256'
'198'
'236'
'41'
'97'
['256', '198', '236', '41', '97']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="70" ssid = "3">.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'97'", "'236'", "'70'"]
'256'
'178'
'97'
'236'
'70'
['256', '178', '97', '236', '70']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="125" ssid = "58">It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'198'", "'125'", "'41'"]
'256'
'178'
'198'
'125'
'41'
['256', '178', '198', '125', '41']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'97'", "'178'", "'198'", "'236'"]
'256'
'97'
'178'
'198'
'236'
['256', '97', '178', '198', '236']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'70'", "'198'", "'178'", "'236'"]
'256'
'70'
'198'
'178'
'236'
['256', '70', '198', '178', '236']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/E03-1005_swastika.csv
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
    <S sid="41" ssid="38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



['41']
41
['41']
parsed_discourse_facet ['aim_citation']
<S sid="80" ssid="32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S>
original cit marker offset is 0
new cit marker offset is 0



['80']
80
['80']
parsed_discourse_facet ['result_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



['143']
143
['143']
parsed_discourse_facet ['method_citation']
<S sid="146" ssid="11">This paper also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic depen dencies (as e.g. in Collins 1999 and Charniak 2000).</S>
original cit marker offset is 0
new cit marker offset is 0



['146']
146
['146']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
<S sid="100" ssid="3">In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['result_citation']
    <S sid="30" ssid="27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S>
original cit marker offset is 0
new cit marker offset is 0



['30']
30
['30']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
    <S sid="30" ssid="27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S>
original cit marker offset is 0
new cit marker offset is 0



['30']
30
['30']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="95" ssid = "47">By using these PCFG-reductions we can thus parse with all subtrees in polynomial time.</S><S sid ="25" ssid = "22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="87" ssid = "39">Yet  his grammar contains more than 5 million subtrees and processing times of over 200 seconds per WSJ sentence are reported (Bod 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'95'", "'25'", "'22'", "'87'"]
'130'
'95'
'25'
'22'
'87'
['130', '95', '25', '22', '87']
parsed_discourse_facet ['implication_citation']
<S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S><S sid ="58" ssid = "10">The notation A@k denotes the node at address k where A is the nonterminal labeling that node.</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="112" ssid = "15">Moreover  if n gets large  SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'", "'58'", "'40'", "'79'", "'112'"]
'85'
'58'
'40'
'79'
'112'
['85', '58', '40', '79', '112']
parsed_discourse_facet ['implication_citation']
<S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="25" ssid = "22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="105" ssid = "8">The derivation with the smallest sum  or highest rank  is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP  its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'25'", "'40'", "'108'", "'105'"]
'130'
'25'
'40'
'108'
'105'
['130', '25', '40', '108', '105']
parsed_discourse_facet ['results_citation']
<S sid ="30" ssid = "27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S><S sid ="139" ssid = "4">But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP  the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.</S><S sid ="32" ssid = "29">However  ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'139'", "'32'", "'22'", "'40'"]
'30'
'139'
'32'
'22'
'40'
['30', '139', '32', '22', '40']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S><S sid ="112" ssid = "15">Moreover  if n gets large  SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="71" ssid = "23">For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.</S><S sid ="139" ssid = "4">But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP  the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'112'", "'79'", "'71'", "'139'"]
'80'
'112'
'79'
'71'
'139'
['80', '112', '79', '71', '139']
parsed_discourse_facet ['method_citation']
<S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="92" ssid = "44">Tested on the OVIS corpus  Bonnema et al. 's proposal obtains results that are comparable to Sima'an (1999) -- see Bonnema et al.</S><S sid ="51" ssid = "3">That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S sid ="101" ssid = "4">We will refer to this model as Simplicity-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'82'", "'140'", "'92'", "'51'", "'101'"]
'82'
'140'
'92'
'51'
'101'
['82', '140', '92', '51', '101']
parsed_discourse_facet ['method_citation']
<S sid ="94" ssid = "46">This paper presents the first published results with this estimator on the WSJ.</S><S sid ="42" ssid = "39">We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.</S><S sid ="12" ssid = "9">This approach has now gained wide usage  as exemplified by the work of Collins (1996  1999)  Charniak (1996  1997)  Johnson (1998)  Chiang (2000)  and many others.</S><S sid ="51" ssid = "3">That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'", "'42'", "'12'", "'51'", "'45'"]
'94'
'42'
'12'
'51'
'45'
['94', '42', '12', '51', '45']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "4">We will refer to this model as Simplicity-DOP.</S><S sid ="51" ssid = "3">That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S sid ="139" ssid = "4">But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP  the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.</S><S sid ="67" ssid = "19">We can create a subtree by choosing any possible left subtree and any possible right subtree.</S><S sid ="126" ssid = "7">We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores  as these are commonly used to rank parsing systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'51'", "'139'", "'67'", "'126'"]
'101'
'51'
'139'
'67'
'126'
['101', '51', '139', '67', '126']
parsed_discourse_facet ['method_citation']
<S sid ="111" ssid = "14">Note that for n=1  SL-DOP is equal to Likelihood-DOP  since there is only one most probable tree to select from  and LS-DOP is equal to Simplicity-DOP  since there is only one simplest tree to select from.</S><S sid ="0" ssid = "0">An Efficient Implementation of a New DOP Model</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="71" ssid = "23">For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'111'", "'0'", "'82'", "'71'", "'140'"]
'111'
'0'
'82'
'71'
'140'
['111', '0', '82', '71', '140']
parsed_discourse_facet ['method_citation']
<S sid ="112" ssid = "15">Moreover  if n gets large  SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.</S><S sid ="145" ssid = "10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S><S sid ="106" ssid = "9">What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.</S><S sid ="101" ssid = "4">We will refer to this model as Simplicity-DOP.</S><S sid ="68" ssid = "20">Thus  there are aj= (bk+ 1)(ci + 1) possible subtrees headed by A @j. Goodman then gives a simple small PCFG with the following property: for every subtree in the training corpus headed by A  the grammar will generate an isomorphic subderivation with probability 1/a.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'145'", "'106'", "'101'", "'68'"]
'112'
'145'
'106'
'101'
'68'
['112', '145', '106', '101', '68']
parsed_discourse_facet ['implication_citation']
<S sid ="58" ssid = "10">The notation A@k denotes the node at address k where A is the nonterminal labeling that node.</S><S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S><S sid ="38" ssid = "35">Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task  the parsing time was reported to be over 200 seconds per sentence (Bod 2003).</S><S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="106" ssid = "9">What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'28'", "'38'", "'13'", "'106'"]
'58'
'28'
'38'
'13'
'106'
['58', '28', '38', '13', '106']
parsed_discourse_facet ['method_citation']
<S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="29" ssid = "26">Johnson (1998b  2002) showed that DOP1's subtree estimation method is statistically biased and inconsistent.</S><S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'", "'40'", "'39'", "'29'", "'130'"]
'28'
'40'
'39'
'29'
'130'
['28', '40', '39', '29', '130']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S><S sid ="44" ssid = "41">But while Bod's estimator obtains state-of-the-art results on the WSJ  comparable to Charniak (2000) and Collins (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'22'", "'45'", "'44'", "'82'"]
'39'
'22'
'45'
'44'
'82'
['39', '22', '45', '44', '82']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="0" ssid = "0">An Efficient Implementation of a New DOP Model</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'22'", "'0'", "'79'", "'15'"]
'108'
'22'
'0'
'79'
'15'
['108', '22', '0', '79', '15']
parsed_discourse_facet ['results_citation']
<S sid ="25" ssid = "22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S><S sid ="114" ssid = "17">Note that Goodman's PCFG-reduction method summarized in Section 2 applies not only to Likelihood-DOP but also to Simplicity-DOP.</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="89" ssid = "41">Let a be the number of times nonterminals of type A occur in the training data.</S><S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'114'", "'82'", "'89'", "'28'"]
'25'
'114'
'82'
'89'
'28'
['25', '114', '82', '89', '28']
parsed_discourse_facet ['implication_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



E03-1005
P04-1013
0
result_citation
['method_citation']
IGNORE THIS: Key error 5
parsing: input/ref/Task1/A97-1014_swastika.csv
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



['151']
151
['151']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
<S sid="47" ssid="37">Argument structure can be represented in terms of unordered trees (with crossing branches).</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



['167']
167
['167']
parsed_discourse_facet ['method_citation']
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



['160']
160
['160']
parsed_discourse_facet ['method_citation']
<S sid="127" ssid="8">As the need for certain functionalities becomes obvious with growing annotation experience, we have decided to implement the tool in two stages.</S>
original cit marker offset is 0
new cit marker offset is 0



['127']
127
['127']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



['39']
39
['39']
parsed_discourse_facet ['method_citation']
<S sid="72" ssid="17">In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.</S>
original cit marker offset is 0
new cit marker offset is 0



['72']
72
['72']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/A97-1014.csv
<S sid ="73" ssid = "18">While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.</S><S sid ="115" ssid = "28">3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here  where C stands for coordination  and VP for the actual category.</S><S sid ="135" ssid = "16">Figure 1 shows a screen dump of the tool.</S><S sid ="39" ssid = "29">Consider the German sentence (1) daran wird ihn Anna erkennen  &di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies  marked by co-references between traces and the corresponding nodes.</S><S sid ="84" ssid = "29">Consider (qui verbs where the subject of the infinitival VP is not realised syntactically  but co-referent with the subject or object. of the matrix equi verb: (3) er bat mich zu kommen he asked me to come (mich is the understood subject. of kommt n).</S>
original cit marker offset is 0
new cit marker offset is 0



["'73'", "'115'", "'135'", "'39'", "'84'"]
'73'
'115'
'135'
'39'
'84'
['73', '115', '135', '39', '84']
parsed_discourse_facet ['implication_citation']
<S sid ="110" ssid = "23">A similar convention ha.s been adopted for constructions in which scope ambiguities have syntactic effects but a one-to-one correspondence between scope and attachment. does not seem reasonable  cf. focus particles such as only or also.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="30" ssid = "20">Thus the context-free constituent backbone plays a pivotal role in the annotation scheme.</S><S sid ="113" ssid = "26">Since a precise structural description of non-constituent coordination would require a. rich inventory of incomplete phrase types  we have agreed on a sort of unde.rspe.cified representations: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'", "'80'", "'30'", "'113'", "'56'"]
'110'
'80'
'30'
'113'
'56'
['110', '80', '30', '113', '56']
parsed_discourse_facet ['implication_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S><S sid ="78" ssid = "23">Morphological information: Another set of labels represents morphological information.</S><S sid ="8" ssid = "5">A formalism complying with these requirements is described in section 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'56'", "'60'", "'78'", "'8'"]
'80'
'56'
'60'
'78'
'8'
['80', '56', '60', '78', '8']
parsed_discourse_facet ['results_citation']
<S sid ="0" ssid = "0">An Annotation Scheme for Free Word Order Languages</S><S sid ="18" ssid = "8">(Marcus et al.  1994).</S><S sid ="151" ssid = "32">For evaluation  the already annotated sentences were divided into two disjoint sets  one for training (90% of the corpus)  the other one for testing (10%).</S><S sid ="49" ssid = "39">Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.</S><S sid ="114" ssid = "27">Fig.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'18'", "'151'", "'49'", "'114'"]
'0'
'18'
'151'
'49'
'114'
['0', '18', '151', '49', '114']
parsed_discourse_facet ['method_citation']
<S sid ="65" ssid = "10">The tree resembles traditional constituent structures.</S><S sid ="62" ssid = "7">2.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="77" ssid = "22">Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'62'", "'80'", "'77'", "'5'"]
'65'
'62'
'80'
'77'
'5'
['65', '62', '80', '77', '5']
parsed_discourse_facet ['method_citation']
<S sid ="49" ssid = "39">Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="118" ssid = "31">An explicit coordinating conjunction need not be present.</S><S sid ="28" ssid = "18">(Bies et al.  1995)  (Sampson  1995)).</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'56'", "'118'", "'28'", "'80'"]
'49'
'56'
'118'
'28'
'80'
['49', '56', '118', '28', '80']
parsed_discourse_facet ['method_citation']
<S sid ="65" ssid = "10">The tree resembles traditional constituent structures.</S><S sid ="108" ssid = "21">However  full or partial disambiguation takes place in context  and the annotators do not consider unrealistic readings.</S><S sid ="96" ssid = "9">In (5)  either a lexical nominalisation rule for the adjective Gliickliche is stipulated  or the existence of an empty nominal head.</S><S sid ="86" ssid = "31">We call such additional edges secondary links and represent them as dotted lines  see fig.</S><S sid ="135" ssid = "16">Figure 1 shows a screen dump of the tool.</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'108'", "'96'", "'86'", "'135'"]
'65'
'108'
'96'
'86'
'135'
['65', '108', '96', '86', '135']
parsed_discourse_facet ['method_citation']
<S sid ="115" ssid = "28">3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here  where C stands for coordination  and VP for the actual category.</S><S sid ="77" ssid = "22">Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.</S><S sid ="109" ssid = "22">In addition  we have adopted a simple convention for those cases in which context information is insufficient  for total disambiguation: the highest possible attachment  site is chosen.</S><S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'77'", "'109'", "'60'", "'21'"]
'115'
'77'
'109'
'60'
'21'
['115', '77', '109', '60', '21']
parsed_discourse_facet ['method_citation']
<S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="73" ssid = "18">While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.</S><S sid ="86" ssid = "31">We call such additional edges secondary links and represent them as dotted lines  see fig.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S><S sid ="77" ssid = "22">Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'73'", "'86'", "'21'", "'77'"]
'56'
'73'
'86'
'21'
'77'
['56', '73', '86', '21', '77']
parsed_discourse_facet ['method_citation']
<S sid ="138" ssid = "19">This allows easy modification if needed.</S><S sid ="125" ssid = "6">The tool should also permit a convenient handling of node and edge labels.</S><S sid ="89" ssid = "2">However  some other standard analysts turn out to be problematic  mainly due to the partial  idealised character of competence grammars  which often marginalise or ignore such important. phenomena. as 'deficient' (e.g. headless) constructions  appositions  temporal expressions  etc.</S><S sid ="49" ssid = "39">Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.</S><S sid ="110" ssid = "23">A similar convention ha.s been adopted for constructions in which scope ambiguities have syntactic effects but a one-to-one correspondence between scope and attachment. does not seem reasonable  cf. focus particles such as only or also.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'", "'125'", "'89'", "'49'", "'110'"]
'138'
'125'
'89'
'49'
'110'
['138', '125', '89', '49', '110']
parsed_discourse_facet ['implication_citation']
<S sid ="147" ssid = "28">(Cutting et al.  1992) and (Feldweg  1995)).</S><S sid ="7" ssid = "4">On the basis of these considerations  we formulate several additional requirements.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="144" ssid = "25">We distinguish five degrees of automation: So far  about 1100 sentences of our corpus have been annotated.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'7'", "'80'", "'144'", "'21'"]
'147'
'7'
'80'
'144'
'21'
['147', '7', '80', '144', '21']
parsed_discourse_facet ['method_citation']
<S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S><S sid ="73" ssid = "18">While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.</S><S sid ="96" ssid = "9">In (5)  either a lexical nominalisation rule for the adjective Gliickliche is stipulated  or the existence of an empty nominal head.</S><S sid ="46" ssid = "36">These approaches provide an adequate explanation for several issues problematic for phrase-structure grammars (clause union  extraposition  diverse second-position phenomena).</S><S sid ="33" ssid = "23">(Lehmann et al.  1996)  (Marcus et al.  1994)  (Sampson  1995)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'73'", "'96'", "'46'", "'33'"]
'60'
'73'
'96'
'46'
'33'
['60', '73', '96', '46', '33']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "13">As the annotation scheme does not distinguish different bar levels or any similar intermediate categories  only a small set of node labels is needed (currently 16 tags  S  NP  AP ...).</S><S sid ="51" ssid = "41">This requirement speaks against the traditional sort of dependency trees  in which heads a re represented as non-terminal nodes  cf.</S><S sid ="109" ssid = "22">In addition  we have adopted a simple convention for those cases in which context information is insufficient  for total disambiguation: the highest possible attachment  site is chosen.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'51'", "'109'", "'11'", "'21'"]
'68'
'51'
'109'
'11'
'21'
['68', '51', '109', '11', '21']
parsed_discourse_facet ['method_citation']
<S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="138" ssid = "19">This allows easy modification if needed.</S><S sid ="115" ssid = "28">3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here  where C stands for coordination  and VP for the actual category.</S><S sid ="112" ssid = "25">A problem for the rudimentary argument. structure representations is the use of incomplete structures in natural language  i.e. phenomena such as coordination and ellipsis.</S><S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'138'", "'115'", "'112'", "'60'"]
'56'
'138'
'115'
'112'
'60'
['56', '138', '115', '112', '60']
parsed_discourse_facet ['results_citation']
parsing: input/ref/Task1/A00-2030_sweta.csv
<S sid="18" ssid="1">Almost all approaches to information extraction &#8212; even at the sentence level &#8212; are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="100" ssid="5">Given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a While our focus throughout the project was on TE and TR, we became curious about how well the model did at part-of-speech tagging, syntactic parsing, and at name finding.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="15">Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
 <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="1">Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.<
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="102" ssid = "7">The results are summarized in Table 2.</S><S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'79'", "'82'", "'102'", "'56'"]
'68'
'79'
'82'
'102'
'56'
['68', '79', '82', '102', '56']
parsed_discourse_facet ['implication_citation']
<S sid ="65" ssid = "6">We illustrate the generation process by walking through a few of the steps of the parse shown in Figure 3.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="9" ssid = "7">Manually creating sourcespecific training data for syntax was not required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'96'", "'23'", "'88'", "'9'"]
'65'
'96'
'23'
'88'
'9'
['65', '96', '23', '88', '9']
parsed_discourse_facet ['implication_citation']
<S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'68'", "'36'", "'6'", "'96'"]
'88'
'68'
'36'
'6'
'96'
['88', '68', '36', '6', '96']
parsed_discourse_facet ['results_citation']
<S sid ="60" ssid = "1">In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'62'", "'13'", "'36'", "'105'"]
'60'
'62'
'13'
'36'
'105'
['60', '62', '13', '36', '105']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="47" ssid = "7">It soon became painfully obvious that this task could not be performed in the available time.</S><S sid ="73" ssid = "14">We now briefly summarize the probability structure of the model.</S><S sid ="97" ssid = "2">The evaluation results are summarized in Table 1.</S><S sid ="80" ssid = "2">However  because these estimates are too sparse to be relied upon  we use interpolated estimates consisting of mixtures of successively lowerorder estimates (as in Placeway et al. 1993).</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'47'", "'73'", "'97'", "'80'"]
'38'
'47'
'73'
'97'
'80'
['38', '47', '73', '97', '80']
parsed_discourse_facet ['method_citation']
<S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'", "'15'", "'45'", "'88'", "'56'"]
'79'
'15'
'45'
'88'
'56'
['79', '15', '45', '88', '56']
parsed_discourse_facet ['method_citation']
<S sid ="49" ssid = "9">By necessity  we adopted the strategy of hand marking only the semantics.</S><S sid ="70" ssid = "11">Post-modifier constituents for the PER/NP.</S><S sid ="69" ssid = "10">8.</S><S sid ="95" ssid = "14">The semantics  that is  the entities and relations  can then be directly extracted from these sentential trees.</S><S sid ="111" ssid = "3">The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies  either expressed or implied  of the Defense Advanced Research Projects Agency or the United States Government.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'70'", "'69'", "'95'", "'111'"]
'49'
'70'
'69'
'95'
'111'
['49', '70', '69', '95', '111']
parsed_discourse_facet ['method_citation']
<S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="4" ssid = "2">Yet  relatively few have embedded one of these algorithms in a task.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="67" ssid = "8">We pick up the derivation just after the topmost S and its head word  said  have been produced.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'79'", "'4'", "'82'", "'67'"]
'23'
'79'
'4'
'82'
'67'
['23', '79', '4', '82', '67']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "3">However  pipelined architectures suffer from a serious disadvantage: errors accumulate as they propagate through the pipeline.</S><S sid ="31" ssid = "14">If the single generalized model could then be extended to semantic analysis  all necessary sentence level processing would be contained in that model.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="84" ssid = "3">Although mathematically the model predicts tree elements in a top-down fashion  we search the space bottom-up using a chartbased search.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'31'", "'0'", "'38'", "'84'"]
'20'
'31'
'0'
'38'
'84'
['20', '31', '0', '38', '84']
parsed_discourse_facet ['method_citation']
<S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'38'", "'107'", "'104'", "'23'"]
'88'
'38'
'107'
'104'
'23'
['88', '38', '107', '104', '23']
parsed_discourse_facet ['implication_citation']
<S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="74" ssid = "15">The categories for head constituents  cl are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c _1  and the head word of their parent  wp.</S><S sid ="69" ssid = "10">8.</S><S sid ="111" ssid = "3">The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies  either expressed or implied  of the Defense Advanced Research Projects Agency or the United States Government.</S>
original cit marker offset is 0
new cit marker offset is 0



["'82'", "'45'", "'74'", "'69'", "'111'"]
'82'
'45'
'74'
'69'
'111'
['82', '45', '74', '69', '111']
parsed_discourse_facet ['method_citation']
<S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="102" ssid = "7">The results are summarized in Table 2.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'", "'102'", "'82'", "'62'", "'68'"]
'45'
'102'
'82'
'62'
'68'
['45', '102', '82', '62', '68']
parsed_discourse_facet ['method_citation']
<S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="97" ssid = "2">The evaluation results are summarized in Table 1.</S><S sid ="75" ssid = "16">Separate probabilities are maintained for left (pre) and right (post) modifiers: Part-of-speech tags  t    for modifiers are predicted based on the modifier  cm  the partof-speech tag of the head word  th  and the head word itself  wh: Head words  w for modifiers are predicted based on the modifier  cm  the part-of-speech tag of the modifier word   t the part-ofspeech tag of the head word   th  and the head word itself  wh: lAwmicm tm th wh)  e.g.</S><S sid ="95" ssid = "14">The semantics  that is  the entities and relations  can then be directly extracted from these sentential trees.</S><S sid ="59" ssid = "5">These labels serve to form a continuous chain between the relation and its argument.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'", "'97'", "'75'", "'95'", "'59'"]
'45'
'97'
'75'
'95'
'59'
['45', '97', '75', '95', '59']
parsed_discourse_facet ['method_citation']
<S sid ="76" ssid = "17">Finally  word features  fm  for modifiers are predicted based on the modifier  cm  the partof-speech tag of the modifier word   t the part-of-speech tag of the head word th  the head word itself  wh  and whether or not the modifier head word  w is known or unknown.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="112" ssid = "4">We thank Michael Collins of the University of Pennsylvania for his valuable suggestions.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'", "'36'", "'112'", "'101'", "'5'"]
'76'
'36'
'112'
'101'
'5'
['76', '36', '112', '101', '5']
parsed_discourse_facet ['results_citation']
<S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'68'", "'13'", "'38'", "'23'"]
'56'
'68'
'13'
'38'
'23'
['56', '68', '13', '38', '23']
parsed_discourse_facet ['implication_citation']
<S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'62'", "'36'", "'79'", "'107'"]
'51'
'62'
'36'
'79'
'107'
['51', '62', '36', '79', '107']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="49" ssid = "9">By necessity  we adopted the strategy of hand marking only the semantics.</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="39" ssid = "7">For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'49'", "'38'", "'5'", "'39'"]
'6'
'49'
'38'
'5'
'39'
['6', '49', '38', '5', '39']
parsed_discourse_facet ['results_citation']
<S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="31" ssid = "14">If the single generalized model could then be extended to semantic analysis  all necessary sentence level processing would be contained in that model.</S><S sid ="83" ssid = "2">More precisely  it must find the most likely augmented parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'57'", "'88'", "'31'", "'83'"]
'23'
'57'
'88'
'31'
'83'
['23', '57', '88', '31', '83']
parsed_discourse_facet ['aim_citation']
<S sid ="95" ssid = "14">The semantics  that is  the entities and relations  can then be directly extracted from these sentential trees.</S><S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="34" ssid = "2">In these trees  the standard TREEBANK structures are augmented to convey semantic information  that is  entities and relations.</S><S sid ="4" ssid = "2">Yet  relatively few have embedded one of these algorithms in a task.</S><S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'45'", "'34'", "'4'", "'6'"]
'95'
'45'
'34'
'4'
'6'
['95', '45', '34', '4', '6']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
parsing: input/ref/Task1/P08-1028_aakansha.csv
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="185" ssid="19">The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.</S>
    <S sid="186" ssid="20">The combined model is best overall with &#961; = 0.19.</S>
    <S sid="187" ssid="21">However, the difference between the two models is not statistically significant.</S>
original cit marker offset is 0
new cit marker offset is 0



["'185'", "'186'"]
'185'
'186'
['185', '186']
parsed_discourse_facet ['result_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="21">The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'"]
'48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="53" ssid="1">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.</S><S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'53'", "'57'"]
'53'
'57'
['53', '57']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="16">Although the composition model in (5) is commonly used in the literature, from a linguistic perspective, the model in (6) is more appealing.</S>
    <S sid="69" ssid="17">Simply adding the vectors u and v lumps their contents together rather than allowing the content of one vector to pick out the relevant content of the other.</S>
    <S sid="70" ssid="18">Instead, it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v, and vice versa.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'69'", "'70'"]
'68'
'69'
'70'
['68', '69', '70']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
    <S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'", "'25'"]
'24'
'25'
['24', '25']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="12">Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'"]
'64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="176" ssid="10">The multiplicative and combined models yield means closer to the human ratings.</S>
    <S sid="177" ssid="11">The difference between High and Low similarity values estimated by these models are statistically significant (p &lt; 0.01 using the Wilcoxon rank sum test).</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'", "'177'"]
'176'
'177'
['176', '177']
parsed_discourse_facet ['method_citation']
<S sid="191" ssid="3">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'191'"]
'191'
['191']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1028.csv
<S sid ="200" ssid = "12">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid ="136" ssid = "49">We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine-grained semantic distinctions (see Table 1).</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants similarity judgments with those of the models.6 We used Spearmans p for our correlation analyses.</S><S sid ="135" ssid = "48">The average inter-subject agreement5 was  = 0.40.</S><S sid ="149" ssid = "62">Our composition models have no additional parameters beyond the semantic space just described  with three exceptions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'200'", "'136'", "'164'", "'135'", "'149'"]
'200'
'136'
'164'
'135'
'149'
['200', '136', '164', '135', '149']
parsed_discourse_facet ['implication_citation']
<S sid ="107" ssid = "20">Landmarks were taken from WordNet (Fellbaum  1998).</S><S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="45" ssid = "18">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid ="192" ssid = "4">We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'63'", "'42'", "'45'", "'192'"]
'107'
'63'
'42'
'45'
'192'
['107', '63', '42', '45', '192']
parsed_discourse_facet ['implication_citation']
<S sid ="30" ssid = "3">For the hierarchical structure of natural language this binding problem becomes particularly acute.</S><S sid ="78" ssid = "26">These vectors are not arbitrary and ideally they must exhibit some relation to the words of the construction under consideration.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants similarity judgments with those of the models.6 We used Spearmans p for our correlation analyses.</S><S sid ="171" ssid = "5">For comparison  we also show the human ratings for these items (UpperBound).</S><S sid ="176" ssid = "10">The multiplicative and combined models yield means closer to the human ratings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'78'", "'164'", "'171'", "'176'"]
'30'
'78'
'164'
'171'
'176'
['30', '78', '164', '171', '176']
parsed_discourse_facet ['results_citation']
<S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S><S sid ="85" ssid = "33">One potential drawback of multiplicative models is the effect of components with value zero.</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'", "'103'", "'85'", "'101'", "'63'"]
'42'
'103'
'85'
'101'
'63'
['42', '103', '85', '101', '63']
parsed_discourse_facet ['method_citation']
<S sid ="97" ssid = "10">Moreover by using a minimal structure we factor out inessential degrees of freedom and are able to assess the merits of different models on an equal footing.</S><S sid ="126" ssid = "39">49 unpaid volunteers completed the experiment  all native speakers of English.</S><S sid ="192" ssid = "4">We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials.</S><S sid ="120" ssid = "33">Examples of our items are given in Table 1.</S><S sid ="165" ssid = "78">Again  better models should correlate better with the experimental data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'126'", "'192'", "'120'", "'165'"]
'97'
'126'
'192'
'120'
'165'
['97', '126', '192', '120', '165']
parsed_discourse_facet ['method_citation']
<S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="45" ssid = "18">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="68" ssid = "16">Although the composition model in (5) is commonly used in the literature  from a linguistic perspective  the model in (6) is more appealing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'", "'121'", "'45'", "'116'", "'68'"]
'42'
'121'
'45'
'116'
'68'
['42', '121', '45', '116', '68']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "13">In the following we describe our data collection procedure and give details on how our composition models were constructed and evaluated.</S><S sid ="123" ssid = "36">Sentence pairs were presented serially in random order.</S><S sid ="112" ssid = "25">The stimuli were administered to four separate groups; each group saw one set of 100 sentences.</S><S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'123'", "'112'", "'25'", "'63'"]
'100'
'123'
'112'
'25'
'63'
['100', '123', '112', '25', '63']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S><S sid ="85" ssid = "33">One potential drawback of multiplicative models is the effect of components with value zero.</S><S sid ="145" ssid = "58">The latter were the most common context words (excluding a stop list of function words).</S><S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'85'", "'145'", "'121'", "'103'"]
'4'
'85'
'145'
'121'
'103'
['4', '85', '145', '121', '103']
parsed_discourse_facet ['method_citation']
<S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="45" ssid = "18">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid ="95" ssid = "8">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'45'", "'95'", "'150'", "'42'"]
'25'
'45'
'95'
'150'
'42'
['25', '45', '95', '150', '42']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "49">We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine-grained semantic distinctions (see Table 1).</S><S sid ="132" ssid = "45">We also measured how well humans agree in their ratings.</S><S sid ="158" ssid = "71">The m neighbors most similar to the predicate  and the k of m neighbors closest to its argument.</S><S sid ="200" ssid = "12">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid ="17" ssid = "13">It was not the sales manager who hit the bottle that day  but the office worker with the serious drinking problem. b.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'132'", "'158'", "'200'", "'17'"]
'136'
'132'
'158'
'200'
'17'
['136', '132', '158', '200', '17']
parsed_discourse_facet ['implication_citation']
<S sid ="95" ssid = "8">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="190" ssid = "2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid ="46" ssid = "19">However  since it is order independent  it cannot capture meaning differences that are modulated by differences in syntactic structure.</S><S sid ="29" ssid = "2">While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'150'", "'190'", "'46'", "'29'"]
'95'
'150'
'190'
'46'
'29'
['95', '150', '190', '46', '29']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="156" ssid = "69">This yielded a weighted sum consisting of 95% verb  0% noun and 5% of their multiplicative combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'103'", "'116'", "'150'", "'156'"]
'34'
'103'
'116'
'150'
'156'
['34', '103', '116', '150', '156']
parsed_discourse_facet ['method_citation']
<S sid ="176" ssid = "10">The multiplicative and combined models yield means closer to the human ratings.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="157" ssid = "70">Finally  Kintschs (2001) additive model has two extra parameters.</S><S sid ="88" ssid = "1">We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).</S><S sid ="38" ssid = "11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'", "'42'", "'157'", "'88'", "'38'"]
'176'
'42'
'157'
'88'
'38'
['176', '42', '157', '88', '38']
parsed_discourse_facet ['method_citation']
<S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="114" ssid = "27">For each reference verb  the subjects responses were entered into a contingency table  whose rows corresponded to nouns and columns to each possible answer (i.e.  one of the two landmarks).</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'116'", "'150'", "'59'", "'114'"]
'63'
'116'
'150'
'59'
'114'
['63', '116', '150', '59', '114']
parsed_discourse_facet ['results_citation']
<S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="64" ssid = "12">Now  if we assume that p lies in the same space as u and v  avoiding the issues of dimensionality associated with tensor products  and that f is a linear function  for simplicity  of the cartesian product of u and v  then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast  if we assume that f is a linear function of the tensor product of u and v  then we obtain multiplicative models: where C is a tensor of rank 3  which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'", "'121'", "'116'", "'64'", "'103'"]
'42'
'121'
'116'
'64'
'103'
['42', '121', '116', '64', '103']
parsed_discourse_facet ['implication_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="77" ssid = "25">For additive models  a natural way to achieve this is to include further vectors into the summation.</S><S sid ="70" ssid = "18">Instead  it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v  and vice versa.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="85" ssid = "33">One potential drawback of multiplicative models is the effect of components with value zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'77'", "'70'", "'59'", "'85'"]
'194'
'77'
'70'
'59'
'85'
['194', '77', '70', '59', '85']
parsed_discourse_facet ['method_citation']
<S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S><S sid ="130" ssid = "43">As we can see sentences with high similarity landmarks are perceived as more similar to the reference sentence.</S><S sid ="82" ssid = "30">In contrast to the simple additive model  this extended model is sensitive to syntactic structure  since n is chosen from among the neighbors of the predicate  distinguishing it from the argument.</S><S sid ="73" ssid = "21">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware  since semantically important constituents can participate more actively in the composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'121'", "'103'", "'130'", "'82'", "'73'"]
'121'
'103'
'130'
'82'
'73'
['121', '103', '130', '82', '73']
parsed_discourse_facet ['results_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



P08-1028
P10-1021
0
method_citation
['method_citation']
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P05-1013_swastika.csv
<S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



['49']
49
['49']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="6">In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
<S sid="38" ssid="9">Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="6">In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
Error in Reference Offset
S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
Error in Reference Offset
    <S sid="86" ssid="13">As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
 <S sid="51" ssid="22">In the second scheme, Head+Path, we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p&#8595;.</S>
original cit marker offset is 0
new cit marker offset is 0



['51']
51
['51']
parsed_discourse_facet ['method_citation']
<S sid="99" ssid="10">This may seem surprising, given the experiments reported in section 4, but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift, where the encoding of path information is often redundant.</S>
original cit marker offset is 0
new cit marker offset is 0



['99']
99
['99']
parsed_discourse_facet ['result_citation']
<S sid="7" ssid="3">From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



['2']
2
['2']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/P05-1013.csv
<S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S><S sid ="53" ssid = "24">In the third and final scheme  denoted Path  we keep the extra infor2Note that this is a baseline for the parsing experiment only (Experiment 2).</S><S sid ="37" ssid = "8">Here we use a slightly different notion of lift  applying to individual arcs and moving their head upwards one step at a time: Intuitively  lifting an arc makes the word wk dependent on the head wi of its original head wj (which is unique in a well-formed dependency graph)  unless wj is a root in which case the operation is undefined (but then wj * wk is necessarily projective if the dependency graph is well-formed).</S><S sid ="83" ssid = "10">It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'53'", "'37'", "'83'", "'75'"]
'51'
'53'
'37'
'83'
'75'
['51', '53', '37', '83', '75']
parsed_discourse_facet ['implication_citation']
<S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S><S sid ="38" ssid = "9">Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'38'", "'78'", "'34'", "'51'"]
'75'
'38'
'78'
'34'
'51'
['75', '38', '78', '34', '51']
parsed_discourse_facet ['implication_citation']
<S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S><S sid ="23" ssid = "19">By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S><S sid ="110" ssid = "2">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="25" ssid = "21">The rest of the paper is structured as follows.</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'70'", "'23'", "'110'", "'25'", "'11'"]
'70'
'23'
'110'
'25'
'11'
['70', '23', '110', '25', '11']
parsed_discourse_facet ['results_citation']
<S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="77" ssid = "4">The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'24'", "'95'", "'77'", "'11'"]
'34'
'24'
'95'
'77'
'11'
['34', '24', '95', '77', '11']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "14">Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.</S><S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="18" ssid = "14">In addition  there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington  1990; Kahane et al.  1998; Duchier and Debusmann  2001; Holan et al.  2001; Hellwig  2003).</S><S sid ="103" ssid = "14">On the other hand  given that all schemes have similar parsing accuracy overall  this means that the Path scheme is the least likely to introduce errors on projective arcs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'65'", "'14'", "'18'", "'103'"]
'43'
'65'
'14'
'18'
'103'
['43', '65', '14', '18', '103']
parsed_discourse_facet ['method_citation']
<S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S><S sid ="42" ssid = "13">Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="55" ssid = "26">As can be seen from the last column in Table 1  both Head and Head+Path may theoretically lead to a quadratic increase in the number of distinct arc labels (Head+Path being worse than Head only by a constant factor)  while the increase is only linear in the case of Path.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'42'", "'14'", "'55'", "'104'"]
'75'
'42'
'14'
'55'
'104'
['75', '42', '14', '55', '104']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="107" ssid = "18">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="66" ssid = "5">The choice between different actions is in general nondeterministic  and the parser relies on a memorybased classifier  trained on treebank data  to predict the next action based on features of the current parser configuration.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'107'", "'14'", "'95'", "'66'"]
'11'
'107'
'14'
'95'
'66'
['11', '107', '14', '95', '66']
parsed_discourse_facet ['method_citation']
<S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S><S sid ="12" ssid = "8">Prague Dependency Treebank (Hajic et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'70'", "'81'", "'12'", "'78'", "'89'"]
'70'
'81'
'12'
'78'
'89'
['70', '81', '12', '78', '89']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Prague Dependency Treebank (Hajic et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S><S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="5" ssid = "1">It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order  where discontinuous syntactic constructions are more common than in languages like English (Melcuk  1988; Covington  1990).</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S><S sid ="66" ssid = "5">The choice between different actions is in general nondeterministic  and the parser relies on a memorybased classifier  trained on treebank data  to predict the next action based on features of the current parser configuration.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'65'", "'5'", "'71'", "'66'"]
'12'
'65'
'5'
'71'
'66'
['12', '65', '5', '71', '66']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S><S sid ="82" ssid = "9">The last four columns in Table 3 show the distribution of nonprojective arcs with respect to the number of lifts required.</S><S sid ="86" ssid = "13">As expected  the most informative encoding  Head+Path  gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S><S sid ="84" ssid = "11">In the second part of the experiment  we applied the inverse transformation based on breadth-first search under the three different encoding schemes.</S><S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'82'", "'86'", "'84'", "'70'"]
'51'
'82'
'86'
'84'
'70'
['51', '82', '86', '84', '70']
parsed_discourse_facet ['implication_citation']
<S sid ="85" ssid = "12">The results are given in Table 4.</S><S sid ="33" ssid = "4">If (wi  r  wj) E A  we say that wi is the head of wj and wj a dependent of wi.</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="43" ssid = "14">Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.</S><S sid ="44" ssid = "15">Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'", "'33'", "'60'", "'43'", "'44'"]
'85'
'33'
'60'
'43'
'44'
['85', '33', '60', '43', '44']
parsed_discourse_facet ['method_citation']
<S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="25" ssid = "21">The rest of the paper is structured as follows.</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'14'", "'25'", "'60'", "'75'"]
'65'
'14'
'25'
'60'
'75'
['65', '14', '25', '60', '75']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="26" ssid = "22">In section 2 we introduce the graph transformation techniques used to projectivize and deprojectivize dependency graphs  and in section 3 we describe the data-driven dependency parser that is the core of our system.</S><S sid ="61" ssid = "32">Before we turn to the evaluation  however  we need to introduce the data-driven dependency parser used in the latter experiments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'104'", "'21'", "'26'", "'61'"]
'11'
'104'
'21'
'26'
'61'
['11', '104', '21', '26', '61']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="43" ssid = "14">Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S><S sid ="63" ssid = "2">The parser builds dependency graphs by traversing the input from left to right  using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'11'", "'43'", "'81'", "'63'"]
'34'
'11'
'43'
'81'
'63'
['34', '11', '43', '81', '63']
parsed_discourse_facet ['results_citation']
<S sid ="86" ssid = "13">As expected  the most informative encoding  Head+Path  gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S><S sid ="57" ssid = "28">In approaching this problem  a variety of different methods are conceivable  including a more or less sophisticated use of machine learning.</S><S sid ="73" ssid = "12">More details on the memory-based prediction can be found in Nivre et al. (2004) and Nivre and Scholz (2004).</S><S sid ="61" ssid = "32">Before we turn to the evaluation  however  we need to introduce the data-driven dependency parser used in the latter experiments.</S><S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'57'", "'73'", "'61'", "'34'"]
'86'
'57'
'73'
'61'
'34'
['86', '57', '73', '61', '34']
parsed_discourse_facet ['implication_citation']
<S sid ="83" ssid = "10">It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="101" ssid = "12">However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.</S><S sid ="25" ssid = "21">The rest of the paper is structured as follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'106'", "'78'", "'101'", "'25'"]
'83'
'106'
'78'
'101'
'25'
['83', '106', '78', '101', '25']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="77" ssid = "4">The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'24'", "'95'", "'77'", "'11'"]
'34'
'24'
'95'
'77'
'11'
['34', '24', '95', '77', '11']
parsed_discourse_facet ['results_citation']
<S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="92" ssid = "3">Evaluation metrics used are Attachment Score (AS)  i.e. the proportion of tokens that are attached to the correct head  and Exact Match (EM)  i.e. the proportion of sentences for which the dependency graph exactly matches the gold standard.</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'65'", "'92'", "'60'", "'71'"]
'95'
'65'
'92'
'60'
'71'
['95', '65', '92', '60', '71']
parsed_discourse_facet ['aim_citation']
<S sid ="67" ssid = "6">Table 2 shows the features used in the current version of the parser.</S><S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S><S sid ="82" ssid = "9">The last four columns in Table 3 show the distribution of nonprojective arcs with respect to the number of lifts required.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'70'", "'82'", "'14'", "'81'"]
'67'
'70'
'82'
'14'
'81'
['67', '70', '82', '14', '81']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S><S sid ="63" ssid = "2">The parser builds dependency graphs by traversing the input from left to right  using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'11'", "'34'", "'75'", "'63'"]
'51'
'11'
'34'
'75'
'63'
['51', '11', '34', '75', '63']
parsed_discourse_facet ['aim_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/A00-2018_vardha.csv
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'91'"]
'91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="8">In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'"]
'39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="162" ssid="53">Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'"]
'162'
['162']
parsed_discourse_facet ['method_citation']
 <S sid="87" ssid="56">In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17].</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'"]
'87'
['87']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'91'"]
'91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="176" ssid="3">That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'"]
'176'
['176']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['method_citation']
 <S sid="101" ssid="12">In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning).</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'"]
'101'
['101']
parsed_discourse_facet ['method_citation']
<S sid="155" ssid="46">For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
  <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="162" ssid="53">Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'"]
'162'
['162']
parsed_discourse_facet ['method_citation']
 <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
 <S sid="79" ssid="48">We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
    <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
    <S sid="175" ssid="2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["'175'"]
'175'
['175']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2018.csv
<S sid ="114" ssid = "5">That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.</S><S sid ="159" ssid = "50">Something very much like this is done in [15].</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="165" ssid = "56">Note that we also tried including this information using a standard deleted-interpolation model.</S><S sid ="163" ssid = "54">Next we add the less obvious conditioning events noted in our previous discussion of the final model  grandparent label lg and left sibling label /b.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'159'", "'2'", "'165'", "'163'"]
'114'
'159'
'2'
'165'
'163'
['114', '159', '2', '165', '163']
parsed_discourse_facet ['implication_citation']
<S sid ="114" ssid = "5">That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.</S><S sid ="3" ssid = "3">The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.</S><S sid ="99" ssid = "10">Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S><S sid ="129" ssid = "20">It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments)  it does not guess the pre-terminal before guessing the lexical head  and it uses a tree-bank grammar rather than a Markov grammar.</S><S sid ="92" ssid = "3">For runs with the generative model based upon Markov grammar statistics  the first pass uses the same statistics  but conditioned only on standard PCFG information.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'3'", "'99'", "'129'", "'92'"]
'114'
'3'
'99'
'129'
'92'
['114', '3', '99', '129', '92']
parsed_discourse_facet ['implication_citation']
<S sid ="132" ssid = "23">Between the Old model and the Best model  Figure 2 gives precision/recall measurements for several different versions of our parser.</S><S sid ="69" ssid = "38">For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.</S><S sid ="184" ssid = "11">The first is the slight  but important  improvement achieved by using this model over conventional deleted interpolation  as indicated in Figure 2.</S><S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="85" ssid = "54">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models  this simplifies the model significantly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'132'", "'69'", "'184'", "'157'", "'85'"]
'132'
'69'
'184'
'157'
'85'
['132', '69', '184', '157', '85']
parsed_discourse_facet ['results_citation']
<S sid ="137" ssid = "28">However  Collins in [10] does not stress the decision to guess the head's pre-terminal first  and it might be lost on the casual reader.</S><S sid ="51" ssid = "20">Second  and this is a point we have not yet mentioned  the features used in these models need have no particular independence of one another.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="58" ssid = "27">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid ="150" ssid = "41">The second modification is the explicit marking of noun and verb-phrase coordination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'51'", "'2'", "'58'", "'150'"]
'137'
'51'
'2'
'58'
'150'
['137', '51', '2', '58', '150']
parsed_discourse_facet ['method_citation']
<S sid ="49" ssid = "18">First  as already implicit in our discussion  factoring the probability computation into a sequence of values corresponding to various &quot;features&quot; suggests that the probability model should be easily changeable  just change the set of features used.</S><S sid ="4" ssid = "4">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c  information outside c that our probability model deems important in determining the probability in question.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="165" ssid = "56">Note that we also tried including this information using a standard deleted-interpolation model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'4'", "'13'", "'2'", "'165'"]
'49'
'4'
'13'
'2'
'165'
['49', '4', '13', '2', '165']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "3">Also  remember that H is a pla ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.</S><S sid ="11" ssid = "7">What fundamentally distinguishes probabilistic generative parsers is how they compute p(r)  and it is to that topic we turn next.</S><S sid ="96" ssid = "7">As noted above  the probability model uses five smoothed probability distributions  one each for Li  M Ri t  and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7.</S><S sid ="99" ssid = "10">Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S><S sid ="19" ssid = "8">The method we use follows that of [10].</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'11'", "'96'", "'99'", "'19'"]
'34'
'11'
'96'
'99'
'19'
['34', '11', '96', '99', '19']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.</S><S sid ="185" ssid = "12">We expect that as we experiment with other  more semantic conditioning information  the importance of this aspect of the model will increase.</S><S sid ="80" ssid = "49">(Our experience is that rather than requiring 50 or so iterations  three suffice.)</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="144" ssid = "35">This quantity is a relatively intuitive one (as  for example  it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it  in effect  as the unsmoothed probability upon which all smoothing of p(h) is based.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'185'", "'80'", "'54'", "'144'"]
'3'
'185'
'80'
'54'
'144'
['3', '185', '80', '54', '144']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "16">The intuitive idea is that each factor gi is larger than one if the feature in question makes the probability more likely  one if the feature has no effect  and smaller than one if it makes the probability less likely.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S><S sid ="85" ssid = "54">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models  this simplifies the model significantly.</S><S sid ="22" ssid = "11">For us the non-terminal symbols are those of the tree-bank  augmented by the symbols aux and auxg  which have been assigned deterministically to certain auxiliary verbs such as &quot;have&quot; or &quot;having&quot;.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'2'", "'41'", "'85'", "'22'"]
'47'
'2'
'41'
'85'
'22'
['47', '2', '41', '85', '22']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="55" ssid = "24">This method is known as &quot;deleted interpolation&quot; smoothing.</S><S sid ="66" ssid = "35">In many cases this is clearly warranted.</S><S sid ="4" ssid = "4">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid ="121" ssid = "12">Without these enhancements Char97 performs at the 86.6% level for sentences of length < 40.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'55'", "'66'", "'4'", "'121'"]
'157'
'55'
'66'
'4'
'121'
['157', '55', '66', '4', '121']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="0" ssid = "0">A Maximum-Entropy-Inspired Parser *</S><S sid ="37" ssid = "6">Rather  we concentrate on the aspects of these models that most directly influenced the model presented here.</S><S sid ="21" ssid = "10">(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'157'", "'0'", "'37'", "'21'"]
'2'
'157'
'0'
'37'
'21'
['2', '157', '0', '37', '21']
parsed_discourse_facet ['implication_citation']
<S sid ="48" ssid = "17">Maximum-entropy models have two benefits for a parser builder.</S><S sid ="21" ssid = "10">(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="177" ssid = "4">The results reported here disprove this conjecture.</S><S sid ="159" ssid = "50">Something very much like this is done in [15].</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'21'", "'54'", "'177'", "'159'"]
'48'
'21'
'54'
'177'
'159'
['48', '21', '54', '177', '159']
parsed_discourse_facet ['method_citation']
<S sid ="35" ssid = "4">In the past few years the maximum entropy  or log-linear  approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing [1 17].</S><S sid ="121" ssid = "12">Without these enhancements Char97 performs at the 86.6% level for sentences of length < 40.</S><S sid ="69" ssid = "38">For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.</S><S sid ="138" ssid = "29">Indeed  it was lost on the present author until he went back after the fact and found it there.</S><S sid ="114" ssid = "5">That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'", "'121'", "'69'", "'138'", "'114'"]
'35'
'121'
'69'
'138'
'114'
['35', '121', '69', '138', '114']
parsed_discourse_facet ['method_citation']
<S sid ="169" ssid = "60">Up to this point all the models considered in this section are tree-bank grammar models.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="134" ssid = "25">As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.</S><S sid ="99" ssid = "10">Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S><S sid ="155" ssid = "46">For example  in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'", "'125'", "'134'", "'99'", "'155'"]
'169'
'125'
'134'
'99'
'155'
['169', '125', '134', '99', '155']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid ="34" ssid = "3">Also  remember that H is a pla ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.</S><S sid ="165" ssid = "56">Note that we also tried including this information using a standard deleted-interpolation model.</S><S sid ="69" ssid = "38">For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.</S><S sid ="0" ssid = "0">A Maximum-Entropy-Inspired Parser *</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'34'", "'165'", "'69'", "'0'"]
'7'
'34'
'165'
'69'
'0'
['7', '34', '165', '69', '0']
parsed_discourse_facet ['results_citation']
<S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S><S sid ="37" ssid = "6">Rather  we concentrate on the aspects of these models that most directly influenced the model presented here.</S><S sid ="169" ssid = "60">Up to this point all the models considered in this section are tree-bank grammar models.</S><S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'", "'37'", "'169'", "'7'", "'125'"]
'41'
'37'
'169'
'7'
'125'
['41', '37', '169', '7', '125']
parsed_discourse_facet ['implication_citation']
<S sid ="88" ssid = "57">While we could have smoothed in the same fashion  we choose instead to use standard deleted interpolation.</S><S sid ="124" ssid = "15">We note here that this corpus is somewhat more difficult than the &quot;official&quot; test corpus.</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="21" ssid = "10">(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)</S><S sid ="63" ssid = "32">As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'124'", "'54'", "'21'", "'63'"]
'88'
'124'
'54'
'21'
'63'
['88', '124', '54', '21', '63']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P11-1060_swastika.csv
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['aim_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['aim_citation']
    <S sid="148" ssid="33">This bootstrapping behavior occurs naturally: The &#8220;easy&#8221; examples are processed first, where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
    <S sid="13" ssid="9">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['result_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="47" ssid="23">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
S sid="44" ssid="20">The recurrence is as follows: At each node, we compute the set of tuples v consistent with the predicate at that node (v &#8712; w(p)), and S(x)}, where a set of pairs S is treated as a set-valued function S(x) = {y : (x, y) &#8712; S} with domain S1 = {x : (x, y) &#8712; S}.</S>
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
Error in Reference Offset
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



['106']
106
['106']
parsed_discourse_facet ['aim_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
    <S sid="172" ssid="57">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms, we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus, and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S>
original cit marker offset is 0
new cit marker offset is 0



['172']
172
['172']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



['171']
171
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S sid ="57" ssid = "33">But consider Figure 4: (a) is headed by borders  but states needs to be extracted; in (b)  the quantifier no is syntactically dominated by the head verb borders but needs to take wider scope.</S><S sid ="39" ssid = "15">Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.</S><S sid ="60" ssid = "36">Then higher up in the tree  we invoke it with an execute relation Xi to create the desired semantic scope.2 This mark-execute construct acts non-locally  so to maintain compositionality  we must augment the denotation d = JzKw to include any information about the marked nodes in z that can be accessed by an execute relation later on.</S><S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="167" ssid = "52">In DCS  we start with lexical triggers  which are 6 Conclusion more basic than CCG lexical entries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'", "'39'", "'60'", "'22'", "'167'"]
'57'
'39'
'60'
'22'
'167'
['57', '39', '60', '22', '167']
parsed_discourse_facet ['implication_citation']
<S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="71" ssid = "47">Let z be a DCS tree.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="120" ssid = "5">GEO has 48 non-value predicates and JOBS has 26.</S><S sid ="85" ssid = "61">Extraction allows us to return the set of consistent values of a marked non-root node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'71'", "'141'", "'120'", "'85'"]
'15'
'71'
'141'
'120'
'85'
['15', '71', '141', '120', '85']
parsed_discourse_facet ['implication_citation']
<S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S><S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parents denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="39" ssid = "15">Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.</S><S sid ="147" ssid = "32">However  training on just these examples is enough to improve the parameters  and this 29% increases to 66% and then to 95% over the next few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'51'", "'3'", "'39'", "'147'"]
'46'
'51'
'3'
'39'
'147'
['46', '51', '3', '39', '147']
parsed_discourse_facet ['results_citation']
<S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parents denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid ="84" ssid = "60">There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.</S><S sid ="170" ssid = "55">Our features as soft preferences.</S><S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'84'", "'170'", "'46'", "'98'"]
'51'
'84'
'170'
'46'
'98'
['51', '84', '170', '46', '98']
parsed_discourse_facet ['method_citation']
<S sid ="122" ssid = "7">For JOBS  if we use the standard Jobs database  close to half the ys are empty  which makes it uninteresting.</S><S sid ="94" ssid = "70">We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.</S><S sid ="102" ssid = "78">As a running example  consider x = city that is in California and z = hcity; 11:hloc; 21:hCAiii  where city triggers city and California triggers CA.</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S><S sid ="95" ssid = "71">Our first question is: given notation of the DCS tree before execution is an utterance x  what trees z  Z are permissible?</S>
original cit marker offset is 0
new cit marker offset is 0



["'122'", "'94'", "'102'", "'98'", "'95'"]
'122'
'94'
'102'
'98'
'95'
['122', '94', '102', '98', '95']
parsed_discourse_facet ['method_citation']
<S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="84" ssid = "60">There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'", "'3'", "'6'", "'84'", "'162'"]
'22'
'3'
'6'
'84'
'162'
['22', '3', '6', '84', '162']
parsed_discourse_facet ['method_citation']
<S sid ="13" ssid = "9">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x  y)  which is much cheaper to obtain than (x  z) pairs.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="29" ssid = "5">Let P be a set of predicates (e.g.  state  count  P)  which are just symbols.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="172" ssid = "57">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms  we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus  and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'134'", "'29'", "'77'", "'172'"]
'13'
'134'
'29'
'77'
'172'
['13', '134', '29', '77', '172']
parsed_discourse_facet ['method_citation']
<S sid ="96" ssid = "72">To California cities)  and it also allows us to underspecify L. In particular  our L will not include verbs or prepositions; rather  we rely on the predicates corresponding to those words to be triggered by traces.</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S><S sid ="94" ssid = "70">We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.</S><S sid ="124" ssid = "9">For each data predicate p (e.g.  language)  we add each possible tuple (e.g.  (job37  Java)) to w(p) independently with probability 0.8.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S>
original cit marker offset is 0
new cit marker offset is 0



["'96'", "'88'", "'94'", "'124'", "'162'"]
'96'
'88'
'94'
'124'
'162'
['96', '88', '94', '124', '162']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "84">However  in order to learn  we need to sum over {z  ZL(x) : JzKw = y}  and unfortunately  the additional constraint JzKw = y does not factorize.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="84" ssid = "60">There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'3'", "'84'", "'134'", "'46'"]
'108'
'3'
'84'
'134'
'46'
['108', '3', '84', '134', '46']
parsed_discourse_facet ['method_citation']
<S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'", "'98'", "'77'", "'162'", "'88'"]
'22'
'98'
'77'
'162'
'88'
['22', '98', '77', '162', '88']
parsed_discourse_facet ['implication_citation']
<S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'134'", "'3'", "'141'", "'98'"]
'12'
'134'
'3'
'141'
'98'
['12', '134', '3', '141', '98']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="29" ssid = "5">Let P be a set of predicates (e.g.  state  count  P)  which are just symbols.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="129" ssid = "14">We also define an augmented lexicon L+ which includes a prototype word x for each predicate appearing in (iii) above (e.g.  (large  size))  which cancels the predicates triggered by xs POS tag.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'29'", "'77'", "'129'", "'8'"]
'3'
'29'
'77'
'129'
'8'
['3', '29', '77', '129', '8']
parsed_discourse_facet ['method_citation']
<S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S><S sid ="142" ssid = "27">This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.</S><S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="144" ssid = "29">Intuitions How is our system learning?</S><S sid ="112" ssid = "88">Our learning algorithm alternates between (i) using the current parameters  to generate the K-best set ZL (x) for each training example x  and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'142'", "'10'", "'144'", "'112'"]
'46'
'142'
'10'
'144'
'112'
['46', '142', '10', '144', '112']
parsed_discourse_facet ['method_citation']
<S sid ="133" ssid = "18">Table 2 shows that our system using lexical triggers L (henceforth  DCS) outperforms SEMRESP (78.9% over 73.2%).</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="163" ssid = "48">Feedback from the context; for example  the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x  y)  which means (Schuler  2003) and semantic parsing (Popescu et borders looks right for the first argument and left al.  2003; Clarke et al.  2010).</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'134'", "'163'", "'141'", "'10'"]
'133'
'134'
'163'
'141'
'10'
['133', '134', '163', '141', '10']
parsed_discourse_facet ['results_citation']
<S sid ="86" ssid = "62">Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  (i  )]{1 = }.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="113" ssid = "89">Formally  let O(  ') be the objective function O() with ZL(x) ZL I(x).</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'162'", "'77'", "'3'", "'113'"]
'86'
'162'
'77'
'3'
'113'
['86', '162', '77', '3', '113']
parsed_discourse_facet ['implication_citation']
<S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="55" ssid = "31">The tree structure still enables us to compute denotations efficiently based on (1) and (2).</S><S sid ="80" ssid = "56">The full definition of join is as follows: Aggregate The aggregate operation takes a denotation and forms a set out of the tuples in the first column for each setting of the rest of the columns: Now we turn to the mark (M) and execute (Xi) operations  which handles the divergence between syntactic and semantic scope.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'134'", "'3'", "'55'", "'80'"]
'10'
'134'
'3'
'55'
'80'
['10', '134', '3', '55', '80']
parsed_discourse_facet ['method_citation']
<S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="50" ssid = "26">For example  consider the phrase number of major cities  and suppose that number corresponds to the count predicate.</S><S sid ="113" ssid = "89">Formally  let O(  ') be the objective function O() with ZL(x) ZL I(x).</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'", "'134'", "'154'", "'50'", "'113'"]
'22'
'134'
'154'
'50'
'113'
['22', '134', '154', '50', '113']
parsed_discourse_facet ['results_citation']
<S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="126" ssid = "11">During development  we further held out a random 30% of the training sets for validation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'134'", "'3'", "'12'", "'126'"]
'6'
'134'
'3'
'12'
'126'
['6', '134', '3', '12', '126']
parsed_discourse_facet ['aim_citation']
<S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S><S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="172" ssid = "57">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms  we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus  and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'132'", "'9'", "'10'", "'172'", "'134'"]
'132'
'9'
'10'
'172'
'134'
['132', '9', '10', '172', '134']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P08-1028_sweta.csv
<S sid="65" ssid="13">So, if we assume that only the ith components of u and v contribute to the ith component of p, that these components are not dependent on i, and that the function is symmetric with regard to the interchange of u and v, we obtain a simpler instantiation of an additive model: Analogously, under the same assumptions, we obtain the following simpler multiplicative model: only the ith components of u and v contribute to the ith component of p. Another class of models can be derived by relaxing this constraint.</S>
original cit marker offset is 0
new cit marker offset is 0



["65'"]
65'
['65']
parsed_discourse_facet ['method_citation']
 <S sid="75" ssid="23">An extreme form of this differential in the contribution of constituents is where one of the vectors, say u, contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic, however it can serve as a simple baseline against which to compare more sophisticated models.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["42'"]
42'
['42']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="17">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="195" ssid="7">The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately.</S>
original cit marker offset is 0
new cit marker offset is 0



["195'"]
195'
['195']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["57'"]
57'
['57']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["190'"]
190'
['190']
parsed_discourse_facet ['method_citation']
 <S sid="29" ssid="2">While neural networks can readily represent single distinct objects, in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>
original cit marker offset is 0
new cit marker offset is 0



["29'"]
29'
['29']
parsed_discourse_facet ['method_citation']
    <S sid="38" ssid="11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="12">Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="163" ssid="76">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["163'"]
163'
['163']
parsed_discourse_facet ['method_citation']
<S sid="185" ssid="19">The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.</S>
original cit marker offset is 0
new cit marker offset is 0



["185'"]
185'
['185']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["190'"]
190'
['190']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1028.csv
<S sid ="200" ssid = "12">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid ="136" ssid = "49">We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine-grained semantic distinctions (see Table 1).</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants similarity judgments with those of the models.6 We used Spearmans p for our correlation analyses.</S><S sid ="135" ssid = "48">The average inter-subject agreement5 was  = 0.40.</S><S sid ="149" ssid = "62">Our composition models have no additional parameters beyond the semantic space just described  with three exceptions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'200'", "'136'", "'164'", "'135'", "'149'"]
'200'
'136'
'164'
'135'
'149'
['200', '136', '164', '135', '149']
parsed_discourse_facet ['implication_citation']
<S sid ="107" ssid = "20">Landmarks were taken from WordNet (Fellbaum  1998).</S><S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="45" ssid = "18">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid ="192" ssid = "4">We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'63'", "'42'", "'45'", "'192'"]
'107'
'63'
'42'
'45'
'192'
['107', '63', '42', '45', '192']
parsed_discourse_facet ['implication_citation']
<S sid ="30" ssid = "3">For the hierarchical structure of natural language this binding problem becomes particularly acute.</S><S sid ="78" ssid = "26">These vectors are not arbitrary and ideally they must exhibit some relation to the words of the construction under consideration.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants similarity judgments with those of the models.6 We used Spearmans p for our correlation analyses.</S><S sid ="171" ssid = "5">For comparison  we also show the human ratings for these items (UpperBound).</S><S sid ="176" ssid = "10">The multiplicative and combined models yield means closer to the human ratings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'78'", "'164'", "'171'", "'176'"]
'30'
'78'
'164'
'171'
'176'
['30', '78', '164', '171', '176']
parsed_discourse_facet ['results_citation']
<S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S><S sid ="85" ssid = "33">One potential drawback of multiplicative models is the effect of components with value zero.</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'", "'103'", "'85'", "'101'", "'63'"]
'42'
'103'
'85'
'101'
'63'
['42', '103', '85', '101', '63']
parsed_discourse_facet ['method_citation']
<S sid ="97" ssid = "10">Moreover by using a minimal structure we factor out inessential degrees of freedom and are able to assess the merits of different models on an equal footing.</S><S sid ="126" ssid = "39">49 unpaid volunteers completed the experiment  all native speakers of English.</S><S sid ="192" ssid = "4">We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials.</S><S sid ="120" ssid = "33">Examples of our items are given in Table 1.</S><S sid ="165" ssid = "78">Again  better models should correlate better with the experimental data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'126'", "'192'", "'120'", "'165'"]
'97'
'126'
'192'
'120'
'165'
['97', '126', '192', '120', '165']
parsed_discourse_facet ['method_citation']
<S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="45" ssid = "18">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="68" ssid = "16">Although the composition model in (5) is commonly used in the literature  from a linguistic perspective  the model in (6) is more appealing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'", "'121'", "'45'", "'116'", "'68'"]
'42'
'121'
'45'
'116'
'68'
['42', '121', '45', '116', '68']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "13">In the following we describe our data collection procedure and give details on how our composition models were constructed and evaluated.</S><S sid ="123" ssid = "36">Sentence pairs were presented serially in random order.</S><S sid ="112" ssid = "25">The stimuli were administered to four separate groups; each group saw one set of 100 sentences.</S><S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'123'", "'112'", "'25'", "'63'"]
'100'
'123'
'112'
'25'
'63'
['100', '123', '112', '25', '63']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S><S sid ="85" ssid = "33">One potential drawback of multiplicative models is the effect of components with value zero.</S><S sid ="145" ssid = "58">The latter were the most common context words (excluding a stop list of function words).</S><S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'85'", "'145'", "'121'", "'103'"]
'4'
'85'
'145'
'121'
'103'
['4', '85', '145', '121', '103']
parsed_discourse_facet ['method_citation']
<S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="45" ssid = "18">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid ="95" ssid = "8">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'45'", "'95'", "'150'", "'42'"]
'25'
'45'
'95'
'150'
'42'
['25', '45', '95', '150', '42']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "49">We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine-grained semantic distinctions (see Table 1).</S><S sid ="132" ssid = "45">We also measured how well humans agree in their ratings.</S><S sid ="158" ssid = "71">The m neighbors most similar to the predicate  and the k of m neighbors closest to its argument.</S><S sid ="200" ssid = "12">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid ="17" ssid = "13">It was not the sales manager who hit the bottle that day  but the office worker with the serious drinking problem. b.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'132'", "'158'", "'200'", "'17'"]
'136'
'132'
'158'
'200'
'17'
['136', '132', '158', '200', '17']
parsed_discourse_facet ['implication_citation']
<S sid ="95" ssid = "8">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="190" ssid = "2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid ="46" ssid = "19">However  since it is order independent  it cannot capture meaning differences that are modulated by differences in syntactic structure.</S><S sid ="29" ssid = "2">While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'150'", "'190'", "'46'", "'29'"]
'95'
'150'
'190'
'46'
'29'
['95', '150', '190', '46', '29']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="156" ssid = "69">This yielded a weighted sum consisting of 95% verb  0% noun and 5% of their multiplicative combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'103'", "'116'", "'150'", "'156'"]
'34'
'103'
'116'
'150'
'156'
['34', '103', '116', '150', '156']
parsed_discourse_facet ['method_citation']
<S sid ="176" ssid = "10">The multiplicative and combined models yield means closer to the human ratings.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="157" ssid = "70">Finally  Kintschs (2001) additive model has two extra parameters.</S><S sid ="88" ssid = "1">We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).</S><S sid ="38" ssid = "11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'", "'42'", "'157'", "'88'", "'38'"]
'176'
'42'
'157'
'88'
'38'
['176', '42', '157', '88', '38']
parsed_discourse_facet ['method_citation']
<S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="114" ssid = "27">For each reference verb  the subjects responses were entered into a contingency table  whose rows corresponded to nouns and columns to each possible answer (i.e.  one of the two landmarks).</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'116'", "'150'", "'59'", "'114'"]
'63'
'116'
'150'
'59'
'114'
['63', '116', '150', '59', '114']
parsed_discourse_facet ['results_citation']
<S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="64" ssid = "12">Now  if we assume that p lies in the same space as u and v  avoiding the issues of dimensionality associated with tensor products  and that f is a linear function  for simplicity  of the cartesian product of u and v  then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast  if we assume that f is a linear function of the tensor product of u and v  then we obtain multiplicative models: where C is a tensor of rank 3  which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'", "'121'", "'116'", "'64'", "'103'"]
'42'
'121'
'116'
'64'
'103'
['42', '121', '116', '64', '103']
parsed_discourse_facet ['implication_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="77" ssid = "25">For additive models  a natural way to achieve this is to include further vectors into the summation.</S><S sid ="70" ssid = "18">Instead  it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v  and vice versa.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="85" ssid = "33">One potential drawback of multiplicative models is the effect of components with value zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'77'", "'70'", "'59'", "'85'"]
'194'
'77'
'70'
'59'
'85'
['194', '77', '70', '59', '85']
parsed_discourse_facet ['method_citation']
<S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S><S sid ="130" ssid = "43">As we can see sentences with high similarity landmarks are perceived as more similar to the reference sentence.</S><S sid ="82" ssid = "30">In contrast to the simple additive model  this extended model is sensitive to syntactic structure  since n is chosen from among the neighbors of the predicate  distinguishing it from the argument.</S><S sid ="73" ssid = "21">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware  since semantically important constituents can participate more actively in the composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'121'", "'103'", "'130'", "'82'", "'73'"]
'121'
'103'
'130'
'82'
'73'
['121', '103', '130', '82', '73']
parsed_discourse_facet ['results_citation']



P08-1028
P14-1060
0
method_citation
['implication_citation']
parsing: input/ref/Task1/D09-1092_vardha.csv
<S sid="17" ssid="13">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
 <S sid="20" ssid="16">We also explore how the characteristics of different languages affect topic model performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
    <S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
   <S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'"]
'55'
['55']
parsed_discourse_facet ['method_citation']
 sid="9" ssid="5">In this paper, we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
Error in Reference Offset
<S sid="118" ssid="67">We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
 <S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'"]
'55'
['55']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="51">In contrast, PLTM assigns a significant number of tokens to almost all 800 topics, in very similar proportions in both languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'"]
'102'
['102']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="4">This is unlike LDA, in which each document is assumed to have its own document-specific distribution over topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'"]
'38'
['38']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="105">We use both Jensen-Shannon divergence and cosine distance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'"]
'156'
['156']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="2">Each tuple is a set of documents that are loosely equivalent to each other, but written in different languages, e.g., corresponding Wikipedia articles in French, English and German.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
  <S sid="170" ssid="4">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
    <S sid="29" ssid="5">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
  <S sid="170" ssid="4">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="168" ssid="2">However, the growth of the web, and in particular Wikipedia, has made comparable text corpora &#8211; documents that are topically similar but are not direct translations of one another &#8211; considerably more abundant than true parallel corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'168'"]
'168'
['168']
parsed_discourse_facet ['method_citation']
    <S sid="29" ssid="5">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
 <S sid="110" ssid="59">Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'"]
'110'
['110']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="176" ssid = "10">We dropped all articles in non-English languages that did not link to an English article.</S><S sid ="145" ssid = "94">For T = 800  the top English and Spanish words in 448 topics were exact translations of one another.</S><S sid ="155" ssid = "104">Finally  for each pair of languages (query and target) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'18'", "'176'", "'145'", "'155'"]
'147'
'18'
'176'
'145'
'155'
['147', '18', '176', '145', '155']
parsed_discourse_facet ['implication_citation']
<S sid ="63" ssid = "12">Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.</S><S sid ="156" ssid = "105">We use both Jensen-Shannon divergence and cosine distance.</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="64" ssid = "13">The first topic contains words relating to the European Central Bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'156'", "'32'", "'23'", "'64'"]
'63'
'156'
'32'
'23'
'64'
['63', '156', '32', '23', '64']
parsed_discourse_facet ['implication_citation']
<S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S><S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="182" ssid = "16">As with EuroParl  we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.</S><S sid ="190" ssid = "24">Meanwhile  interest in actors and actresses (center) is consistent across all languages.</S><S sid ="104" ssid = "53">This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'30'", "'182'", "'190'", "'104'"]
'38'
'30'
'182'
'190'
'104'
['38', '30', '182', '190', '104']
parsed_discourse_facet ['results_citation']
<S sid ="164" ssid = "113">Results vary by language.</S><S sid ="160" ssid = "109">It is important to note that the length of documents matters.</S><S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of objective. The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S><S sid ="104" ssid = "53">This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.</S><S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of 1  .</S>
original cit marker offset is 0
new cit marker offset is 0



["'164'", "'160'", "'69'", "'104'", "'43'"]
'164'
'160'
'69'
'104'
'43'
['164', '160', '69', '104', '43']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S><S sid ="172" ssid = "6">Second  because comparable texts may not use exactly the same topics  it becomes crucially important to be able to characterize differences in topic prevalence at the document level (do different languages have different perspectives on the same article?) and at the language-wide level (which topics do particular languages focus on?).</S><S sid ="63" ssid = "12">Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'23'", "'38'", "'172'", "'63'"]
'32'
'23'
'38'
'172'
'63'
['32', '23', '38', '172', '63']
parsed_discourse_facet ['method_citation']
<S sid ="135" ssid = "84">We do not consider multi-word terms.</S><S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of objective. The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S><S sid ="175" ssid = "9">We preprocessed the data by removing tables  references  images and info-boxes.</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="176" ssid = "10">We dropped all articles in non-English languages that did not link to an English article.</S>
original cit marker offset is 0
new cit marker offset is 0



["'135'", "'69'", "'175'", "'32'", "'176'"]
'135'
'69'
'175'
'32'
'176'
['135', '69', '175', '32', '176']
parsed_discourse_facet ['method_citation']
<S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="97" ssid = "46">Rather  these results are intended as a quantitative analysis of the difference between the two models.</S><S sid ="40" ssid = "6">Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter  and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl  P(wl  |zl l) = 11n lwl |zl .</S><S sid ="21" ssid = "17">The second corpus  Wikipedia articles in twelve languages  contains sets of documents that are not translations of one another  but are very likely to be about similar concepts.</S><S sid ="75" ssid = "24">We compute histograms of these maximum topic probabilities for T  {50 100  200  400  800}.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'97'", "'40'", "'21'", "'75'"]
'147'
'97'
'40'
'21'
'75'
['147', '97', '40', '21', '75']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="184" ssid = "18">Interestingly  we find that almost all languages in our corpus  including several pairs that have historically been in conflict  show average JS divergences of between approximately 0.08 and 0.12 for T = 400  consistent with our findings for EuroParl translations.</S><S sid ="87" ssid = "36">The probability of previously unseen held-out document tuples given these estimates can then be computed.</S><S sid ="145" ssid = "94">For T = 800  the top English and Spanish words in 448 topics were exact translations of one another.</S><S sid ="178" ssid = "12">For efficiency  we truncated each article to the nearest word after 1000 characters and dropped the 50 most common word types in each language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'184'", "'87'", "'145'", "'178'"]
'32'
'184'
'87'
'145'
'178'
['32', '184', '87', '145', '178']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="48" ssid = "14">.</S><S sid ="124" ssid = "73">At p = 0.01 (1% glue documents)  German and French both include words relating to Russia  while the English and Italian word distributions appear locally consistent but unrelated to Russia.</S><S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'48'", "'124'", "'1'", "'38'"]
'30'
'48'
'124'
'1'
'38'
['30', '48', '124', '1', '38']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of objective. The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="9" ssid = "5">In this paper  we present the polylingual topic model (PLTM).</S><S sid ="40" ssid = "6">Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter  and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl  P(wl  |zl l) = 11n lwl |zl .</S><S sid ="186" ssid = "20">Although we find that if Wikipedia contains an article on a particular subject in some language  the article will tend to be topically similar to the articles about that subject in other languages  we also find that across the whole collection different languages emphasize topics to different extents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'23'", "'9'", "'40'", "'186'"]
'69'
'23'
'9'
'40'
'186'
['69', '23', '9', '40', '186']
parsed_discourse_facet ['implication_citation']
<S sid ="46" ssid = "12">  L and m from P(1  ...   L  m  |W'  ) or by evaluating a point estimate.</S><S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="72" ssid = "21">To answer this question  we compute the posterior probability of each topic in each tuple under the trained model.</S><S sid ="28" ssid = "4">We take a simpler approach that is more suitable for topically similar document tuples (where documents are not direct translations of one another) in more than two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'30'", "'32'", "'72'", "'28'"]
'46'
'30'
'32'
'72'
'28'
['46', '30', '32', '72', '28']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual synsets by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="45" ssid = "11">.</S><S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="67" ssid = "16">(Interestingly  all languages except Greek and Finnish use closely related words for youth or young in a separate topic.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'131'", "'45'", "'59'", "'67'"]
'30'
'131'
'45'
'59'
'67'
['30', '131', '45', '59', '67']
parsed_discourse_facet ['method_citation']
<S sid ="155" ssid = "104">Finally  for each pair of languages (query and target) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S><S sid ="29" ssid = "5">A recent extended abstract  developed concurrently by Ni et al. (Ni et al.  2009)  discusses a multilingual topic model similar to the one presented here.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="173" ssid = "7">We downloaded XML copies of all Wikipedia articles in twelve different languages: Welsh  German  Greek  English  Farsi  Finnish  French  Hebrew  Italian  Polish  Russian and Turkish.</S><S sid ="103" ssid = "52">PLTM topics therefore have a higher granularity  i.e.  they are more specific.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'", "'29'", "'19'", "'173'", "'103'"]
'155'
'29'
'19'
'173'
'103'
['155', '29', '19', '173', '103']
parsed_discourse_facet ['method_citation']
<S sid ="135" ssid = "84">We do not consider multi-word terms.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual synsets by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="89" ssid = "38">Analytically calculating the probability of a set of held-out document tuples given 1  ...   L and m is intractable  due to the summation over an exponential number of topic assignments for these held-out documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'135'", "'131'", "'147'", "'1'", "'89'"]
'135'
'131'
'147'
'1'
'89'
['135', '131', '147', '1', '89']
parsed_discourse_facet ['results_citation']
<S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of 1  .</S><S sid ="136" ssid = "85">We expect that simple analysis of topic assignments for sequential words would yield such collocations  but we leave this for future work.</S><S sid ="156" ssid = "105">We use both Jensen-Shannon divergence and cosine distance.</S><S sid ="45" ssid = "11">.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'136'", "'156'", "'45'", "'23'"]
'43'
'136'
'156'
'45'
'23'
['43', '136', '156', '45', '23']
parsed_discourse_facet ['implication_citation']
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual synsets by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="84" ssid = "33">As the number of topics increases  greater variability in topic distributions causes divergence to increase.</S><S sid ="63" ssid = "12">Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.</S><S sid ="175" ssid = "9">We preprocessed the data by removing tables  references  images and info-boxes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'147'", "'84'", "'63'", "'175'"]
'131'
'147'
'84'
'63'
'175'
['131', '147', '84', '63', '175']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of 1  .</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="175" ssid = "9">We preprocessed the data by removing tables  references  images and info-boxes.</S><S sid ="53" ssid = "2">In this case  we can be confident that the topic distribution is genuinely shared across all languages.</S><S sid ="193" ssid = "2">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'32'", "'175'", "'53'", "'193'"]
'43'
'32'
'175'
'53'
'193'
['43', '32', '175', '53', '193']
parsed_discourse_facet ['results_citation']
<S sid ="97" ssid = "46">Rather  these results are intended as a quantitative analysis of the difference between the two models.</S><S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="135" ssid = "84">We do not consider multi-word terms.</S><S sid ="103" ssid = "52">PLTM topics therefore have a higher granularity  i.e.  they are more specific.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'52'", "'30'", "'135'", "'103'"]
'97'
'52'
'30'
'135'
'103'
['97', '52', '30', '135', '103']
parsed_discourse_facet ['aim_citation']
<S sid ="14" ssid = "10">Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.</S><S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="190" ssid = "24">Meanwhile  interest in actors and actresses (center) is consistent across all languages.</S><S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'59'", "'190'", "'1'", "'31'"]
'14'
'59'
'190'
'1'
'31'
['14', '59', '190', '1', '31']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P08-1102_sweta.csv
<S sid="21" ssid="17">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="24">A subsequence of boundary-POS labelling result indicates a word with POS t only if the boundary tag sequence composed of its boundary part conforms to s or bm*e style, and all POS tags in its POS part equal to t. For example, a tag sequence b NN m NN e NN represents a threecharacter word with POS tag NN.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="43" ssid="15">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["43'"]
43'
['43']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["92'"]
92'
['92']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="5">To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&amp;T).</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="5">In following subsections, we describe the feature templates and the perceptron training algorithm.</S>
    <S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["33'", "'34'"]
33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="4">By maintaining a stack of size N at each position i of the sequence, we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S>
original cit marker offset is 0
new cit marker offset is 0



["79'"]
79'
['79']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="7">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["96'"]
96'
['96']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="16" ssid="12">Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="7">To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S sid ="120" ssid = "31">Without the perceptron  the cascaded model (if we can still call it cascaded) performs poorly on both segmentation and Joint S&T.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'", "'58'", "'136'", "'31'", "'134'"]
'120'
'58'
'136'
'31'
'134'
['120', '58', '136', '31', '134']
parsed_discourse_facet ['implication_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="117" ssid = "28">Table 4 shows experiments results.</S><S sid ="93" ssid = "4">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid ="18" ssid = "14">In this architecture  knowledge sources that are intractable to incorporate into the perceptron  can be easily incorporated into the outside linear model.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'117'", "'93'", "'18'", "'136'"]
'68'
'117'
'93'
'18'
'136'
['68', '117', '93', '18', '136']
parsed_discourse_facet ['implication_citation']
<S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x  X to outputs y  Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="78" ssid = "3">Given a Chinese character sequence C1:n  the decoding procedure can proceed in a left-right fashion with a dynamic programming approach.</S><S sid ="79" ssid = "4">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="46" ssid = "18">Following Collins  we use a function GEN(x) generating all candidate results of an input x   a representation 4) mapping each training example (x  y)  X  Y to a feature vector 4)(x  y)  Rd  and a parameter vector   Rd corresponding to the feature vector. d means the dimension of the vector space  it equals to the amount of features in the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'", "'78'", "'79'", "'7'", "'46'"]
'45'
'78'
'79'
'7'
'46'
['45', '78', '79', '7', '46']
parsed_discourse_facet ['results_citation']
<S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="65" ssid = "16">As shown in Figure 1  the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.</S><S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="95" ssid = "6">For convenience of comparing with others  we focus only on the close test  which means that any extra resource is forbidden except the designated training corpus.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'65'", "'101'", "'95'", "'58'"]
'133'
'65'
'101'
'95'
'58'
['133', '65', '101', '95', '58']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="95" ssid = "6">For convenience of comparing with others  we focus only on the close test  which means that any extra resource is forbidden except the designated training corpus.</S><S sid ="61" ssid = "12">In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'47'", "'95'", "'61'", "'136'"]
'101'
'47'
'95'
'61'
'136'
['101', '47', '95', '61', '136']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called averaged parameters (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="71" ssid = "22">Using W = w1:m to denote the word sequence  T = t1:m to denote the corresponding POS sequence  P (T |W) to denote the probability that W is labelled as T  and P(W|T) to denote the probability that T generates W  we can define the cooccurrence model as follows: wt and tw denote the corresponding weights of the two components.</S><S sid ="92" ssid = "3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.</S><S sid ="82" ssid = "7">In addition  we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'49'", "'71'", "'92'", "'82'"]
'68'
'49'
'71'
'92'
'82'
['68', '49', '71', '92', '82']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "10">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid ="40" ssid = "12">Templates in the column below are expanded from the upper ones.</S><S sid ="43" ssid = "15">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S><S sid ="25" ssid = "21">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'58'", "'40'", "'43'", "'25'"]
'38'
'58'
'40'
'43'
'25'
['38', '58', '40', '43', '25']
parsed_discourse_facet ['method_citation']
<S sid ="93" ssid = "4">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid ="57" ssid = "8">It has a two-layer architecture  with a perceptron as the core and another linear model as the outside-layer.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="40" ssid = "12">Templates in the column below are expanded from the upper ones.</S><S sid ="17" ssid = "13">We will describe it in detail in Section 4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'93'", "'57'", "'136'", "'40'", "'17'"]
'93'
'57'
'136'
'40'
'17'
['93', '57', '136', '40', '17']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="33" ssid = "5">In following subsections  we describe the feature templates and the perceptron training algorithm.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="96" ssid = "7">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus  we randomly chosen 2  000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84  294 sentences)  then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'68'", "'33'", "'134'", "'96'"]
'101'
'68'
'33'
'134'
'96'
['101', '68', '33', '134', '96']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="89" ssid = "14">Function D derives the candidate result from the word-POS pair p and the candidate q at prior position of p.</S><S sid ="9" ssid = "5">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid ="79" ssid = "4">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid ="104" ssid = "15">According to the usual practice in syntactic analysis  we choose chapters 1  260 (18074 sentences) as training set  chapter 271  300 (348 sentences) as test set and chapter 301  325 (350 sentences) as development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'89'", "'9'", "'79'", "'104'"]
'68'
'89'
'9'
'79'
'104'
['68', '89', '9', '79', '104']
parsed_discourse_facet ['implication_citation']
<S sid ="125" ssid = "36">However unlike the three features  the word LM brings very tiny improvement.</S><S sid ="40" ssid = "12">Templates in the column below are expanded from the upper ones.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'40'", "'94'", "'2'", "'58'"]
'125'
'40'
'94'
'2'
'58'
['125', '40', '94', '2', '58']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="56" ssid = "7">To alleviate the drawbacks  we propose a cascaded linear model.</S><S sid ="61" ssid = "12">In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.</S><S sid ="93" ssid = "4">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid ="112" ssid = "23">Here the core perceptron was just the POS+ model in experiments above.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'56'", "'61'", "'93'", "'112'"]
'136'
'56'
'61'
'93'
'112'
['136', '56', '61', '93', '112']
parsed_discourse_facet ['method_citation']
<S sid ="99" ssid = "10">Then we trained LEX on each of the four corpora for 7 iterations.</S><S sid ="120" ssid = "31">Without the perceptron  the cascaded model (if we can still call it cascaded) performs poorly on both segmentation and Joint S&T.</S><S sid ="121" ssid = "32">Among other features  the 4-gram POS LM plays the most important role  removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&T.</S><S sid ="42" ssid = "14">As predications generated from such templates depend on the current character  we name these templates lexical-target.</S><S sid ="117" ssid = "28">Table 4 shows experiments results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'99'", "'120'", "'121'", "'42'", "'117'"]
'99'
'120'
'121'
'42'
'117'
['99', '120', '121', '42', '117']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="64" ssid = "15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S><S sid ="53" ssid = "4">Figure 2 shows the growing tendency of feature space with the introduction of these features as well as the character-based ones.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'134'", "'64'", "'53'", "'7'"]
'68'
'134'
'64'
'53'
'7'
['68', '134', '64', '53', '7']
parsed_discourse_facet ['results_citation']
<S sid ="3" ssid = "3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="84" ssid = "9">Algorithm 2 shows the decoding algorithm.</S><S sid ="18" ssid = "14">In this architecture  knowledge sources that are intractable to incorporate into the perceptron  can be easily incorporated into the outside linear model.</S><S sid ="92" ssid = "3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'136'", "'84'", "'18'", "'92'"]
'3'
'136'
'84'
'18'
'92'
['3', '136', '84', '18', '92']
parsed_discourse_facet ['implication_citation']
<S sid ="51" ssid = "2">Additional features most widely used are related to word or POS ngrams.</S><S sid ="13" ssid = "9">However  as such features are generated dynamically during the decoding procedure  two limitation arise: on the one hand  the amount of parameters increases rapidly  which is apt to overfit on training corpus; on the other hand  exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called averaged parameters (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="36" ssid = "8">All feature templates and their instances are shown in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'13'", "'7'", "'49'", "'36'"]
'51'
'13'
'7'
'49'
'36'
['51', '13', '7', '49', '36']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/A97-1014_vardha.csv
<S sid="72" ssid="17">In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
 <S sid="144" ssid="25">We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="4">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
  <S sid="36" ssid="26">As for free word order languages, the following features may cause problems: sition between the two poles.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
 <S sid="24" ssid="14">The typical treebank architecture is as follows: Structures: A context-free backbone is augmented with trace-filler representations of non-local dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
 <S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'"]
'151'
['151']
parsed_discourse_facet ['method_citation']
 <S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
 <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["'167'"]
'167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'"]
'39'
['39']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'"]
'151'
['151']
parsed_discourse_facet ['method_citation']
<S sid="71" ssid="16">However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'71'"]
'71'
['71']
parsed_discourse_facet ['method_citation']
 <S sid="144" ssid="25">We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A97-1014.csv
<S sid ="73" ssid = "18">While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.</S><S sid ="115" ssid = "28">3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here  where C stands for coordination  and VP for the actual category.</S><S sid ="135" ssid = "16">Figure 1 shows a screen dump of the tool.</S><S sid ="39" ssid = "29">Consider the German sentence (1) daran wird ihn Anna erkennen  &di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies  marked by co-references between traces and the corresponding nodes.</S><S sid ="84" ssid = "29">Consider (qui verbs where the subject of the infinitival VP is not realised syntactically  but co-referent with the subject or object. of the matrix equi verb: (3) er bat mich zu kommen he asked me to come (mich is the understood subject. of kommt n).</S>
original cit marker offset is 0
new cit marker offset is 0



["'73'", "'115'", "'135'", "'39'", "'84'"]
'73'
'115'
'135'
'39'
'84'
['73', '115', '135', '39', '84']
parsed_discourse_facet ['implication_citation']
<S sid ="110" ssid = "23">A similar convention ha.s been adopted for constructions in which scope ambiguities have syntactic effects but a one-to-one correspondence between scope and attachment. does not seem reasonable  cf. focus particles such as only or also.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="30" ssid = "20">Thus the context-free constituent backbone plays a pivotal role in the annotation scheme.</S><S sid ="113" ssid = "26">Since a precise structural description of non-constituent coordination would require a. rich inventory of incomplete phrase types  we have agreed on a sort of unde.rspe.cified representations: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'", "'80'", "'30'", "'113'", "'56'"]
'110'
'80'
'30'
'113'
'56'
['110', '80', '30', '113', '56']
parsed_discourse_facet ['implication_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S><S sid ="78" ssid = "23">Morphological information: Another set of labels represents morphological information.</S><S sid ="8" ssid = "5">A formalism complying with these requirements is described in section 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'56'", "'60'", "'78'", "'8'"]
'80'
'56'
'60'
'78'
'8'
['80', '56', '60', '78', '8']
parsed_discourse_facet ['results_citation']
<S sid ="0" ssid = "0">An Annotation Scheme for Free Word Order Languages</S><S sid ="18" ssid = "8">(Marcus et al.  1994).</S><S sid ="151" ssid = "32">For evaluation  the already annotated sentences were divided into two disjoint sets  one for training (90% of the corpus)  the other one for testing (10%).</S><S sid ="49" ssid = "39">Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.</S><S sid ="114" ssid = "27">Fig.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'18'", "'151'", "'49'", "'114'"]
'0'
'18'
'151'
'49'
'114'
['0', '18', '151', '49', '114']
parsed_discourse_facet ['method_citation']
<S sid ="65" ssid = "10">The tree resembles traditional constituent structures.</S><S sid ="62" ssid = "7">2.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="77" ssid = "22">Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'62'", "'80'", "'77'", "'5'"]
'65'
'62'
'80'
'77'
'5'
['65', '62', '80', '77', '5']
parsed_discourse_facet ['method_citation']
<S sid ="49" ssid = "39">Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="118" ssid = "31">An explicit coordinating conjunction need not be present.</S><S sid ="28" ssid = "18">(Bies et al.  1995)  (Sampson  1995)).</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'56'", "'118'", "'28'", "'80'"]
'49'
'56'
'118'
'28'
'80'
['49', '56', '118', '28', '80']
parsed_discourse_facet ['method_citation']
<S sid ="65" ssid = "10">The tree resembles traditional constituent structures.</S><S sid ="108" ssid = "21">However  full or partial disambiguation takes place in context  and the annotators do not consider unrealistic readings.</S><S sid ="96" ssid = "9">In (5)  either a lexical nominalisation rule for the adjective Gliickliche is stipulated  or the existence of an empty nominal head.</S><S sid ="86" ssid = "31">We call such additional edges secondary links and represent them as dotted lines  see fig.</S><S sid ="135" ssid = "16">Figure 1 shows a screen dump of the tool.</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'108'", "'96'", "'86'", "'135'"]
'65'
'108'
'96'
'86'
'135'
['65', '108', '96', '86', '135']
parsed_discourse_facet ['method_citation']
<S sid ="115" ssid = "28">3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here  where C stands for coordination  and VP for the actual category.</S><S sid ="77" ssid = "22">Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.</S><S sid ="109" ssid = "22">In addition  we have adopted a simple convention for those cases in which context information is insufficient  for total disambiguation: the highest possible attachment  site is chosen.</S><S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'77'", "'109'", "'60'", "'21'"]
'115'
'77'
'109'
'60'
'21'
['115', '77', '109', '60', '21']
parsed_discourse_facet ['method_citation']
<S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="73" ssid = "18">While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.</S><S sid ="86" ssid = "31">We call such additional edges secondary links and represent them as dotted lines  see fig.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S><S sid ="77" ssid = "22">Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'73'", "'86'", "'21'", "'77'"]
'56'
'73'
'86'
'21'
'77'
['56', '73', '86', '21', '77']
parsed_discourse_facet ['method_citation']
<S sid ="138" ssid = "19">This allows easy modification if needed.</S><S sid ="125" ssid = "6">The tool should also permit a convenient handling of node and edge labels.</S><S sid ="89" ssid = "2">However  some other standard analysts turn out to be problematic  mainly due to the partial  idealised character of competence grammars  which often marginalise or ignore such important. phenomena. as 'deficient' (e.g. headless) constructions  appositions  temporal expressions  etc.</S><S sid ="49" ssid = "39">Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.</S><S sid ="110" ssid = "23">A similar convention ha.s been adopted for constructions in which scope ambiguities have syntactic effects but a one-to-one correspondence between scope and attachment. does not seem reasonable  cf. focus particles such as only or also.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'", "'125'", "'89'", "'49'", "'110'"]
'138'
'125'
'89'
'49'
'110'
['138', '125', '89', '49', '110']
parsed_discourse_facet ['implication_citation']
<S sid ="147" ssid = "28">(Cutting et al.  1992) and (Feldweg  1995)).</S><S sid ="7" ssid = "4">On the basis of these considerations  we formulate several additional requirements.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="144" ssid = "25">We distinguish five degrees of automation: So far  about 1100 sentences of our corpus have been annotated.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'7'", "'80'", "'144'", "'21'"]
'147'
'7'
'80'
'144'
'21'
['147', '7', '80', '144', '21']
parsed_discourse_facet ['method_citation']
<S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S><S sid ="73" ssid = "18">While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.</S><S sid ="96" ssid = "9">In (5)  either a lexical nominalisation rule for the adjective Gliickliche is stipulated  or the existence of an empty nominal head.</S><S sid ="46" ssid = "36">These approaches provide an adequate explanation for several issues problematic for phrase-structure grammars (clause union  extraposition  diverse second-position phenomena).</S><S sid ="33" ssid = "23">(Lehmann et al.  1996)  (Marcus et al.  1994)  (Sampson  1995)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'73'", "'96'", "'46'", "'33'"]
'60'
'73'
'96'
'46'
'33'
['60', '73', '96', '46', '33']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "13">As the annotation scheme does not distinguish different bar levels or any similar intermediate categories  only a small set of node labels is needed (currently 16 tags  S  NP  AP ...).</S><S sid ="51" ssid = "41">This requirement speaks against the traditional sort of dependency trees  in which heads a re represented as non-terminal nodes  cf.</S><S sid ="109" ssid = "22">In addition  we have adopted a simple convention for those cases in which context information is insufficient  for total disambiguation: the highest possible attachment  site is chosen.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'51'", "'109'", "'11'", "'21'"]
'68'
'51'
'109'
'11'
'21'
['68', '51', '109', '11', '21']
parsed_discourse_facet ['method_citation']
<S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="138" ssid = "19">This allows easy modification if needed.</S><S sid ="115" ssid = "28">3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here  where C stands for coordination  and VP for the actual category.</S><S sid ="112" ssid = "25">A problem for the rudimentary argument. structure representations is the use of incomplete structures in natural language  i.e. phenomena such as coordination and ellipsis.</S><S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'138'", "'115'", "'112'", "'60'"]
'56'
'138'
'115'
'112'
'60'
['56', '138', '115', '112', '60']
parsed_discourse_facet ['results_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
parsing: input/ref/Task1/P08-1102_aakansha.csv
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="14">As predications generated from such templates depend on the current character, we name these templates lexical-target.</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="121" ssid="32">Among other features, the 4-gram POS LM plays the most important role, removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'121'"]
'121'
['121']
parsed_discourse_facet ['method_citation']
<S sid="37" ssid="9">C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'"]
'37'
['37']
parsed_discourse_facet ['method_citation']
<S sid="73" ssid="24">For instance, if the word w appears N times in training corpus and is labelled as POS t for n times, the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'73'"]
'73'
['73']
parsed_discourse_facet ['method_citation']
<S sid="46" ssid="18">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) &#8712; X &#215; Y to a feature vector 4)(x, y) &#8712; Rd, and a parameter vector &#945;&#65533; &#8712; Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'"]
'46'
['46']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S sid ="120" ssid = "31">Without the perceptron  the cascaded model (if we can still call it cascaded) performs poorly on both segmentation and Joint S&T.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'", "'58'", "'136'", "'31'", "'134'"]
'120'
'58'
'136'
'31'
'134'
['120', '58', '136', '31', '134']
parsed_discourse_facet ['implication_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="117" ssid = "28">Table 4 shows experiments results.</S><S sid ="93" ssid = "4">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid ="18" ssid = "14">In this architecture  knowledge sources that are intractable to incorporate into the perceptron  can be easily incorporated into the outside linear model.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'117'", "'93'", "'18'", "'136'"]
'68'
'117'
'93'
'18'
'136'
['68', '117', '93', '18', '136']
parsed_discourse_facet ['implication_citation']
<S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x  X to outputs y  Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="78" ssid = "3">Given a Chinese character sequence C1:n  the decoding procedure can proceed in a left-right fashion with a dynamic programming approach.</S><S sid ="79" ssid = "4">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="46" ssid = "18">Following Collins  we use a function GEN(x) generating all candidate results of an input x   a representation 4) mapping each training example (x  y)  X  Y to a feature vector 4)(x  y)  Rd  and a parameter vector   Rd corresponding to the feature vector. d means the dimension of the vector space  it equals to the amount of features in the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'", "'78'", "'79'", "'7'", "'46'"]
'45'
'78'
'79'
'7'
'46'
['45', '78', '79', '7', '46']
parsed_discourse_facet ['results_citation']
<S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="65" ssid = "16">As shown in Figure 1  the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.</S><S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="95" ssid = "6">For convenience of comparing with others  we focus only on the close test  which means that any extra resource is forbidden except the designated training corpus.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'65'", "'101'", "'95'", "'58'"]
'133'
'65'
'101'
'95'
'58'
['133', '65', '101', '95', '58']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="95" ssid = "6">For convenience of comparing with others  we focus only on the close test  which means that any extra resource is forbidden except the designated training corpus.</S><S sid ="61" ssid = "12">In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'47'", "'95'", "'61'", "'136'"]
'101'
'47'
'95'
'61'
'136'
['101', '47', '95', '61', '136']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called averaged parameters (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="71" ssid = "22">Using W = w1:m to denote the word sequence  T = t1:m to denote the corresponding POS sequence  P (T |W) to denote the probability that W is labelled as T  and P(W|T) to denote the probability that T generates W  we can define the cooccurrence model as follows: wt and tw denote the corresponding weights of the two components.</S><S sid ="92" ssid = "3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.</S><S sid ="82" ssid = "7">In addition  we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'49'", "'71'", "'92'", "'82'"]
'68'
'49'
'71'
'92'
'82'
['68', '49', '71', '92', '82']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "10">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid ="40" ssid = "12">Templates in the column below are expanded from the upper ones.</S><S sid ="43" ssid = "15">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S><S sid ="25" ssid = "21">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'58'", "'40'", "'43'", "'25'"]
'38'
'58'
'40'
'43'
'25'
['38', '58', '40', '43', '25']
parsed_discourse_facet ['method_citation']
<S sid ="93" ssid = "4">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid ="57" ssid = "8">It has a two-layer architecture  with a perceptron as the core and another linear model as the outside-layer.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="40" ssid = "12">Templates in the column below are expanded from the upper ones.</S><S sid ="17" ssid = "13">We will describe it in detail in Section 4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'93'", "'57'", "'136'", "'40'", "'17'"]
'93'
'57'
'136'
'40'
'17'
['93', '57', '136', '40', '17']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="33" ssid = "5">In following subsections  we describe the feature templates and the perceptron training algorithm.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="96" ssid = "7">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus  we randomly chosen 2  000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84  294 sentences)  then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'68'", "'33'", "'134'", "'96'"]
'101'
'68'
'33'
'134'
'96'
['101', '68', '33', '134', '96']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="89" ssid = "14">Function D derives the candidate result from the word-POS pair p and the candidate q at prior position of p.</S><S sid ="9" ssid = "5">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid ="79" ssid = "4">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid ="104" ssid = "15">According to the usual practice in syntactic analysis  we choose chapters 1  260 (18074 sentences) as training set  chapter 271  300 (348 sentences) as test set and chapter 301  325 (350 sentences) as development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'89'", "'9'", "'79'", "'104'"]
'68'
'89'
'9'
'79'
'104'
['68', '89', '9', '79', '104']
parsed_discourse_facet ['implication_citation']
<S sid ="125" ssid = "36">However unlike the three features  the word LM brings very tiny improvement.</S><S sid ="40" ssid = "12">Templates in the column below are expanded from the upper ones.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'40'", "'94'", "'2'", "'58'"]
'125'
'40'
'94'
'2'
'58'
['125', '40', '94', '2', '58']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="56" ssid = "7">To alleviate the drawbacks  we propose a cascaded linear model.</S><S sid ="61" ssid = "12">In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.</S><S sid ="93" ssid = "4">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid ="112" ssid = "23">Here the core perceptron was just the POS+ model in experiments above.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'56'", "'61'", "'93'", "'112'"]
'136'
'56'
'61'
'93'
'112'
['136', '56', '61', '93', '112']
parsed_discourse_facet ['method_citation']
<S sid ="99" ssid = "10">Then we trained LEX on each of the four corpora for 7 iterations.</S><S sid ="120" ssid = "31">Without the perceptron  the cascaded model (if we can still call it cascaded) performs poorly on both segmentation and Joint S&T.</S><S sid ="121" ssid = "32">Among other features  the 4-gram POS LM plays the most important role  removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&T.</S><S sid ="42" ssid = "14">As predications generated from such templates depend on the current character  we name these templates lexical-target.</S><S sid ="117" ssid = "28">Table 4 shows experiments results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'99'", "'120'", "'121'", "'42'", "'117'"]
'99'
'120'
'121'
'42'
'117'
['99', '120', '121', '42', '117']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="64" ssid = "15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S><S sid ="53" ssid = "4">Figure 2 shows the growing tendency of feature space with the introduction of these features as well as the character-based ones.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'134'", "'64'", "'53'", "'7'"]
'68'
'134'
'64'
'53'
'7'
['68', '134', '64', '53', '7']
parsed_discourse_facet ['results_citation']
<S sid ="3" ssid = "3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="84" ssid = "9">Algorithm 2 shows the decoding algorithm.</S><S sid ="18" ssid = "14">In this architecture  knowledge sources that are intractable to incorporate into the perceptron  can be easily incorporated into the outside linear model.</S><S sid ="92" ssid = "3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'136'", "'84'", "'18'", "'92'"]
'3'
'136'
'84'
'18'
'92'
['3', '136', '84', '18', '92']
parsed_discourse_facet ['implication_citation']
<S sid ="51" ssid = "2">Additional features most widely used are related to word or POS ngrams.</S><S sid ="13" ssid = "9">However  as such features are generated dynamically during the decoding procedure  two limitation arise: on the one hand  the amount of parameters increases rapidly  which is apt to overfit on training corpus; on the other hand  exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called averaged parameters (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="36" ssid = "8">All feature templates and their instances are shown in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'13'", "'7'", "'49'", "'36'"]
'51'
'13'
'7'
'49'
'36'
['51', '13', '7', '49', '36']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/A97-1014_sweta.csv
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["168'"]
168'
['168']
parsed_discourse_facet ['method_citation']
<S sid="165" ssid="7">In addition the approach provides empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linea.rizations, and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.</S>
original cit marker offset is 0
new cit marker offset is 0



["165'"]
165'
['165']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="26">This amount of data suffices as training material to reliably assign the grammatical functions if the user determines the elements of a phrase and its type (step 1 of the list above).</S>
original cit marker offset is 0
new cit marker offset is 0



["145'"]
145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.<
original cit marker offset is 0
new cit marker offset is 0



["15'"]
15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="31">Apart from this rather technical problem, two further arguments speak against phrase structure as the structural pivot of the annotation scheme: Finally, the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies, which has proved tedious.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
<S sid="47" ssid="37">Argument structure can be represented in terms of unordered trees (with crossing branches).</S>
original cit marker offset is 0
new cit marker offset is 0



["47'"]
47'
['47']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
 <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["160'"]
160'
['160']
parsed_discourse_facet ['method_citation']
<S sid="166" ssid="8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
<S sid="166" ssid="8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="24">Sentences annotated in previous steps are used as training material for further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
  <S sid="71" ssid="16">However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["71'"]
71'
['71']
parsed_discourse_facet ['method_citation']
  <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A97-1014.csv
<S sid ="73" ssid = "18">While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.</S><S sid ="115" ssid = "28">3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here  where C stands for coordination  and VP for the actual category.</S><S sid ="135" ssid = "16">Figure 1 shows a screen dump of the tool.</S><S sid ="39" ssid = "29">Consider the German sentence (1) daran wird ihn Anna erkennen  &di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies  marked by co-references between traces and the corresponding nodes.</S><S sid ="84" ssid = "29">Consider (qui verbs where the subject of the infinitival VP is not realised syntactically  but co-referent with the subject or object. of the matrix equi verb: (3) er bat mich zu kommen he asked me to come (mich is the understood subject. of kommt n).</S>
original cit marker offset is 0
new cit marker offset is 0



["'73'", "'115'", "'135'", "'39'", "'84'"]
'73'
'115'
'135'
'39'
'84'
['73', '115', '135', '39', '84']
parsed_discourse_facet ['implication_citation']
<S sid ="110" ssid = "23">A similar convention ha.s been adopted for constructions in which scope ambiguities have syntactic effects but a one-to-one correspondence between scope and attachment. does not seem reasonable  cf. focus particles such as only or also.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="30" ssid = "20">Thus the context-free constituent backbone plays a pivotal role in the annotation scheme.</S><S sid ="113" ssid = "26">Since a precise structural description of non-constituent coordination would require a. rich inventory of incomplete phrase types  we have agreed on a sort of unde.rspe.cified representations: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'", "'80'", "'30'", "'113'", "'56'"]
'110'
'80'
'30'
'113'
'56'
['110', '80', '30', '113', '56']
parsed_discourse_facet ['implication_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S><S sid ="78" ssid = "23">Morphological information: Another set of labels represents morphological information.</S><S sid ="8" ssid = "5">A formalism complying with these requirements is described in section 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'56'", "'60'", "'78'", "'8'"]
'80'
'56'
'60'
'78'
'8'
['80', '56', '60', '78', '8']
parsed_discourse_facet ['results_citation']
<S sid ="0" ssid = "0">An Annotation Scheme for Free Word Order Languages</S><S sid ="18" ssid = "8">(Marcus et al.  1994).</S><S sid ="151" ssid = "32">For evaluation  the already annotated sentences were divided into two disjoint sets  one for training (90% of the corpus)  the other one for testing (10%).</S><S sid ="49" ssid = "39">Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.</S><S sid ="114" ssid = "27">Fig.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'18'", "'151'", "'49'", "'114'"]
'0'
'18'
'151'
'49'
'114'
['0', '18', '151', '49', '114']
parsed_discourse_facet ['method_citation']
<S sid ="65" ssid = "10">The tree resembles traditional constituent structures.</S><S sid ="62" ssid = "7">2.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="77" ssid = "22">Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'62'", "'80'", "'77'", "'5'"]
'65'
'62'
'80'
'77'
'5'
['65', '62', '80', '77', '5']
parsed_discourse_facet ['method_citation']
<S sid ="49" ssid = "39">Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="118" ssid = "31">An explicit coordinating conjunction need not be present.</S><S sid ="28" ssid = "18">(Bies et al.  1995)  (Sampson  1995)).</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'56'", "'118'", "'28'", "'80'"]
'49'
'56'
'118'
'28'
'80'
['49', '56', '118', '28', '80']
parsed_discourse_facet ['method_citation']
<S sid ="65" ssid = "10">The tree resembles traditional constituent structures.</S><S sid ="108" ssid = "21">However  full or partial disambiguation takes place in context  and the annotators do not consider unrealistic readings.</S><S sid ="96" ssid = "9">In (5)  either a lexical nominalisation rule for the adjective Gliickliche is stipulated  or the existence of an empty nominal head.</S><S sid ="86" ssid = "31">We call such additional edges secondary links and represent them as dotted lines  see fig.</S><S sid ="135" ssid = "16">Figure 1 shows a screen dump of the tool.</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'108'", "'96'", "'86'", "'135'"]
'65'
'108'
'96'
'86'
'135'
['65', '108', '96', '86', '135']
parsed_discourse_facet ['method_citation']
<S sid ="115" ssid = "28">3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here  where C stands for coordination  and VP for the actual category.</S><S sid ="77" ssid = "22">Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.</S><S sid ="109" ssid = "22">In addition  we have adopted a simple convention for those cases in which context information is insufficient  for total disambiguation: the highest possible attachment  site is chosen.</S><S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'77'", "'109'", "'60'", "'21'"]
'115'
'77'
'109'
'60'
'21'
['115', '77', '109', '60', '21']
parsed_discourse_facet ['method_citation']
<S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="73" ssid = "18">While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.</S><S sid ="86" ssid = "31">We call such additional edges secondary links and represent them as dotted lines  see fig.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S><S sid ="77" ssid = "22">Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'73'", "'86'", "'21'", "'77'"]
'56'
'73'
'86'
'21'
'77'
['56', '73', '86', '21', '77']
parsed_discourse_facet ['method_citation']
<S sid ="138" ssid = "19">This allows easy modification if needed.</S><S sid ="125" ssid = "6">The tool should also permit a convenient handling of node and edge labels.</S><S sid ="89" ssid = "2">However  some other standard analysts turn out to be problematic  mainly due to the partial  idealised character of competence grammars  which often marginalise or ignore such important. phenomena. as 'deficient' (e.g. headless) constructions  appositions  temporal expressions  etc.</S><S sid ="49" ssid = "39">Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.</S><S sid ="110" ssid = "23">A similar convention ha.s been adopted for constructions in which scope ambiguities have syntactic effects but a one-to-one correspondence between scope and attachment. does not seem reasonable  cf. focus particles such as only or also.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'", "'125'", "'89'", "'49'", "'110'"]
'138'
'125'
'89'
'49'
'110'
['138', '125', '89', '49', '110']
parsed_discourse_facet ['implication_citation']
<S sid ="147" ssid = "28">(Cutting et al.  1992) and (Feldweg  1995)).</S><S sid ="7" ssid = "4">On the basis of these considerations  we formulate several additional requirements.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="144" ssid = "25">We distinguish five degrees of automation: So far  about 1100 sentences of our corpus have been annotated.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'7'", "'80'", "'144'", "'21'"]
'147'
'7'
'80'
'144'
'21'
['147', '7', '80', '144', '21']
parsed_discourse_facet ['method_citation']
<S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S><S sid ="73" ssid = "18">While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.</S><S sid ="96" ssid = "9">In (5)  either a lexical nominalisation rule for the adjective Gliickliche is stipulated  or the existence of an empty nominal head.</S><S sid ="46" ssid = "36">These approaches provide an adequate explanation for several issues problematic for phrase-structure grammars (clause union  extraposition  diverse second-position phenomena).</S><S sid ="33" ssid = "23">(Lehmann et al.  1996)  (Marcus et al.  1994)  (Sampson  1995)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'73'", "'96'", "'46'", "'33'"]
'60'
'73'
'96'
'46'
'33'
['60', '73', '96', '46', '33']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "13">As the annotation scheme does not distinguish different bar levels or any similar intermediate categories  only a small set of node labels is needed (currently 16 tags  S  NP  AP ...).</S><S sid ="51" ssid = "41">This requirement speaks against the traditional sort of dependency trees  in which heads a re represented as non-terminal nodes  cf.</S><S sid ="109" ssid = "22">In addition  we have adopted a simple convention for those cases in which context information is insufficient  for total disambiguation: the highest possible attachment  site is chosen.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'51'", "'109'", "'11'", "'21'"]
'68'
'51'
'109'
'11'
'21'
['68', '51', '109', '11', '21']
parsed_discourse_facet ['method_citation']
<S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="138" ssid = "19">This allows easy modification if needed.</S><S sid ="115" ssid = "28">3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here  where C stands for coordination  and VP for the actual category.</S><S sid ="112" ssid = "25">A problem for the rudimentary argument. structure representations is the use of incomplete structures in natural language  i.e. phenomena such as coordination and ellipsis.</S><S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'138'", "'115'", "'112'", "'60'"]
'56'
'138'
'115'
'112'
'60'
['56', '138', '115', '112', '60']
parsed_discourse_facet ['results_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
parsing: input/ref/Task1/A00-2030_vardha.csv
 <S sid="11" ssid="1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="10">Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'"]
'50'
['50']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
 <S sid="106" ssid="3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
 <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
 <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="2">The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
 <S sid="3" ssid="1">Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'"]
'3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="15">Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'"]
'32'
['32']
parsed_discourse_facet ['method_citation']
 <S sid="10" ssid="8">Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="13">Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
    <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
 <S sid="26" ssid="9">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'"]
'26'
['26']
parsed_discourse_facet ['method_citation']
    <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="2">Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="102" ssid = "7">The results are summarized in Table 2.</S><S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'79'", "'82'", "'102'", "'56'"]
'68'
'79'
'82'
'102'
'56'
['68', '79', '82', '102', '56']
parsed_discourse_facet ['implication_citation']
<S sid ="65" ssid = "6">We illustrate the generation process by walking through a few of the steps of the parse shown in Figure 3.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="9" ssid = "7">Manually creating sourcespecific training data for syntax was not required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'96'", "'23'", "'88'", "'9'"]
'65'
'96'
'23'
'88'
'9'
['65', '96', '23', '88', '9']
parsed_discourse_facet ['implication_citation']
<S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'68'", "'36'", "'6'", "'96'"]
'88'
'68'
'36'
'6'
'96'
['88', '68', '36', '6', '96']
parsed_discourse_facet ['results_citation']
<S sid ="60" ssid = "1">In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'62'", "'13'", "'36'", "'105'"]
'60'
'62'
'13'
'36'
'105'
['60', '62', '13', '36', '105']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="47" ssid = "7">It soon became painfully obvious that this task could not be performed in the available time.</S><S sid ="73" ssid = "14">We now briefly summarize the probability structure of the model.</S><S sid ="97" ssid = "2">The evaluation results are summarized in Table 1.</S><S sid ="80" ssid = "2">However  because these estimates are too sparse to be relied upon  we use interpolated estimates consisting of mixtures of successively lowerorder estimates (as in Placeway et al. 1993).</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'47'", "'73'", "'97'", "'80'"]
'38'
'47'
'73'
'97'
'80'
['38', '47', '73', '97', '80']
parsed_discourse_facet ['method_citation']
<S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'", "'15'", "'45'", "'88'", "'56'"]
'79'
'15'
'45'
'88'
'56'
['79', '15', '45', '88', '56']
parsed_discourse_facet ['method_citation']
<S sid ="49" ssid = "9">By necessity  we adopted the strategy of hand marking only the semantics.</S><S sid ="70" ssid = "11">Post-modifier constituents for the PER/NP.</S><S sid ="69" ssid = "10">8.</S><S sid ="95" ssid = "14">The semantics  that is  the entities and relations  can then be directly extracted from these sentential trees.</S><S sid ="111" ssid = "3">The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies  either expressed or implied  of the Defense Advanced Research Projects Agency or the United States Government.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'70'", "'69'", "'95'", "'111'"]
'49'
'70'
'69'
'95'
'111'
['49', '70', '69', '95', '111']
parsed_discourse_facet ['method_citation']
<S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="4" ssid = "2">Yet  relatively few have embedded one of these algorithms in a task.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="67" ssid = "8">We pick up the derivation just after the topmost S and its head word  said  have been produced.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'79'", "'4'", "'82'", "'67'"]
'23'
'79'
'4'
'82'
'67'
['23', '79', '4', '82', '67']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "3">However  pipelined architectures suffer from a serious disadvantage: errors accumulate as they propagate through the pipeline.</S><S sid ="31" ssid = "14">If the single generalized model could then be extended to semantic analysis  all necessary sentence level processing would be contained in that model.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="84" ssid = "3">Although mathematically the model predicts tree elements in a top-down fashion  we search the space bottom-up using a chartbased search.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'31'", "'0'", "'38'", "'84'"]
'20'
'31'
'0'
'38'
'84'
['20', '31', '0', '38', '84']
parsed_discourse_facet ['method_citation']
<S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'38'", "'107'", "'104'", "'23'"]
'88'
'38'
'107'
'104'
'23'
['88', '38', '107', '104', '23']
parsed_discourse_facet ['implication_citation']
<S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="74" ssid = "15">The categories for head constituents  cl are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c _1  and the head word of their parent  wp.</S><S sid ="69" ssid = "10">8.</S><S sid ="111" ssid = "3">The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies  either expressed or implied  of the Defense Advanced Research Projects Agency or the United States Government.</S>
original cit marker offset is 0
new cit marker offset is 0



["'82'", "'45'", "'74'", "'69'", "'111'"]
'82'
'45'
'74'
'69'
'111'
['82', '45', '74', '69', '111']
parsed_discourse_facet ['method_citation']
<S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="102" ssid = "7">The results are summarized in Table 2.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'", "'102'", "'82'", "'62'", "'68'"]
'45'
'102'
'82'
'62'
'68'
['45', '102', '82', '62', '68']
parsed_discourse_facet ['method_citation']
<S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="97" ssid = "2">The evaluation results are summarized in Table 1.</S><S sid ="75" ssid = "16">Separate probabilities are maintained for left (pre) and right (post) modifiers: Part-of-speech tags  t    for modifiers are predicted based on the modifier  cm  the partof-speech tag of the head word  th  and the head word itself  wh: Head words  w for modifiers are predicted based on the modifier  cm  the part-of-speech tag of the modifier word   t the part-ofspeech tag of the head word   th  and the head word itself  wh: lAwmicm tm th wh)  e.g.</S><S sid ="95" ssid = "14">The semantics  that is  the entities and relations  can then be directly extracted from these sentential trees.</S><S sid ="59" ssid = "5">These labels serve to form a continuous chain between the relation and its argument.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'", "'97'", "'75'", "'95'", "'59'"]
'45'
'97'
'75'
'95'
'59'
['45', '97', '75', '95', '59']
parsed_discourse_facet ['method_citation']
<S sid ="76" ssid = "17">Finally  word features  fm  for modifiers are predicted based on the modifier  cm  the partof-speech tag of the modifier word   t the part-of-speech tag of the head word th  the head word itself  wh  and whether or not the modifier head word  w is known or unknown.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="112" ssid = "4">We thank Michael Collins of the University of Pennsylvania for his valuable suggestions.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'", "'36'", "'112'", "'101'", "'5'"]
'76'
'36'
'112'
'101'
'5'
['76', '36', '112', '101', '5']
parsed_discourse_facet ['results_citation']
<S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'68'", "'13'", "'38'", "'23'"]
'56'
'68'
'13'
'38'
'23'
['56', '68', '13', '38', '23']
parsed_discourse_facet ['implication_citation']
<S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'62'", "'36'", "'79'", "'107'"]
'51'
'62'
'36'
'79'
'107'
['51', '62', '36', '79', '107']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="49" ssid = "9">By necessity  we adopted the strategy of hand marking only the semantics.</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="39" ssid = "7">For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'49'", "'38'", "'5'", "'39'"]
'6'
'49'
'38'
'5'
'39'
['6', '49', '38', '5', '39']
parsed_discourse_facet ['results_citation']
<S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="31" ssid = "14">If the single generalized model could then be extended to semantic analysis  all necessary sentence level processing would be contained in that model.</S><S sid ="83" ssid = "2">More precisely  it must find the most likely augmented parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'57'", "'88'", "'31'", "'83'"]
'23'
'57'
'88'
'31'
'83'
['23', '57', '88', '31', '83']
parsed_discourse_facet ['aim_citation']
<S sid ="95" ssid = "14">The semantics  that is  the entities and relations  can then be directly extracted from these sentential trees.</S><S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="34" ssid = "2">In these trees  the standard TREEBANK structures are augmented to convey semantic information  that is  entities and relations.</S><S sid ="4" ssid = "2">Yet  relatively few have embedded one of these algorithms in a task.</S><S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'45'", "'34'", "'4'", "'6'"]
'95'
'45'
'34'
'4'
'6'
['95', '45', '34', '4', '6']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P11-1061_swastika.csv
    <S sid="144" ssid="7">For comparison, the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%, and goes up to 88.7% with a treebank dictionary.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
    <S sid="44" ssid="10">Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.</S
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
parsed_discourse_facet ['method_citation']
    <S sid="16" ssid="12">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (&#167;3), and then use graph label propagation to project syntactic information from English to the foreign language (&#167;4).</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="44" ssid="10">Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.</S
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
parsed_discourse_facet ['method_citation']
<S sid="110" ssid="10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S>
original cit marker offset is 0
new cit marker offset is 0



['110']
110
['110']
parsed_discourse_facet ['aim_citation']
    <S sid="115" ssid="15">The taggers were trained on datasets labeled with the universal tags.</S>
original cit marker offset is 0
new cit marker offset is 0



['115']
115
['115']
parsed_discourse_facet ['method_citation']
    <S sid="115" ssid="15">The taggers were trained on datasets labeled with the universal tags.</S>
original cit marker offset is 0
new cit marker offset is 0



['115']
115
['115']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['158']
158
['158']
parsed_discourse_facet ['result_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



['23']
23
['23']
parsed_discourse_facet ['result_citation']
<S sid="24" ssid="1">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['24']
24
['24']
parsed_discourse_facet ['aim_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['???']
???
['???']
parsed_discourse_facet ['method_citation']
    <S sid="16" ssid="12">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (&#167;3), and then use graph label propagation to project syntactic information from English to the foreign language (&#167;4).</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



['23']
23
['23']
parsed_discourse_facet ['result_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['158']
158
['158']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
<S sid="56" ssid="22">To define a similarity function between the English and the foreign vertices, we rely on high-confidence word alignments.</S>
original cit marker offset is 0
new cit marker offset is 0



['56']
56
['56']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



['161']
161
['161']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P11-1061.csv
<S sid ="14" ssid = "10">Our work is closest to that of Yarowsky and Ngai (2001)  but differs in two important ways.</S><S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S><S sid ="103" ssid = "3">The availability of these resources guided our selection of foreign languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'7'", "'56'", "'135'", "'103'"]
'14'
'7'
'56'
'135'
'103'
['14', '7', '56', '135', '103']
parsed_discourse_facet ['implication_citation']
<S sid ="75" ssid = "6">We use a squared loss to penalize neighboring vertices that have different label distributions: kqi  qjk2 = Ey(qi(y)  qj(y))2  and additionally regularize the label distributions towards the uniform distribution U over all possible labels Y.</S><S sid ="53" ssid = "19">Finally  note that while most feature concepts are lexicalized  others  such as the suffix concept  are not.</S><S sid ="34" ssid = "11">The following three sections elaborate these different stages is more detail.</S><S sid ="129" ssid = "29">Fortunately  performance was stable across various values  and we were able to use the same hyperparameters for all languages.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'53'", "'34'", "'129'", "'26'"]
'75'
'53'
'34'
'129'
'26'
['75', '53', '34', '129', '26']
parsed_discourse_facet ['implication_citation']
<S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="59" ssid = "25">So far the graph has been completely unlabeled.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'107'", "'59'", "'56'", "'39'"]
'1'
'107'
'59'
'56'
'39'
['1', '107', '59', '56', '39']
parsed_discourse_facet ['results_citation']
<S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : We describe how we choose  in 6.4.</S><S sid ="66" ssid = "32">In general  the neighborhoods can be more diverse and we allow a soft label distribution over the vertices.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'66'", "'107'", "'117'", "'135'"]
'83'
'66'
'107'
'117'
'135'
['83', '66', '107', '117', '135']
parsed_discourse_facet ['method_citation']
<S sid ="104" ssid = "4">For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi  2006; Nivre et al.  2007).</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="155" ssid = "18">Examining the word fidanzato for the No LP and With LP models is particularly instructive.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="98" ssid = "29">7).</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'", "'1'", "'155'", "'39'", "'98'"]
'104'
'1'
'155'
'39'
'98'
['104', '1', '155', '39', '98']
parsed_discourse_facet ['method_citation']
<S sid ="143" ssid = "6">Overall  it gives improvements ranging from 1.1% for German to 14.7% for Italian  for an average improvement of 8.3% over the unsupervised feature-HMM model.</S><S sid ="96" ssid = "27">The function A : F * C maps from the language specific fine-grained tagset F to the coarser universal tagset C and is described in detail in 6.2: Note that when tx(y) = 1 the feature value is 0 and has no effect on the model  while its value is oc when tx(y) = 0 and constrains the HMMs state space.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="68" ssid = "34">In the label propagation stage  we propagate the automatic English tags to the aligned Italian trigram types  followed by further propagation solely among the Italian vertices. the Italian vertices are connected to an automatically labeled English vertex.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'96'", "'97'", "'68'", "'26'"]
'143'
'96'
'97'
'68'
'26'
['143', '96', '97', '68', '26']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="129" ssid = "29">Fortunately  performance was stable across various values  and we were able to use the same hyperparameters for all languages.</S><S sid ="108" ssid = "8">However  we needed to restrict ourselves to these languages in order to be able to evaluate the performance of our approach.</S><S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'26'", "'129'", "'108'", "'37'"]
'7'
'26'
'129'
'108'
'37'
['7', '26', '129', '108', '37']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S><S sid ="122" ssid = "22">For each language  we took the same number of sentences from the bitext as there are in its treebank  and trained a supervised feature-HMM.</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="31" ssid = "8">By aggregating the POS labels of the English tokens to types  we can generate label distributions for the English vertices.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'110'", "'122'", "'19'", "'31'"]
'39'
'110'
'122'
'19'
'31'
['39', '110', '122', '19', '31']
parsed_discourse_facet ['method_citation']
<S sid ="73" ssid = "4">Note that because we extracted only high-confidence alignments  many foreign vertices will not be connected to any English vertices.</S><S sid ="24" ssid = "1">The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.</S><S sid ="150" ssid = "13">For all languages  the vocabulary sizes increase by several thousand words.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="139" ssid = "2">As expected  the vanilla HMM trained with EM performs the worst.</S>
original cit marker offset is 0
new cit marker offset is 0



["'73'", "'24'", "'150'", "'39'", "'139'"]
'73'
'24'
'150'
'39'
'139'
['73', '24', '150', '39', '139']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="45" ssid = "11">Furthermore  we do not connect the English vertices to each other  but only to foreign language vertices.4 The graph vertices are extracted from the different sides of a parallel corpus (De  Df) and an additional unlabeled monolingual foreign corpus Ff  which will be used later for training.</S><S sid ="69" ssid = "35">Label propagation is used to propagate these tags inwards and results in tag distributions for the middle word of each Italian trigram.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'45'", "'69'", "'135'", "'110'"]
'107'
'45'
'69'
'135'
'110'
['107', '45', '69', '135', '110']
parsed_discourse_facet ['implication_citation']
<S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : We describe how we choose  in 6.4.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'83'", "'25'", "'4'"]
'37'
'1'
'83'
'25'
'4'
['37', '1', '83', '25', '4']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="120" ssid = "20">We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="44" ssid = "10">Because all English vertices are going to be labeled  we do not need to disambiguate them by embedding them in trigrams.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'120'", "'117'", "'44'", "'26'"]
'54'
'120'
'117'
'44'
'26'
['54', '120', '117', '44', '26']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="47" ssid = "13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S><S sid ="161" ssid = "4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections  and bridge the gap between purely supervised and unsupervised POS tagging models.</S><S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'117'", "'47'", "'161'", "'7'"]
'39'
'117'
'47'
'161'
'7'
['39', '117', '47', '161', '7']
parsed_discourse_facet ['method_citation']
<S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="32" ssid = "9">Label propagation can then be used to transfer the labels to the peripheral foreign vertices (i.e. the ones adjacent to the English vertices) first  and then among all of the foreign vertices (4).</S><S sid ="86" ssid = "17">For a sentence x and a state sequence z  a first order Markov model defines a distribution: (9) where Val(X) corresponds to the entire vocabulary.</S><S sid ="132" ssid = "32">When extracting the vector t  used to compute the constraint feature from the graph  we tried three threshold values for r (see Eq.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'117'", "'32'", "'86'", "'132'"]
'37'
'117'
'32'
'86'
'132'
['37', '117', '32', '86', '132']
parsed_discourse_facet ['results_citation']
<S sid ="163" ssid = "2">We would also like to thank Amarnag Subramanya for helping us with the implementation of label propagation and Shankar Kumar for access to the parallel data.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S><S sid ="139" ssid = "2">As expected  the vanilla HMM trained with EM performs the worst.</S><S sid ="58" ssid = "24">Based on these high-confidence alignments we can extract tuples of the form [u H v]  where u is a foreign trigram type  whose middle word aligns to an English word type v. Our bilingual similarity function then sets the edge weights in proportion to these tuple counts.</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : We describe how we choose  in 6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'110'", "'139'", "'58'", "'83'"]
'163'
'110'
'139'
'58'
'83'
['163', '110', '139', '58', '83']
parsed_discourse_facet ['implication_citation']
<S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="134" ssid = "34">Because we dont have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S><S sid ="72" ssid = "3">In the first stage  we run a single step of label propagation  which transfers the label distributions from the English vertices to the connected foreign language vertices (say  Vf) at the periphery of the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'117'", "'134'", "'110'", "'72'"]
'37'
'117'
'134'
'110'
'72'
['37', '117', '134', '110', '72']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="104" ssid = "4">For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi  2006; Nivre et al.  2007).</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'39'", "'26'", "'104'", "'135'"]
'54'
'39'
'26'
'104'
'135'
['54', '39', '26', '104', '135']
parsed_discourse_facet ['results_citation']
<S sid ="33" ssid = "10">The POS distributions over the foreign trigram types are used as features to learn a better unsupervised POS tagger (5).</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S><S sid ="90" ssid = "21">All features were conjoined with the state z.</S><S sid ="96" ssid = "27">The function A : F * C maps from the language specific fine-grained tagset F to the coarser universal tagset C and is described in detail in 6.2: Note that when tx(y) = 1 the feature value is 0 and has no effect on the model  while its value is oc when tx(y) = 0 and constrains the HMMs state space.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'117'", "'4'", "'90'", "'96'"]
'33'
'117'
'4'
'90'
'96'
['33', '117', '4', '90', '96']
parsed_discourse_facet ['aim_citation']
parsing: input/ref/Task1/W99-0623_vardha.csv
    <S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="117" ssid="46">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'117'"]
'117'
['117']
parsed_discourse_facet ['method_citation']
<S sid="72" ssid="1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
    <S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'"]
'120'
['120']
parsed_discourse_facet ['method_citation']
  <S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'139'"]
'139'
['139']
parsed_discourse_facet ['method_citation']
 <S sid="84" ssid="13">The first shows how constituent features and context do not help in deciding which parser to trust.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'"]
'84'
['84']
parsed_discourse_facet ['method_citation']
    <S sid="76" ssid="5">The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'"]
'76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'"]
'38'
['38']
parsed_discourse_facet ['method_citation']
  <S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
 <S sid="72" ssid="1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
    <S sid="87" ssid="16">It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'"]
'87'
['87']
parsed_discourse_facet ['method_citation']
    <S sid="51" ssid="37">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
    <S sid="79" ssid="8">Precision is the portion of hypothesized constituents that are correct and recall is the portion of the Treebank constituents that are hypothesized.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
  <S sid="27" ssid="13">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'"]
'27'
['27']
parsed_discourse_facet ['method_citation']
 <S sid="77" ssid="6">Each parse is converted into a set of constituents represented as a tuples: (label, start, end).</S>
original cit marker offset is 0
new cit marker offset is 0



["'77'"]
'77'
['77']
parsed_discourse_facet ['method_citation']
  <S sid="11" ssid="7">Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
  <S sid="116" ssid="45">The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'116'"]
'116'
['116']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0623.csv
<S sid ="34" ssid = "20">Mi(c) is a binary function returning t when parser i (from among the k parsers) suggests constituent c should be in the parse.</S><S sid ="106" ssid = "35">In each figure the upper graph shows the isolated constituent precision and the bottom graph shows the corresponding number of hypothesized constituents.</S><S sid ="66" ssid = "52">Each decision determines the inclusion or exclusion of a candidate constituent.</S><S sid ="65" ssid = "51">We model each parse as the decisions made to create it  and model those decisions as independent events.</S><S sid ="2" ssid = "2">Two general approaches are presented and two combination techniques are described for each approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'106'", "'66'", "'65'", "'2'"]
'34'
'106'
'66'
'65'
'2'
['34', '106', '66', '65', '2']
Error in Discourse Facet
<S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'15'", "'130'", "'57'", "'50'"]
'18'
'15'
'130'
'57'
'50'
['18', '15', '130', '57', '50']
Error in Discourse Facet
<S sid ="122" ssid = "51">All of these systems were run on data that was not seen during their development.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="24" ssid = "10">We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="107" ssid = "36">Again we notice that the isolated constituent precision is larger than 0.5 only in those partitions that contain very few samples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'122'", "'11'", "'24'", "'61'", "'107'"]
'122'
'11'
'24'
'61'
'107'
['122', '11', '24', '61', '107']
Error in Discourse Facet
<S sid ="92" ssid = "21">While we cannot prove there are no such useful features on which one should condition trust  we can give some insight into why the features we explored offered no gain.</S><S sid ="111" ssid = "40">The first row represents the average accuracy of the three parsers we combine.</S><S sid ="104" ssid = "33">In the cases where isolated constituent precision is larger than 0.5 the affected portion of the hypotheses is negligible.</S><S sid ="42" ssid = "28">Call the crossing constituents A and B.</S><S sid ="117" ssid = "46">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'111'", "'104'", "'42'", "'117'"]
'92'
'111'
'104'
'42'
'117'
['92', '111', '104', '42', '117']
Error in Discourse Facet
<S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="39" ssid = "25">There are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.</S><S sid ="63" ssid = "49">The probabilistic version of this procedure is straightforward: We once again assume independence among our various member parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'139'", "'11'", "'39'", "'63'"]
'15'
'139'
'11'
'39'
'63'
['15', '139', '11', '39', '63']
Error in Discourse Facet
<S sid ="39" ssid = "25">There are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.</S><S sid ="86" ssid = "15">Finally we show the combining techniques degrade very little when a poor parser is added to the set.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a nave Bayes classifier to determine which constituents to include in the parse.</S><S sid ="17" ssid = "3">The substructures that are unanimously hypothesized by the parsers should be preserved after combination  and the combination technique should not foolishly create substructures for which there is no supporting evidence.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'86'", "'27'", "'17'", "'61'"]
'39'
'86'
'27'
'17'
'61'
['39', '86', '27', '17', '61']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="24" ssid = "10">We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'72'", "'21'", "'24'", "'139'"]
'112'
'72'
'21'
'24'
'139'
['112', '72', '21', '24', '139']
Error in Discourse Facet
<S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S><S sid ="96" ssid = "25">We call such a constituent an isolated constituent.</S><S sid ="132" ssid = "61">The results of this experiment can be seen in Table 5.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and nave Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="125" ssid = "54">The constituent voting and nave Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'139'", "'96'", "'132'", "'38'", "'125'"]
'139'
'96'
'132'
'38'
'125'
['139', '96', '132', '38', '125']
Error in Discourse Facet
<S sid ="43" ssid = "29">A receives a votes  and B receives b votes.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="84" ssid = "13">The first shows how constituent features and context do not help in deciding which parser to trust.</S><S sid ="93" ssid = "22">Because we are working with only three parsers  the only situation in which context will help us is when it can indicate we should choose to believe a single parser that disagrees with the majority hypothesis instead of the majority hypothesis itself.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'15'", "'52'", "'84'", "'93'"]
'43'
'15'
'52'
'84'
'93'
['43', '15', '52', '84', '93']
Error in Discourse Facet
<S sid ="101" ssid = "30">We show the results of three of the experiments we conducted to measure isolated constituent precision under various partitioning schemes.</S><S sid ="126" ssid = "55">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a nave Bayes classifier to determine which constituents to include in the parse.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="43" ssid = "29">A receives a votes  and B receives b votes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'126'", "'27'", "'52'", "'43'"]
'101'
'126'
'27'
'52'
'43'
['101', '126', '27', '52', '43']
Error in Discourse Facet
<S sid ="126" ssid = "55">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S><S sid ="142" ssid = "4">Both of the switching techniques  as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.</S><S sid ="111" ssid = "40">The first row represents the average accuracy of the three parsers we combine.</S><S sid ="43" ssid = "29">A receives a votes  and B receives b votes.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'142'", "'111'", "'43'", "'52'"]
'126'
'142'
'111'
'43'
'52'
['126', '142', '111', '43', '52']
Error in Discourse Facet
<S sid ="101" ssid = "30">We show the results of three of the experiments we conducted to measure isolated constituent precision under various partitioning schemes.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="106" ssid = "35">In each figure the upper graph shows the isolated constituent precision and the bottom graph shows the corresponding number of hypothesized constituents.</S><S sid ="22" ssid = "8">If enough parsers suggest that a particular constituent belongs in the parse  we include it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'50'", "'15'", "'106'", "'22'"]
'101'
'50'
'15'
'106'
'22'
['101', '50', '15', '106', '22']
Error in Discourse Facet
<S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S><S sid ="111" ssid = "40">The first row represents the average accuracy of the three parsers we combine.</S><S sid ="30" ssid = "16">This is equivalent to the assumption used in probability estimation for nave Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.</S><S sid ="28" ssid = "14">The development of a nave Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a nave Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'111'", "'30'", "'28'", "'27'"]
'50'
'111'
'30'
'28'
'27'
['50', '111', '30', '28', '27']
Error in Discourse Facet
<S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="140" ssid = "2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S><S sid ="131" ssid = "60">It was then tested on section 22 of the Treebank in conjunction with the other parsers.</S><S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="34" ssid = "20">Mi(c) is a binary function returning t when parser i (from among the k parsers) suggests constituent c should be in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'140'", "'131'", "'112'", "'34'"]
'61'
'140'
'131'
'112'
'34'
['61', '140', '131', '112', '34']
Error in Discourse Facet
<S sid ="87" ssid = "16">It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S><S sid ="37" ssid = "23">Here NO counts the number of hypothesized constituents in the development set that match the binary predicate specified as an argument.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="100" ssid = "29">When this metric is less than 0.5  we expect to incur more errors' than we will remove by adding those constituents to the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'", "'57'", "'37'", "'11'", "'100'"]
'87'
'57'
'37'
'11'
'100'
['87', '57', '37', '11', '100']
Error in Discourse Facet
<S sid ="83" ssid = "12">We performed three experiments to evaluate our techniques.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="108" ssid = "37">From this we see that a finer-grained model for parser combination  at least for the features we have examined  will not give us any additional power.</S><S sid ="107" ssid = "36">Again we notice that the isolated constituent precision is larger than 0.5 only in those partitions that contain very few samples.</S><S sid ="118" ssid = "47">The maximum precision oracle is an upper bound on the possible gain we can achieve by parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'11'", "'108'", "'107'", "'118'"]
'83'
'11'
'108'
'107'
'118'
['83', '11', '108', '107', '118']
Error in Discourse Facet
<S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="30" ssid = "16">This is equivalent to the assumption used in probability estimation for nave Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="113" ssid = "42">The next two rows are results of oracle experiments.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'30'", "'52'", "'113'", "'50'"]
'18'
'30'
'52'
'113'
'50'
['18', '30', '52', '113', '50']
Error in Discourse Facet
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



W99-0623
P01-1005
0
method_citation
parsing: input/ref/Task1/D09-1092_swastika.csv
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="114" ssid="63">Ideally, the &#8220;glue&#8221; documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.</S>
original cit marker offset is 0
new cit marker offset is 0



['114']
114
['114']
parsed_discourse_facet ['aim_citation']
<S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



['138']
138
['138']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



['55']
55
['55']
parsed_discourse_facet ['method_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['192']
192
['192']
parsed_discourse_facet ['result_citation']
<S sid="119" ssid="68">The lower the divergence, the more similar the distributions are to each other.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['result_citation']
<S sid="148" ssid="97">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="163" ssid="112">Performance continues to improve with longer documents, most likely due to better topic inference.</S>
original cit marker offset is 0
new cit marker offset is 0



['163']
163
['163']
parsed_discourse_facet ['result_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="148" ssid="97">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['192']
192
['192']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['method_citation']
    <S sid="10" ssid="6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="176" ssid = "10">We dropped all articles in non-English languages that did not link to an English article.</S><S sid ="145" ssid = "94">For T = 800  the top English and Spanish words in 448 topics were exact translations of one another.</S><S sid ="155" ssid = "104">Finally  for each pair of languages (query and target) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'18'", "'176'", "'145'", "'155'"]
'147'
'18'
'176'
'145'
'155'
['147', '18', '176', '145', '155']
parsed_discourse_facet ['implication_citation']
<S sid ="63" ssid = "12">Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.</S><S sid ="156" ssid = "105">We use both Jensen-Shannon divergence and cosine distance.</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="64" ssid = "13">The first topic contains words relating to the European Central Bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'156'", "'32'", "'23'", "'64'"]
'63'
'156'
'32'
'23'
'64'
['63', '156', '32', '23', '64']
parsed_discourse_facet ['implication_citation']
<S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S><S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="182" ssid = "16">As with EuroParl  we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.</S><S sid ="190" ssid = "24">Meanwhile  interest in actors and actresses (center) is consistent across all languages.</S><S sid ="104" ssid = "53">This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'30'", "'182'", "'190'", "'104'"]
'38'
'30'
'182'
'190'
'104'
['38', '30', '182', '190', '104']
parsed_discourse_facet ['results_citation']
<S sid ="164" ssid = "113">Results vary by language.</S><S sid ="160" ssid = "109">It is important to note that the length of documents matters.</S><S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of objective. The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S><S sid ="104" ssid = "53">This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.</S><S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of 1  .</S>
original cit marker offset is 0
new cit marker offset is 0



["'164'", "'160'", "'69'", "'104'", "'43'"]
'164'
'160'
'69'
'104'
'43'
['164', '160', '69', '104', '43']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S><S sid ="172" ssid = "6">Second  because comparable texts may not use exactly the same topics  it becomes crucially important to be able to characterize differences in topic prevalence at the document level (do different languages have different perspectives on the same article?) and at the language-wide level (which topics do particular languages focus on?).</S><S sid ="63" ssid = "12">Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'23'", "'38'", "'172'", "'63'"]
'32'
'23'
'38'
'172'
'63'
['32', '23', '38', '172', '63']
parsed_discourse_facet ['method_citation']
<S sid ="135" ssid = "84">We do not consider multi-word terms.</S><S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of objective. The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S><S sid ="175" ssid = "9">We preprocessed the data by removing tables  references  images and info-boxes.</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="176" ssid = "10">We dropped all articles in non-English languages that did not link to an English article.</S>
original cit marker offset is 0
new cit marker offset is 0



["'135'", "'69'", "'175'", "'32'", "'176'"]
'135'
'69'
'175'
'32'
'176'
['135', '69', '175', '32', '176']
parsed_discourse_facet ['method_citation']
<S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="97" ssid = "46">Rather  these results are intended as a quantitative analysis of the difference between the two models.</S><S sid ="40" ssid = "6">Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter  and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl  P(wl  |zl l) = 11n lwl |zl .</S><S sid ="21" ssid = "17">The second corpus  Wikipedia articles in twelve languages  contains sets of documents that are not translations of one another  but are very likely to be about similar concepts.</S><S sid ="75" ssid = "24">We compute histograms of these maximum topic probabilities for T  {50 100  200  400  800}.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'97'", "'40'", "'21'", "'75'"]
'147'
'97'
'40'
'21'
'75'
['147', '97', '40', '21', '75']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="184" ssid = "18">Interestingly  we find that almost all languages in our corpus  including several pairs that have historically been in conflict  show average JS divergences of between approximately 0.08 and 0.12 for T = 400  consistent with our findings for EuroParl translations.</S><S sid ="87" ssid = "36">The probability of previously unseen held-out document tuples given these estimates can then be computed.</S><S sid ="145" ssid = "94">For T = 800  the top English and Spanish words in 448 topics were exact translations of one another.</S><S sid ="178" ssid = "12">For efficiency  we truncated each article to the nearest word after 1000 characters and dropped the 50 most common word types in each language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'184'", "'87'", "'145'", "'178'"]
'32'
'184'
'87'
'145'
'178'
['32', '184', '87', '145', '178']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="48" ssid = "14">.</S><S sid ="124" ssid = "73">At p = 0.01 (1% glue documents)  German and French both include words relating to Russia  while the English and Italian word distributions appear locally consistent but unrelated to Russia.</S><S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'48'", "'124'", "'1'", "'38'"]
'30'
'48'
'124'
'1'
'38'
['30', '48', '124', '1', '38']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of objective. The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="9" ssid = "5">In this paper  we present the polylingual topic model (PLTM).</S><S sid ="40" ssid = "6">Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter  and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl  P(wl  |zl l) = 11n lwl |zl .</S><S sid ="186" ssid = "20">Although we find that if Wikipedia contains an article on a particular subject in some language  the article will tend to be topically similar to the articles about that subject in other languages  we also find that across the whole collection different languages emphasize topics to different extents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'23'", "'9'", "'40'", "'186'"]
'69'
'23'
'9'
'40'
'186'
['69', '23', '9', '40', '186']
parsed_discourse_facet ['implication_citation']
<S sid ="46" ssid = "12">  L and m from P(1  ...   L  m  |W'  ) or by evaluating a point estimate.</S><S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="72" ssid = "21">To answer this question  we compute the posterior probability of each topic in each tuple under the trained model.</S><S sid ="28" ssid = "4">We take a simpler approach that is more suitable for topically similar document tuples (where documents are not direct translations of one another) in more than two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'30'", "'32'", "'72'", "'28'"]
'46'
'30'
'32'
'72'
'28'
['46', '30', '32', '72', '28']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual synsets by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="45" ssid = "11">.</S><S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="67" ssid = "16">(Interestingly  all languages except Greek and Finnish use closely related words for youth or young in a separate topic.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'131'", "'45'", "'59'", "'67'"]
'30'
'131'
'45'
'59'
'67'
['30', '131', '45', '59', '67']
parsed_discourse_facet ['method_citation']
<S sid ="155" ssid = "104">Finally  for each pair of languages (query and target) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S><S sid ="29" ssid = "5">A recent extended abstract  developed concurrently by Ni et al. (Ni et al.  2009)  discusses a multilingual topic model similar to the one presented here.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="173" ssid = "7">We downloaded XML copies of all Wikipedia articles in twelve different languages: Welsh  German  Greek  English  Farsi  Finnish  French  Hebrew  Italian  Polish  Russian and Turkish.</S><S sid ="103" ssid = "52">PLTM topics therefore have a higher granularity  i.e.  they are more specific.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'", "'29'", "'19'", "'173'", "'103'"]
'155'
'29'
'19'
'173'
'103'
['155', '29', '19', '173', '103']
parsed_discourse_facet ['method_citation']
<S sid ="135" ssid = "84">We do not consider multi-word terms.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual synsets by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="89" ssid = "38">Analytically calculating the probability of a set of held-out document tuples given 1  ...   L and m is intractable  due to the summation over an exponential number of topic assignments for these held-out documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'135'", "'131'", "'147'", "'1'", "'89'"]
'135'
'131'
'147'
'1'
'89'
['135', '131', '147', '1', '89']
parsed_discourse_facet ['results_citation']
<S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of 1  .</S><S sid ="136" ssid = "85">We expect that simple analysis of topic assignments for sequential words would yield such collocations  but we leave this for future work.</S><S sid ="156" ssid = "105">We use both Jensen-Shannon divergence and cosine distance.</S><S sid ="45" ssid = "11">.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'136'", "'156'", "'45'", "'23'"]
'43'
'136'
'156'
'45'
'23'
['43', '136', '156', '45', '23']
parsed_discourse_facet ['implication_citation']
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual synsets by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="84" ssid = "33">As the number of topics increases  greater variability in topic distributions causes divergence to increase.</S><S sid ="63" ssid = "12">Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.</S><S sid ="175" ssid = "9">We preprocessed the data by removing tables  references  images and info-boxes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'147'", "'84'", "'63'", "'175'"]
'131'
'147'
'84'
'63'
'175'
['131', '147', '84', '63', '175']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of 1  .</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="175" ssid = "9">We preprocessed the data by removing tables  references  images and info-boxes.</S><S sid ="53" ssid = "2">In this case  we can be confident that the topic distribution is genuinely shared across all languages.</S><S sid ="193" ssid = "2">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'32'", "'175'", "'53'", "'193'"]
'43'
'32'
'175'
'53'
'193'
['43', '32', '175', '53', '193']
parsed_discourse_facet ['results_citation']
<S sid ="97" ssid = "46">Rather  these results are intended as a quantitative analysis of the difference between the two models.</S><S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="135" ssid = "84">We do not consider multi-word terms.</S><S sid ="103" ssid = "52">PLTM topics therefore have a higher granularity  i.e.  they are more specific.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'52'", "'30'", "'135'", "'103'"]
'97'
'52'
'30'
'135'
'103'
['97', '52', '30', '135', '103']
parsed_discourse_facet ['aim_citation']
<S sid ="14" ssid = "10">Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.</S><S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="190" ssid = "24">Meanwhile  interest in actors and actresses (center) is consistent across all languages.</S><S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'59'", "'190'", "'1'", "'31'"]
'14'
'59'
'190'
'1'
'31'
['14', '59', '190', '1', '31']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/D09-1092_sweta.csv
 <S sid="32" ssid="8">Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="5">Additionally, PLTM assumes that each &#8220;topic&#8221; consists of a set of discrete distributions over words&#8212;one for each language l = 1, ... , L. In other words, rather than using a single set of topics &#934; = {&#966;1, ... , &#966;T}, as in LDA, there are L sets of language-specific topics, &#934;1, ... , &#934;L, each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter &#946;l.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



["138'"]
138'
['138']
parsed_discourse_facet ['method_citation']
 <S sid="196" ssid="5">When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>
original cit marker offset is 0
new cit marker offset is 0



["196'"]
196'
['196']
parsed_discourse_facet ['method_citation']
<S sid="111" ssid="60">In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts &#8211; i.e., we put each of these documents in a single-document tuple.</S>
original cit marker offset is 0
new cit marker offset is 0



["111'"]
111'
['111']
parsed_discourse_facet ['method_citation']
<S sid="128" ssid="77">Although the PLTM is clearly not a substitute for a machine translation system&#8212;it has no way to represent syntax or even multi-word phrases&#8212;it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.</S>
original cit marker offset is 0
new cit marker offset is 0



["128'"]
128'
['128']
parsed_discourse_facet ['method_citation']
<S sid="122" ssid="71">Divergence drops significantly when the proportion of &#8220;glue&#8221; tuples increases from 0.01 to 0.25.</S>
original cit marker offset is 0
new cit marker offset is 0



["122'"]
122'
['122']
parsed_discourse_facet ['method_citation']
 <S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
 <S sid="110" ssid="59">Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S>
original cit marker offset is 0
new cit marker offset is 0



["110'"]
110'
['110']
parsed_discourse_facet ['method_citation']
<S sid="30" ssid="6">However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["30'"]
30'
['30']
parsed_discourse_facet ['method_citation']
    <S sid="146" ssid="95">In addition to enhancing lexicons by aligning topic-specific vocabulary, PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["146'"]
146'
['146']
parsed_discourse_facet ['method_citation']
<S sid="77" ssid="26">Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.</
original cit marker offset is 0
new cit marker offset is 0



["77'"]
77'
['77']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="105">We use both Jensen-Shannon divergence and cosine distance.</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="7">They also provide little analysis of the differences between polylingual and single-language topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>
original cit marker offset is 0
new cit marker offset is 0



["19'"]
19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="131" ssid="80">We evaluate sets of high-probability words in each topic and multilingual &#8220;synsets&#8221; by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["131'"]
131'
['131']
parsed_discourse_facet ['method_citation']
<S sid="196" ssid="5">When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>
original cit marker offset is 0
new cit marker offset is 0



["196'"]
196'
['196']
parsed_discourse_facet ['method_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["192'"]
192'
['192']
parsed_discourse_facet ['method_citation']
<S sid="195" ssid="4">Additionally, PLTM can support the creation of bilingual lexica for low resource language pairs, providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["195'"]
195'
['195']
parsed_discourse_facet ['method_citation']
<S sid="6" ssid="2">Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["6'"]
6'
['6']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="176" ssid = "10">We dropped all articles in non-English languages that did not link to an English article.</S><S sid ="145" ssid = "94">For T = 800  the top English and Spanish words in 448 topics were exact translations of one another.</S><S sid ="155" ssid = "104">Finally  for each pair of languages (query and target) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'18'", "'176'", "'145'", "'155'"]
'147'
'18'
'176'
'145'
'155'
['147', '18', '176', '145', '155']
parsed_discourse_facet ['implication_citation']
<S sid ="63" ssid = "12">Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.</S><S sid ="156" ssid = "105">We use both Jensen-Shannon divergence and cosine distance.</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="64" ssid = "13">The first topic contains words relating to the European Central Bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'156'", "'32'", "'23'", "'64'"]
'63'
'156'
'32'
'23'
'64'
['63', '156', '32', '23', '64']
parsed_discourse_facet ['implication_citation']
<S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S><S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="182" ssid = "16">As with EuroParl  we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.</S><S sid ="190" ssid = "24">Meanwhile  interest in actors and actresses (center) is consistent across all languages.</S><S sid ="104" ssid = "53">This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'30'", "'182'", "'190'", "'104'"]
'38'
'30'
'182'
'190'
'104'
['38', '30', '182', '190', '104']
parsed_discourse_facet ['results_citation']
<S sid ="164" ssid = "113">Results vary by language.</S><S sid ="160" ssid = "109">It is important to note that the length of documents matters.</S><S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of objective. The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S><S sid ="104" ssid = "53">This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.</S><S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of 1  .</S>
original cit marker offset is 0
new cit marker offset is 0



["'164'", "'160'", "'69'", "'104'", "'43'"]
'164'
'160'
'69'
'104'
'43'
['164', '160', '69', '104', '43']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S><S sid ="172" ssid = "6">Second  because comparable texts may not use exactly the same topics  it becomes crucially important to be able to characterize differences in topic prevalence at the document level (do different languages have different perspectives on the same article?) and at the language-wide level (which topics do particular languages focus on?).</S><S sid ="63" ssid = "12">Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'23'", "'38'", "'172'", "'63'"]
'32'
'23'
'38'
'172'
'63'
['32', '23', '38', '172', '63']
parsed_discourse_facet ['method_citation']
<S sid ="135" ssid = "84">We do not consider multi-word terms.</S><S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of objective. The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S><S sid ="175" ssid = "9">We preprocessed the data by removing tables  references  images and info-boxes.</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="176" ssid = "10">We dropped all articles in non-English languages that did not link to an English article.</S>
original cit marker offset is 0
new cit marker offset is 0



["'135'", "'69'", "'175'", "'32'", "'176'"]
'135'
'69'
'175'
'32'
'176'
['135', '69', '175', '32', '176']
parsed_discourse_facet ['method_citation']
<S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="97" ssid = "46">Rather  these results are intended as a quantitative analysis of the difference between the two models.</S><S sid ="40" ssid = "6">Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter  and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl  P(wl  |zl l) = 11n lwl |zl .</S><S sid ="21" ssid = "17">The second corpus  Wikipedia articles in twelve languages  contains sets of documents that are not translations of one another  but are very likely to be about similar concepts.</S><S sid ="75" ssid = "24">We compute histograms of these maximum topic probabilities for T  {50 100  200  400  800}.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'97'", "'40'", "'21'", "'75'"]
'147'
'97'
'40'
'21'
'75'
['147', '97', '40', '21', '75']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="184" ssid = "18">Interestingly  we find that almost all languages in our corpus  including several pairs that have historically been in conflict  show average JS divergences of between approximately 0.08 and 0.12 for T = 400  consistent with our findings for EuroParl translations.</S><S sid ="87" ssid = "36">The probability of previously unseen held-out document tuples given these estimates can then be computed.</S><S sid ="145" ssid = "94">For T = 800  the top English and Spanish words in 448 topics were exact translations of one another.</S><S sid ="178" ssid = "12">For efficiency  we truncated each article to the nearest word after 1000 characters and dropped the 50 most common word types in each language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'184'", "'87'", "'145'", "'178'"]
'32'
'184'
'87'
'145'
'178'
['32', '184', '87', '145', '178']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="48" ssid = "14">.</S><S sid ="124" ssid = "73">At p = 0.01 (1% glue documents)  German and French both include words relating to Russia  while the English and Italian word distributions appear locally consistent but unrelated to Russia.</S><S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'48'", "'124'", "'1'", "'38'"]
'30'
'48'
'124'
'1'
'38'
['30', '48', '124', '1', '38']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of objective. The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="9" ssid = "5">In this paper  we present the polylingual topic model (PLTM).</S><S sid ="40" ssid = "6">Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter  and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl  P(wl  |zl l) = 11n lwl |zl .</S><S sid ="186" ssid = "20">Although we find that if Wikipedia contains an article on a particular subject in some language  the article will tend to be topically similar to the articles about that subject in other languages  we also find that across the whole collection different languages emphasize topics to different extents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'23'", "'9'", "'40'", "'186'"]
'69'
'23'
'9'
'40'
'186'
['69', '23', '9', '40', '186']
parsed_discourse_facet ['implication_citation']
<S sid ="46" ssid = "12">  L and m from P(1  ...   L  m  |W'  ) or by evaluating a point estimate.</S><S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="72" ssid = "21">To answer this question  we compute the posterior probability of each topic in each tuple under the trained model.</S><S sid ="28" ssid = "4">We take a simpler approach that is more suitable for topically similar document tuples (where documents are not direct translations of one another) in more than two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'30'", "'32'", "'72'", "'28'"]
'46'
'30'
'32'
'72'
'28'
['46', '30', '32', '72', '28']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual synsets by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="45" ssid = "11">.</S><S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="67" ssid = "16">(Interestingly  all languages except Greek and Finnish use closely related words for youth or young in a separate topic.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'131'", "'45'", "'59'", "'67'"]
'30'
'131'
'45'
'59'
'67'
['30', '131', '45', '59', '67']
parsed_discourse_facet ['method_citation']
<S sid ="155" ssid = "104">Finally  for each pair of languages (query and target) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S><S sid ="29" ssid = "5">A recent extended abstract  developed concurrently by Ni et al. (Ni et al.  2009)  discusses a multilingual topic model similar to the one presented here.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="173" ssid = "7">We downloaded XML copies of all Wikipedia articles in twelve different languages: Welsh  German  Greek  English  Farsi  Finnish  French  Hebrew  Italian  Polish  Russian and Turkish.</S><S sid ="103" ssid = "52">PLTM topics therefore have a higher granularity  i.e.  they are more specific.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'", "'29'", "'19'", "'173'", "'103'"]
'155'
'29'
'19'
'173'
'103'
['155', '29', '19', '173', '103']
parsed_discourse_facet ['method_citation']
<S sid ="135" ssid = "84">We do not consider multi-word terms.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual synsets by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="89" ssid = "38">Analytically calculating the probability of a set of held-out document tuples given 1  ...   L and m is intractable  due to the summation over an exponential number of topic assignments for these held-out documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'135'", "'131'", "'147'", "'1'", "'89'"]
'135'
'131'
'147'
'1'
'89'
['135', '131', '147', '1', '89']
parsed_discourse_facet ['results_citation']
<S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of 1  .</S><S sid ="136" ssid = "85">We expect that simple analysis of topic assignments for sequential words would yield such collocations  but we leave this for future work.</S><S sid ="156" ssid = "105">We use both Jensen-Shannon divergence and cosine distance.</S><S sid ="45" ssid = "11">.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'136'", "'156'", "'45'", "'23'"]
'43'
'136'
'156'
'45'
'23'
['43', '136', '156', '45', '23']
parsed_discourse_facet ['implication_citation']
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual synsets by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="84" ssid = "33">As the number of topics increases  greater variability in topic distributions causes divergence to increase.</S><S sid ="63" ssid = "12">Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.</S><S sid ="175" ssid = "9">We preprocessed the data by removing tables  references  images and info-boxes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'147'", "'84'", "'63'", "'175'"]
'131'
'147'
'84'
'63'
'175'
['131', '147', '84', '63', '175']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of 1  .</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="175" ssid = "9">We preprocessed the data by removing tables  references  images and info-boxes.</S><S sid ="53" ssid = "2">In this case  we can be confident that the topic distribution is genuinely shared across all languages.</S><S sid ="193" ssid = "2">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'32'", "'175'", "'53'", "'193'"]
'43'
'32'
'175'
'53'
'193'
['43', '32', '175', '53', '193']
parsed_discourse_facet ['results_citation']
<S sid ="97" ssid = "46">Rather  these results are intended as a quantitative analysis of the difference between the two models.</S><S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="135" ssid = "84">We do not consider multi-word terms.</S><S sid ="103" ssid = "52">PLTM topics therefore have a higher granularity  i.e.  they are more specific.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'52'", "'30'", "'135'", "'103'"]
'97'
'52'
'30'
'135'
'103'
['97', '52', '30', '135', '103']
parsed_discourse_facet ['aim_citation']
<S sid ="14" ssid = "10">Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.</S><S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="190" ssid = "24">Meanwhile  interest in actors and actresses (center) is consistent across all languages.</S><S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'59'", "'190'", "'1'", "'31'"]
'14'
'59'
'190'
'1'
'31'
['14', '59', '190', '1', '31']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/W99-0613_vardha.csv
    <S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="35" ssid="29">AdaBoost finds a weighted combination of simple (weak) classifiers, where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
    <S sid="134" ssid="1">This section describes an algorithm based on boosting algorithms, which were previously developed for supervised machine learning problems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'"]
'134'
['134']
parsed_discourse_facet ['method_citation']
    <S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'"]
'236'
['236']
parsed_discourse_facet ['method_citation']
    <S sid="8" ssid="2">Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
    <S sid="42" ssid="36">(Riloff and Shepherd 97) describe a bootstrapping approach for acquiring nouns in particular categories (such as &amp;quot;vehicle&amp;quot; or &amp;quot;weapon&amp;quot; categories).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
    <S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'"]
'236'
['236']
parsed_discourse_facet ['method_citation']
    <S sid="222" ssid="1">The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'222'"]
'222'
['222']
parsed_discourse_facet ['method_citation']
    <S sid="30" ssid="24">(Blum and Mitchell 98) offer a promising formulation of redundancy, also prove some results about how the use of can help classification, and suggest an objective function when training with unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'"]
'30'
['30']
parsed_discourse_facet ['method_citation']
 <S sid="26" ssid="20">We present two algorithms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'"]
'26'
['26']
parsed_discourse_facet ['method_citation']
    <S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="26">The algorithm can be viewed as heuristically optimizing an objective function suggested by (Blum and Mitchell 98); empirically it is shown to be quite successful in optimizing this criterion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'"]
'32'
['32']
parsed_discourse_facet ['method_citation']
    <S sid="47" ssid="1">971,746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'"]
'47'
['47']
parsed_discourse_facet ['method_citation']
    <S sid="127" ssid="60">The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'"]
'127'
['127']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="125" ssid = "58">It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'178'", "'125'", "'41'"]
'256'
'198'
'178'
'125'
'41'
['256', '198', '178', '125', '41']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'97'", "'70'", "'198'"]
'256'
'178'
'97'
'70'
'198'
['256', '178', '97', '70', '198']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="128" ssid = "61">At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'128'", "'198'", "'178'", "'70'"]
'256'
'128'
'198'
'178'
'70'
['256', '128', '198', '178', '70']
parsed_discourse_facet ['results_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'97'", "'70'", "'198'", "'178'"]
'256'
'97'
'70'
'198'
'178'
['256', '97', '70', '198', '178']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="70" ssid = "3">.</S><S sid ="220" ssid = "87">(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'198'", "'70'", "'220'"]
'256'
'178'
'198'
'70'
'220'
['256', '178', '198', '70', '220']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'178'", "'70'", "'236'"]
'256'
'198'
'178'
'70'
'236'
['256', '198', '178', '70', '236']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="70" ssid = "3">.</S><S sid ="128" ssid = "61">At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'70'", "'128'", "'97'"]
'256'
'198'
'70'
'128'
'97'
['256', '198', '70', '128', '97']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S><S sid ="220" ssid = "87">(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'70'", "'220'", "'198'"]
'256'
'178'
'70'
'220'
'198'
['256', '178', '70', '220', '198']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'236'", "'41'", "'97'"]
'256'
'198'
'236'
'41'
'97'
['256', '198', '236', '41', '97']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="70" ssid = "3">.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'97'", "'236'", "'70'"]
'256'
'178'
'97'
'236'
'70'
['256', '178', '97', '236', '70']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="125" ssid = "58">It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'198'", "'125'", "'41'"]
'256'
'178'
'198'
'125'
'41'
['256', '178', '198', '125', '41']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'97'", "'178'", "'198'", "'236'"]
'256'
'97'
'178'
'198'
'236'
['256', '97', '178', '198', '236']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'70'", "'198'", "'178'", "'236'"]
'256'
'70'
'198'
'178'
'236'
['256', '70', '198', '178', '236']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W99-0613_sweta.csv
<S sid="121" ssid="54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page, and other pages pointing to the page).</S>
original cit marker offset is 0
new cit marker offset is 0



["121'"]
121'
['121']
parsed_discourse_facet ['method_citation']
<S sid="252" ssid="3">The method uses a &amp;quot;soft&amp;quot; measure of the agreement between two classifiers as an objective function; we described an algorithm which directly optimizes this function.</S>
original cit marker offset is 0
new cit marker offset is 0



["252'"]
252'
['252']
parsed_discourse_facet ['method_citation']
 <S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



["213'"]
213'
['213']
parsed_discourse_facet ['method_citation']
 <S sid="250" ssid="1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S>
original cit marker offset is 0
new cit marker offset is 0



["250'"]
250'
['250']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="33">(Brin 98) ,describes a system for extracting (author, book-title) pairs from the World Wide Web using an approach that bootstraps from an initial seed set of examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="202" ssid="69">The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels, and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.</S>
original cit marker offset is 0
new cit marker offset is 0



["202'"]
202'
['202']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="15">The following features were used: full-string=x The full string (e.g., for Maury Cooper, full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word, this feature applies for any words that the string contains (e.g., Maury Cooper contributes two such features, contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g., IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods, and contains at least one period.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="176" ssid="43">(7) is at 0 when: 1) Vi : sign(gi (xi)) = sign(g2 (xi)); 2) Ig3(xi)l oo; and 3) sign(gi (xi)) = yi for i = 1, , m. In fact, Zco provides a bound on the sum of the classification error of the labeled examples and the number of disagreements between the two classifiers on the unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["176'"]
176'
['176']
parsed_discourse_facet ['method_citation']
 <S sid="108" ssid="41">In the cotraining case, (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples, and (2) must agree with each other on the unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S>
    <S sid="28" ssid="22">(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'", "'28'"]
27'
'28'
['27', '28']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["7'"]
7'
['7']
parsed_discourse_facet ['method_citation']
 <S sid="172" ssid="39">To see this, note thai the first two terms in the above equation correspond to the function that AdaBoost attempts to minimize in the standard supervised setting (Equ.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["85'"]
85'
['85']
parsed_discourse_facet ['method_citation']
 <S sid="214" ssid="81">This modification brings the method closer to the DL-CoTrain algorithm described earlier, and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples, preventing one label from dominating &#8212; this deserves more theoretical investigation.</S>
original cit marker offset is 0
new cit marker offset is 0



["214'"]
214'
['214']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="125" ssid = "58">It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'178'", "'125'", "'41'"]
'256'
'198'
'178'
'125'
'41'
['256', '198', '178', '125', '41']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'97'", "'70'", "'198'"]
'256'
'178'
'97'
'70'
'198'
['256', '178', '97', '70', '198']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="128" ssid = "61">At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'128'", "'198'", "'178'", "'70'"]
'256'
'128'
'198'
'178'
'70'
['256', '128', '198', '178', '70']
parsed_discourse_facet ['results_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'97'", "'70'", "'198'", "'178'"]
'256'
'97'
'70'
'198'
'178'
['256', '97', '70', '198', '178']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="70" ssid = "3">.</S><S sid ="220" ssid = "87">(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'198'", "'70'", "'220'"]
'256'
'178'
'198'
'70'
'220'
['256', '178', '198', '70', '220']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'178'", "'70'", "'236'"]
'256'
'198'
'178'
'70'
'236'
['256', '198', '178', '70', '236']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="70" ssid = "3">.</S><S sid ="128" ssid = "61">At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'70'", "'128'", "'97'"]
'256'
'198'
'70'
'128'
'97'
['256', '198', '70', '128', '97']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S><S sid ="220" ssid = "87">(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'70'", "'220'", "'198'"]
'256'
'178'
'70'
'220'
'198'
['256', '178', '70', '220', '198']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'236'", "'41'", "'97'"]
'256'
'198'
'236'
'41'
'97'
['256', '198', '236', '41', '97']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="70" ssid = "3">.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'97'", "'236'", "'70'"]
'256'
'178'
'97'
'236'
'70'
['256', '178', '97', '236', '70']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="125" ssid = "58">It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'198'", "'125'", "'41'"]
'256'
'178'
'198'
'125'
'41'
['256', '178', '198', '125', '41']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'97'", "'178'", "'198'", "'236'"]
'256'
'97'
'178'
'198'
'236'
['256', '97', '178', '198', '236']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'70'", "'198'", "'178'", "'236'"]
'256'
'70'
'198'
'178'
'236'
['256', '70', '198', '178', '236']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
parsing: input/ref/Task1/D10-1044_aakansha.csv
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S>
    <S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'96'"]
'95'
'96'
['95', '96']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'"]
'75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="28">For comparison to information-retrieval inspired baselines, eg (L&#168;u et al., 2007), we select sentences from OUT using language model perplexities from IN.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'"]
'22'
['22']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="23">The 2nd block contains the IR system, which was tuned by selecting text in multiples of the size of the EMEA training corpus, according to dev set performance.</S>
    <S sid="120" ssid="24">This significantly underperforms log-linear combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'", "'120'"]
'119'
'120'
['119', '120']
parsed_discourse_facet ['result_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
    <S sid="24" ssid="21">Sentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="4">We focus here instead on adapting the two most important features: the language model (LM), which estimates the probability p(wIh) of a target word w following an ngram h; and the translation models (TM) p(slt) and p(t1s), which give the probability of source phrase s translating to target phrase t, and vice versa.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D10-1044.csv
<S sid ="32" ssid = "29">This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.</S><S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="101" ssid = "5">The second setting uses the news-related subcorpora for the NIST09 MT Chinese to English evaluation8 as IN  and the remaining NIST parallel Chinese/English corpora (UN  Hong Kong Laws  and Hong Kong Hansard) as OUT.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'2'", "'114'", "'101'", "'104'"]
'32'
'2'
'114'
'101'
'104'
['32', '2', '114', '101', '104']
parsed_discourse_facet ['implication_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="138" ssid = "7">However  for multinomial models like our LMs and TMs  there is a one to one correspondence between instances and features  eg the correspondence between a phrase pair (s  t) and its conditional multinomial probability p(s1t).</S><S sid ="144" ssid = "1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S><S sid ="133" ssid = "2">It is difficult to directly compare the Matsoukas et al results with ours  since our out-of-domain corpus is homogeneous; given heterogeneous training data  however  it would be trivial to include Matsoukas-style identity features in our instance-weighting model.</S><S sid ="99" ssid = "3">The dev and test sets were randomly chosen from the EMEA corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'138'", "'144'", "'133'", "'99'"]
'67'
'138'
'144'
'133'
'99'
['67', '138', '144', '133', '99']
parsed_discourse_facet ['implication_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="142" ssid = "11">There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).</S><S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'142'", "'8'", "'2'", "'38'"]
'67'
'142'
'8'
'2'
'38'
['67', '142', '8', '2', '38']
parsed_discourse_facet ['results_citation']
<S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="32" ssid = "29">This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.</S><S sid ="83" ssid = "20">We have not yet tried this.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daumes (2007) work that splits each feature into domain-specific and general copies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'32'", "'83'", "'78'", "'136'"]
'8'
'32'
'83'
'78'
'136'
['8', '32', '83', '78', '136']
parsed_discourse_facet ['method_citation']
<S sid ="133" ssid = "2">It is difficult to directly compare the Matsoukas et al results with ours  since our out-of-domain corpus is homogeneous; given heterogeneous training data  however  it would be trivial to include Matsoukas-style identity features in our instance-weighting model.</S><S sid ="118" ssid = "22">Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.</S><S sid ="5" ssid = "2">Even when there is training data available in the domain of interest  there is often additional data from other domains that could in principle be used to improve performance.</S><S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'118'", "'5'", "'1'", "'10'"]
'133'
'118'
'5'
'1'
'10'
['133', '118', '5', '1', '10']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="5" ssid = "2">Even when there is training data available in the domain of interest  there is often additional data from other domains that could in principle be used to improve performance.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daumes (2007) work that splits each feature into domain-specific and general copies.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S><S sid ="14" ssid = "11">There is a fairly large body of work on SMT adaptation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'5'", "'136'", "'104'", "'14'"]
'10'
'5'
'136'
'104'
'14'
['10', '5', '136', '104', '14']
parsed_discourse_facet ['method_citation']
<S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="95" ssid = "32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="125" ssid = "29">Somewhat surprisingly  there do not appear to be large systematic differences between linear and MAP combinations.</S><S sid ="20" ssid = "17">Daume (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'", "'95'", "'78'", "'125'", "'20'"]
'64'
'95'
'78'
'125'
'20'
['64', '95', '78', '125', '20']
parsed_discourse_facet ['method_citation']
<S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="36" ssid = "33">Section 5 covers relevant previous work on SMT adaptation  and section 6 concludes.</S><S sid ="19" ssid = "16">The idea of distinguishing between general and domain-specific examples is due to Daume and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.</S><S sid ="79" ssid = "16">This combination generalizes (2) and (3): we use either at = a to obtain a fixed-weight linear combination  or at = cI(t)/(cI(t) + 0) to obtain a MAP combination.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'36'", "'19'", "'79'", "'114'"]
'151'
'36'
'19'
'79'
'114'
['151', '36', '19', '79', '114']
parsed_discourse_facet ['method_citation']
<S sid ="118" ssid = "22">Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.</S><S sid ="50" ssid = "14">Linear weights are difficult to incorporate into the standard MERT procedure because they are hidden within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.</S><S sid ="88" ssid = "25">We have not explored this strategy.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S><S sid ="19" ssid = "16">The idea of distinguishing between general and domain-specific examples is due to Daume and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'", "'50'", "'88'", "'104'", "'19'"]
'118'
'50'
'88'
'104'
'19'
['118', '50', '88', '104', '19']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "15">For the LM  adaptive weights are set as follows: where  is a weight vector containing an element i for each domain (just IN and OUT in our case)  pi are the corresponding domain-specific models  and p(w  h) is an empirical distribution from a targetlanguage training corpuswe used the IN dev set for this.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the flattened variant described in section 3.2.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="118" ssid = "22">Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'114'", "'127'", "'67'", "'118'"]
'51'
'114'
'127'
'67'
'118'
['51', '114', '127', '67', '118']
parsed_discourse_facet ['implication_citation']
<S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="96" ssid = "33">We used it to score all phrase pairs in the OUT table  in order to provide a feature for the instance-weighting model.</S><S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="106" ssid = "10">The corpora for both settings are summarized in table 1.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'96'", "'128'", "'106'", "'111'"]
'1'
'96'
'128'
'106'
'111'
['1', '96', '128', '106', '111']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "7">Its success depends on the two domains being relatively close  and on the OUT corpus not being so large as to overwhelm the contribution of IN.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="41" ssid = "5">We do not adapt the alignment procedure for generating the phrase table from which the TM distributions are derived.</S><S sid ="60" ssid = "24">The matching sentence pairs are then added to the IN corpus  and the system is re-trained.</S><S sid ="0" ssid = "0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'64'", "'41'", "'60'", "'0'"]
'43'
'64'
'41'
'60'
'0'
['43', '64', '41', '60', '0']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="15" ssid = "12">We introduce several new ideas.</S><S sid ="71" ssid = "8">Finally  we incorporate the instance-weighting model into a general linear combination  and learn weights and mixing parameters simultaneously. where c(s  t) is a modified count for pair (s  t) in OUT  u(s|t) is a prior distribution  and y is a prior weight.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'67'", "'15'", "'71'", "'104'"]
'8'
'67'
'15'
'71'
'104'
['8', '67', '15', '71', '104']
parsed_discourse_facet ['method_citation']
<S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S><S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="43" ssid = "7">Its success depends on the two domains being relatively close  and on the OUT corpus not being so large as to overwhelm the contribution of IN.</S><S sid ="135" ssid = "4">Finally  we note that Jiangs instance-weighting framework is broader than we have presented above  encompassing among other possibilities the use of unlabelled IN data  which is applicable to SMT settings where source-only IN corpora are available.</S><S sid ="146" ssid = "3">The features are weighted within a logistic model to give an overall weight that is applied to the phrase pairs frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'", "'151'", "'43'", "'135'", "'146'"]
'104'
'151'
'43'
'135'
'146'
['104', '151', '43', '135', '146']
parsed_discourse_facet ['results_citation']
<S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="83" ssid = "20">We have not yet tried this.</S><S sid ="28" ssid = "25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'114'", "'83'", "'28'", "'17'"]
'16'
'114'
'83'
'28'
'17'
['16', '114', '83', '28', '17']
parsed_discourse_facet ['implication_citation']
<S sid ="89" ssid = "26">We used 22 features for the logistic weighting model  divided into two groups: one intended to reflect the degree to which a phrase pair belongs to general language  and one intended to capture similarity to the IN domain.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="110" ssid = "14">Je voudrais preciser  a` ladresse du commissaire Liikanen  quil nest pas aise de recourir aux tribunaux nationaux.</S><S sid ="109" ssid = "13"> I would also like to point out to commissioner Liikanen that it is not easy to take a matter to a national court.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'78'", "'110'", "'109'", "'67'"]
'89'
'78'
'110'
'109'
'67'
['89', '78', '110', '109', '67']
parsed_discourse_facet ['method_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="36" ssid = "33">Section 5 covers relevant previous work on SMT adaptation  and section 6 concludes.</S><S sid ="131" ssid = "35">The general-language features have a slight advantage over the similarity features  and both are better than the SVM feature.</S><S sid ="47" ssid = "11">Apart from MERT difficulties  a conceptual problem with log-linear combination is that it multiplies feature probabilities  essentially forcing different features to agree on high-scoring candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'64'", "'36'", "'131'", "'47'"]
'67'
'64'
'36'
'131'
'47'
['67', '64', '36', '131', '47']
parsed_discourse_facet ['results_citation']
<S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daumes (2007) work that splits each feature into domain-specific and general copies.</S><S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'136'", "'8'", "'11'", "'111'"]
'114'
'136'
'8'
'11'
'111'
['114', '136', '8', '11', '111']
parsed_discourse_facet ['aim_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S><S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="92" ssid = "29">6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.</S><S sid ="95" ssid = "32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'111'", "'128'", "'92'", "'95'"]
'67'
'111'
'128'
'92'
'95'
['67', '111', '128', '92', '95']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P11-1061_aakansha.csv
<S sid="40" ssid="6">We extend Subramanya et al.&#8217;s intuitions to our bilingual setup.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="11">First, we use a novel graph-based framework for projecting syntactic information across language boundaries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="29" ssid="6">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'"]
'158'
['158']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'"]
'158'
['158']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="1">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="17" ssid="13">Second, we treat the projected labels as features in an unsupervised model (&#167;5), rather than using them directly for supervised training.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="111" ssid="11">We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).</S>
original cit marker offset is 0
new cit marker offset is 0



["'111'"]
'111'
['111']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="29" ssid="6">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'161'"]
'161'
['161']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1061.csv
<S sid ="14" ssid = "10">Our work is closest to that of Yarowsky and Ngai (2001)  but differs in two important ways.</S><S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S><S sid ="103" ssid = "3">The availability of these resources guided our selection of foreign languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'7'", "'56'", "'135'", "'103'"]
'14'
'7'
'56'
'135'
'103'
['14', '7', '56', '135', '103']
parsed_discourse_facet ['implication_citation']
<S sid ="75" ssid = "6">We use a squared loss to penalize neighboring vertices that have different label distributions: kqi  qjk2 = Ey(qi(y)  qj(y))2  and additionally regularize the label distributions towards the uniform distribution U over all possible labels Y.</S><S sid ="53" ssid = "19">Finally  note that while most feature concepts are lexicalized  others  such as the suffix concept  are not.</S><S sid ="34" ssid = "11">The following three sections elaborate these different stages is more detail.</S><S sid ="129" ssid = "29">Fortunately  performance was stable across various values  and we were able to use the same hyperparameters for all languages.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'53'", "'34'", "'129'", "'26'"]
'75'
'53'
'34'
'129'
'26'
['75', '53', '34', '129', '26']
parsed_discourse_facet ['implication_citation']
<S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="59" ssid = "25">So far the graph has been completely unlabeled.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'107'", "'59'", "'56'", "'39'"]
'1'
'107'
'59'
'56'
'39'
['1', '107', '59', '56', '39']
parsed_discourse_facet ['results_citation']
<S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : We describe how we choose  in 6.4.</S><S sid ="66" ssid = "32">In general  the neighborhoods can be more diverse and we allow a soft label distribution over the vertices.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'66'", "'107'", "'117'", "'135'"]
'83'
'66'
'107'
'117'
'135'
['83', '66', '107', '117', '135']
parsed_discourse_facet ['method_citation']
<S sid ="104" ssid = "4">For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi  2006; Nivre et al.  2007).</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="155" ssid = "18">Examining the word fidanzato for the No LP and With LP models is particularly instructive.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="98" ssid = "29">7).</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'", "'1'", "'155'", "'39'", "'98'"]
'104'
'1'
'155'
'39'
'98'
['104', '1', '155', '39', '98']
parsed_discourse_facet ['method_citation']
<S sid ="143" ssid = "6">Overall  it gives improvements ranging from 1.1% for German to 14.7% for Italian  for an average improvement of 8.3% over the unsupervised feature-HMM model.</S><S sid ="96" ssid = "27">The function A : F * C maps from the language specific fine-grained tagset F to the coarser universal tagset C and is described in detail in 6.2: Note that when tx(y) = 1 the feature value is 0 and has no effect on the model  while its value is oc when tx(y) = 0 and constrains the HMMs state space.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="68" ssid = "34">In the label propagation stage  we propagate the automatic English tags to the aligned Italian trigram types  followed by further propagation solely among the Italian vertices. the Italian vertices are connected to an automatically labeled English vertex.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'96'", "'97'", "'68'", "'26'"]
'143'
'96'
'97'
'68'
'26'
['143', '96', '97', '68', '26']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="129" ssid = "29">Fortunately  performance was stable across various values  and we were able to use the same hyperparameters for all languages.</S><S sid ="108" ssid = "8">However  we needed to restrict ourselves to these languages in order to be able to evaluate the performance of our approach.</S><S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'26'", "'129'", "'108'", "'37'"]
'7'
'26'
'129'
'108'
'37'
['7', '26', '129', '108', '37']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S><S sid ="122" ssid = "22">For each language  we took the same number of sentences from the bitext as there are in its treebank  and trained a supervised feature-HMM.</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="31" ssid = "8">By aggregating the POS labels of the English tokens to types  we can generate label distributions for the English vertices.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'110'", "'122'", "'19'", "'31'"]
'39'
'110'
'122'
'19'
'31'
['39', '110', '122', '19', '31']
parsed_discourse_facet ['method_citation']
<S sid ="73" ssid = "4">Note that because we extracted only high-confidence alignments  many foreign vertices will not be connected to any English vertices.</S><S sid ="24" ssid = "1">The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.</S><S sid ="150" ssid = "13">For all languages  the vocabulary sizes increase by several thousand words.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="139" ssid = "2">As expected  the vanilla HMM trained with EM performs the worst.</S>
original cit marker offset is 0
new cit marker offset is 0



["'73'", "'24'", "'150'", "'39'", "'139'"]
'73'
'24'
'150'
'39'
'139'
['73', '24', '150', '39', '139']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="45" ssid = "11">Furthermore  we do not connect the English vertices to each other  but only to foreign language vertices.4 The graph vertices are extracted from the different sides of a parallel corpus (De  Df) and an additional unlabeled monolingual foreign corpus Ff  which will be used later for training.</S><S sid ="69" ssid = "35">Label propagation is used to propagate these tags inwards and results in tag distributions for the middle word of each Italian trigram.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'45'", "'69'", "'135'", "'110'"]
'107'
'45'
'69'
'135'
'110'
['107', '45', '69', '135', '110']
parsed_discourse_facet ['implication_citation']
<S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : We describe how we choose  in 6.4.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'83'", "'25'", "'4'"]
'37'
'1'
'83'
'25'
'4'
['37', '1', '83', '25', '4']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="120" ssid = "20">We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="44" ssid = "10">Because all English vertices are going to be labeled  we do not need to disambiguate them by embedding them in trigrams.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'120'", "'117'", "'44'", "'26'"]
'54'
'120'
'117'
'44'
'26'
['54', '120', '117', '44', '26']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="47" ssid = "13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S><S sid ="161" ssid = "4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections  and bridge the gap between purely supervised and unsupervised POS tagging models.</S><S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'117'", "'47'", "'161'", "'7'"]
'39'
'117'
'47'
'161'
'7'
['39', '117', '47', '161', '7']
parsed_discourse_facet ['method_citation']
<S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="32" ssid = "9">Label propagation can then be used to transfer the labels to the peripheral foreign vertices (i.e. the ones adjacent to the English vertices) first  and then among all of the foreign vertices (4).</S><S sid ="86" ssid = "17">For a sentence x and a state sequence z  a first order Markov model defines a distribution: (9) where Val(X) corresponds to the entire vocabulary.</S><S sid ="132" ssid = "32">When extracting the vector t  used to compute the constraint feature from the graph  we tried three threshold values for r (see Eq.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'117'", "'32'", "'86'", "'132'"]
'37'
'117'
'32'
'86'
'132'
['37', '117', '32', '86', '132']
parsed_discourse_facet ['results_citation']
<S sid ="163" ssid = "2">We would also like to thank Amarnag Subramanya for helping us with the implementation of label propagation and Shankar Kumar for access to the parallel data.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S><S sid ="139" ssid = "2">As expected  the vanilla HMM trained with EM performs the worst.</S><S sid ="58" ssid = "24">Based on these high-confidence alignments we can extract tuples of the form [u H v]  where u is a foreign trigram type  whose middle word aligns to an English word type v. Our bilingual similarity function then sets the edge weights in proportion to these tuple counts.</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : We describe how we choose  in 6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'110'", "'139'", "'58'", "'83'"]
'163'
'110'
'139'
'58'
'83'
['163', '110', '139', '58', '83']
parsed_discourse_facet ['implication_citation']
<S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="134" ssid = "34">Because we dont have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S><S sid ="72" ssid = "3">In the first stage  we run a single step of label propagation  which transfers the label distributions from the English vertices to the connected foreign language vertices (say  Vf) at the periphery of the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'117'", "'134'", "'110'", "'72'"]
'37'
'117'
'134'
'110'
'72'
['37', '117', '134', '110', '72']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="104" ssid = "4">For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi  2006; Nivre et al.  2007).</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'39'", "'26'", "'104'", "'135'"]
'54'
'39'
'26'
'104'
'135'
['54', '39', '26', '104', '135']
parsed_discourse_facet ['results_citation']
<S sid ="33" ssid = "10">The POS distributions over the foreign trigram types are used as features to learn a better unsupervised POS tagger (5).</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S><S sid ="90" ssid = "21">All features were conjoined with the state z.</S><S sid ="96" ssid = "27">The function A : F * C maps from the language specific fine-grained tagset F to the coarser universal tagset C and is described in detail in 6.2: Note that when tx(y) = 1 the feature value is 0 and has no effect on the model  while its value is oc when tx(y) = 0 and constrains the HMMs state space.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'117'", "'4'", "'90'", "'96'"]
'33'
'117'
'4'
'90'
'96'
['33', '117', '4', '90', '96']
parsed_discourse_facet ['aim_citation']
parsing: input/ref/Task1/P05-1013_aakansha.csv
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="17">However, the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech, with a best performance of 73% UAS (Holan, 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="86" ssid="13">As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'"]
'86'
['86']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="7">As shown in Table 3, the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.</S>
    <S sid="81"  ssid="8">However, the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'81'"]
'80'
'81'
['80', '81']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'"]
'49'
['49']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="10">While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P05-1013.csv
<S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S><S sid ="53" ssid = "24">In the third and final scheme  denoted Path  we keep the extra infor2Note that this is a baseline for the parsing experiment only (Experiment 2).</S><S sid ="37" ssid = "8">Here we use a slightly different notion of lift  applying to individual arcs and moving their head upwards one step at a time: Intuitively  lifting an arc makes the word wk dependent on the head wi of its original head wj (which is unique in a well-formed dependency graph)  unless wj is a root in which case the operation is undefined (but then wj * wk is necessarily projective if the dependency graph is well-formed).</S><S sid ="83" ssid = "10">It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'53'", "'37'", "'83'", "'75'"]
'51'
'53'
'37'
'83'
'75'
['51', '53', '37', '83', '75']
parsed_discourse_facet ['implication_citation']
<S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S><S sid ="38" ssid = "9">Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'38'", "'78'", "'34'", "'51'"]
'75'
'38'
'78'
'34'
'51'
['75', '38', '78', '34', '51']
parsed_discourse_facet ['implication_citation']
<S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S><S sid ="23" ssid = "19">By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S><S sid ="110" ssid = "2">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="25" ssid = "21">The rest of the paper is structured as follows.</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'70'", "'23'", "'110'", "'25'", "'11'"]
'70'
'23'
'110'
'25'
'11'
['70', '23', '110', '25', '11']
parsed_discourse_facet ['results_citation']
<S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="77" ssid = "4">The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'24'", "'95'", "'77'", "'11'"]
'34'
'24'
'95'
'77'
'11'
['34', '24', '95', '77', '11']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "14">Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.</S><S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="18" ssid = "14">In addition  there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington  1990; Kahane et al.  1998; Duchier and Debusmann  2001; Holan et al.  2001; Hellwig  2003).</S><S sid ="103" ssid = "14">On the other hand  given that all schemes have similar parsing accuracy overall  this means that the Path scheme is the least likely to introduce errors on projective arcs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'65'", "'14'", "'18'", "'103'"]
'43'
'65'
'14'
'18'
'103'
['43', '65', '14', '18', '103']
parsed_discourse_facet ['method_citation']
<S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S><S sid ="42" ssid = "13">Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="55" ssid = "26">As can be seen from the last column in Table 1  both Head and Head+Path may theoretically lead to a quadratic increase in the number of distinct arc labels (Head+Path being worse than Head only by a constant factor)  while the increase is only linear in the case of Path.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'42'", "'14'", "'55'", "'104'"]
'75'
'42'
'14'
'55'
'104'
['75', '42', '14', '55', '104']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="107" ssid = "18">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="66" ssid = "5">The choice between different actions is in general nondeterministic  and the parser relies on a memorybased classifier  trained on treebank data  to predict the next action based on features of the current parser configuration.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'107'", "'14'", "'95'", "'66'"]
'11'
'107'
'14'
'95'
'66'
['11', '107', '14', '95', '66']
parsed_discourse_facet ['method_citation']
<S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S><S sid ="12" ssid = "8">Prague Dependency Treebank (Hajic et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'70'", "'81'", "'12'", "'78'", "'89'"]
'70'
'81'
'12'
'78'
'89'
['70', '81', '12', '78', '89']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Prague Dependency Treebank (Hajic et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S><S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="5" ssid = "1">It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order  where discontinuous syntactic constructions are more common than in languages like English (Melcuk  1988; Covington  1990).</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S><S sid ="66" ssid = "5">The choice between different actions is in general nondeterministic  and the parser relies on a memorybased classifier  trained on treebank data  to predict the next action based on features of the current parser configuration.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'65'", "'5'", "'71'", "'66'"]
'12'
'65'
'5'
'71'
'66'
['12', '65', '5', '71', '66']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S><S sid ="82" ssid = "9">The last four columns in Table 3 show the distribution of nonprojective arcs with respect to the number of lifts required.</S><S sid ="86" ssid = "13">As expected  the most informative encoding  Head+Path  gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S><S sid ="84" ssid = "11">In the second part of the experiment  we applied the inverse transformation based on breadth-first search under the three different encoding schemes.</S><S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'82'", "'86'", "'84'", "'70'"]
'51'
'82'
'86'
'84'
'70'
['51', '82', '86', '84', '70']
parsed_discourse_facet ['implication_citation']
<S sid ="85" ssid = "12">The results are given in Table 4.</S><S sid ="33" ssid = "4">If (wi  r  wj) E A  we say that wi is the head of wj and wj a dependent of wi.</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="43" ssid = "14">Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.</S><S sid ="44" ssid = "15">Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'", "'33'", "'60'", "'43'", "'44'"]
'85'
'33'
'60'
'43'
'44'
['85', '33', '60', '43', '44']
parsed_discourse_facet ['method_citation']
<S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="25" ssid = "21">The rest of the paper is structured as follows.</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'14'", "'25'", "'60'", "'75'"]
'65'
'14'
'25'
'60'
'75'
['65', '14', '25', '60', '75']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="26" ssid = "22">In section 2 we introduce the graph transformation techniques used to projectivize and deprojectivize dependency graphs  and in section 3 we describe the data-driven dependency parser that is the core of our system.</S><S sid ="61" ssid = "32">Before we turn to the evaluation  however  we need to introduce the data-driven dependency parser used in the latter experiments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'104'", "'21'", "'26'", "'61'"]
'11'
'104'
'21'
'26'
'61'
['11', '104', '21', '26', '61']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="43" ssid = "14">Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S><S sid ="63" ssid = "2">The parser builds dependency graphs by traversing the input from left to right  using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'11'", "'43'", "'81'", "'63'"]
'34'
'11'
'43'
'81'
'63'
['34', '11', '43', '81', '63']
parsed_discourse_facet ['results_citation']
<S sid ="86" ssid = "13">As expected  the most informative encoding  Head+Path  gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S><S sid ="57" ssid = "28">In approaching this problem  a variety of different methods are conceivable  including a more or less sophisticated use of machine learning.</S><S sid ="73" ssid = "12">More details on the memory-based prediction can be found in Nivre et al. (2004) and Nivre and Scholz (2004).</S><S sid ="61" ssid = "32">Before we turn to the evaluation  however  we need to introduce the data-driven dependency parser used in the latter experiments.</S><S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'57'", "'73'", "'61'", "'34'"]
'86'
'57'
'73'
'61'
'34'
['86', '57', '73', '61', '34']
parsed_discourse_facet ['implication_citation']
<S sid ="83" ssid = "10">It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="101" ssid = "12">However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.</S><S sid ="25" ssid = "21">The rest of the paper is structured as follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'106'", "'78'", "'101'", "'25'"]
'83'
'106'
'78'
'101'
'25'
['83', '106', '78', '101', '25']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="77" ssid = "4">The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'24'", "'95'", "'77'", "'11'"]
'34'
'24'
'95'
'77'
'11'
['34', '24', '95', '77', '11']
parsed_discourse_facet ['results_citation']
<S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="92" ssid = "3">Evaluation metrics used are Attachment Score (AS)  i.e. the proportion of tokens that are attached to the correct head  and Exact Match (EM)  i.e. the proportion of sentences for which the dependency graph exactly matches the gold standard.</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'65'", "'92'", "'60'", "'71'"]
'95'
'65'
'92'
'60'
'71'
['95', '65', '92', '60', '71']
parsed_discourse_facet ['aim_citation']
<S sid ="67" ssid = "6">Table 2 shows the features used in the current version of the parser.</S><S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S><S sid ="82" ssid = "9">The last four columns in Table 3 show the distribution of nonprojective arcs with respect to the number of lifts required.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'70'", "'82'", "'14'", "'81'"]
'67'
'70'
'82'
'14'
'81'
['67', '70', '82', '14', '81']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S><S sid ="63" ssid = "2">The parser builds dependency graphs by traversing the input from left to right  using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'11'", "'34'", "'75'", "'63'"]
'51'
'11'
'34'
'75'
'63'
['51', '11', '34', '75', '63']
parsed_discourse_facet ['aim_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P08-1043_aakansha.csv
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="69" ssid="1">We represent all morphological analyses of a given utterance using a lattice structure.</S>
    <S sid="70" ssid="2">Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'70'"]
'69'
'70'
['69', '70']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="17">The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty, 2006) and (Cohen and Smith, 2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="11">Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
    <S sid="134" ssid="12">Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training 	similar to (Cohen and Smith, 2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'134'"]
'133'
'134'
['133', '134']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="17">The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="155" ssid="33">Evaluation We use 8 different measures to evaluate the performance of our system on the joint disambiguation task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S sid ="54" ssid = "1">A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.</S><S sid ="89" ssid = "21">Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree  s.t.</S><S sid ="179" ssid = "17">On the surface  our model may seem as a special case of Cohen and Smith in which  = 0.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the  hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="148" ssid = "26">Removing the leaves from the resulting tree yields a parse for L under G  with the desired probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'89'", "'179'", "'183'", "'148'"]
'54'
'89'
'179'
'183'
'148'
['54', '89', '179', '183', '148']
parsed_discourse_facet ['implication_citation']
<S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="33" ssid = "12">The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="169" ssid = "7">Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.</S><S sid ="188" ssid = "2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'33'", "'156'", "'169'", "'188'"]
'80'
'33'
'156'
'169'
'188'
['80', '33', '156', '169', '188']
parsed_discourse_facet ['implication_citation']
<S sid ="130" ssid = "8">To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and malformed7 were removed.</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="48" ssid = "6">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S><S sid ="176" ssid = "14">Furthermore  the combination of pruning and vertical markovization of the grammar outperforms the Oracle results reported by Cohen and Smith.</S><S sid ="97" ssid = "29">Thus our proposed model is a proper model assigning probability mass to all (7r  L) pairs  where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'80'", "'48'", "'176'", "'97'"]
'130'
'80'
'48'
'176'
'97'
['130', '80', '48', '176', '97']
parsed_discourse_facet ['results_citation']
<S sid ="49" ssid = "7">Tsarfaty and Simaan (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S><S sid ="163" ssid = "1">The accuracy results for segmentation  tagging and parsing using our different models and our standard data split are summarized in Table 1.</S><S sid ="71" ssid = "3">This is by now a fairly standard representation for multiple morphological segmentation of Hebrew utterances (Adler  2001; Bar-Haim et al.  2005; Smith et al.  2005; Cohen and Smith  2007; Adler  2007).</S><S sid ="33" ssid = "12">The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).</S><S sid ="148" ssid = "26">Removing the leaves from the resulting tree yields a parse for L under G  with the desired probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'163'", "'71'", "'33'", "'148'"]
'49'
'163'
'71'
'33'
'148'
['49', '163', '71', '33', '148']
parsed_discourse_facet ['method_citation']
<S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="45" ssid = "3">Morphological disambiguators that consider a token in context (an utterance) and propose the most likely morphological analysis of an utterance (including segmentation) were presented by Bar-Haim et al. (2005)  Adler and Elhadad (2006)  Shacham and Wintner (2007)  and achieved good results (the best segmentation result so far is around 98%).</S><S sid ="157" ssid = "35">SEGTok(noH) is the segmentation accuracy ignoring mistakes involving the implicit definite article h.11 To evaluate our performance on the tagging task we report CPOS and FPOS corresponding to coarse- and fine-grained PoS tagging results (F1) measure.</S><S sid ="169" ssid = "7">Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'156'", "'45'", "'157'", "'169'"]
'180'
'156'
'45'
'157'
'169'
['180', '156', '45', '157', '169']
parsed_discourse_facet ['method_citation']
<S sid ="169" ssid = "7">Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="161" ssid = "39">We report the F1 value of both measures.</S><S sid ="67" ssid = "14">Hence  we take the probability of the event fmnh analyzed as REL VB to be This means that we generate f and mnh independently depending on their corresponding PoS tags  and the context (as well as the syntactic relation between the two) is modeled via the derivation resulting in a sequence REL VB spanning the form fmnh. based on linear context.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'", "'156'", "'80'", "'161'", "'67'"]
'169'
'156'
'80'
'161'
'67'
['169', '156', '80', '161', '67']
parsed_discourse_facet ['method_citation']
<S sid ="88" ssid = "20">M(wi) = Li).</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="36" ssid = "15">Furthermore  the systematic way in which particles are prefixed to one another and onto an open-class category gives rise to a distinct sort of morphological ambiguity: space-delimited tokens may be ambiguous between several different segmentation possibilities.</S><S sid ="26" ssid = "5">The relativizer f(that) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.</S><S sid ="73" ssid = "5">We use double-circles to indicate the space-delimited token boundaries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'156'", "'36'", "'26'", "'73'"]
'88'
'156'
'36'
'26'
'73'
['88', '156', '36', '26', '73']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="38" ssid = "17">The form mnh itself can be read as at least three different verbs (counted  appointed  was appointed)  a noun (a portion)  and a possessed noun (her kind).</S><S sid ="108" ssid = "40">Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p * (s  p)) > 0  while other segments have never been observed as a lexical event before.</S><S sid ="49" ssid = "7">Tsarfaty and Simaan (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S><S sid ="158" ssid = "36">Evaluating parsing results in our joint framework  as argued by Tsarfaty (2006)  is not trivial under the joint disambiguation task  as the hypothesized yield need not coincide with the correct one.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'38'", "'108'", "'49'", "'158'"]
'80'
'38'
'108'
'49'
'158'
['80', '38', '108', '49', '158']
parsed_discourse_facet ['method_citation']
<S sid ="182" ssid = "20">In addition  as the CRF and PCFG look at similar sorts of information from within two inherently different models  they are far from independent and optimizing their product is meaningless.</S><S sid ="101" ssid = "33">The possible analyses of a surface token pose constraints on the analyses of specific segments.</S><S sid ="163" ssid = "1">The accuracy results for segmentation  tagging and parsing using our different models and our standard data split are summarized in Table 1.</S><S sid ="58" ssid = "5">Such tag sequences are often treated as complex tags (e.g.</S><S sid ="44" ssid = "2">Such analyzers propose multiple segmentation possibilities and their corresponding analyses for a token in isolation but have no means to determine the most likely ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["'182'", "'101'", "'163'", "'58'", "'44'"]
'182'
'101'
'163'
'58'
'44'
['182', '101', '163', '58', '44']
parsed_discourse_facet ['method_citation']
<S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="195" ssid = "9">We further thank Khalil Simaan (ILLCUvA) for his careful advise concerning the formal details of the proposal.</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="98" ssid = "30">The Grammar Our parser looks for the most likely tree spanning a single path through the lattice of which the yield is a sequence of lexemes.</S><S sid ="191" ssid = "5">In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'195'", "'80'", "'98'", "'191'"]
'180'
'195'
'80'
'98'
'191'
['180', '195', '80', '98', '191']
parsed_discourse_facet ['implication_citation']
<S sid ="120" ssid = "52">From now on all lattice arcs are tagged segments and the assignment of probability P(p * (s  p)) to lattice arcs proceeds as usual.4 A rather pathological case is when our lexical heuristics prune away all segmentation possibilities and we remain with an empty lattice.</S><S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="158" ssid = "36">Evaluating parsing results in our joint framework  as argued by Tsarfaty (2006)  is not trivial under the joint disambiguation task  as the hypothesized yield need not coincide with the correct one.</S><S sid ="157" ssid = "35">SEGTok(noH) is the segmentation accuracy ignoring mistakes involving the implicit definite article h.11 To evaluate our performance on the tagging task we report CPOS and FPOS corresponding to coarse- and fine-grained PoS tagging results (F1) measure.</S><S sid ="49" ssid = "7">Tsarfaty and Simaan (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'", "'180'", "'158'", "'157'", "'49'"]
'120'
'180'
'158'
'157'
'49'
['120', '180', '158', '157', '49']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="26" ssid = "5">The relativizer f(that) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.</S><S sid ="54" ssid = "1">A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.</S><S sid ="130" ssid = "8">To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and malformed7 were removed.</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'26'", "'54'", "'130'", "'80'"]
'14'
'26'
'54'
'130'
'80'
['14', '26', '54', '130', '80']
parsed_discourse_facet ['method_citation']
<S sid ="154" ssid = "32">For all grammars  we use fine-grained PoS tags indicating various morphological features annotated therein.</S><S sid ="194" ssid = "8">Acknowledgments We thank Meni Adler and Michael Elhadad (BGU) for helpful comments and discussion.</S><S sid ="97" ssid = "29">Thus our proposed model is a proper model assigning probability mass to all (7r  L) pairs  where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S><S sid ="130" ssid = "8">To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and malformed7 were removed.</S><S sid ="149" ssid = "27">We use a patched version of BitPar allowing for direct input of probabilities instead of counts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'154'", "'194'", "'97'", "'130'", "'149'"]
'154'
'194'
'97'
'130'
'149'
['154', '194', '97', '130', '149']
parsed_discourse_facet ['method_citation']
<S sid ="5" ssid = "1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  tokens) that constitute the unanalyzed surface forms (utterances).</S><S sid ="76" ssid = "8">Furthermore  some of the arcs represent lexemes not present in the input tokens (e.g. h/DT  fl/POS)  however these are parts of valid analyses of the token (cf. super-segmental morphology section 2).</S><S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="108" ssid = "40">Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p * (s  p)) > 0  while other segments have never been observed as a lexical event before.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'76'", "'180'", "'108'", "'133'"]
'5'
'76'
'180'
'108'
'133'
['5', '76', '180', '108', '133']
parsed_discourse_facet ['results_citation']
<S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="48" ssid = "6">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S><S sid ="181" ssid = "19">Many morphological decisions are based on long distance dependencies  and when the global syntactic evidence disagrees with evidence based on local linear context  the two models compete with one another  despite the fact that the PCFG takes also local context into account.</S><S sid ="95" ssid = "27">A compatible view is presented by Charniak et al. (1996) who consider the kind of probabilities a generative parser should get from a PoS tagger  and concludes that these should be P(w|t) and nothing fancier.3 In our setting  therefore  the Lattice is not used to induce a probability distribution on a linear context  but rather  it is used as a common-denominator of state-indexation of all segmentations possibilities of a surface form.</S><S sid ="54" ssid = "1">A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'48'", "'181'", "'95'", "'54'"]
'80'
'48'
'181'
'95'
'54'
['80', '48', '181', '95', '54']
parsed_discourse_facet ['implication_citation']
<S sid ="164" ssid = "2">In addition we report for each model its performance on goldsegmented input (GS) to indicate the upper bound 11Overt definiteness errors may be seen as a wrong feature rather than as wrong constituent and it is by now an accepted standard to report accuracy with and without such errors. for the grammars performance on the parsing task.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S><S sid ="22" ssid = "1">Segmental morphology Hebrew consists of seven particles m(from) f(when/who/that) h(the) w(and) k(like) l(to) and b(in). which may never appear in isolation and must always attach as prefixes to the following open-class category item we refer to as stem.</S><S sid ="126" ssid = "4">When a comparison against previous results requires additional pre-processing  we state it explicitly to allow for the reader to replicate the reported results.</S><S sid ="173" ssid = "11">The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter () which is tuned separately for each of the tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'164'", "'133'", "'22'", "'126'", "'173'"]
'164'
'133'
'22'
'126'
'173'
['164', '133', '22', '126', '173']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/J01-2004_aakansha.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'"]
'372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="17" ssid="5">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



["'302'"]
'302'
['302']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="37">This underspecification of the nonterminal predictions (e.g., VP-VBD in the example in Figure 2, as opposed to NP), allows lexical items to become part of the left context, and so be used to condition production probabilities, even the production probabilities of constituents that dominate them in the unfactored tree.</S>
    <S sid="80" ssid="38">It also brings words further downstream into the look-ahead at the point of specification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'", "'80'"]
'79'
'80'
['79', '80']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="42">Our approach is found to yield very accurate parses efficiently, and, in addition, to lend itself straightforwardly to estimating word probabilities on-line, that is, in a single pass from left to right.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="291" ssid="47">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'291'"]
'291'
['291']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['method_citation']
<S sid="209" ssid="113">This parser is essentially a stochastic version of the top-down parser described in Aho, Sethi, and Ullman (1986).</S>
    <S sid="210" ssid="114">It uses a PCFG with a conditional probability model of the sort defined in the previous section.</S>
original cit marker offset is 0
new cit marker offset is 0



["'209'", "'210'"]
'209'
'210'
['209', '210']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'"]
'372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="8">Two features of our top-down parsing approach will emerge as key to its success.</S>
    <S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S><S sid="32" ssid="20">A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'21'", "'32'"]
'20'
'21'
'32'
['20', '21', '32']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="21">Because the rooted partial derivation is fully connected, all of the conditioning information that might be extracted from the top-down left context has already been specified, and a conditional probability model built on this information will not impose any additional burden on the search.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'"]
'33'
['33']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S sid ="321" ssid = "77">In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.</S><S sid ="387" ssid = "143">Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.</S><S sid ="315" ssid = "71">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;the sum of the probabilities over the vocabulary is less than one.</S><S sid ="344" ssid = "100">However  when we interpolate with the trigram  we see that the additional improvement is greater than the one they experienced.</S><S sid ="403" ssid = "16">Perhaps some compromise between the fully connected structures and extreme underspecification will yield an efficiency improvement.</S>
original cit marker offset is 0
new cit marker offset is 0



["'321'", "'387'", "'315'", "'344'", "'403'"]
'321'
'387'
'315'
'344'
'403'
['321', '387', '315', '344', '403']
parsed_discourse_facet ['implication_citation']
<S sid ="380" ssid = "136">Future work will include more substantial word recognition experiments.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S><S sid ="344" ssid = "100">However  when we interpolate with the trigram  we see that the additional improvement is greater than the one they experienced.</S><S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S>
original cit marker offset is 0
new cit marker offset is 0



["'380'", "'280'", "'391'", "'344'", "'336'"]
'380'
'280'
'391'
'344'
'336'
['380', '280', '391', '344', '336']
parsed_discourse_facet ['implication_citation']
<S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="338" ssid = "94">Table 4 compares the perplexity of our model with Chelba and Jelinek (1998a  1998b) on the same training and testing corpora.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="326" ssid = "82">We obtained the training and testing corpora from them (which we will denote C&J corpus)  and also created intermediate corpora  upon which only the first two modifications were carried out (which we will denote no punct).</S><S sid ="343" ssid = "99">Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'</S>
original cit marker offset is 0
new cit marker offset is 0



["'349'", "'338'", "'280'", "'326'", "'343'"]
'349'
'338'
'280'
'326'
'343'
['349', '338', '280', '326', '343']
parsed_discourse_facet ['results_citation']
<S sid ="372" ssid = "128">The small size of our training data  as well as the fact that we are rescoring n-best lists  rather than working directly on lattices  makes comparison with the other models not particularly informative.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="309" ssid = "65">Let Ht be the priority queue H  before any processing has begun with word w  in the look-ahead.</S><S sid ="266" ssid = "22">From this set of measures  we will also include the crossing bracket scores: average crossing brackets (CB)  percentage of sentences with no crossing brackets (0 CB)  and the percentage of sentences with two crossing brackets or fewer (< 2 CB).</S><S sid ="268" ssid = "24">This is an incremental parser with a pruning strategy and no backtracking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'", "'301'", "'309'", "'266'", "'268'"]
'372'
'301'
'309'
'266'
'268'
['372', '301', '309', '266', '268']
parsed_discourse_facet ['method_citation']
<S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="355" ssid = "111">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task  we performed a very small preliminary experiment on n-best lists.</S><S sid ="340" ssid = "96">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid ="354" ssid = "110">Interestingly  at the word where the failure occurred  the sum of the probabilities was 0.9301.</S>
original cit marker offset is 0
new cit marker offset is 0



["'336'", "'301'", "'355'", "'340'", "'354'"]
'336'
'301'
'355'
'340'
'354'
['336', '301', '355', '340', '354']
parsed_discourse_facet ['method_citation']
<S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="361" ssid = "117">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S><S sid ="319" ssid = "75">The hope  however  is that the improved parsing model provided by our conditional probability model will cause the distribution over structures to be more peaked  thus enabling us to capture more of the total probability mass  and making this a fairly snug upper bound on the perplexity.</S><S sid ="355" ssid = "111">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task  we performed a very small preliminary experiment on n-best lists.</S>
original cit marker offset is 0
new cit marker offset is 0



["'301'", "'349'", "'361'", "'319'", "'355'"]
'301'
'349'
'361'
'319'
'355'
['301', '349', '361', '319', '355']
parsed_discourse_facet ['method_citation']
<S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="324" ssid = "80">Their modifications included: (i) removing orthographic cues to structure (e.g.  punctuation); (ii) replacing all numbers with the single token N; and (iii) closing the vocabulary at 10 000  replacing all other words with the UNK token.</S><S sid ="343" ssid = "99">Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'</S><S sid ="387" ssid = "143">Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.</S><S sid ="346" ssid = "102">These results are particularly remarkable  given that we did not build our model as a language model per se  but rather as a parsing model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'349'", "'324'", "'343'", "'387'", "'346'"]
'349'
'324'
'343'
'387'
'346'
['349', '324', '343', '387', '346']
parsed_discourse_facet ['method_citation']
<S sid ="315" ssid = "71">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;the sum of the probabilities over the vocabulary is less than one.</S><S sid ="377" ssid = "133">The point of this small experiment was to see if our parsing model could provide useful information even in the case that recognition errors occur  as opposed to the (generally) fully grammatical strings upon which the perplexity results were obtained.</S><S sid ="288" ssid = "44">Interestingly  conditioning all POS expansions on two c-commanding heads made no difference in accuracy compared to conditioning only leftmost POS expansions on a single c-commanding head; but it did improve the efficiency.</S><S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="382" ssid = "138">The base beam factor that we have used to this point is 10'  which is quite wide.</S>
original cit marker offset is 0
new cit marker offset is 0



["'315'", "'377'", "'288'", "'358'", "'382'"]
'315'
'377'
'288'
'358'
'382'
['315', '377', '288', '358', '382']
parsed_discourse_facet ['method_citation']
<S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="382" ssid = "138">The base beam factor that we have used to this point is 10'  which is quite wide.</S><S sid ="283" ssid = "39">Unlike the Roark and Johnson parser  however  our coverage did not substantially drop as the amount of conditioning information increased  and in some cases  coverage improved slightly.</S><S sid ="380" ssid = "136">Future work will include more substantial word recognition experiments.</S><S sid ="268" ssid = "24">This is an incremental parser with a pruning strategy and no backtracking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'301'", "'382'", "'283'", "'380'", "'268'"]
'301'
'382'
'283'
'380'
'268'
['301', '382', '283', '380', '268']
parsed_discourse_facet ['method_citation']
<S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S><S sid ="321" ssid = "77">In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.</S><S sid ="262" ssid = "18">Recall is the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD.</S>
original cit marker offset is 0
new cit marker offset is 0



["'336'", "'280'", "'391'", "'321'", "'262'"]
'336'
'280'
'391'
'321'
'262'
['336', '280', '391', '321', '262']
parsed_discourse_facet ['implication_citation']
<S sid ="315" ssid = "71">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;the sum of the probabilities over the vocabulary is less than one.</S><S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S><S sid ="295" ssid = "51">Our observed times look polynomial  which is to be expected given our pruning strategy: the denser the competitors within a narrow probability range of the best analysis  the more time will be spent working on these competitors; and the farther along in the sentence  the more chance for ambiguities that can lead to such a situation.</S><S sid ="382" ssid = "138">The base beam factor that we have used to this point is 10'  which is quite wide.</S><S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'315'", "'336'", "'295'", "'382'", "'391'"]
'315'
'336'
'295'
'382'
'391'
['315', '336', '295', '382', '391']
parsed_discourse_facet ['method_citation']
<S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="258" ssid = "14">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S><S sid ="372" ssid = "128">The small size of our training data  as well as the fact that we are rescoring n-best lists  rather than working directly on lattices  makes comparison with the other models not particularly informative.</S><S sid ="298" ssid = "54">What is perhaps surprising is that the difference is not greater.</S><S sid ="270" ssid = "26">In such a case  the parser fails to return a complete parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'301'", "'258'", "'372'", "'298'", "'270'"]
'301'
'258'
'372'
'298'
'270'
['301', '258', '372', '298', '270']
parsed_discourse_facet ['method_citation']
<S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S><S sid ="398" ssid = "11">By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="354" ssid = "110">Interestingly  at the word where the failure occurred  the sum of the probabilities was 0.9301.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'391'", "'398'", "'301'", "'354'", "'404'"]
'391'
'398'
'301'
'354'
'404'
['391', '398', '301', '354', '404']
parsed_discourse_facet ['method_citation']
<S sid ="402" ssid = "15">In fact  left-corner parsing can be simulated by a top-down parser by transforming the grammar  as was done in Roark and Johnson (1999)  and so an approach very similar to the one outlined here could be used in that case.</S><S sid ="398" ssid = "11">By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="371" ssid = "127">For our model and the Treebank trigram model  the LM weight that resulted in the lowest error rates is given.</S><S sid ="343" ssid = "99">Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'</S>
original cit marker offset is 0
new cit marker offset is 0



["'402'", "'398'", "'280'", "'371'", "'343'"]
'402'
'398'
'280'
'371'
'343'
['402', '398', '280', '371', '343']
parsed_discourse_facet ['results_citation']
<S sid ="398" ssid = "11">By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="361" ssid = "117">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S><S sid ="258" ssid = "14">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S>
original cit marker offset is 0
new cit marker offset is 0



["'398'", "'301'", "'390'", "'361'", "'258'"]
'398'
'301'
'390'
'361'
'258'
['398', '301', '390', '361', '258']
parsed_discourse_facet ['implication_citation']
<S sid ="258" ssid = "14">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S><S sid ="262" ssid = "18">Recall is the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD.</S><S sid ="361" ssid = "117">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S><S sid ="339" ssid = "95">We built an interpolated trigram model to serve as a baseline (as they did)  and also interpolated our model's perplexity with the trigram  using the same mixing coefficient as they did in their trials (taking 36 percent of the estimate from the trigram).'</S><S sid ="324" ssid = "80">Their modifications included: (i) removing orthographic cues to structure (e.g.  punctuation); (ii) replacing all numbers with the single token N; and (iii) closing the vocabulary at 10 000  replacing all other words with the UNK token.</S>
original cit marker offset is 0
new cit marker offset is 0



["'258'", "'262'", "'361'", "'339'", "'324'"]
'258'
'262'
'361'
'339'
'324'
['258', '262', '361', '339', '324']
parsed_discourse_facet ['method_citation']
<S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="257" ssid = "13">A constituent for evaluation purposes consists of a label (e.g.  NP) and a span (beginning and ending word positions).</S><S sid ="340" ssid = "96">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="387" ssid = "143">Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.</S>
original cit marker offset is 0
new cit marker offset is 0



["'349'", "'257'", "'340'", "'301'", "'387'"]
'349'
'257'
'340'
'301'
'387'
['349', '257', '340', '301', '387']
parsed_discourse_facet ['results_citation']
parsing: input/ref/Task1/P08-1102_swastika.csv
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="10">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="8">All feature templates and their instances are shown in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



['36']
36
['36']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
<S sid="97" ssid="8">Figure 3 shows their learning curves depicting the F-measure on the development set after 1 to 10 training iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



['97']
97
['97']
parsed_discourse_facet ['result_citation']
<S sid="91" ssid="2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['aim_citation']
<S sid="16" ssid="12">Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



['34']
34
['34']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S sid ="120" ssid = "31">Without the perceptron  the cascaded model (if we can still call it cascaded) performs poorly on both segmentation and Joint S&T.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'", "'58'", "'136'", "'31'", "'134'"]
'120'
'58'
'136'
'31'
'134'
['120', '58', '136', '31', '134']
parsed_discourse_facet ['implication_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="117" ssid = "28">Table 4 shows experiments results.</S><S sid ="93" ssid = "4">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid ="18" ssid = "14">In this architecture  knowledge sources that are intractable to incorporate into the perceptron  can be easily incorporated into the outside linear model.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'117'", "'93'", "'18'", "'136'"]
'68'
'117'
'93'
'18'
'136'
['68', '117', '93', '18', '136']
parsed_discourse_facet ['implication_citation']
<S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x  X to outputs y  Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="78" ssid = "3">Given a Chinese character sequence C1:n  the decoding procedure can proceed in a left-right fashion with a dynamic programming approach.</S><S sid ="79" ssid = "4">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="46" ssid = "18">Following Collins  we use a function GEN(x) generating all candidate results of an input x   a representation 4) mapping each training example (x  y)  X  Y to a feature vector 4)(x  y)  Rd  and a parameter vector   Rd corresponding to the feature vector. d means the dimension of the vector space  it equals to the amount of features in the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'", "'78'", "'79'", "'7'", "'46'"]
'45'
'78'
'79'
'7'
'46'
['45', '78', '79', '7', '46']
parsed_discourse_facet ['results_citation']
<S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="65" ssid = "16">As shown in Figure 1  the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.</S><S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="95" ssid = "6">For convenience of comparing with others  we focus only on the close test  which means that any extra resource is forbidden except the designated training corpus.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'65'", "'101'", "'95'", "'58'"]
'133'
'65'
'101'
'95'
'58'
['133', '65', '101', '95', '58']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="95" ssid = "6">For convenience of comparing with others  we focus only on the close test  which means that any extra resource is forbidden except the designated training corpus.</S><S sid ="61" ssid = "12">In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'47'", "'95'", "'61'", "'136'"]
'101'
'47'
'95'
'61'
'136'
['101', '47', '95', '61', '136']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called averaged parameters (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="71" ssid = "22">Using W = w1:m to denote the word sequence  T = t1:m to denote the corresponding POS sequence  P (T |W) to denote the probability that W is labelled as T  and P(W|T) to denote the probability that T generates W  we can define the cooccurrence model as follows: wt and tw denote the corresponding weights of the two components.</S><S sid ="92" ssid = "3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.</S><S sid ="82" ssid = "7">In addition  we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'49'", "'71'", "'92'", "'82'"]
'68'
'49'
'71'
'92'
'82'
['68', '49', '71', '92', '82']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "10">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid ="40" ssid = "12">Templates in the column below are expanded from the upper ones.</S><S sid ="43" ssid = "15">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S><S sid ="25" ssid = "21">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'58'", "'40'", "'43'", "'25'"]
'38'
'58'
'40'
'43'
'25'
['38', '58', '40', '43', '25']
parsed_discourse_facet ['method_citation']
<S sid ="93" ssid = "4">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid ="57" ssid = "8">It has a two-layer architecture  with a perceptron as the core and another linear model as the outside-layer.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="40" ssid = "12">Templates in the column below are expanded from the upper ones.</S><S sid ="17" ssid = "13">We will describe it in detail in Section 4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'93'", "'57'", "'136'", "'40'", "'17'"]
'93'
'57'
'136'
'40'
'17'
['93', '57', '136', '40', '17']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="33" ssid = "5">In following subsections  we describe the feature templates and the perceptron training algorithm.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="96" ssid = "7">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus  we randomly chosen 2  000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84  294 sentences)  then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'68'", "'33'", "'134'", "'96'"]
'101'
'68'
'33'
'134'
'96'
['101', '68', '33', '134', '96']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="89" ssid = "14">Function D derives the candidate result from the word-POS pair p and the candidate q at prior position of p.</S><S sid ="9" ssid = "5">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid ="79" ssid = "4">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid ="104" ssid = "15">According to the usual practice in syntactic analysis  we choose chapters 1  260 (18074 sentences) as training set  chapter 271  300 (348 sentences) as test set and chapter 301  325 (350 sentences) as development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'89'", "'9'", "'79'", "'104'"]
'68'
'89'
'9'
'79'
'104'
['68', '89', '9', '79', '104']
parsed_discourse_facet ['implication_citation']
<S sid ="125" ssid = "36">However unlike the three features  the word LM brings very tiny improvement.</S><S sid ="40" ssid = "12">Templates in the column below are expanded from the upper ones.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'40'", "'94'", "'2'", "'58'"]
'125'
'40'
'94'
'2'
'58'
['125', '40', '94', '2', '58']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="56" ssid = "7">To alleviate the drawbacks  we propose a cascaded linear model.</S><S sid ="61" ssid = "12">In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.</S><S sid ="93" ssid = "4">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid ="112" ssid = "23">Here the core perceptron was just the POS+ model in experiments above.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'56'", "'61'", "'93'", "'112'"]
'136'
'56'
'61'
'93'
'112'
['136', '56', '61', '93', '112']
parsed_discourse_facet ['method_citation']
<S sid ="99" ssid = "10">Then we trained LEX on each of the four corpora for 7 iterations.</S><S sid ="120" ssid = "31">Without the perceptron  the cascaded model (if we can still call it cascaded) performs poorly on both segmentation and Joint S&T.</S><S sid ="121" ssid = "32">Among other features  the 4-gram POS LM plays the most important role  removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&T.</S><S sid ="42" ssid = "14">As predications generated from such templates depend on the current character  we name these templates lexical-target.</S><S sid ="117" ssid = "28">Table 4 shows experiments results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'99'", "'120'", "'121'", "'42'", "'117'"]
'99'
'120'
'121'
'42'
'117'
['99', '120', '121', '42', '117']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="64" ssid = "15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S><S sid ="53" ssid = "4">Figure 2 shows the growing tendency of feature space with the introduction of these features as well as the character-based ones.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'134'", "'64'", "'53'", "'7'"]
'68'
'134'
'64'
'53'
'7'
['68', '134', '64', '53', '7']
parsed_discourse_facet ['results_citation']
<S sid ="3" ssid = "3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="84" ssid = "9">Algorithm 2 shows the decoding algorithm.</S><S sid ="18" ssid = "14">In this architecture  knowledge sources that are intractable to incorporate into the perceptron  can be easily incorporated into the outside linear model.</S><S sid ="92" ssid = "3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'136'", "'84'", "'18'", "'92'"]
'3'
'136'
'84'
'18'
'92'
['3', '136', '84', '18', '92']
parsed_discourse_facet ['implication_citation']
<S sid ="51" ssid = "2">Additional features most widely used are related to word or POS ngrams.</S><S sid ="13" ssid = "9">However  as such features are generated dynamically during the decoding procedure  two limitation arise: on the one hand  the amount of parameters increases rapidly  which is apt to overfit on training corpus; on the other hand  exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called averaged parameters (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="36" ssid = "8">All feature templates and their instances are shown in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'13'", "'7'", "'49'", "'36'"]
'51'
'13'
'7'
'49'
'36'
['51', '13', '7', '49', '36']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W11-2123_aakansha.csv
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="136" ssid="8">We offer a state function s(wn1) = wn&#65533; where substring wn&#65533; is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'"]
'136'
['136']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="3">Queries take the form p(wn|wn&#8722;1 1 ) where wn1 is an n-gram.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="205" ssid="24">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al., 2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'205'"]
'205'
['205']
parsed_discourse_facet ['method_citation']
<S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
<S sid="204" ssid="23">For RandLM, we used the settings in the documentation: 8 bits per value and false positive probability 1 256.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'"]
'204'
['204']
parsed_discourse_facet ['method_citation']
<S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="229" ssid="48">Then we ran binary search to determine the least amount of memory with which it would run.</S>
original cit marker offset is 0
new cit marker offset is 0



["'229'"]
'229'
['229']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="93" ssid="71">The cost of storing these averages, in bits, is Because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.</S>
original cit marker offset is 0
new cit marker offset is 0



["'93'"]
'93'
['93']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W11-2123.csv
<S sid ="270" ssid = "12">This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S>
original cit marker offset is 0
new cit marker offset is 0



["'270'", "'276'", "'265'", "'286'", "'284'"]
'270'
'276'
'265'
'286'
'284'
['270', '276', '265', '286', '284']
parsed_discourse_facet ['implication_citation']
<S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S>
original cit marker offset is 0
new cit marker offset is 0



["'262'", "'280'", "'265'", "'277'", "'284'"]
'262'
'280'
'265'
'277'
'284'
['262', '280', '265', '277', '284']
parsed_discourse_facet ['implication_citation']
<S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="285" ssid = "6">Juri Ganitkevitch answered questions about Joshua.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'265'", "'284'", "'278'", "'285'", "'276'"]
'265'
'284'
'278'
'285'
'276'
['265', '284', '278', '285', '276']
parsed_discourse_facet ['results_citation']
<S sid ="260" ssid = "2">For speed  we plan to implement the direct-mapped cache from BerkeleyLM.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>
original cit marker offset is 0
new cit marker offset is 0



["'260'", "'262'", "'280'", "'283'", "'266'"]
'260'
'262'
'280'
'283'
'266'
['260', '262', '280', '283', '266']
parsed_discourse_facet ['method_citation']
<S sid ="272" ssid = "14">Generalizing state minimization  the model could also provide explicit bounds on probability for both backward and forward extension.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="268" ssid = "10">For example  syntactic decoders (Koehn et al.  2007; Dyer et al.  2010; Li et al.  2009) perform dynamic programming parametrized by both backward- and forward-looking state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'272'", "'265'", "'283'", "'286'", "'268'"]
'272'
'265'
'283'
'286'
'268'
['272', '265', '283', '286', '268']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="264" ssid = "6">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'284'", "'265'", "'264'", "'267'"]
'256'
'284'
'265'
'264'
'267'
['256', '284', '265', '264', '267']
parsed_discourse_facet ['method_citation']
<S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S>
original cit marker offset is 0
new cit marker offset is 0



["'283'", "'284'", "'280'", "'265'", "'275'"]
'283'
'284'
'280'
'265'
'275'
['283', '284', '280', '265', '275']
parsed_discourse_facet ['method_citation']
<S sid ="261" ssid = "3">Much could be done to further reduce memory consumption.</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'261'", "'277'", "'286'", "'284'", "'283'"]
'261'
'277'
'286'
'284'
'283'
['261', '277', '286', '284', '283']
parsed_discourse_facet ['method_citation']
<S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S><S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'286'", "'280'", "'267'", "'279'", "'283'"]
'286'
'280'
'267'
'279'
'283'
['286', '280', '267', '279', '283']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'287'", "'266'", "'262'", "'265'"]
'256'
'287'
'266'
'262'
'265'
['256', '287', '266', '262', '265']
parsed_discourse_facet ['implication_citation']
<S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="264" ssid = "6">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'262'", "'278'", "'277'", "'265'", "'264'"]
'262'
'278'
'277'
'265'
'264'
['262', '278', '277', '265', '264']
parsed_discourse_facet ['method_citation']
<S sid ="261" ssid = "3">Much could be done to further reduce memory consumption.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S><S sid ="269" ssid = "11">If they knew that the first four words in a hypergraph node would never extend to the left and form a 5-gram  then three or even fewer words could be kept in the backward state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'261'", "'265'", "'266'", "'267'", "'269'"]
'261'
'265'
'266'
'267'
'269'
['261', '265', '266', '267', '269']
parsed_discourse_facet ['method_citation']
<S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'287'", "'266'", "'286'", "'262'", "'274'"]
'287'
'266'
'286'
'262'
'274'
['287', '266', '286', '262', '274']
parsed_discourse_facet ['method_citation']
<S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'286'", "'256'", "'265'", "'284'", "'276'"]
'286'
'256'
'265'
'284'
'276'
['286', '256', '265', '284', '276']
parsed_discourse_facet ['results_citation']
<S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S><S sid ="270" ssid = "12">This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.</S>
original cit marker offset is 0
new cit marker offset is 0



["'278'", "'262'", "'265'", "'275'", "'270'"]
'278'
'262'
'265'
'275'
'270'
['278', '262', '265', '275', '270']
parsed_discourse_facet ['implication_citation']
<S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'280'", "'262'", "'265'", "'279'", "'256'"]
'280'
'262'
'265'
'279'
'256'
['280', '262', '265', '279', '256']
parsed_discourse_facet ['method_citation']
<S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'279'", "'256'", "'262'", "'265'", "'276'"]
'279'
'256'
'262'
'265'
'276'
['279', '256', '262', '265', '276']
parsed_discourse_facet ['results_citation']
<S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="264" ssid = "6">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'277'", "'286'", "'287'", "'264'"]
'256'
'277'
'286'
'287'
'264'
['256', '277', '286', '287', '264']
parsed_discourse_facet ['aim_citation']
<S sid ="263" ssid = "5">Quantization can be improved by jointly encoding probability and backoff.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="258" ssid = "77">However  the point of RandLM is to scale to even larger data  compensating for this loss in quality.</S>
original cit marker offset is 0
new cit marker offset is 0



["'263'", "'265'", "'287'", "'283'", "'258'"]
'263'
'265'
'287'
'283'
'258'
['263', '265', '287', '283', '258']
parsed_discourse_facet ['method_citation']
<S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'265'", "'280'", "'256'", "'278'", "'267'"]
'265'
'280'
'256'
'278'
'267'
['265', '280', '256', '278', '267']
parsed_discourse_facet ['aim_citation']
parsing: input/ref/Task1/P87-1015_vardha.csv
<S sid="165" ssid="50">This class of formalisms have the properties that their derivation trees are local sets, and manipulate objects, using a finite number of composition operations that use a finite number of symbols.</S>
original cit marker offset is 0
new cit marker offset is 0



["'165'"]
'165'
['165']
parsed_discourse_facet ['method_citation']
 <S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'"]
'119'
['119']
parsed_discourse_facet ['method_citation']
    <S sid="3" ssid="1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'"]
'3'
['3']
parsed_discourse_facet ['method_citation']
    <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
    <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
    <S sid="133" ssid="18">To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'"]
'133'
['133']
parsed_discourse_facet ['method_citation']
 <S sid="138" ssid="23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
    <S sid="156" ssid="41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'"]
'156'
['156']
parsed_discourse_facet ['method_citation']
 <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
 <S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'"]
'119'
['119']
parsed_discourse_facet ['method_citation']
  <S sid="133" ssid="18">To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'"]
'133'
['133']
parsed_discourse_facet ['method_citation']
 <S sid="164" ssid="49">Although embedding this version of LCFRS's in the framework of ILFP developed by Rounds (1985) is straightforward, our motivation was to capture properties shared by a family of grammatical systems and generalize them defining a class of related formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'164'"]
'164'
['164']
parsed_discourse_facet ['method_citation']
 <S sid="204" ssid="10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's, HG's, TAG's, and MCTAG's are all local sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'"]
'204'
['204']
parsed_discourse_facet ['method_citation']
    <S sid="9" ssid="7">We examine both the complexity of the paths of trees in the tree sets, and the kinds of dependencies that the formalisms can impose between paths.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="28" ssid="13">A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
 <S sid="138" ssid="23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P87-1015.csv
<S sid ="191" ssid = "76">In addition to the tapes required to store the indices  M requires one work tape for splitting the substrings.</S><S sid ="206" ssid = "12">As suggested in Section 4.3.2  a derivation with independent paths can be divided into subcomputations with limited sharing of information.</S><S sid ="186" ssid = "71">To do this  the x's and y's are stored in the next 2ni + 2n2 tapes  and M goes to a universal state.</S><S sid ="104" ssid = "10">Pumping t2 will change only one branch and leave the other branch unaffected.</S><S sid ="192" ssid = "77">Thus  the ATM has no more than 6km&quot; + 1 work tapes  where km&quot; is the maximum number of substrings spanned by a derived structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'191'", "'206'", "'186'", "'104'", "'192'"]
'191'
'206'
'186'
'104'
'192'
['191', '206', '186', '104', '192']
parsed_discourse_facet ['implication_citation']
<S sid ="84" ssid = "69">((fii  Q2  Pa)    (01  i32  03)   e) (02 e) Oa  en) The path complexity of the tee set generated by a MCTAG is not necessarily context-free.</S><S sid ="54" ssid = "39">An IG can be viewed as a CFG in which each nonterminal is associated with a stack.</S><S sid ="101" ssid = "7">The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently.</S><S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="71" ssid = "56">0n0'i'i0'2&quot;bin242bn I n = 711 + n2 } On the other hand  no linguistic use is made of this general form of composition and Steedman (personal communication) and Steedman (1986) argues that a more limited definition of composition is more natural.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'54'", "'101'", "'59'", "'71'"]
'84'
'54'
'101'
'59'
'71'
['84', '54', '101', '59', '71']
parsed_discourse_facet ['implication_citation']
<S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="44" ssid = "29">The edge from the root to the subtree for the derivation of 7i is labeled by the address ni.</S><S sid ="136" ssid = "21">These two restrictions impose the constraint that the result of composing any two structures should be a structure whose &quot;size&quot; is the sum of its constituents plus some constant For example  the operation 4  discussed in the case of CFG's (in Section 4.1) adds the constant equal to the sum of the length of the strings VI  un+r Since we are considering formalisms with arbitrary structures it is difficult to precisely specify all of the restrictions on the composition operations that we believe would appropriately generalize the concatenation operation for the particular structures used by the formalism.</S><S sid ="28" ssid = "13">A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'44'", "'136'", "'28'", "'153'"]
'151'
'44'
'136'
'28'
'153'
['151', '44', '136', '28', '153']
parsed_discourse_facet ['results_citation']
<S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="134" ssid = "19">These systems are similar to those described by Pollard (1984) as Generalized Context-Free Grammars (GCFG's).</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="112" ssid = "18">.t The path set of tree sets at level k +1 have the complexity of the string language of level k. The independence of paths in a tree set appears to be an important property.</S><S sid ="20" ssid = "5">As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'134'", "'153'", "'112'", "'20'"]
'59'
'134'
'153'
'112'
'20'
['59', '134', '153', '112', '20']
parsed_discourse_facet ['method_citation']
<S sid ="106" ssid = "12">We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths.</S><S sid ="54" ssid = "39">An IG can be viewed as a CFG in which each nonterminal is associated with a stack.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="101" ssid = "7">The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently.</S><S sid ="182" ssid = "67">Since each zi is a contiguous substring of the input (say ai )  and no two substrings overlap  we can represent zi by the pair of integers (i2  i2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'54'", "'143'", "'101'", "'182'"]
'106'
'54'
'143'
'101'
'182'
['106', '54', '143', '101', '182']
parsed_discourse_facet ['method_citation']
<S sid ="55" ssid = "40">Each production can push or pop symbols on the stack as can be seen in the following productions that generate tree of the form shown in Figure 4b.</S><S sid ="117" ssid = "2">Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.</S><S sid ="200" ssid = "6">The importance of this property becomes clear in contrasting theories underlying GPSG (Gazdar  Klein  Pulluna  and Sag  1985)  and GB (as described by Berwick  1984) with those underlying LFG and FUG.</S><S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="106" ssid = "12">We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'117'", "'200'", "'59'", "'106'"]
'55'
'117'
'200'
'59'
'106'
['55', '117', '200', '59', '106']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "17">When 7' is adjoined at ?I in the tree 7 we obtain a tree v&quot;.</S><S sid ="121" ssid = "6">First  any grammar must involve a finite number of elementary structures  composed using a finite number of composition operations.</S><S sid ="19" ssid = "4">It can be easily shown from Thatcher's result that the path set of every local set is a regular set.</S><S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="138" ssid = "23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'121'", "'19'", "'151'", "'138'"]
'32'
'121'
'19'
'151'
'138'
['32', '121', '19', '151', '138']
parsed_discourse_facet ['method_citation']
<S sid ="188" ssid = "73">Thus  for example  one successor process will be have M to be in the existential state qa with the indices encoding xi     xn  in the first 2n i tapes.</S><S sid ="118" ssid = "3">In the remainder of the paper  we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S><S sid ="89" ssid = "74">Hence  trees shown in Figure 8 can not be generated by any MCTAG (but can be generated by an IG) because the number of pairs of dependent paths grows with n. Since the derivation tees of TAG's  MCTAG's  and HG's are local sets  the choice of the structure used at each point in a derivation in these systems does not depend on the context at that point within the derivation.</S><S sid ="96" ssid = "2">A tree set may be said to have dependencies between paths if some &quot;appropriate&quot; subset can be shown to have dependent paths as defined above.</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'188'", "'118'", "'89'", "'96'", "'227'"]
'188'
'118'
'89'
'96'
'227'
['188', '118', '89', '96', '227']
parsed_discourse_facet ['method_citation']
<S sid ="84" ssid = "69">((fii  Q2  Pa)    (01  i32  03)   e) (02 e) Oa  en) The path complexity of the tee set generated by a MCTAG is not necessarily context-free.</S><S sid ="98" ssid = "4">Thatcher (1973) describes a tee pumping lemma for recognizable sets related to the string pumping lemma for regular sets.</S><S sid ="12" ssid = "10">We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.</S><S sid ="179" ssid = "64">It can be seen that M performs a top-down recognition of the input al ... nin logspace.</S><S sid ="185" ssid = "70">Each spawned process must check if xi     xn  and   yn  can be derived from B and C  respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'98'", "'12'", "'179'", "'185'"]
'84'
'98'
'12'
'179'
'185'
['84', '98', '12', '179', '185']
parsed_discourse_facet ['method_citation']
<S sid ="207" ssid = "13">We outlined the definition of a family of constrained grammatical formalisms  called Linear Context-Free Rewriting Systems.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="26" ssid = "11">The productions of HG's are very similar to those of CFG's except that the operation used must be made explicit.</S><S sid ="178" ssid = "63">We define an ATM  M  recognizing a language generated by a grammar  G  having the properties discussed in Section 43.</S><S sid ="155" ssid = "40">CFG's  TAG's  MCTAG's and HG's are all members of this class since they satisfy these restrictions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'153'", "'26'", "'178'", "'155'"]
'207'
'153'
'26'
'178'
'155'
['207', '153', '26', '178', '155']
parsed_discourse_facet ['implication_citation']
<S sid ="38" ssid = "23">Thus  the derivation trees for TAG's have the same structure as local sets.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="165" ssid = "50">This class of formalisms have the properties that their derivation trees are local sets  and manipulate objects  using a finite number of composition operations that use a finite number of symbols.</S><S sid ="62" ssid = "47">TAG's can be shown to be equivalent to this restricted system.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'153'", "'165'", "'62'", "'143'"]
'38'
'153'
'165'
'62'
'143'
['38', '153', '165', '62', '143']
parsed_discourse_facet ['method_citation']
<S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="132" ssid = "17">In TAG's the elementary tree and addresses where adjunction takes place are used to instantiate the operation.</S><S sid ="101" ssid = "7">The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently.</S><S sid ="96" ssid = "2">A tree set may be said to have dependencies between paths if some &quot;appropriate&quot; subset can be shown to have dependent paths as defined above.</S><S sid ="94" ssid = "79">The semilinearity of Tree Adjoining Languages (TAL's)  MCTAL's  and Head Languages (HL's) can be proved using this property  with suitable restrictions on the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'132'", "'101'", "'96'", "'94'"]
'143'
'132'
'101'
'96'
'94'
['143', '132', '101', '96', '94']
parsed_discourse_facet ['method_citation']
<S sid ="89" ssid = "74">Hence  trees shown in Figure 8 can not be generated by any MCTAG (but can be generated by an IG) because the number of pairs of dependent paths grows with n. Since the derivation tees of TAG's  MCTAG's  and HG's are local sets  the choice of the structure used at each point in a derivation in these systems does not depend on the context at that point within the derivation.</S><S sid ="164" ssid = "49">Although embedding this version of LCFRS's in the framework of ILFP developed by Rounds (1985) is straightforward  our motivation was to capture properties shared by a family of grammatical systems and generalize them defining a class of related formalisms.</S><S sid ="17" ssid = "2">We define the path set of a tree 1 as the set of strings that label a path from the root to frontier of 7.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="26" ssid = "11">The productions of HG's are very similar to those of CFG's except that the operation used must be made explicit.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'164'", "'17'", "'143'", "'26'"]
'89'
'164'
'17'
'143'
'26'
['89', '164', '17', '143', '26']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">For example  Gazdar (1985) discusses the applicability of Indexed Grammars (IG's) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar (LFG) and Government and Bindings grammars (GB).</S><S sid ="91" ssid = "76">We characterize a class of formalisms that have this property in Section 4.</S><S sid ="68" ssid = "53">This kind of dependency arises from the use of the composition operation to compose two arbitrarily large categories.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="189" ssid = "74">For rules p : A fpo such that fp is constant function  giving an elementary structure  fp is defined such that fp() = (Si ... xi() where each z is a constant string.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'91'", "'68'", "'143'", "'189'"]
'6'
'91'
'68'
'143'
'189'
['6', '91', '68', '143', '189']
parsed_discourse_facet ['results_citation']
<S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="90" ssid = "75">Thus  as in CFG's  at any point in the derivation  the set of structures that can be applied is determined only by a finite set of rules encapsulated by the grammar.</S><S sid ="185" ssid = "70">Each spawned process must check if xi     xn  and   yn  can be derived from B and C  respectively.</S><S sid ="141" ssid = "26">Thus  the length of any string in L is a linear combination of the length of strings in some fixed finite subset of L  and thus L is said to have the constant growth property.</S><S sid ="158" ssid = "43">We can represent any derived tree of a TAG by the two substrings that appear in its frontier  and then define how the adjunction operation concatenates the substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'90'", "'185'", "'141'", "'158'"]
'151'
'90'
'185'
'141'
'158'
['151', '90', '185', '141', '158']
parsed_discourse_facet ['implication_citation']
<S sid ="156" ssid = "41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S><S sid ="106" ssid = "12">We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths.</S><S sid ="32" ssid = "17">When 7' is adjoined at ?I in the tree 7 we obtain a tree v&quot;.</S><S sid ="12" ssid = "10">We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.</S><S sid ="55" ssid = "40">Each production can push or pop symbols on the stack as can be seen in the following productions that generate tree of the form shown in Figure 4b.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'", "'106'", "'32'", "'12'", "'55'"]
'156'
'106'
'32'
'12'
'55'
['156', '106', '32', '12', '55']
parsed_discourse_facet ['method_citation']



P87-1015
P07-1021
0
method_citation
['method_citation']
parsing: input/ref/Task1/A00-2018_akanksha.csv
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'"]
'90'
'91'
['90', '91']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="17">Maximum-entropy models have two benefits for a parser builder.</S>
    <S sid="49" ssid="18">First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &amp;quot;features&amp;quot; suggests that the probability model should be easily changeable &#8212; just change the set of features used.</S>
    <S sid="51" ssid="20">Second, and this is a point we have not yet mentioned, the features used in these models need have no particular independence of one another.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'49'", "'51'"]
'48'
'49'
'51'
['48', '49', '51']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
    <S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
    <S sid="93" ssid="4">This allows the second pass to see expansions not present in the training corpus.</S>
    <S sid="94" ssid="5">We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'", "'92'", "'93'", "'94'"]
'90'
'91'
'92'
'93'
'94'
['90', '91', '92', '93', '94']
parsed_discourse_facet ['method_citation']
NA
original cit marker offset is 0
new cit marker offset is 0



['0']
0
['0']
Error in Reference Offset
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
    <S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
    <S sid="93" ssid="4">This allows the second pass to see expansions not present in the training corpus.</S>
    <S sid="94" ssid="5">We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'", "'92'", "'93'", "'94'"]
'90'
'91'
'92'
'93'
'94'
['90', '91', '92', '93', '94']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="7">To compute a probability in a log-linear model one first defines a set of &amp;quot;features&amp;quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.</S>
    <S sid="39" ssid="8">In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.</S>
    <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'39'", "'40'"]
'38'
'39'
'40'
['38', '39', '40']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['result_citation']
<S sid="85" ssid="54">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models, this simplifies the model significantly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="32">As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &amp;quot;before&amp;quot;), and the label of the grandparent of c (la).</S><S sid="143" ssid="34">The first is simply that if we first guess the pre-terminal, when we go to guess the head the first thing we can condition upon is the pre-terminal, i.e., we compute p(h I t).</S><S sid="146" ssid="37">The second major reason why first guessing the pre-terminal makes so much difference is that it can be used when backing off the lexical head in computing the probability of the rule expansion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'143'", "'146'"]
'63'
'143'
'146'
['63', '143', '146']
parsed_discourse_facet ['method_citation']
???<S sid="78" ssid="47">With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.</S>                                        <S sid="79" ssid="48">We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.</S>
original cit marker offset is 0
new cit marker offset is 0



["'78'", "'79'"]
'78'
'79'
['78', '79']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/A00-2018.csv
<S sid ="114" ssid = "5">That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.</S><S sid ="159" ssid = "50">Something very much like this is done in [15].</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="165" ssid = "56">Note that we also tried including this information using a standard deleted-interpolation model.</S><S sid ="163" ssid = "54">Next we add the less obvious conditioning events noted in our previous discussion of the final model  grandparent label lg and left sibling label /b.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'159'", "'2'", "'165'", "'163'"]
'114'
'159'
'2'
'165'
'163'
['114', '159', '2', '165', '163']
parsed_discourse_facet ['implication_citation']
<S sid ="114" ssid = "5">That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.</S><S sid ="3" ssid = "3">The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.</S><S sid ="99" ssid = "10">Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S><S sid ="129" ssid = "20">It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments)  it does not guess the pre-terminal before guessing the lexical head  and it uses a tree-bank grammar rather than a Markov grammar.</S><S sid ="92" ssid = "3">For runs with the generative model based upon Markov grammar statistics  the first pass uses the same statistics  but conditioned only on standard PCFG information.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'3'", "'99'", "'129'", "'92'"]
'114'
'3'
'99'
'129'
'92'
['114', '3', '99', '129', '92']
parsed_discourse_facet ['implication_citation']
<S sid ="132" ssid = "23">Between the Old model and the Best model  Figure 2 gives precision/recall measurements for several different versions of our parser.</S><S sid ="69" ssid = "38">For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.</S><S sid ="184" ssid = "11">The first is the slight  but important  improvement achieved by using this model over conventional deleted interpolation  as indicated in Figure 2.</S><S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="85" ssid = "54">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models  this simplifies the model significantly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'132'", "'69'", "'184'", "'157'", "'85'"]
'132'
'69'
'184'
'157'
'85'
['132', '69', '184', '157', '85']
parsed_discourse_facet ['results_citation']
<S sid ="137" ssid = "28">However  Collins in [10] does not stress the decision to guess the head's pre-terminal first  and it might be lost on the casual reader.</S><S sid ="51" ssid = "20">Second  and this is a point we have not yet mentioned  the features used in these models need have no particular independence of one another.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="58" ssid = "27">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid ="150" ssid = "41">The second modification is the explicit marking of noun and verb-phrase coordination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'51'", "'2'", "'58'", "'150'"]
'137'
'51'
'2'
'58'
'150'
['137', '51', '2', '58', '150']
parsed_discourse_facet ['method_citation']
<S sid ="49" ssid = "18">First  as already implicit in our discussion  factoring the probability computation into a sequence of values corresponding to various &quot;features&quot; suggests that the probability model should be easily changeable  just change the set of features used.</S><S sid ="4" ssid = "4">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c  information outside c that our probability model deems important in determining the probability in question.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="165" ssid = "56">Note that we also tried including this information using a standard deleted-interpolation model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'4'", "'13'", "'2'", "'165'"]
'49'
'4'
'13'
'2'
'165'
['49', '4', '13', '2', '165']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "3">Also  remember that H is a pla ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.</S><S sid ="11" ssid = "7">What fundamentally distinguishes probabilistic generative parsers is how they compute p(r)  and it is to that topic we turn next.</S><S sid ="96" ssid = "7">As noted above  the probability model uses five smoothed probability distributions  one each for Li  M Ri t  and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7.</S><S sid ="99" ssid = "10">Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S><S sid ="19" ssid = "8">The method we use follows that of [10].</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'11'", "'96'", "'99'", "'19'"]
'34'
'11'
'96'
'99'
'19'
['34', '11', '96', '99', '19']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.</S><S sid ="185" ssid = "12">We expect that as we experiment with other  more semantic conditioning information  the importance of this aspect of the model will increase.</S><S sid ="80" ssid = "49">(Our experience is that rather than requiring 50 or so iterations  three suffice.)</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="144" ssid = "35">This quantity is a relatively intuitive one (as  for example  it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it  in effect  as the unsmoothed probability upon which all smoothing of p(h) is based.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'185'", "'80'", "'54'", "'144'"]
'3'
'185'
'80'
'54'
'144'
['3', '185', '80', '54', '144']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "16">The intuitive idea is that each factor gi is larger than one if the feature in question makes the probability more likely  one if the feature has no effect  and smaller than one if it makes the probability less likely.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S><S sid ="85" ssid = "54">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models  this simplifies the model significantly.</S><S sid ="22" ssid = "11">For us the non-terminal symbols are those of the tree-bank  augmented by the symbols aux and auxg  which have been assigned deterministically to certain auxiliary verbs such as &quot;have&quot; or &quot;having&quot;.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'2'", "'41'", "'85'", "'22'"]
'47'
'2'
'41'
'85'
'22'
['47', '2', '41', '85', '22']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="55" ssid = "24">This method is known as &quot;deleted interpolation&quot; smoothing.</S><S sid ="66" ssid = "35">In many cases this is clearly warranted.</S><S sid ="4" ssid = "4">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid ="121" ssid = "12">Without these enhancements Char97 performs at the 86.6% level for sentences of length < 40.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'55'", "'66'", "'4'", "'121'"]
'157'
'55'
'66'
'4'
'121'
['157', '55', '66', '4', '121']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="0" ssid = "0">A Maximum-Entropy-Inspired Parser *</S><S sid ="37" ssid = "6">Rather  we concentrate on the aspects of these models that most directly influenced the model presented here.</S><S sid ="21" ssid = "10">(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'157'", "'0'", "'37'", "'21'"]
'2'
'157'
'0'
'37'
'21'
['2', '157', '0', '37', '21']
parsed_discourse_facet ['implication_citation']
<S sid ="48" ssid = "17">Maximum-entropy models have two benefits for a parser builder.</S><S sid ="21" ssid = "10">(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="177" ssid = "4">The results reported here disprove this conjecture.</S><S sid ="159" ssid = "50">Something very much like this is done in [15].</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'21'", "'54'", "'177'", "'159'"]
'48'
'21'
'54'
'177'
'159'
['48', '21', '54', '177', '159']
parsed_discourse_facet ['method_citation']
<S sid ="35" ssid = "4">In the past few years the maximum entropy  or log-linear  approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing [1 17].</S><S sid ="121" ssid = "12">Without these enhancements Char97 performs at the 86.6% level for sentences of length < 40.</S><S sid ="69" ssid = "38">For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.</S><S sid ="138" ssid = "29">Indeed  it was lost on the present author until he went back after the fact and found it there.</S><S sid ="114" ssid = "5">That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'", "'121'", "'69'", "'138'", "'114'"]
'35'
'121'
'69'
'138'
'114'
['35', '121', '69', '138', '114']
parsed_discourse_facet ['method_citation']
<S sid ="169" ssid = "60">Up to this point all the models considered in this section are tree-bank grammar models.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="134" ssid = "25">As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.</S><S sid ="99" ssid = "10">Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S><S sid ="155" ssid = "46">For example  in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'", "'125'", "'134'", "'99'", "'155'"]
'169'
'125'
'134'
'99'
'155'
['169', '125', '134', '99', '155']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid ="34" ssid = "3">Also  remember that H is a pla ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.</S><S sid ="165" ssid = "56">Note that we also tried including this information using a standard deleted-interpolation model.</S><S sid ="69" ssid = "38">For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.</S><S sid ="0" ssid = "0">A Maximum-Entropy-Inspired Parser *</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'34'", "'165'", "'69'", "'0'"]
'7'
'34'
'165'
'69'
'0'
['7', '34', '165', '69', '0']
parsed_discourse_facet ['results_citation']
<S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S><S sid ="37" ssid = "6">Rather  we concentrate on the aspects of these models that most directly influenced the model presented here.</S><S sid ="169" ssid = "60">Up to this point all the models considered in this section are tree-bank grammar models.</S><S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'", "'37'", "'169'", "'7'", "'125'"]
'41'
'37'
'169'
'7'
'125'
['41', '37', '169', '7', '125']
parsed_discourse_facet ['implication_citation']
<S sid ="88" ssid = "57">While we could have smoothed in the same fashion  we choose instead to use standard deleted interpolation.</S><S sid ="124" ssid = "15">We note here that this corpus is somewhat more difficult than the &quot;official&quot; test corpus.</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="21" ssid = "10">(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)</S><S sid ="63" ssid = "32">As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'124'", "'54'", "'21'", "'63'"]
'88'
'124'
'54'
'21'
'63'
['88', '124', '54', '21', '63']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



A00-2018
N03-2024
0
method_citation
['method_citation']
parsing: input/ref/Task1/A00-2030_aakansha.csv
<S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="9">By necessity, we adopted the strategy of hand marking only the semantics.</S>
    <S sid="50" ssid="10">Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'50'"]
'49'
'50'
['49', '50']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="8">Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'"]
'16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S><S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'", "'33'", "'34'"]
'23'
'24'
'33'
'34'
['23', '24', '33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="60" ssid="1">In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).</S>
    <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'61'"]
'60'
'61'
['60', '61']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'"]
'33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).</S>
    <S sid="12" ssid="2">The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).</S><S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'12'", "'16'"]
'11'
'12'
'16'
['11', '12', '16']
parsed_discourse_facet ['method_citation']
<S sid="105" ssid="2">A single model proved capable of performing all necessary sentential processing, both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'"]
'105'
['105']
parsed_discourse_facet ['result_citation']
<S sid="60" ssid="1">In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).</S>
    <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'61'"]
'60'
'61'
['60', '61']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'"]
'16'
['16']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="102" ssid = "7">The results are summarized in Table 2.</S><S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'79'", "'82'", "'102'", "'56'"]
'68'
'79'
'82'
'102'
'56'
['68', '79', '82', '102', '56']
parsed_discourse_facet ['implication_citation']
<S sid ="65" ssid = "6">We illustrate the generation process by walking through a few of the steps of the parse shown in Figure 3.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="9" ssid = "7">Manually creating sourcespecific training data for syntax was not required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'96'", "'23'", "'88'", "'9'"]
'65'
'96'
'23'
'88'
'9'
['65', '96', '23', '88', '9']
parsed_discourse_facet ['implication_citation']
<S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'68'", "'36'", "'6'", "'96'"]
'88'
'68'
'36'
'6'
'96'
['88', '68', '36', '6', '96']
parsed_discourse_facet ['results_citation']
<S sid ="60" ssid = "1">In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'62'", "'13'", "'36'", "'105'"]
'60'
'62'
'13'
'36'
'105'
['60', '62', '13', '36', '105']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="47" ssid = "7">It soon became painfully obvious that this task could not be performed in the available time.</S><S sid ="73" ssid = "14">We now briefly summarize the probability structure of the model.</S><S sid ="97" ssid = "2">The evaluation results are summarized in Table 1.</S><S sid ="80" ssid = "2">However  because these estimates are too sparse to be relied upon  we use interpolated estimates consisting of mixtures of successively lowerorder estimates (as in Placeway et al. 1993).</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'47'", "'73'", "'97'", "'80'"]
'38'
'47'
'73'
'97'
'80'
['38', '47', '73', '97', '80']
parsed_discourse_facet ['method_citation']
<S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'", "'15'", "'45'", "'88'", "'56'"]
'79'
'15'
'45'
'88'
'56'
['79', '15', '45', '88', '56']
parsed_discourse_facet ['method_citation']
<S sid ="49" ssid = "9">By necessity  we adopted the strategy of hand marking only the semantics.</S><S sid ="70" ssid = "11">Post-modifier constituents for the PER/NP.</S><S sid ="69" ssid = "10">8.</S><S sid ="95" ssid = "14">The semantics  that is  the entities and relations  can then be directly extracted from these sentential trees.</S><S sid ="111" ssid = "3">The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies  either expressed or implied  of the Defense Advanced Research Projects Agency or the United States Government.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'70'", "'69'", "'95'", "'111'"]
'49'
'70'
'69'
'95'
'111'
['49', '70', '69', '95', '111']
parsed_discourse_facet ['method_citation']
<S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="4" ssid = "2">Yet  relatively few have embedded one of these algorithms in a task.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="67" ssid = "8">We pick up the derivation just after the topmost S and its head word  said  have been produced.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'79'", "'4'", "'82'", "'67'"]
'23'
'79'
'4'
'82'
'67'
['23', '79', '4', '82', '67']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "3">However  pipelined architectures suffer from a serious disadvantage: errors accumulate as they propagate through the pipeline.</S><S sid ="31" ssid = "14">If the single generalized model could then be extended to semantic analysis  all necessary sentence level processing would be contained in that model.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="84" ssid = "3">Although mathematically the model predicts tree elements in a top-down fashion  we search the space bottom-up using a chartbased search.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'31'", "'0'", "'38'", "'84'"]
'20'
'31'
'0'
'38'
'84'
['20', '31', '0', '38', '84']
parsed_discourse_facet ['method_citation']
<S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'38'", "'107'", "'104'", "'23'"]
'88'
'38'
'107'
'104'
'23'
['88', '38', '107', '104', '23']
parsed_discourse_facet ['implication_citation']
<S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="74" ssid = "15">The categories for head constituents  cl are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c _1  and the head word of their parent  wp.</S><S sid ="69" ssid = "10">8.</S><S sid ="111" ssid = "3">The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies  either expressed or implied  of the Defense Advanced Research Projects Agency or the United States Government.</S>
original cit marker offset is 0
new cit marker offset is 0



["'82'", "'45'", "'74'", "'69'", "'111'"]
'82'
'45'
'74'
'69'
'111'
['82', '45', '74', '69', '111']
parsed_discourse_facet ['method_citation']
<S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="102" ssid = "7">The results are summarized in Table 2.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'", "'102'", "'82'", "'62'", "'68'"]
'45'
'102'
'82'
'62'
'68'
['45', '102', '82', '62', '68']
parsed_discourse_facet ['method_citation']
<S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="97" ssid = "2">The evaluation results are summarized in Table 1.</S><S sid ="75" ssid = "16">Separate probabilities are maintained for left (pre) and right (post) modifiers: Part-of-speech tags  t    for modifiers are predicted based on the modifier  cm  the partof-speech tag of the head word  th  and the head word itself  wh: Head words  w for modifiers are predicted based on the modifier  cm  the part-of-speech tag of the modifier word   t the part-ofspeech tag of the head word   th  and the head word itself  wh: lAwmicm tm th wh)  e.g.</S><S sid ="95" ssid = "14">The semantics  that is  the entities and relations  can then be directly extracted from these sentential trees.</S><S sid ="59" ssid = "5">These labels serve to form a continuous chain between the relation and its argument.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'", "'97'", "'75'", "'95'", "'59'"]
'45'
'97'
'75'
'95'
'59'
['45', '97', '75', '95', '59']
parsed_discourse_facet ['method_citation']
<S sid ="76" ssid = "17">Finally  word features  fm  for modifiers are predicted based on the modifier  cm  the partof-speech tag of the modifier word   t the part-of-speech tag of the head word th  the head word itself  wh  and whether or not the modifier head word  w is known or unknown.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="112" ssid = "4">We thank Michael Collins of the University of Pennsylvania for his valuable suggestions.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'", "'36'", "'112'", "'101'", "'5'"]
'76'
'36'
'112'
'101'
'5'
['76', '36', '112', '101', '5']
parsed_discourse_facet ['results_citation']
<S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'68'", "'13'", "'38'", "'23'"]
'56'
'68'
'13'
'38'
'23'
['56', '68', '13', '38', '23']
parsed_discourse_facet ['implication_citation']
<S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'62'", "'36'", "'79'", "'107'"]
'51'
'62'
'36'
'79'
'107'
['51', '62', '36', '79', '107']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="49" ssid = "9">By necessity  we adopted the strategy of hand marking only the semantics.</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="39" ssid = "7">For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'49'", "'38'", "'5'", "'39'"]
'6'
'49'
'38'
'5'
'39'
['6', '49', '38', '5', '39']
parsed_discourse_facet ['results_citation']
<S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="31" ssid = "14">If the single generalized model could then be extended to semantic analysis  all necessary sentence level processing would be contained in that model.</S><S sid ="83" ssid = "2">More precisely  it must find the most likely augmented parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'57'", "'88'", "'31'", "'83'"]
'23'
'57'
'88'
'31'
'83'
['23', '57', '88', '31', '83']
parsed_discourse_facet ['aim_citation']
<S sid ="95" ssid = "14">The semantics  that is  the entities and relations  can then be directly extracted from these sentential trees.</S><S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="34" ssid = "2">In these trees  the standard TREEBANK structures are augmented to convey semantic information  that is  entities and relations.</S><S sid ="4" ssid = "2">Yet  relatively few have embedded one of these algorithms in a task.</S><S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'45'", "'34'", "'4'", "'6'"]
'95'
'45'
'34'
'4'
'6'
['95', '45', '34', '4', '6']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W06-3114_sweta.csv
 <S sid="108" ssid="1">The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="27">For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="44">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages, as demonstrated by the results for English-German and English-French.</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
 <S sid="172" ssid="3">Due to many similarly performing systems, we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
 <S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["36'"]
36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="103" ssid="19">Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["103'"]
103'
['103']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="60">One annotator suggested that this was the case for as much as 10% of our test sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["102'"]
102'
['102']
parsed_discourse_facet ['method_citation']
<S sid="123" ssid="16">For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.</S>
original cit marker offset is 0
new cit marker offset is 0



["123'"]
123'
['123']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="27">For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="1">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="19">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["126'"]
126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="173" ssid="4">The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["173'"]
173'
['173']
parsed_discourse_facet ['method_citation']
 <S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["170'"]
170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="84" ssid="23">The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["84'"]
84'
['84']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="22" ssid = "15">The text type are editorials instead of speech transcripts.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S><S sid ="11" ssid = "4">To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.</S><S sid ="167" ssid = "60">One annotator suggested that this was the case for as much as 10% of our test sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'22'", "'155'", "'11'", "'167'"]
'59'
'22'
'155'
'11'
'167'
['59', '22', '155', '11', '167']
parsed_discourse_facet ['implication_citation']
<S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="34" ssid = "27">For more on the participating systems  please refer to the respective system description in the proceedings of the workshop.</S><S sid ="89" ssid = "5">In words  the judgements are normalized  so that the average normalized judgement per judge is 3.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S><S sid ="160" ssid = "53">Annotators suggested that long sentences are almost impossible to judge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'34'", "'89'", "'155'", "'160'"]
'136'
'34'
'89'
'155'
'160'
['136', '34', '89', '155', '160']
parsed_discourse_facet ['implication_citation']
<S sid ="27" ssid = "20">Microsofts approach uses dependency trees  others use hierarchical phrase models.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="115" ssid = "8">Often  two systems can not be distinguished with a confidence of over 95%  so there are ranked the same.</S><S sid ="15" ssid = "8">Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.</S><S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'", "'1'", "'115'", "'15'", "'59'"]
'27'
'1'
'115'
'15'
'59'
['27', '1', '115', '15', '59']
parsed_discourse_facet ['results_citation']
<S sid ="140" ssid = "33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid ="22" ssid = "15">The text type are editorials instead of speech transcripts.</S><S sid ="26" ssid = "19">Most of these groups follow a phrase-based statistical approach to machine translation.</S><S sid ="82" ssid = "21">This decreases the statistical significance of our results compared to those studies.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'22'", "'26'", "'82'", "'136'"]
'140'
'22'
'26'
'82'
'136'
['140', '22', '26', '82', '136']
parsed_discourse_facet ['method_citation']
<S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="131" ssid = "24">The manual scores are averages over the raw unnormalized scores.</S><S sid ="119" ssid = "12">There may be occasionally a system clearly at the top or at the bottom  but most systems are so close that it is hard to distinguish them.</S><S sid ="44" ssid = "10">We computed BLEU scores for each submission with a single reference translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'84'", "'131'", "'119'", "'44'"]
'125'
'84'
'131'
'119'
'44'
['125', '84', '131', '119', '44']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "12">We aligned the texts at a sentence level across all four languages  resulting in 1064 sentence per language.</S><S sid ="63" ssid = "2">Many human evaluation metrics have been proposed.</S><S sid ="135" ssid = "28">The differences in difficulty are better reflected in the BLEU scores than in the raw un-normalized manual judgements.</S><S sid ="11" ssid = "4">To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'63'", "'135'", "'11'", "'1'"]
'19'
'63'
'135'
'11'
'1'
['19', '63', '135', '11', '1']
parsed_discourse_facet ['method_citation']
<S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="51" ssid = "17">When dropping the top and bottom 2.5% the remaining BLEU scores define the range of the confidence interval.</S><S sid ="146" ssid = "39">This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.</S><S sid ="139" ssid = "32">Given the closeness of most systems and the wide over-lapping confidence intervals it is hard to make strong statements about the correlation between human judgements and automatic scoring methods such as BLEU.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'51'", "'146'", "'139'", "'1'"]
'163'
'51'
'146'
'139'
'1'
['163', '51', '146', '139', '1']
parsed_discourse_facet ['method_citation']
<S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="67" ssid = "6">Participants and other volunteers contributed about 180 hours of labor in the manual evaluation.</S><S sid ="26" ssid = "19">Most of these groups follow a phrase-based statistical approach to machine translation.</S><S sid ="55" ssid = "21">If one system is better in 95% of the sample sets  we conclude that its higher BLEU score is statistically significantly better.</S><S sid ="89" ssid = "5">In words  the judgements are normalized  so that the average normalized judgement per judge is 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'67'", "'26'", "'55'", "'89'"]
'163'
'67'
'26'
'55'
'89'
['163', '67', '26', '55', '89']
parsed_discourse_facet ['method_citation']
<S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="121" ssid = "14">For the automatic scoring method BLEU  we can distinguish three quarters of the systems.</S><S sid ="161" ssid = "54">Since all long sentence translation are somewhat muddled  even a contrastive evaluation between systems was difficult.</S><S sid ="30" ssid = "23">The other half was replaced by other participants  so we ended up with roughly the same number.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'136'", "'121'", "'161'", "'30'"]
'84'
'136'
'121'
'161'
'30'
['84', '136', '121', '161', '30']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="50" ssid = "16">Following this method  we repeatedly  say  1000 times  sample sets of sentences from the output of each system  measure their BLEU score  and use these 1000 BLEU scores as basis for estimating a confidence interval.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="83" ssid = "22">The number of judgements is additionally fragmented by our breakup of sentences into in-domain and out-of-domain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'163'", "'50'", "'84'", "'83'"]
'136'
'163'
'50'
'84'
'83'
['136', '163', '50', '84', '83']
parsed_discourse_facet ['implication_citation']
<S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="119" ssid = "12">There may be occasionally a system clearly at the top or at the bottom  but most systems are so close that it is hard to distinguish them.</S><S sid ="82" ssid = "21">This decreases the statistical significance of our results compared to those studies.</S><S sid ="43" ssid = "9">At the very least  we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.</S><S sid ="10" ssid = "3">Figure 1 provides some statistics about this corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'119'", "'82'", "'43'", "'10'"]
'59'
'119'
'82'
'43'
'10'
['59', '119', '82', '43', '10']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "8">We settled on contrastive evaluations of 5 system outputs for a single test sentence.</S><S sid ="99" ssid = "15">Systems that generally do worse than others will receive a negative one.</S><S sid ="112" ssid = "5">The normalization on a per-judge basis gave very similar ranking  only slightly less consistent with the ranking from the pairwise comparisons.</S><S sid ="62" ssid = "1">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems  they are only a imperfect substitute for human assessment of translation quality  or as the acronym BLEU puts it  a bilingual evaluation understudy.</S><S sid ="98" ssid = "14">Systems that generally do better than others will receive a positive average normalizedjudgement per sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'99'", "'112'", "'62'", "'98'"]
'69'
'99'
'112'
'62'
'98'
['69', '99', '112', '62', '98']
parsed_discourse_facet ['method_citation']
<S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="110" ssid = "3">In the graphs  system scores are indicated by a point  the confidence intervals by shaded areas around the point.</S><S sid ="146" ssid = "39">This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.</S><S sid ="90" ssid = "6">Another way to view the judgements is that they are less quality judgements of machine translation systems per se  but rankings of machine translation systems.</S><S sid ="33" ssid = "26">While building a machine translation system is a serious undertaking  in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'110'", "'146'", "'90'", "'33'"]
'125'
'110'
'146'
'90'
'33'
['125', '110', '146', '90', '33']
parsed_discourse_facet ['method_citation']
<S sid ="98" ssid = "14">Systems that generally do better than others will receive a positive average normalizedjudgement per sentence.</S><S sid ="148" ssid = "41">The best answer to this is: many research labs have very competitive systems whose performance is hard to tell apart.</S><S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S><S sid ="37" ssid = "3">It rewards matches of n-gram sequences  but measures only at most indirectly overall grammatical coherence.</S><S sid ="102" ssid = "18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems  we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'98'", "'148'", "'6'", "'37'", "'102'"]
'98'
'148'
'6'
'37'
'102'
['98', '148', '6', '37', '102']
parsed_discourse_facet ['results_citation']
<S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="177" ssid = "1">This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency  Contract No.</S><S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="3" ssid = "1">was done by the participants.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'177'", "'59'", "'1'", "'3'"]
'125'
'177'
'59'
'1'
'3'
['125', '177', '59', '1', '3']
parsed_discourse_facet ['implication_citation']
<S sid ="90" ssid = "6">Another way to view the judgements is that they are less quality judgements of machine translation systems per se  but rankings of machine translation systems.</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="135" ssid = "28">The differences in difficulty are better reflected in the BLEU scores than in the raw un-normalized manual judgements.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'0'", "'125'", "'126'", "'135'"]
'90'
'0'
'125'
'126'
'135'
['90', '0', '125', '126', '135']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "17">When dropping the top and bottom 2.5% the remaining BLEU scores define the range of the confidence interval.</S><S sid ="39" ssid = "5">However  a recent study (Callison-Burch et al.  2006)  pointed out that this correlation may not always be strong.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S><S sid ="26" ssid = "19">Most of these groups follow a phrase-based statistical approach to machine translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'39'", "'1'", "'162'", "'26'"]
'51'
'39'
'1'
'162'
'26'
['51', '39', '1', '162', '26']
parsed_discourse_facet ['results_citation']
<S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="112" ssid = "5">The normalization on a per-judge basis gave very similar ranking  only slightly less consistent with the ranking from the pairwise comparisons.</S><S sid ="174" ssid = "5">The manual evaluation of scoring translation on a graded scale from 15 seems to be very hard to perform.</S><S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'112'", "'174'", "'59'", "'125'"]
'136'
'112'
'174'
'59'
'125'
['136', '112', '174', '59', '125']
parsed_discourse_facet ['aim_citation']



W06-3114
D07-1092
0
method_citation
['implication_citation']
parsing: input/ref/Task1/J01-2004_swastika.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="3">In the past few years, however, some improvements have been made over these language models through the use of statistical methods of natural language processing, and the development of innovative, linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
<S sid="215" ssid="119">The first word in the string remaining to be parsed, w1, we will call the look-ahead word.</S>
original cit marker offset is 0
new cit marker offset is 0



['215']
215
['215']
parsed_discourse_facet ['method_citation']
    <S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



['302']
302
['302']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="100" ssid="4">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['method_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S sid ="321" ssid = "77">In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.</S><S sid ="387" ssid = "143">Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.</S><S sid ="315" ssid = "71">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;the sum of the probabilities over the vocabulary is less than one.</S><S sid ="344" ssid = "100">However  when we interpolate with the trigram  we see that the additional improvement is greater than the one they experienced.</S><S sid ="403" ssid = "16">Perhaps some compromise between the fully connected structures and extreme underspecification will yield an efficiency improvement.</S>
original cit marker offset is 0
new cit marker offset is 0



["'321'", "'387'", "'315'", "'344'", "'403'"]
'321'
'387'
'315'
'344'
'403'
['321', '387', '315', '344', '403']
parsed_discourse_facet ['implication_citation']
<S sid ="380" ssid = "136">Future work will include more substantial word recognition experiments.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S><S sid ="344" ssid = "100">However  when we interpolate with the trigram  we see that the additional improvement is greater than the one they experienced.</S><S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S>
original cit marker offset is 0
new cit marker offset is 0



["'380'", "'280'", "'391'", "'344'", "'336'"]
'380'
'280'
'391'
'344'
'336'
['380', '280', '391', '344', '336']
parsed_discourse_facet ['implication_citation']
<S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="338" ssid = "94">Table 4 compares the perplexity of our model with Chelba and Jelinek (1998a  1998b) on the same training and testing corpora.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="326" ssid = "82">We obtained the training and testing corpora from them (which we will denote C&J corpus)  and also created intermediate corpora  upon which only the first two modifications were carried out (which we will denote no punct).</S><S sid ="343" ssid = "99">Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'</S>
original cit marker offset is 0
new cit marker offset is 0



["'349'", "'338'", "'280'", "'326'", "'343'"]
'349'
'338'
'280'
'326'
'343'
['349', '338', '280', '326', '343']
parsed_discourse_facet ['results_citation']
<S sid ="372" ssid = "128">The small size of our training data  as well as the fact that we are rescoring n-best lists  rather than working directly on lattices  makes comparison with the other models not particularly informative.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="309" ssid = "65">Let Ht be the priority queue H  before any processing has begun with word w  in the look-ahead.</S><S sid ="266" ssid = "22">From this set of measures  we will also include the crossing bracket scores: average crossing brackets (CB)  percentage of sentences with no crossing brackets (0 CB)  and the percentage of sentences with two crossing brackets or fewer (< 2 CB).</S><S sid ="268" ssid = "24">This is an incremental parser with a pruning strategy and no backtracking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'", "'301'", "'309'", "'266'", "'268'"]
'372'
'301'
'309'
'266'
'268'
['372', '301', '309', '266', '268']
parsed_discourse_facet ['method_citation']
<S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="355" ssid = "111">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task  we performed a very small preliminary experiment on n-best lists.</S><S sid ="340" ssid = "96">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid ="354" ssid = "110">Interestingly  at the word where the failure occurred  the sum of the probabilities was 0.9301.</S>
original cit marker offset is 0
new cit marker offset is 0



["'336'", "'301'", "'355'", "'340'", "'354'"]
'336'
'301'
'355'
'340'
'354'
['336', '301', '355', '340', '354']
parsed_discourse_facet ['method_citation']
<S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="361" ssid = "117">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S><S sid ="319" ssid = "75">The hope  however  is that the improved parsing model provided by our conditional probability model will cause the distribution over structures to be more peaked  thus enabling us to capture more of the total probability mass  and making this a fairly snug upper bound on the perplexity.</S><S sid ="355" ssid = "111">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task  we performed a very small preliminary experiment on n-best lists.</S>
original cit marker offset is 0
new cit marker offset is 0



["'301'", "'349'", "'361'", "'319'", "'355'"]
'301'
'349'
'361'
'319'
'355'
['301', '349', '361', '319', '355']
parsed_discourse_facet ['method_citation']
<S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="324" ssid = "80">Their modifications included: (i) removing orthographic cues to structure (e.g.  punctuation); (ii) replacing all numbers with the single token N; and (iii) closing the vocabulary at 10 000  replacing all other words with the UNK token.</S><S sid ="343" ssid = "99">Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'</S><S sid ="387" ssid = "143">Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.</S><S sid ="346" ssid = "102">These results are particularly remarkable  given that we did not build our model as a language model per se  but rather as a parsing model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'349'", "'324'", "'343'", "'387'", "'346'"]
'349'
'324'
'343'
'387'
'346'
['349', '324', '343', '387', '346']
parsed_discourse_facet ['method_citation']
<S sid ="315" ssid = "71">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;the sum of the probabilities over the vocabulary is less than one.</S><S sid ="377" ssid = "133">The point of this small experiment was to see if our parsing model could provide useful information even in the case that recognition errors occur  as opposed to the (generally) fully grammatical strings upon which the perplexity results were obtained.</S><S sid ="288" ssid = "44">Interestingly  conditioning all POS expansions on two c-commanding heads made no difference in accuracy compared to conditioning only leftmost POS expansions on a single c-commanding head; but it did improve the efficiency.</S><S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="382" ssid = "138">The base beam factor that we have used to this point is 10'  which is quite wide.</S>
original cit marker offset is 0
new cit marker offset is 0



["'315'", "'377'", "'288'", "'358'", "'382'"]
'315'
'377'
'288'
'358'
'382'
['315', '377', '288', '358', '382']
parsed_discourse_facet ['method_citation']
<S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="382" ssid = "138">The base beam factor that we have used to this point is 10'  which is quite wide.</S><S sid ="283" ssid = "39">Unlike the Roark and Johnson parser  however  our coverage did not substantially drop as the amount of conditioning information increased  and in some cases  coverage improved slightly.</S><S sid ="380" ssid = "136">Future work will include more substantial word recognition experiments.</S><S sid ="268" ssid = "24">This is an incremental parser with a pruning strategy and no backtracking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'301'", "'382'", "'283'", "'380'", "'268'"]
'301'
'382'
'283'
'380'
'268'
['301', '382', '283', '380', '268']
parsed_discourse_facet ['method_citation']
<S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S><S sid ="321" ssid = "77">In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.</S><S sid ="262" ssid = "18">Recall is the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD.</S>
original cit marker offset is 0
new cit marker offset is 0



["'336'", "'280'", "'391'", "'321'", "'262'"]
'336'
'280'
'391'
'321'
'262'
['336', '280', '391', '321', '262']
parsed_discourse_facet ['implication_citation']
<S sid ="315" ssid = "71">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;the sum of the probabilities over the vocabulary is less than one.</S><S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S><S sid ="295" ssid = "51">Our observed times look polynomial  which is to be expected given our pruning strategy: the denser the competitors within a narrow probability range of the best analysis  the more time will be spent working on these competitors; and the farther along in the sentence  the more chance for ambiguities that can lead to such a situation.</S><S sid ="382" ssid = "138">The base beam factor that we have used to this point is 10'  which is quite wide.</S><S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'315'", "'336'", "'295'", "'382'", "'391'"]
'315'
'336'
'295'
'382'
'391'
['315', '336', '295', '382', '391']
parsed_discourse_facet ['method_citation']
<S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="258" ssid = "14">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S><S sid ="372" ssid = "128">The small size of our training data  as well as the fact that we are rescoring n-best lists  rather than working directly on lattices  makes comparison with the other models not particularly informative.</S><S sid ="298" ssid = "54">What is perhaps surprising is that the difference is not greater.</S><S sid ="270" ssid = "26">In such a case  the parser fails to return a complete parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'301'", "'258'", "'372'", "'298'", "'270'"]
'301'
'258'
'372'
'298'
'270'
['301', '258', '372', '298', '270']
parsed_discourse_facet ['method_citation']
<S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S><S sid ="398" ssid = "11">By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="354" ssid = "110">Interestingly  at the word where the failure occurred  the sum of the probabilities was 0.9301.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'391'", "'398'", "'301'", "'354'", "'404'"]
'391'
'398'
'301'
'354'
'404'
['391', '398', '301', '354', '404']
parsed_discourse_facet ['method_citation']
<S sid ="402" ssid = "15">In fact  left-corner parsing can be simulated by a top-down parser by transforming the grammar  as was done in Roark and Johnson (1999)  and so an approach very similar to the one outlined here could be used in that case.</S><S sid ="398" ssid = "11">By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="371" ssid = "127">For our model and the Treebank trigram model  the LM weight that resulted in the lowest error rates is given.</S><S sid ="343" ssid = "99">Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'</S>
original cit marker offset is 0
new cit marker offset is 0



["'402'", "'398'", "'280'", "'371'", "'343'"]
'402'
'398'
'280'
'371'
'343'
['402', '398', '280', '371', '343']
parsed_discourse_facet ['results_citation']
<S sid ="398" ssid = "11">By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="361" ssid = "117">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S><S sid ="258" ssid = "14">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S>
original cit marker offset is 0
new cit marker offset is 0



["'398'", "'301'", "'390'", "'361'", "'258'"]
'398'
'301'
'390'
'361'
'258'
['398', '301', '390', '361', '258']
parsed_discourse_facet ['implication_citation']
<S sid ="258" ssid = "14">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S><S sid ="262" ssid = "18">Recall is the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD.</S><S sid ="361" ssid = "117">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S><S sid ="339" ssid = "95">We built an interpolated trigram model to serve as a baseline (as they did)  and also interpolated our model's perplexity with the trigram  using the same mixing coefficient as they did in their trials (taking 36 percent of the estimate from the trigram).'</S><S sid ="324" ssid = "80">Their modifications included: (i) removing orthographic cues to structure (e.g.  punctuation); (ii) replacing all numbers with the single token N; and (iii) closing the vocabulary at 10 000  replacing all other words with the UNK token.</S>
original cit marker offset is 0
new cit marker offset is 0



["'258'", "'262'", "'361'", "'339'", "'324'"]
'258'
'262'
'361'
'339'
'324'
['258', '262', '361', '339', '324']
parsed_discourse_facet ['method_citation']
<S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="257" ssid = "13">A constituent for evaluation purposes consists of a label (e.g.  NP) and a span (beginning and ending word positions).</S><S sid ="340" ssid = "96">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="387" ssid = "143">Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.</S>
original cit marker offset is 0
new cit marker offset is 0



["'349'", "'257'", "'340'", "'301'", "'387'"]
'349'
'257'
'340'
'301'
'387'
['349', '257', '340', '301', '387']
parsed_discourse_facet ['results_citation']
parsing: input/ref/Task1/D10-1044_swastika.csv
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
<S sid="71" ssid="8">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c&#955;(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



['71']
71
['71']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
<S sid="71" ssid="8">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c&#955;(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



['71']
71
['71']
parsed_discourse_facet ['result_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="9">An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (Och, 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



['45']
45
['45']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



['75']
75
['75']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
  <S sid="42" ssid="6">The natural baseline approach is to concatenate data from IN and OUT.</S>
original cit marker offset is 0
new cit marker offset is 0



['42']
42
['42']
parsed_discourse_facet ['aim_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/D10-1044.csv
<S sid ="32" ssid = "29">This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.</S><S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="101" ssid = "5">The second setting uses the news-related subcorpora for the NIST09 MT Chinese to English evaluation8 as IN  and the remaining NIST parallel Chinese/English corpora (UN  Hong Kong Laws  and Hong Kong Hansard) as OUT.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'2'", "'114'", "'101'", "'104'"]
'32'
'2'
'114'
'101'
'104'
['32', '2', '114', '101', '104']
parsed_discourse_facet ['implication_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="138" ssid = "7">However  for multinomial models like our LMs and TMs  there is a one to one correspondence between instances and features  eg the correspondence between a phrase pair (s  t) and its conditional multinomial probability p(s1t).</S><S sid ="144" ssid = "1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S><S sid ="133" ssid = "2">It is difficult to directly compare the Matsoukas et al results with ours  since our out-of-domain corpus is homogeneous; given heterogeneous training data  however  it would be trivial to include Matsoukas-style identity features in our instance-weighting model.</S><S sid ="99" ssid = "3">The dev and test sets were randomly chosen from the EMEA corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'138'", "'144'", "'133'", "'99'"]
'67'
'138'
'144'
'133'
'99'
['67', '138', '144', '133', '99']
parsed_discourse_facet ['implication_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="142" ssid = "11">There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).</S><S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'142'", "'8'", "'2'", "'38'"]
'67'
'142'
'8'
'2'
'38'
['67', '142', '8', '2', '38']
parsed_discourse_facet ['results_citation']
<S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="32" ssid = "29">This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.</S><S sid ="83" ssid = "20">We have not yet tried this.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daumes (2007) work that splits each feature into domain-specific and general copies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'32'", "'83'", "'78'", "'136'"]
'8'
'32'
'83'
'78'
'136'
['8', '32', '83', '78', '136']
parsed_discourse_facet ['method_citation']
<S sid ="133" ssid = "2">It is difficult to directly compare the Matsoukas et al results with ours  since our out-of-domain corpus is homogeneous; given heterogeneous training data  however  it would be trivial to include Matsoukas-style identity features in our instance-weighting model.</S><S sid ="118" ssid = "22">Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.</S><S sid ="5" ssid = "2">Even when there is training data available in the domain of interest  there is often additional data from other domains that could in principle be used to improve performance.</S><S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'118'", "'5'", "'1'", "'10'"]
'133'
'118'
'5'
'1'
'10'
['133', '118', '5', '1', '10']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="5" ssid = "2">Even when there is training data available in the domain of interest  there is often additional data from other domains that could in principle be used to improve performance.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daumes (2007) work that splits each feature into domain-specific and general copies.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S><S sid ="14" ssid = "11">There is a fairly large body of work on SMT adaptation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'5'", "'136'", "'104'", "'14'"]
'10'
'5'
'136'
'104'
'14'
['10', '5', '136', '104', '14']
parsed_discourse_facet ['method_citation']
<S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="95" ssid = "32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="125" ssid = "29">Somewhat surprisingly  there do not appear to be large systematic differences between linear and MAP combinations.</S><S sid ="20" ssid = "17">Daume (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'", "'95'", "'78'", "'125'", "'20'"]
'64'
'95'
'78'
'125'
'20'
['64', '95', '78', '125', '20']
parsed_discourse_facet ['method_citation']
<S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="36" ssid = "33">Section 5 covers relevant previous work on SMT adaptation  and section 6 concludes.</S><S sid ="19" ssid = "16">The idea of distinguishing between general and domain-specific examples is due to Daume and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.</S><S sid ="79" ssid = "16">This combination generalizes (2) and (3): we use either at = a to obtain a fixed-weight linear combination  or at = cI(t)/(cI(t) + 0) to obtain a MAP combination.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'36'", "'19'", "'79'", "'114'"]
'151'
'36'
'19'
'79'
'114'
['151', '36', '19', '79', '114']
parsed_discourse_facet ['method_citation']
<S sid ="118" ssid = "22">Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.</S><S sid ="50" ssid = "14">Linear weights are difficult to incorporate into the standard MERT procedure because they are hidden within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.</S><S sid ="88" ssid = "25">We have not explored this strategy.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S><S sid ="19" ssid = "16">The idea of distinguishing between general and domain-specific examples is due to Daume and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'", "'50'", "'88'", "'104'", "'19'"]
'118'
'50'
'88'
'104'
'19'
['118', '50', '88', '104', '19']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "15">For the LM  adaptive weights are set as follows: where  is a weight vector containing an element i for each domain (just IN and OUT in our case)  pi are the corresponding domain-specific models  and p(w  h) is an empirical distribution from a targetlanguage training corpuswe used the IN dev set for this.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the flattened variant described in section 3.2.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="118" ssid = "22">Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'114'", "'127'", "'67'", "'118'"]
'51'
'114'
'127'
'67'
'118'
['51', '114', '127', '67', '118']
parsed_discourse_facet ['implication_citation']
<S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="96" ssid = "33">We used it to score all phrase pairs in the OUT table  in order to provide a feature for the instance-weighting model.</S><S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="106" ssid = "10">The corpora for both settings are summarized in table 1.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'96'", "'128'", "'106'", "'111'"]
'1'
'96'
'128'
'106'
'111'
['1', '96', '128', '106', '111']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "7">Its success depends on the two domains being relatively close  and on the OUT corpus not being so large as to overwhelm the contribution of IN.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="41" ssid = "5">We do not adapt the alignment procedure for generating the phrase table from which the TM distributions are derived.</S><S sid ="60" ssid = "24">The matching sentence pairs are then added to the IN corpus  and the system is re-trained.</S><S sid ="0" ssid = "0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'64'", "'41'", "'60'", "'0'"]
'43'
'64'
'41'
'60'
'0'
['43', '64', '41', '60', '0']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="15" ssid = "12">We introduce several new ideas.</S><S sid ="71" ssid = "8">Finally  we incorporate the instance-weighting model into a general linear combination  and learn weights and mixing parameters simultaneously. where c(s  t) is a modified count for pair (s  t) in OUT  u(s|t) is a prior distribution  and y is a prior weight.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'67'", "'15'", "'71'", "'104'"]
'8'
'67'
'15'
'71'
'104'
['8', '67', '15', '71', '104']
parsed_discourse_facet ['method_citation']
<S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S><S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="43" ssid = "7">Its success depends on the two domains being relatively close  and on the OUT corpus not being so large as to overwhelm the contribution of IN.</S><S sid ="135" ssid = "4">Finally  we note that Jiangs instance-weighting framework is broader than we have presented above  encompassing among other possibilities the use of unlabelled IN data  which is applicable to SMT settings where source-only IN corpora are available.</S><S sid ="146" ssid = "3">The features are weighted within a logistic model to give an overall weight that is applied to the phrase pairs frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'", "'151'", "'43'", "'135'", "'146'"]
'104'
'151'
'43'
'135'
'146'
['104', '151', '43', '135', '146']
parsed_discourse_facet ['results_citation']
<S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="83" ssid = "20">We have not yet tried this.</S><S sid ="28" ssid = "25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'114'", "'83'", "'28'", "'17'"]
'16'
'114'
'83'
'28'
'17'
['16', '114', '83', '28', '17']
parsed_discourse_facet ['implication_citation']
<S sid ="89" ssid = "26">We used 22 features for the logistic weighting model  divided into two groups: one intended to reflect the degree to which a phrase pair belongs to general language  and one intended to capture similarity to the IN domain.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="110" ssid = "14">Je voudrais preciser  a` ladresse du commissaire Liikanen  quil nest pas aise de recourir aux tribunaux nationaux.</S><S sid ="109" ssid = "13"> I would also like to point out to commissioner Liikanen that it is not easy to take a matter to a national court.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'78'", "'110'", "'109'", "'67'"]
'89'
'78'
'110'
'109'
'67'
['89', '78', '110', '109', '67']
parsed_discourse_facet ['method_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="36" ssid = "33">Section 5 covers relevant previous work on SMT adaptation  and section 6 concludes.</S><S sid ="131" ssid = "35">The general-language features have a slight advantage over the similarity features  and both are better than the SVM feature.</S><S sid ="47" ssid = "11">Apart from MERT difficulties  a conceptual problem with log-linear combination is that it multiplies feature probabilities  essentially forcing different features to agree on high-scoring candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'64'", "'36'", "'131'", "'47'"]
'67'
'64'
'36'
'131'
'47'
['67', '64', '36', '131', '47']
parsed_discourse_facet ['results_citation']
<S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daumes (2007) work that splits each feature into domain-specific and general copies.</S><S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'136'", "'8'", "'11'", "'111'"]
'114'
'136'
'8'
'11'
'111'
['114', '136', '8', '11', '111']
parsed_discourse_facet ['aim_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S><S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="92" ssid = "29">6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.</S><S sid ="95" ssid = "32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'111'", "'128'", "'92'", "'95'"]
'67'
'111'
'128'
'92'
'95'
['67', '111', '128', '92', '95']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P11-1060_sweta.csv
<S sid="8" ssid="4">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
 <S sid="45" ssid="21">The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["45'"]
45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="27">It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent&#8217;s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
 <S sid="166" ssid="51">Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language&#8212;think grounded al., 2010). compositional semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
 <S sid="8" ssid="4">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
 <S sid="11" ssid="7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["11'"]
11'
['11']
parsed_discourse_facet ['method_citation']
  <S sid="115" ssid="91">After training, given a new utterance x, our system outputs the most likely y, summing out the latent logical form z: argmaxy p&#952;(T)(y  |x, z &#8712; &#732;ZL,&#952;(T)).</S>
original cit marker offset is 0
new cit marker offset is 0



["115'"]
115'
['115']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="70">We now turn to the task of mapping natural language For the example in Figure 4(b), the de- utterances to DCS trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["94'"]
94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="157" ssid="42">Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["157'"]
157'
['157']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["106'"]
106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="52">In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="23">Next, we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S>
original cit marker offset is 0
new cit marker offset is 0



["138'"]
138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="16">The CSP has two types of constraints: (i) x &#8712; w(p) for each node x labeled with predicate p &#8712; P; and (ii) xj = yj0 (the j-th component of x must equal the j'-th component of y) for each edge (x, y) labeled with j0j &#8712; R. A solution to the CSP is an assignment of nodes to values that satisfies all the constraints.</S>
original cit marker offset is 0
new cit marker offset is 0



["40'"]
40'
['40']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["171'"]
171'
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S sid ="57" ssid = "33">But consider Figure 4: (a) is headed by borders  but states needs to be extracted; in (b)  the quantifier no is syntactically dominated by the head verb borders but needs to take wider scope.</S><S sid ="39" ssid = "15">Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.</S><S sid ="60" ssid = "36">Then higher up in the tree  we invoke it with an execute relation Xi to create the desired semantic scope.2 This mark-execute construct acts non-locally  so to maintain compositionality  we must augment the denotation d = JzKw to include any information about the marked nodes in z that can be accessed by an execute relation later on.</S><S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="167" ssid = "52">In DCS  we start with lexical triggers  which are 6 Conclusion more basic than CCG lexical entries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'", "'39'", "'60'", "'22'", "'167'"]
'57'
'39'
'60'
'22'
'167'
['57', '39', '60', '22', '167']
parsed_discourse_facet ['implication_citation']
<S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="71" ssid = "47">Let z be a DCS tree.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="120" ssid = "5">GEO has 48 non-value predicates and JOBS has 26.</S><S sid ="85" ssid = "61">Extraction allows us to return the set of consistent values of a marked non-root node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'71'", "'141'", "'120'", "'85'"]
'15'
'71'
'141'
'120'
'85'
['15', '71', '141', '120', '85']
parsed_discourse_facet ['implication_citation']
<S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S><S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parents denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="39" ssid = "15">Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.</S><S sid ="147" ssid = "32">However  training on just these examples is enough to improve the parameters  and this 29% increases to 66% and then to 95% over the next few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'51'", "'3'", "'39'", "'147'"]
'46'
'51'
'3'
'39'
'147'
['46', '51', '3', '39', '147']
parsed_discourse_facet ['results_citation']
<S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parents denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid ="84" ssid = "60">There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.</S><S sid ="170" ssid = "55">Our features as soft preferences.</S><S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'84'", "'170'", "'46'", "'98'"]
'51'
'84'
'170'
'46'
'98'
['51', '84', '170', '46', '98']
parsed_discourse_facet ['method_citation']
<S sid ="122" ssid = "7">For JOBS  if we use the standard Jobs database  close to half the ys are empty  which makes it uninteresting.</S><S sid ="94" ssid = "70">We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.</S><S sid ="102" ssid = "78">As a running example  consider x = city that is in California and z = hcity; 11:hloc; 21:hCAiii  where city triggers city and California triggers CA.</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S><S sid ="95" ssid = "71">Our first question is: given notation of the DCS tree before execution is an utterance x  what trees z  Z are permissible?</S>
original cit marker offset is 0
new cit marker offset is 0



["'122'", "'94'", "'102'", "'98'", "'95'"]
'122'
'94'
'102'
'98'
'95'
['122', '94', '102', '98', '95']
parsed_discourse_facet ['method_citation']
<S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="84" ssid = "60">There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'", "'3'", "'6'", "'84'", "'162'"]
'22'
'3'
'6'
'84'
'162'
['22', '3', '6', '84', '162']
parsed_discourse_facet ['method_citation']
<S sid ="13" ssid = "9">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x  y)  which is much cheaper to obtain than (x  z) pairs.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="29" ssid = "5">Let P be a set of predicates (e.g.  state  count  P)  which are just symbols.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="172" ssid = "57">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms  we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus  and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'134'", "'29'", "'77'", "'172'"]
'13'
'134'
'29'
'77'
'172'
['13', '134', '29', '77', '172']
parsed_discourse_facet ['method_citation']
<S sid ="96" ssid = "72">To California cities)  and it also allows us to underspecify L. In particular  our L will not include verbs or prepositions; rather  we rely on the predicates corresponding to those words to be triggered by traces.</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S><S sid ="94" ssid = "70">We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.</S><S sid ="124" ssid = "9">For each data predicate p (e.g.  language)  we add each possible tuple (e.g.  (job37  Java)) to w(p) independently with probability 0.8.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S>
original cit marker offset is 0
new cit marker offset is 0



["'96'", "'88'", "'94'", "'124'", "'162'"]
'96'
'88'
'94'
'124'
'162'
['96', '88', '94', '124', '162']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "84">However  in order to learn  we need to sum over {z  ZL(x) : JzKw = y}  and unfortunately  the additional constraint JzKw = y does not factorize.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="84" ssid = "60">There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'3'", "'84'", "'134'", "'46'"]
'108'
'3'
'84'
'134'
'46'
['108', '3', '84', '134', '46']
parsed_discourse_facet ['method_citation']
<S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'", "'98'", "'77'", "'162'", "'88'"]
'22'
'98'
'77'
'162'
'88'
['22', '98', '77', '162', '88']
parsed_discourse_facet ['implication_citation']
<S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'134'", "'3'", "'141'", "'98'"]
'12'
'134'
'3'
'141'
'98'
['12', '134', '3', '141', '98']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="29" ssid = "5">Let P be a set of predicates (e.g.  state  count  P)  which are just symbols.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="129" ssid = "14">We also define an augmented lexicon L+ which includes a prototype word x for each predicate appearing in (iii) above (e.g.  (large  size))  which cancels the predicates triggered by xs POS tag.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'29'", "'77'", "'129'", "'8'"]
'3'
'29'
'77'
'129'
'8'
['3', '29', '77', '129', '8']
parsed_discourse_facet ['method_citation']
<S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S><S sid ="142" ssid = "27">This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.</S><S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="144" ssid = "29">Intuitions How is our system learning?</S><S sid ="112" ssid = "88">Our learning algorithm alternates between (i) using the current parameters  to generate the K-best set ZL (x) for each training example x  and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'142'", "'10'", "'144'", "'112'"]
'46'
'142'
'10'
'144'
'112'
['46', '142', '10', '144', '112']
parsed_discourse_facet ['method_citation']
<S sid ="133" ssid = "18">Table 2 shows that our system using lexical triggers L (henceforth  DCS) outperforms SEMRESP (78.9% over 73.2%).</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="163" ssid = "48">Feedback from the context; for example  the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x  y)  which means (Schuler  2003) and semantic parsing (Popescu et borders looks right for the first argument and left al.  2003; Clarke et al.  2010).</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'134'", "'163'", "'141'", "'10'"]
'133'
'134'
'163'
'141'
'10'
['133', '134', '163', '141', '10']
parsed_discourse_facet ['results_citation']
<S sid ="86" ssid = "62">Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  (i  )]{1 = }.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="113" ssid = "89">Formally  let O(  ') be the objective function O() with ZL(x) ZL I(x).</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'162'", "'77'", "'3'", "'113'"]
'86'
'162'
'77'
'3'
'113'
['86', '162', '77', '3', '113']
parsed_discourse_facet ['implication_citation']
<S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="55" ssid = "31">The tree structure still enables us to compute denotations efficiently based on (1) and (2).</S><S sid ="80" ssid = "56">The full definition of join is as follows: Aggregate The aggregate operation takes a denotation and forms a set out of the tuples in the first column for each setting of the rest of the columns: Now we turn to the mark (M) and execute (Xi) operations  which handles the divergence between syntactic and semantic scope.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'134'", "'3'", "'55'", "'80'"]
'10'
'134'
'3'
'55'
'80'
['10', '134', '3', '55', '80']
parsed_discourse_facet ['method_citation']
<S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="50" ssid = "26">For example  consider the phrase number of major cities  and suppose that number corresponds to the count predicate.</S><S sid ="113" ssid = "89">Formally  let O(  ') be the objective function O() with ZL(x) ZL I(x).</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'", "'134'", "'154'", "'50'", "'113'"]
'22'
'134'
'154'
'50'
'113'
['22', '134', '154', '50', '113']
parsed_discourse_facet ['results_citation']
<S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="126" ssid = "11">During development  we further held out a random 30% of the training sets for validation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'134'", "'3'", "'12'", "'126'"]
'6'
'134'
'3'
'12'
'126'
['6', '134', '3', '12', '126']
parsed_discourse_facet ['aim_citation']
<S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S><S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="172" ssid = "57">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms  we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus  and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'132'", "'9'", "'10'", "'172'", "'134'"]
'132'
'9'
'10'
'172'
'134'
['132', '9', '10', '172', '134']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/W06-3114_aakansha.csv
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="2">Training and testing is based on the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="2">Training and testing is based on the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'"]
'102'
['102']
parsed_discourse_facet ['method_citation']
<S sid="84" ssid="23">The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'"]
'84'
['84']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="4">To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="19">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="6">Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="3">&#8226; We evaluated translation from English, in addition to into English.</S>
    <S sid="6" ssid="4">English was again paired with German, French, and Spanish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'6'"]
'5'
'6'
['5', '6']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="22" ssid = "15">The text type are editorials instead of speech transcripts.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S><S sid ="11" ssid = "4">To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.</S><S sid ="167" ssid = "60">One annotator suggested that this was the case for as much as 10% of our test sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'22'", "'155'", "'11'", "'167'"]
'59'
'22'
'155'
'11'
'167'
['59', '22', '155', '11', '167']
parsed_discourse_facet ['implication_citation']
<S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="34" ssid = "27">For more on the participating systems  please refer to the respective system description in the proceedings of the workshop.</S><S sid ="89" ssid = "5">In words  the judgements are normalized  so that the average normalized judgement per judge is 3.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S><S sid ="160" ssid = "53">Annotators suggested that long sentences are almost impossible to judge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'34'", "'89'", "'155'", "'160'"]
'136'
'34'
'89'
'155'
'160'
['136', '34', '89', '155', '160']
parsed_discourse_facet ['implication_citation']
<S sid ="27" ssid = "20">Microsofts approach uses dependency trees  others use hierarchical phrase models.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="115" ssid = "8">Often  two systems can not be distinguished with a confidence of over 95%  so there are ranked the same.</S><S sid ="15" ssid = "8">Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.</S><S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'", "'1'", "'115'", "'15'", "'59'"]
'27'
'1'
'115'
'15'
'59'
['27', '1', '115', '15', '59']
parsed_discourse_facet ['results_citation']
<S sid ="140" ssid = "33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid ="22" ssid = "15">The text type are editorials instead of speech transcripts.</S><S sid ="26" ssid = "19">Most of these groups follow a phrase-based statistical approach to machine translation.</S><S sid ="82" ssid = "21">This decreases the statistical significance of our results compared to those studies.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'22'", "'26'", "'82'", "'136'"]
'140'
'22'
'26'
'82'
'136'
['140', '22', '26', '82', '136']
parsed_discourse_facet ['method_citation']
<S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="131" ssid = "24">The manual scores are averages over the raw unnormalized scores.</S><S sid ="119" ssid = "12">There may be occasionally a system clearly at the top or at the bottom  but most systems are so close that it is hard to distinguish them.</S><S sid ="44" ssid = "10">We computed BLEU scores for each submission with a single reference translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'84'", "'131'", "'119'", "'44'"]
'125'
'84'
'131'
'119'
'44'
['125', '84', '131', '119', '44']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "12">We aligned the texts at a sentence level across all four languages  resulting in 1064 sentence per language.</S><S sid ="63" ssid = "2">Many human evaluation metrics have been proposed.</S><S sid ="135" ssid = "28">The differences in difficulty are better reflected in the BLEU scores than in the raw un-normalized manual judgements.</S><S sid ="11" ssid = "4">To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'63'", "'135'", "'11'", "'1'"]
'19'
'63'
'135'
'11'
'1'
['19', '63', '135', '11', '1']
parsed_discourse_facet ['method_citation']
<S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="51" ssid = "17">When dropping the top and bottom 2.5% the remaining BLEU scores define the range of the confidence interval.</S><S sid ="146" ssid = "39">This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.</S><S sid ="139" ssid = "32">Given the closeness of most systems and the wide over-lapping confidence intervals it is hard to make strong statements about the correlation between human judgements and automatic scoring methods such as BLEU.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'51'", "'146'", "'139'", "'1'"]
'163'
'51'
'146'
'139'
'1'
['163', '51', '146', '139', '1']
parsed_discourse_facet ['method_citation']
<S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="67" ssid = "6">Participants and other volunteers contributed about 180 hours of labor in the manual evaluation.</S><S sid ="26" ssid = "19">Most of these groups follow a phrase-based statistical approach to machine translation.</S><S sid ="55" ssid = "21">If one system is better in 95% of the sample sets  we conclude that its higher BLEU score is statistically significantly better.</S><S sid ="89" ssid = "5">In words  the judgements are normalized  so that the average normalized judgement per judge is 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'67'", "'26'", "'55'", "'89'"]
'163'
'67'
'26'
'55'
'89'
['163', '67', '26', '55', '89']
parsed_discourse_facet ['method_citation']
<S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="121" ssid = "14">For the automatic scoring method BLEU  we can distinguish three quarters of the systems.</S><S sid ="161" ssid = "54">Since all long sentence translation are somewhat muddled  even a contrastive evaluation between systems was difficult.</S><S sid ="30" ssid = "23">The other half was replaced by other participants  so we ended up with roughly the same number.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'136'", "'121'", "'161'", "'30'"]
'84'
'136'
'121'
'161'
'30'
['84', '136', '121', '161', '30']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="50" ssid = "16">Following this method  we repeatedly  say  1000 times  sample sets of sentences from the output of each system  measure their BLEU score  and use these 1000 BLEU scores as basis for estimating a confidence interval.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="83" ssid = "22">The number of judgements is additionally fragmented by our breakup of sentences into in-domain and out-of-domain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'163'", "'50'", "'84'", "'83'"]
'136'
'163'
'50'
'84'
'83'
['136', '163', '50', '84', '83']
parsed_discourse_facet ['implication_citation']
<S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="119" ssid = "12">There may be occasionally a system clearly at the top or at the bottom  but most systems are so close that it is hard to distinguish them.</S><S sid ="82" ssid = "21">This decreases the statistical significance of our results compared to those studies.</S><S sid ="43" ssid = "9">At the very least  we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.</S><S sid ="10" ssid = "3">Figure 1 provides some statistics about this corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'119'", "'82'", "'43'", "'10'"]
'59'
'119'
'82'
'43'
'10'
['59', '119', '82', '43', '10']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "8">We settled on contrastive evaluations of 5 system outputs for a single test sentence.</S><S sid ="99" ssid = "15">Systems that generally do worse than others will receive a negative one.</S><S sid ="112" ssid = "5">The normalization on a per-judge basis gave very similar ranking  only slightly less consistent with the ranking from the pairwise comparisons.</S><S sid ="62" ssid = "1">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems  they are only a imperfect substitute for human assessment of translation quality  or as the acronym BLEU puts it  a bilingual evaluation understudy.</S><S sid ="98" ssid = "14">Systems that generally do better than others will receive a positive average normalizedjudgement per sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'99'", "'112'", "'62'", "'98'"]
'69'
'99'
'112'
'62'
'98'
['69', '99', '112', '62', '98']
parsed_discourse_facet ['method_citation']
<S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="110" ssid = "3">In the graphs  system scores are indicated by a point  the confidence intervals by shaded areas around the point.</S><S sid ="146" ssid = "39">This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.</S><S sid ="90" ssid = "6">Another way to view the judgements is that they are less quality judgements of machine translation systems per se  but rankings of machine translation systems.</S><S sid ="33" ssid = "26">While building a machine translation system is a serious undertaking  in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'110'", "'146'", "'90'", "'33'"]
'125'
'110'
'146'
'90'
'33'
['125', '110', '146', '90', '33']
parsed_discourse_facet ['method_citation']
<S sid ="98" ssid = "14">Systems that generally do better than others will receive a positive average normalizedjudgement per sentence.</S><S sid ="148" ssid = "41">The best answer to this is: many research labs have very competitive systems whose performance is hard to tell apart.</S><S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S><S sid ="37" ssid = "3">It rewards matches of n-gram sequences  but measures only at most indirectly overall grammatical coherence.</S><S sid ="102" ssid = "18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems  we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'98'", "'148'", "'6'", "'37'", "'102'"]
'98'
'148'
'6'
'37'
'102'
['98', '148', '6', '37', '102']
parsed_discourse_facet ['results_citation']
<S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="177" ssid = "1">This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency  Contract No.</S><S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="3" ssid = "1">was done by the participants.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'177'", "'59'", "'1'", "'3'"]
'125'
'177'
'59'
'1'
'3'
['125', '177', '59', '1', '3']
parsed_discourse_facet ['implication_citation']
<S sid ="90" ssid = "6">Another way to view the judgements is that they are less quality judgements of machine translation systems per se  but rankings of machine translation systems.</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="135" ssid = "28">The differences in difficulty are better reflected in the BLEU scores than in the raw un-normalized manual judgements.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'0'", "'125'", "'126'", "'135'"]
'90'
'0'
'125'
'126'
'135'
['90', '0', '125', '126', '135']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "17">When dropping the top and bottom 2.5% the remaining BLEU scores define the range of the confidence interval.</S><S sid ="39" ssid = "5">However  a recent study (Callison-Burch et al.  2006)  pointed out that this correlation may not always be strong.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S><S sid ="26" ssid = "19">Most of these groups follow a phrase-based statistical approach to machine translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'39'", "'1'", "'162'", "'26'"]
'51'
'39'
'1'
'162'
'26'
['51', '39', '1', '162', '26']
parsed_discourse_facet ['results_citation']
<S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="112" ssid = "5">The normalization on a per-judge basis gave very similar ranking  only slightly less consistent with the ranking from the pairwise comparisons.</S><S sid ="174" ssid = "5">The manual evaluation of scoring translation on a graded scale from 15 seems to be very hard to perform.</S><S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'112'", "'174'", "'59'", "'125'"]
'136'
'112'
'174'
'59'
'125'
['136', '112', '174', '59', '125']
parsed_discourse_facet ['aim_citation']
parsing: input/ref/Task1/P11-1061_sweta.csv
 <S sid="9" ssid="5">Unfortunately, the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al., 2010), making its practical usability questionable at best.</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="47" ssid="13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["47'"]
47'
['47']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["10'"]
10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="1">Given the bilingual graph described in the previous section, we can use label propagation to project the English POS labels to the foreign language.</S>
original cit marker offset is 0
new cit marker offset is 0



["70'"]
70'
['70']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="18">This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
 <S sid="83" ssid="14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value &#964;: We describe how we choose &#964; in &#167;6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["83'"]
83'
['83']
parsed_discourse_facet ['method_citation']
<S sid="113" ssid="13">For each language under consideration, Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["113'"]
113'
['113']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
 <S sid="13" ssid="9">(2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available.</S>
original cit marker offset is 0
new cit marker offset is 0



["13'"]
13'
['13']
parsed_discourse_facet ['method_citation']
 <S sid="3" ssid="3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="20">We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Syntactic universals are a well studied concept in linguistics (Carnie, 2002; Newmeyer, 2005), and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["19'"]
19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="16">Figure 2 shows an excerpt of a sentence from the Italian test set and the tags assigned by four different models, as well as the gold tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



["161'"]
161'
['161']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["23'"]
23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["23'"]
23'
['23']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1061.csv
<S sid ="14" ssid = "10">Our work is closest to that of Yarowsky and Ngai (2001)  but differs in two important ways.</S><S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S><S sid ="103" ssid = "3">The availability of these resources guided our selection of foreign languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'7'", "'56'", "'135'", "'103'"]
'14'
'7'
'56'
'135'
'103'
['14', '7', '56', '135', '103']
parsed_discourse_facet ['implication_citation']
<S sid ="75" ssid = "6">We use a squared loss to penalize neighboring vertices that have different label distributions: kqi  qjk2 = Ey(qi(y)  qj(y))2  and additionally regularize the label distributions towards the uniform distribution U over all possible labels Y.</S><S sid ="53" ssid = "19">Finally  note that while most feature concepts are lexicalized  others  such as the suffix concept  are not.</S><S sid ="34" ssid = "11">The following three sections elaborate these different stages is more detail.</S><S sid ="129" ssid = "29">Fortunately  performance was stable across various values  and we were able to use the same hyperparameters for all languages.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'53'", "'34'", "'129'", "'26'"]
'75'
'53'
'34'
'129'
'26'
['75', '53', '34', '129', '26']
parsed_discourse_facet ['implication_citation']
<S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="59" ssid = "25">So far the graph has been completely unlabeled.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'107'", "'59'", "'56'", "'39'"]
'1'
'107'
'59'
'56'
'39'
['1', '107', '59', '56', '39']
parsed_discourse_facet ['results_citation']
<S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : We describe how we choose  in 6.4.</S><S sid ="66" ssid = "32">In general  the neighborhoods can be more diverse and we allow a soft label distribution over the vertices.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'66'", "'107'", "'117'", "'135'"]
'83'
'66'
'107'
'117'
'135'
['83', '66', '107', '117', '135']
parsed_discourse_facet ['method_citation']
<S sid ="104" ssid = "4">For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi  2006; Nivre et al.  2007).</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="155" ssid = "18">Examining the word fidanzato for the No LP and With LP models is particularly instructive.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="98" ssid = "29">7).</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'", "'1'", "'155'", "'39'", "'98'"]
'104'
'1'
'155'
'39'
'98'
['104', '1', '155', '39', '98']
parsed_discourse_facet ['method_citation']
<S sid ="143" ssid = "6">Overall  it gives improvements ranging from 1.1% for German to 14.7% for Italian  for an average improvement of 8.3% over the unsupervised feature-HMM model.</S><S sid ="96" ssid = "27">The function A : F * C maps from the language specific fine-grained tagset F to the coarser universal tagset C and is described in detail in 6.2: Note that when tx(y) = 1 the feature value is 0 and has no effect on the model  while its value is oc when tx(y) = 0 and constrains the HMMs state space.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="68" ssid = "34">In the label propagation stage  we propagate the automatic English tags to the aligned Italian trigram types  followed by further propagation solely among the Italian vertices. the Italian vertices are connected to an automatically labeled English vertex.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'96'", "'97'", "'68'", "'26'"]
'143'
'96'
'97'
'68'
'26'
['143', '96', '97', '68', '26']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="129" ssid = "29">Fortunately  performance was stable across various values  and we were able to use the same hyperparameters for all languages.</S><S sid ="108" ssid = "8">However  we needed to restrict ourselves to these languages in order to be able to evaluate the performance of our approach.</S><S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'26'", "'129'", "'108'", "'37'"]
'7'
'26'
'129'
'108'
'37'
['7', '26', '129', '108', '37']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S><S sid ="122" ssid = "22">For each language  we took the same number of sentences from the bitext as there are in its treebank  and trained a supervised feature-HMM.</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="31" ssid = "8">By aggregating the POS labels of the English tokens to types  we can generate label distributions for the English vertices.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'110'", "'122'", "'19'", "'31'"]
'39'
'110'
'122'
'19'
'31'
['39', '110', '122', '19', '31']
parsed_discourse_facet ['method_citation']
<S sid ="73" ssid = "4">Note that because we extracted only high-confidence alignments  many foreign vertices will not be connected to any English vertices.</S><S sid ="24" ssid = "1">The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.</S><S sid ="150" ssid = "13">For all languages  the vocabulary sizes increase by several thousand words.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="139" ssid = "2">As expected  the vanilla HMM trained with EM performs the worst.</S>
original cit marker offset is 0
new cit marker offset is 0



["'73'", "'24'", "'150'", "'39'", "'139'"]
'73'
'24'
'150'
'39'
'139'
['73', '24', '150', '39', '139']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="45" ssid = "11">Furthermore  we do not connect the English vertices to each other  but only to foreign language vertices.4 The graph vertices are extracted from the different sides of a parallel corpus (De  Df) and an additional unlabeled monolingual foreign corpus Ff  which will be used later for training.</S><S sid ="69" ssid = "35">Label propagation is used to propagate these tags inwards and results in tag distributions for the middle word of each Italian trigram.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'45'", "'69'", "'135'", "'110'"]
'107'
'45'
'69'
'135'
'110'
['107', '45', '69', '135', '110']
parsed_discourse_facet ['implication_citation']
<S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : We describe how we choose  in 6.4.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'83'", "'25'", "'4'"]
'37'
'1'
'83'
'25'
'4'
['37', '1', '83', '25', '4']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="120" ssid = "20">We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="44" ssid = "10">Because all English vertices are going to be labeled  we do not need to disambiguate them by embedding them in trigrams.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'120'", "'117'", "'44'", "'26'"]
'54'
'120'
'117'
'44'
'26'
['54', '120', '117', '44', '26']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="47" ssid = "13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S><S sid ="161" ssid = "4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections  and bridge the gap between purely supervised and unsupervised POS tagging models.</S><S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'117'", "'47'", "'161'", "'7'"]
'39'
'117'
'47'
'161'
'7'
['39', '117', '47', '161', '7']
parsed_discourse_facet ['method_citation']
<S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="32" ssid = "9">Label propagation can then be used to transfer the labels to the peripheral foreign vertices (i.e. the ones adjacent to the English vertices) first  and then among all of the foreign vertices (4).</S><S sid ="86" ssid = "17">For a sentence x and a state sequence z  a first order Markov model defines a distribution: (9) where Val(X) corresponds to the entire vocabulary.</S><S sid ="132" ssid = "32">When extracting the vector t  used to compute the constraint feature from the graph  we tried three threshold values for r (see Eq.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'117'", "'32'", "'86'", "'132'"]
'37'
'117'
'32'
'86'
'132'
['37', '117', '32', '86', '132']
parsed_discourse_facet ['results_citation']
<S sid ="163" ssid = "2">We would also like to thank Amarnag Subramanya for helping us with the implementation of label propagation and Shankar Kumar for access to the parallel data.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S><S sid ="139" ssid = "2">As expected  the vanilla HMM trained with EM performs the worst.</S><S sid ="58" ssid = "24">Based on these high-confidence alignments we can extract tuples of the form [u H v]  where u is a foreign trigram type  whose middle word aligns to an English word type v. Our bilingual similarity function then sets the edge weights in proportion to these tuple counts.</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : We describe how we choose  in 6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'110'", "'139'", "'58'", "'83'"]
'163'
'110'
'139'
'58'
'83'
['163', '110', '139', '58', '83']
parsed_discourse_facet ['implication_citation']
<S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="134" ssid = "34">Because we dont have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S><S sid ="72" ssid = "3">In the first stage  we run a single step of label propagation  which transfers the label distributions from the English vertices to the connected foreign language vertices (say  Vf) at the periphery of the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'117'", "'134'", "'110'", "'72'"]
'37'
'117'
'134'
'110'
'72'
['37', '117', '134', '110', '72']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="104" ssid = "4">For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi  2006; Nivre et al.  2007).</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'39'", "'26'", "'104'", "'135'"]
'54'
'39'
'26'
'104'
'135'
['54', '39', '26', '104', '135']
parsed_discourse_facet ['results_citation']
<S sid ="33" ssid = "10">The POS distributions over the foreign trigram types are used as features to learn a better unsupervised POS tagger (5).</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S><S sid ="90" ssid = "21">All features were conjoined with the state z.</S><S sid ="96" ssid = "27">The function A : F * C maps from the language specific fine-grained tagset F to the coarser universal tagset C and is described in detail in 6.2: Note that when tx(y) = 1 the feature value is 0 and has no effect on the model  while its value is oc when tx(y) = 0 and constrains the HMMs state space.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'117'", "'4'", "'90'", "'96'"]
'33'
'117'
'4'
'90'
'96'
['33', '117', '4', '90', '96']
parsed_discourse_facet ['aim_citation']
parsing: input/ref/Task1/E03-1005_aakansha.csv
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'"]
'105'
['105']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'"]
'145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'"]
'80'
['80']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'"]
'143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid="141" ssid="6">This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'141'"]
'140'
'141'
['140', '141']
parsed_discourse_facet ['result_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="102" ssid="5">In case the shortest derivation is not unique, Bod (2000b) proposes to back off to a frequency ordering of the subtrees.</S>
    <S sid="103" ssid="6">That is, all subtrees of each root label are assigned a rank according to their frequency in the treebank: the most frequent subtree (or subtrees) of each root label gets rank 1, the second most frequent subtree gets rank 2, etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'", "'103'"]
'102'
'103'
['102', '103']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="46" ssid="43">Most previous notions of best parse tree in DOP1 were based on a probabilistic metric, with Bod (2000b) as a notable exception, who used a simplicity metric based on the shortest derivation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'"]
'46'
['46']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="37">For example, Bod (2001) samples a fixed number of subtrees of each depth, which has the effect of assigning roughly equal weight to each node in the training data, and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S>
    <S sid="86" ssid="38">Bod reports state-of-the-art results with this method, and observes no decrease in parse accuracy when larger subtrees are included (using subtrees up to depth 14).</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'", "'86'"]
'85'
'86'
['85', '86']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="115" ssid="18">The only thing that needs to be changed for Simplicity-DOP is that all subtrees should be assigned equal probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="95" ssid = "47">By using these PCFG-reductions we can thus parse with all subtrees in polynomial time.</S><S sid ="25" ssid = "22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="87" ssid = "39">Yet  his grammar contains more than 5 million subtrees and processing times of over 200 seconds per WSJ sentence are reported (Bod 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'95'", "'25'", "'22'", "'87'"]
'130'
'95'
'25'
'22'
'87'
['130', '95', '25', '22', '87']
parsed_discourse_facet ['implication_citation']
<S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S><S sid ="58" ssid = "10">The notation A@k denotes the node at address k where A is the nonterminal labeling that node.</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="112" ssid = "15">Moreover  if n gets large  SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'", "'58'", "'40'", "'79'", "'112'"]
'85'
'58'
'40'
'79'
'112'
['85', '58', '40', '79', '112']
parsed_discourse_facet ['implication_citation']
<S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="25" ssid = "22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="105" ssid = "8">The derivation with the smallest sum  or highest rank  is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP  its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'25'", "'40'", "'108'", "'105'"]
'130'
'25'
'40'
'108'
'105'
['130', '25', '40', '108', '105']
parsed_discourse_facet ['results_citation']
<S sid ="30" ssid = "27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S><S sid ="139" ssid = "4">But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP  the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.</S><S sid ="32" ssid = "29">However  ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'139'", "'32'", "'22'", "'40'"]
'30'
'139'
'32'
'22'
'40'
['30', '139', '32', '22', '40']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S><S sid ="112" ssid = "15">Moreover  if n gets large  SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="71" ssid = "23">For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.</S><S sid ="139" ssid = "4">But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP  the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'112'", "'79'", "'71'", "'139'"]
'80'
'112'
'79'
'71'
'139'
['80', '112', '79', '71', '139']
parsed_discourse_facet ['method_citation']
<S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="92" ssid = "44">Tested on the OVIS corpus  Bonnema et al. 's proposal obtains results that are comparable to Sima'an (1999) -- see Bonnema et al.</S><S sid ="51" ssid = "3">That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S sid ="101" ssid = "4">We will refer to this model as Simplicity-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'82'", "'140'", "'92'", "'51'", "'101'"]
'82'
'140'
'92'
'51'
'101'
['82', '140', '92', '51', '101']
parsed_discourse_facet ['method_citation']
<S sid ="94" ssid = "46">This paper presents the first published results with this estimator on the WSJ.</S><S sid ="42" ssid = "39">We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.</S><S sid ="12" ssid = "9">This approach has now gained wide usage  as exemplified by the work of Collins (1996  1999)  Charniak (1996  1997)  Johnson (1998)  Chiang (2000)  and many others.</S><S sid ="51" ssid = "3">That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'", "'42'", "'12'", "'51'", "'45'"]
'94'
'42'
'12'
'51'
'45'
['94', '42', '12', '51', '45']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "4">We will refer to this model as Simplicity-DOP.</S><S sid ="51" ssid = "3">That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S sid ="139" ssid = "4">But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP  the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.</S><S sid ="67" ssid = "19">We can create a subtree by choosing any possible left subtree and any possible right subtree.</S><S sid ="126" ssid = "7">We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores  as these are commonly used to rank parsing systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'51'", "'139'", "'67'", "'126'"]
'101'
'51'
'139'
'67'
'126'
['101', '51', '139', '67', '126']
parsed_discourse_facet ['method_citation']
<S sid ="111" ssid = "14">Note that for n=1  SL-DOP is equal to Likelihood-DOP  since there is only one most probable tree to select from  and LS-DOP is equal to Simplicity-DOP  since there is only one simplest tree to select from.</S><S sid ="0" ssid = "0">An Efficient Implementation of a New DOP Model</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="71" ssid = "23">For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'111'", "'0'", "'82'", "'71'", "'140'"]
'111'
'0'
'82'
'71'
'140'
['111', '0', '82', '71', '140']
parsed_discourse_facet ['method_citation']
<S sid ="112" ssid = "15">Moreover  if n gets large  SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.</S><S sid ="145" ssid = "10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S><S sid ="106" ssid = "9">What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.</S><S sid ="101" ssid = "4">We will refer to this model as Simplicity-DOP.</S><S sid ="68" ssid = "20">Thus  there are aj= (bk+ 1)(ci + 1) possible subtrees headed by A @j. Goodman then gives a simple small PCFG with the following property: for every subtree in the training corpus headed by A  the grammar will generate an isomorphic subderivation with probability 1/a.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'145'", "'106'", "'101'", "'68'"]
'112'
'145'
'106'
'101'
'68'
['112', '145', '106', '101', '68']
parsed_discourse_facet ['implication_citation']
<S sid ="58" ssid = "10">The notation A@k denotes the node at address k where A is the nonterminal labeling that node.</S><S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S><S sid ="38" ssid = "35">Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task  the parsing time was reported to be over 200 seconds per sentence (Bod 2003).</S><S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="106" ssid = "9">What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'28'", "'38'", "'13'", "'106'"]
'58'
'28'
'38'
'13'
'106'
['58', '28', '38', '13', '106']
parsed_discourse_facet ['method_citation']
<S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="29" ssid = "26">Johnson (1998b  2002) showed that DOP1's subtree estimation method is statistically biased and inconsistent.</S><S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'", "'40'", "'39'", "'29'", "'130'"]
'28'
'40'
'39'
'29'
'130'
['28', '40', '39', '29', '130']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S><S sid ="44" ssid = "41">But while Bod's estimator obtains state-of-the-art results on the WSJ  comparable to Charniak (2000) and Collins (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'22'", "'45'", "'44'", "'82'"]
'39'
'22'
'45'
'44'
'82'
['39', '22', '45', '44', '82']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="0" ssid = "0">An Efficient Implementation of a New DOP Model</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'22'", "'0'", "'79'", "'15'"]
'108'
'22'
'0'
'79'
'15'
['108', '22', '0', '79', '15']
parsed_discourse_facet ['results_citation']
<S sid ="25" ssid = "22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S><S sid ="114" ssid = "17">Note that Goodman's PCFG-reduction method summarized in Section 2 applies not only to Likelihood-DOP but also to Simplicity-DOP.</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="89" ssid = "41">Let a be the number of times nonterminals of type A occur in the training data.</S><S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'114'", "'82'", "'89'", "'28'"]
'25'
'114'
'82'
'89'
'28'
['25', '114', '82', '89', '28']
parsed_discourse_facet ['implication_citation']



E03-1005
P04-1013
0
result_citation
['method_citation']



E03-1005
W06-2905
0
result_citation
['method_citation']
parsing: input/ref/Task1/E03-1005_sweta.csv
<S sid="20" ssid="17">Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).</S>
original cit marker offset is 0
new cit marker offset is 0



["20'"]
20'
['20']
parsed_discourse_facet ['method_citation']
 <S sid="74" ssid="26">Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.</S>
original cit marker offset is 0
new cit marker offset is 0



["74'"]
74'
['74']
parsed_discourse_facet ['method_citation']
<S sid="44" ssid="41">But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["44'"]
44'
['44']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["145'"]
145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S>
original cit marker offset is 0
new cit marker offset is 0



["22'"]
22'
['22']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="14">It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).</S>
original cit marker offset is 0
new cit marker offset is 0



["133'"]
133'
['133']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier &amp; Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="35">Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
 <S sid="100" ssid="3">In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="29">However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
 <S sid="130" ssid="11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["130'"]
130'
['130']
parsed_discourse_facet ['method_citation']
 <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["140'"]
140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="24">Goodman (1996, 1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'"]
27'
['27']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="95" ssid = "47">By using these PCFG-reductions we can thus parse with all subtrees in polynomial time.</S><S sid ="25" ssid = "22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="87" ssid = "39">Yet  his grammar contains more than 5 million subtrees and processing times of over 200 seconds per WSJ sentence are reported (Bod 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'95'", "'25'", "'22'", "'87'"]
'130'
'95'
'25'
'22'
'87'
['130', '95', '25', '22', '87']
parsed_discourse_facet ['implication_citation']
<S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S><S sid ="58" ssid = "10">The notation A@k denotes the node at address k where A is the nonterminal labeling that node.</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="112" ssid = "15">Moreover  if n gets large  SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'", "'58'", "'40'", "'79'", "'112'"]
'85'
'58'
'40'
'79'
'112'
['85', '58', '40', '79', '112']
parsed_discourse_facet ['implication_citation']
<S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="25" ssid = "22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="105" ssid = "8">The derivation with the smallest sum  or highest rank  is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP  its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'25'", "'40'", "'108'", "'105'"]
'130'
'25'
'40'
'108'
'105'
['130', '25', '40', '108', '105']
parsed_discourse_facet ['results_citation']
<S sid ="30" ssid = "27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S><S sid ="139" ssid = "4">But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP  the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.</S><S sid ="32" ssid = "29">However  ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'139'", "'32'", "'22'", "'40'"]
'30'
'139'
'32'
'22'
'40'
['30', '139', '32', '22', '40']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S><S sid ="112" ssid = "15">Moreover  if n gets large  SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="71" ssid = "23">For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.</S><S sid ="139" ssid = "4">But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP  the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'112'", "'79'", "'71'", "'139'"]
'80'
'112'
'79'
'71'
'139'
['80', '112', '79', '71', '139']
parsed_discourse_facet ['method_citation']
<S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="92" ssid = "44">Tested on the OVIS corpus  Bonnema et al. 's proposal obtains results that are comparable to Sima'an (1999) -- see Bonnema et al.</S><S sid ="51" ssid = "3">That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S sid ="101" ssid = "4">We will refer to this model as Simplicity-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'82'", "'140'", "'92'", "'51'", "'101'"]
'82'
'140'
'92'
'51'
'101'
['82', '140', '92', '51', '101']
parsed_discourse_facet ['method_citation']
<S sid ="94" ssid = "46">This paper presents the first published results with this estimator on the WSJ.</S><S sid ="42" ssid = "39">We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.</S><S sid ="12" ssid = "9">This approach has now gained wide usage  as exemplified by the work of Collins (1996  1999)  Charniak (1996  1997)  Johnson (1998)  Chiang (2000)  and many others.</S><S sid ="51" ssid = "3">That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'", "'42'", "'12'", "'51'", "'45'"]
'94'
'42'
'12'
'51'
'45'
['94', '42', '12', '51', '45']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "4">We will refer to this model as Simplicity-DOP.</S><S sid ="51" ssid = "3">That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S sid ="139" ssid = "4">But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP  the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.</S><S sid ="67" ssid = "19">We can create a subtree by choosing any possible left subtree and any possible right subtree.</S><S sid ="126" ssid = "7">We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores  as these are commonly used to rank parsing systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'51'", "'139'", "'67'", "'126'"]
'101'
'51'
'139'
'67'
'126'
['101', '51', '139', '67', '126']
parsed_discourse_facet ['method_citation']
<S sid ="111" ssid = "14">Note that for n=1  SL-DOP is equal to Likelihood-DOP  since there is only one most probable tree to select from  and LS-DOP is equal to Simplicity-DOP  since there is only one simplest tree to select from.</S><S sid ="0" ssid = "0">An Efficient Implementation of a New DOP Model</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="71" ssid = "23">For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'111'", "'0'", "'82'", "'71'", "'140'"]
'111'
'0'
'82'
'71'
'140'
['111', '0', '82', '71', '140']
parsed_discourse_facet ['method_citation']
<S sid ="112" ssid = "15">Moreover  if n gets large  SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.</S><S sid ="145" ssid = "10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S><S sid ="106" ssid = "9">What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.</S><S sid ="101" ssid = "4">We will refer to this model as Simplicity-DOP.</S><S sid ="68" ssid = "20">Thus  there are aj= (bk+ 1)(ci + 1) possible subtrees headed by A @j. Goodman then gives a simple small PCFG with the following property: for every subtree in the training corpus headed by A  the grammar will generate an isomorphic subderivation with probability 1/a.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'145'", "'106'", "'101'", "'68'"]
'112'
'145'
'106'
'101'
'68'
['112', '145', '106', '101', '68']
parsed_discourse_facet ['implication_citation']
<S sid ="58" ssid = "10">The notation A@k denotes the node at address k where A is the nonterminal labeling that node.</S><S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S><S sid ="38" ssid = "35">Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task  the parsing time was reported to be over 200 seconds per sentence (Bod 2003).</S><S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="106" ssid = "9">What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'28'", "'38'", "'13'", "'106'"]
'58'
'28'
'38'
'13'
'106'
['58', '28', '38', '13', '106']
parsed_discourse_facet ['method_citation']
<S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="29" ssid = "26">Johnson (1998b  2002) showed that DOP1's subtree estimation method is statistically biased and inconsistent.</S><S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'", "'40'", "'39'", "'29'", "'130'"]
'28'
'40'
'39'
'29'
'130'
['28', '40', '39', '29', '130']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S><S sid ="44" ssid = "41">But while Bod's estimator obtains state-of-the-art results on the WSJ  comparable to Charniak (2000) and Collins (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'22'", "'45'", "'44'", "'82'"]
'39'
'22'
'45'
'44'
'82'
['39', '22', '45', '44', '82']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="0" ssid = "0">An Efficient Implementation of a New DOP Model</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'22'", "'0'", "'79'", "'15'"]
'108'
'22'
'0'
'79'
'15'
['108', '22', '0', '79', '15']
parsed_discourse_facet ['results_citation']
<S sid ="25" ssid = "22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S><S sid ="114" ssid = "17">Note that Goodman's PCFG-reduction method summarized in Section 2 applies not only to Likelihood-DOP but also to Simplicity-DOP.</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="89" ssid = "41">Let a be the number of times nonterminals of type A occur in the training data.</S><S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'114'", "'82'", "'89'", "'28'"]
'25'
'114'
'82'
'89'
'28'
['25', '114', '82', '89', '28']
parsed_discourse_facet ['implication_citation']
parsing: input/ref/Task1/A00-2018_sweta.csv
 <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["17'"]
17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
 <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["17'"]
17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="11">Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &amp;quot;correct&amp;quot;, and statistics were collected on the resulting parses.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="10">(It is &amp;quot;soft&amp;quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.)</S>
original cit marker offset is 0
new cit marker offset is 0



["119'"]
119'
['119']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.</S>
original cit marker offset is 0
new cit marker offset is 0



["95'"]
95'
['95']
parsed_discourse_facet ['method_citation']
 <S sid="175" ssid="2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["175'"]
175'
['175']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
original cit marker offset is 0
new cit marker offset is 0



["92'"]
92'
['92']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="17">This is indicated in Figure 2, where the model labeled &amp;quot;Best&amp;quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["126'"]
126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="1">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c, t(c) (t for &amp;quot;tag&amp;quot;), then the lexical head of c, h(c), and then the expansion of c into further constituents e(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["174'"]
174'
['174']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="32">As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &amp;quot;before&amp;quot;), and the label of the grandparent of c (la).</S>
original cit marker offset is 0
new cit marker offset is 0



["63'"]
63'
['63']
parsed_discourse_facet ['method_citation']
<S sid="78" ssid="47">With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.</S>
original cit marker offset is 0
new cit marker offset is 0



["78'"]
78'
['78']
parsed_discourse_facet ['method_citation']
 <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="7">From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.</S>
original cit marker offset is 0
new cit marker offset is 0



["180'"]
180'
['180']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2018.csv
<S sid ="114" ssid = "5">That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.</S><S sid ="159" ssid = "50">Something very much like this is done in [15].</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="165" ssid = "56">Note that we also tried including this information using a standard deleted-interpolation model.</S><S sid ="163" ssid = "54">Next we add the less obvious conditioning events noted in our previous discussion of the final model  grandparent label lg and left sibling label /b.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'159'", "'2'", "'165'", "'163'"]
'114'
'159'
'2'
'165'
'163'
['114', '159', '2', '165', '163']
parsed_discourse_facet ['implication_citation']
<S sid ="114" ssid = "5">That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.</S><S sid ="3" ssid = "3">The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.</S><S sid ="99" ssid = "10">Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S><S sid ="129" ssid = "20">It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments)  it does not guess the pre-terminal before guessing the lexical head  and it uses a tree-bank grammar rather than a Markov grammar.</S><S sid ="92" ssid = "3">For runs with the generative model based upon Markov grammar statistics  the first pass uses the same statistics  but conditioned only on standard PCFG information.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'3'", "'99'", "'129'", "'92'"]
'114'
'3'
'99'
'129'
'92'
['114', '3', '99', '129', '92']
parsed_discourse_facet ['implication_citation']
<S sid ="132" ssid = "23">Between the Old model and the Best model  Figure 2 gives precision/recall measurements for several different versions of our parser.</S><S sid ="69" ssid = "38">For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.</S><S sid ="184" ssid = "11">The first is the slight  but important  improvement achieved by using this model over conventional deleted interpolation  as indicated in Figure 2.</S><S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="85" ssid = "54">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models  this simplifies the model significantly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'132'", "'69'", "'184'", "'157'", "'85'"]
'132'
'69'
'184'
'157'
'85'
['132', '69', '184', '157', '85']
parsed_discourse_facet ['results_citation']
<S sid ="137" ssid = "28">However  Collins in [10] does not stress the decision to guess the head's pre-terminal first  and it might be lost on the casual reader.</S><S sid ="51" ssid = "20">Second  and this is a point we have not yet mentioned  the features used in these models need have no particular independence of one another.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="58" ssid = "27">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid ="150" ssid = "41">The second modification is the explicit marking of noun and verb-phrase coordination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'51'", "'2'", "'58'", "'150'"]
'137'
'51'
'2'
'58'
'150'
['137', '51', '2', '58', '150']
parsed_discourse_facet ['method_citation']
<S sid ="49" ssid = "18">First  as already implicit in our discussion  factoring the probability computation into a sequence of values corresponding to various &quot;features&quot; suggests that the probability model should be easily changeable  just change the set of features used.</S><S sid ="4" ssid = "4">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c  information outside c that our probability model deems important in determining the probability in question.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="165" ssid = "56">Note that we also tried including this information using a standard deleted-interpolation model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'4'", "'13'", "'2'", "'165'"]
'49'
'4'
'13'
'2'
'165'
['49', '4', '13', '2', '165']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "3">Also  remember that H is a pla ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.</S><S sid ="11" ssid = "7">What fundamentally distinguishes probabilistic generative parsers is how they compute p(r)  and it is to that topic we turn next.</S><S sid ="96" ssid = "7">As noted above  the probability model uses five smoothed probability distributions  one each for Li  M Ri t  and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7.</S><S sid ="99" ssid = "10">Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S><S sid ="19" ssid = "8">The method we use follows that of [10].</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'11'", "'96'", "'99'", "'19'"]
'34'
'11'
'96'
'99'
'19'
['34', '11', '96', '99', '19']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.</S><S sid ="185" ssid = "12">We expect that as we experiment with other  more semantic conditioning information  the importance of this aspect of the model will increase.</S><S sid ="80" ssid = "49">(Our experience is that rather than requiring 50 or so iterations  three suffice.)</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="144" ssid = "35">This quantity is a relatively intuitive one (as  for example  it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it  in effect  as the unsmoothed probability upon which all smoothing of p(h) is based.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'185'", "'80'", "'54'", "'144'"]
'3'
'185'
'80'
'54'
'144'
['3', '185', '80', '54', '144']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "16">The intuitive idea is that each factor gi is larger than one if the feature in question makes the probability more likely  one if the feature has no effect  and smaller than one if it makes the probability less likely.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S><S sid ="85" ssid = "54">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models  this simplifies the model significantly.</S><S sid ="22" ssid = "11">For us the non-terminal symbols are those of the tree-bank  augmented by the symbols aux and auxg  which have been assigned deterministically to certain auxiliary verbs such as &quot;have&quot; or &quot;having&quot;.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'2'", "'41'", "'85'", "'22'"]
'47'
'2'
'41'
'85'
'22'
['47', '2', '41', '85', '22']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="55" ssid = "24">This method is known as &quot;deleted interpolation&quot; smoothing.</S><S sid ="66" ssid = "35">In many cases this is clearly warranted.</S><S sid ="4" ssid = "4">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid ="121" ssid = "12">Without these enhancements Char97 performs at the 86.6% level for sentences of length < 40.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'55'", "'66'", "'4'", "'121'"]
'157'
'55'
'66'
'4'
'121'
['157', '55', '66', '4', '121']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="0" ssid = "0">A Maximum-Entropy-Inspired Parser *</S><S sid ="37" ssid = "6">Rather  we concentrate on the aspects of these models that most directly influenced the model presented here.</S><S sid ="21" ssid = "10">(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'157'", "'0'", "'37'", "'21'"]
'2'
'157'
'0'
'37'
'21'
['2', '157', '0', '37', '21']
parsed_discourse_facet ['implication_citation']
<S sid ="48" ssid = "17">Maximum-entropy models have two benefits for a parser builder.</S><S sid ="21" ssid = "10">(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="177" ssid = "4">The results reported here disprove this conjecture.</S><S sid ="159" ssid = "50">Something very much like this is done in [15].</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'21'", "'54'", "'177'", "'159'"]
'48'
'21'
'54'
'177'
'159'
['48', '21', '54', '177', '159']
parsed_discourse_facet ['method_citation']
<S sid ="35" ssid = "4">In the past few years the maximum entropy  or log-linear  approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing [1 17].</S><S sid ="121" ssid = "12">Without these enhancements Char97 performs at the 86.6% level for sentences of length < 40.</S><S sid ="69" ssid = "38">For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.</S><S sid ="138" ssid = "29">Indeed  it was lost on the present author until he went back after the fact and found it there.</S><S sid ="114" ssid = "5">That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'", "'121'", "'69'", "'138'", "'114'"]
'35'
'121'
'69'
'138'
'114'
['35', '121', '69', '138', '114']
parsed_discourse_facet ['method_citation']
<S sid ="169" ssid = "60">Up to this point all the models considered in this section are tree-bank grammar models.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="134" ssid = "25">As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.</S><S sid ="99" ssid = "10">Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S><S sid ="155" ssid = "46">For example  in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'", "'125'", "'134'", "'99'", "'155'"]
'169'
'125'
'134'
'99'
'155'
['169', '125', '134', '99', '155']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid ="34" ssid = "3">Also  remember that H is a pla ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.</S><S sid ="165" ssid = "56">Note that we also tried including this information using a standard deleted-interpolation model.</S><S sid ="69" ssid = "38">For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.</S><S sid ="0" ssid = "0">A Maximum-Entropy-Inspired Parser *</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'34'", "'165'", "'69'", "'0'"]
'7'
'34'
'165'
'69'
'0'
['7', '34', '165', '69', '0']
parsed_discourse_facet ['results_citation']
<S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S><S sid ="37" ssid = "6">Rather  we concentrate on the aspects of these models that most directly influenced the model presented here.</S><S sid ="169" ssid = "60">Up to this point all the models considered in this section are tree-bank grammar models.</S><S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'", "'37'", "'169'", "'7'", "'125'"]
'41'
'37'
'169'
'7'
'125'
['41', '37', '169', '7', '125']
parsed_discourse_facet ['implication_citation']
<S sid ="88" ssid = "57">While we could have smoothed in the same fashion  we choose instead to use standard deleted interpolation.</S><S sid ="124" ssid = "15">We note here that this corpus is somewhat more difficult than the &quot;official&quot; test corpus.</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="21" ssid = "10">(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)</S><S sid ="63" ssid = "32">As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'124'", "'54'", "'21'", "'63'"]
'88'
'124'
'54'
'21'
'63'
['88', '124', '54', '21', '63']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/W11-2123_swastika.csv
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



['52']
52
['52']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="177" ssid="49">However, TRIE partitions storage by n-gram length, so walking the trie reads N disjoint pages.</S>
original cit marker offset is 0
new cit marker offset is 0



['177']
177
['177']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="278" ssid="5">We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns.</S>
original cit marker offset is 0
new cit marker offset is 0



['278']
278
['278']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/W11-2123.csv
<S sid ="270" ssid = "12">This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S>
original cit marker offset is 0
new cit marker offset is 0



["'270'", "'276'", "'265'", "'286'", "'284'"]
'270'
'276'
'265'
'286'
'284'
['270', '276', '265', '286', '284']
parsed_discourse_facet ['implication_citation']
<S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S>
original cit marker offset is 0
new cit marker offset is 0



["'262'", "'280'", "'265'", "'277'", "'284'"]
'262'
'280'
'265'
'277'
'284'
['262', '280', '265', '277', '284']
parsed_discourse_facet ['implication_citation']
<S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="285" ssid = "6">Juri Ganitkevitch answered questions about Joshua.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'265'", "'284'", "'278'", "'285'", "'276'"]
'265'
'284'
'278'
'285'
'276'
['265', '284', '278', '285', '276']
parsed_discourse_facet ['results_citation']
<S sid ="260" ssid = "2">For speed  we plan to implement the direct-mapped cache from BerkeleyLM.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>
original cit marker offset is 0
new cit marker offset is 0



["'260'", "'262'", "'280'", "'283'", "'266'"]
'260'
'262'
'280'
'283'
'266'
['260', '262', '280', '283', '266']
parsed_discourse_facet ['method_citation']
<S sid ="272" ssid = "14">Generalizing state minimization  the model could also provide explicit bounds on probability for both backward and forward extension.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="268" ssid = "10">For example  syntactic decoders (Koehn et al.  2007; Dyer et al.  2010; Li et al.  2009) perform dynamic programming parametrized by both backward- and forward-looking state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'272'", "'265'", "'283'", "'286'", "'268'"]
'272'
'265'
'283'
'286'
'268'
['272', '265', '283', '286', '268']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="264" ssid = "6">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'284'", "'265'", "'264'", "'267'"]
'256'
'284'
'265'
'264'
'267'
['256', '284', '265', '264', '267']
parsed_discourse_facet ['method_citation']
<S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S>
original cit marker offset is 0
new cit marker offset is 0



["'283'", "'284'", "'280'", "'265'", "'275'"]
'283'
'284'
'280'
'265'
'275'
['283', '284', '280', '265', '275']
parsed_discourse_facet ['method_citation']
<S sid ="261" ssid = "3">Much could be done to further reduce memory consumption.</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'261'", "'277'", "'286'", "'284'", "'283'"]
'261'
'277'
'286'
'284'
'283'
['261', '277', '286', '284', '283']
parsed_discourse_facet ['method_citation']
<S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S><S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'286'", "'280'", "'267'", "'279'", "'283'"]
'286'
'280'
'267'
'279'
'283'
['286', '280', '267', '279', '283']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'287'", "'266'", "'262'", "'265'"]
'256'
'287'
'266'
'262'
'265'
['256', '287', '266', '262', '265']
parsed_discourse_facet ['implication_citation']
<S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="264" ssid = "6">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'262'", "'278'", "'277'", "'265'", "'264'"]
'262'
'278'
'277'
'265'
'264'
['262', '278', '277', '265', '264']
parsed_discourse_facet ['method_citation']
<S sid ="261" ssid = "3">Much could be done to further reduce memory consumption.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S><S sid ="269" ssid = "11">If they knew that the first four words in a hypergraph node would never extend to the left and form a 5-gram  then three or even fewer words could be kept in the backward state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'261'", "'265'", "'266'", "'267'", "'269'"]
'261'
'265'
'266'
'267'
'269'
['261', '265', '266', '267', '269']
parsed_discourse_facet ['method_citation']
<S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'287'", "'266'", "'286'", "'262'", "'274'"]
'287'
'266'
'286'
'262'
'274'
['287', '266', '286', '262', '274']
parsed_discourse_facet ['method_citation']
<S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'286'", "'256'", "'265'", "'284'", "'276'"]
'286'
'256'
'265'
'284'
'276'
['286', '256', '265', '284', '276']
parsed_discourse_facet ['results_citation']
<S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S><S sid ="270" ssid = "12">This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.</S>
original cit marker offset is 0
new cit marker offset is 0



["'278'", "'262'", "'265'", "'275'", "'270'"]
'278'
'262'
'265'
'275'
'270'
['278', '262', '265', '275', '270']
parsed_discourse_facet ['implication_citation']
<S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'280'", "'262'", "'265'", "'279'", "'256'"]
'280'
'262'
'265'
'279'
'256'
['280', '262', '265', '279', '256']
parsed_discourse_facet ['method_citation']
<S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'279'", "'256'", "'262'", "'265'", "'276'"]
'279'
'256'
'262'
'265'
'276'
['279', '256', '262', '265', '276']
parsed_discourse_facet ['results_citation']
<S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="264" ssid = "6">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'277'", "'286'", "'287'", "'264'"]
'256'
'277'
'286'
'287'
'264'
['256', '277', '286', '287', '264']
parsed_discourse_facet ['aim_citation']
<S sid ="263" ssid = "5">Quantization can be improved by jointly encoding probability and backoff.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="258" ssid = "77">However  the point of RandLM is to scale to even larger data  compensating for this loss in quality.</S>
original cit marker offset is 0
new cit marker offset is 0



["'263'", "'265'", "'287'", "'283'", "'258'"]
'263'
'265'
'287'
'283'
'258'
['263', '265', '287', '283', '258']
parsed_discourse_facet ['method_citation']
<S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'265'", "'280'", "'256'", "'278'", "'267'"]
'265'
'280'
'256'
'278'
'267'
['265', '280', '256', '278', '267']
parsed_discourse_facet ['aim_citation']
parsing: input/ref/Task1/P08-1043_sweta.csv
<S sid="156" ssid="34">To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["4'"]
4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="2">Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.</S>
original cit marker offset is 0
new cit marker offset is 0



["70'"]
70'
['70']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="3">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="9">Tsarfaty (2006) used a morphological analyzer (Segal, 2000), a PoS tagger (Bar-Haim et al., 2005), and a general purpose parser (Schmid, 2000) in an integrated framework in which morphological and syntactic components interact to share information, leading to improved performance on the joint task.</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="107" ssid="39">Firstly, Hebrew unknown tokens are doubly unknown: each unknown token may correspond to several segmentation possibilities, and each segment in such sequences may be able to admit multiple PoS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["107'"]
107'
['107']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="10">The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["14'"]
14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="12">The current work treats both segmental and super-segmental phenomena, yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg, 2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="53" ssid="11">Both (Tsarfaty, 2006; Cohen and Smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has shown a single generative model which handles both tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["53'"]
53'
['53']
parsed_discourse_facet ['method_citation']
 <S sid="48" ssid="6">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["48'"]
48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="141" ssid="19">This analyzer setting is similar to that of (Cohen and Smith, 2007), and models using it are denoted nohsp, Parser and Grammar We used BitPar (Schmid, 2004), an efficient general purpose parser,10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["141'"]
141'
['141']
parsed_discourse_facet ['method_citation']
<S sid="188" ssid="2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



["188'"]
188'
['188']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["94'"]
94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="12">Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training data similar to (Cohen and Smith, 2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
    <S sid="156" ssid="34">To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth, &#8216;tokens&#8217;) that constitute the unanalyzed surface forms (utterances).</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S sid ="54" ssid = "1">A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.</S><S sid ="89" ssid = "21">Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree  s.t.</S><S sid ="179" ssid = "17">On the surface  our model may seem as a special case of Cohen and Smith in which  = 0.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the  hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="148" ssid = "26">Removing the leaves from the resulting tree yields a parse for L under G  with the desired probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'89'", "'179'", "'183'", "'148'"]
'54'
'89'
'179'
'183'
'148'
['54', '89', '179', '183', '148']
parsed_discourse_facet ['implication_citation']
<S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="33" ssid = "12">The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="169" ssid = "7">Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.</S><S sid ="188" ssid = "2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'33'", "'156'", "'169'", "'188'"]
'80'
'33'
'156'
'169'
'188'
['80', '33', '156', '169', '188']
parsed_discourse_facet ['implication_citation']
<S sid ="130" ssid = "8">To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and malformed7 were removed.</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="48" ssid = "6">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S><S sid ="176" ssid = "14">Furthermore  the combination of pruning and vertical markovization of the grammar outperforms the Oracle results reported by Cohen and Smith.</S><S sid ="97" ssid = "29">Thus our proposed model is a proper model assigning probability mass to all (7r  L) pairs  where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'80'", "'48'", "'176'", "'97'"]
'130'
'80'
'48'
'176'
'97'
['130', '80', '48', '176', '97']
parsed_discourse_facet ['results_citation']
<S sid ="49" ssid = "7">Tsarfaty and Simaan (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S><S sid ="163" ssid = "1">The accuracy results for segmentation  tagging and parsing using our different models and our standard data split are summarized in Table 1.</S><S sid ="71" ssid = "3">This is by now a fairly standard representation for multiple morphological segmentation of Hebrew utterances (Adler  2001; Bar-Haim et al.  2005; Smith et al.  2005; Cohen and Smith  2007; Adler  2007).</S><S sid ="33" ssid = "12">The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).</S><S sid ="148" ssid = "26">Removing the leaves from the resulting tree yields a parse for L under G  with the desired probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'163'", "'71'", "'33'", "'148'"]
'49'
'163'
'71'
'33'
'148'
['49', '163', '71', '33', '148']
parsed_discourse_facet ['method_citation']
<S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="45" ssid = "3">Morphological disambiguators that consider a token in context (an utterance) and propose the most likely morphological analysis of an utterance (including segmentation) were presented by Bar-Haim et al. (2005)  Adler and Elhadad (2006)  Shacham and Wintner (2007)  and achieved good results (the best segmentation result so far is around 98%).</S><S sid ="157" ssid = "35">SEGTok(noH) is the segmentation accuracy ignoring mistakes involving the implicit definite article h.11 To evaluate our performance on the tagging task we report CPOS and FPOS corresponding to coarse- and fine-grained PoS tagging results (F1) measure.</S><S sid ="169" ssid = "7">Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'156'", "'45'", "'157'", "'169'"]
'180'
'156'
'45'
'157'
'169'
['180', '156', '45', '157', '169']
parsed_discourse_facet ['method_citation']
<S sid ="169" ssid = "7">Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="161" ssid = "39">We report the F1 value of both measures.</S><S sid ="67" ssid = "14">Hence  we take the probability of the event fmnh analyzed as REL VB to be This means that we generate f and mnh independently depending on their corresponding PoS tags  and the context (as well as the syntactic relation between the two) is modeled via the derivation resulting in a sequence REL VB spanning the form fmnh. based on linear context.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'", "'156'", "'80'", "'161'", "'67'"]
'169'
'156'
'80'
'161'
'67'
['169', '156', '80', '161', '67']
parsed_discourse_facet ['method_citation']
<S sid ="88" ssid = "20">M(wi) = Li).</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="36" ssid = "15">Furthermore  the systematic way in which particles are prefixed to one another and onto an open-class category gives rise to a distinct sort of morphological ambiguity: space-delimited tokens may be ambiguous between several different segmentation possibilities.</S><S sid ="26" ssid = "5">The relativizer f(that) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.</S><S sid ="73" ssid = "5">We use double-circles to indicate the space-delimited token boundaries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'156'", "'36'", "'26'", "'73'"]
'88'
'156'
'36'
'26'
'73'
['88', '156', '36', '26', '73']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="38" ssid = "17">The form mnh itself can be read as at least three different verbs (counted  appointed  was appointed)  a noun (a portion)  and a possessed noun (her kind).</S><S sid ="108" ssid = "40">Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p * (s  p)) > 0  while other segments have never been observed as a lexical event before.</S><S sid ="49" ssid = "7">Tsarfaty and Simaan (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S><S sid ="158" ssid = "36">Evaluating parsing results in our joint framework  as argued by Tsarfaty (2006)  is not trivial under the joint disambiguation task  as the hypothesized yield need not coincide with the correct one.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'38'", "'108'", "'49'", "'158'"]
'80'
'38'
'108'
'49'
'158'
['80', '38', '108', '49', '158']
parsed_discourse_facet ['method_citation']
<S sid ="182" ssid = "20">In addition  as the CRF and PCFG look at similar sorts of information from within two inherently different models  they are far from independent and optimizing their product is meaningless.</S><S sid ="101" ssid = "33">The possible analyses of a surface token pose constraints on the analyses of specific segments.</S><S sid ="163" ssid = "1">The accuracy results for segmentation  tagging and parsing using our different models and our standard data split are summarized in Table 1.</S><S sid ="58" ssid = "5">Such tag sequences are often treated as complex tags (e.g.</S><S sid ="44" ssid = "2">Such analyzers propose multiple segmentation possibilities and their corresponding analyses for a token in isolation but have no means to determine the most likely ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["'182'", "'101'", "'163'", "'58'", "'44'"]
'182'
'101'
'163'
'58'
'44'
['182', '101', '163', '58', '44']
parsed_discourse_facet ['method_citation']
<S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="195" ssid = "9">We further thank Khalil Simaan (ILLCUvA) for his careful advise concerning the formal details of the proposal.</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="98" ssid = "30">The Grammar Our parser looks for the most likely tree spanning a single path through the lattice of which the yield is a sequence of lexemes.</S><S sid ="191" ssid = "5">In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'195'", "'80'", "'98'", "'191'"]
'180'
'195'
'80'
'98'
'191'
['180', '195', '80', '98', '191']
parsed_discourse_facet ['implication_citation']
<S sid ="120" ssid = "52">From now on all lattice arcs are tagged segments and the assignment of probability P(p * (s  p)) to lattice arcs proceeds as usual.4 A rather pathological case is when our lexical heuristics prune away all segmentation possibilities and we remain with an empty lattice.</S><S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="158" ssid = "36">Evaluating parsing results in our joint framework  as argued by Tsarfaty (2006)  is not trivial under the joint disambiguation task  as the hypothesized yield need not coincide with the correct one.</S><S sid ="157" ssid = "35">SEGTok(noH) is the segmentation accuracy ignoring mistakes involving the implicit definite article h.11 To evaluate our performance on the tagging task we report CPOS and FPOS corresponding to coarse- and fine-grained PoS tagging results (F1) measure.</S><S sid ="49" ssid = "7">Tsarfaty and Simaan (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'", "'180'", "'158'", "'157'", "'49'"]
'120'
'180'
'158'
'157'
'49'
['120', '180', '158', '157', '49']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="26" ssid = "5">The relativizer f(that) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.</S><S sid ="54" ssid = "1">A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.</S><S sid ="130" ssid = "8">To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and malformed7 were removed.</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'26'", "'54'", "'130'", "'80'"]
'14'
'26'
'54'
'130'
'80'
['14', '26', '54', '130', '80']
parsed_discourse_facet ['method_citation']
<S sid ="154" ssid = "32">For all grammars  we use fine-grained PoS tags indicating various morphological features annotated therein.</S><S sid ="194" ssid = "8">Acknowledgments We thank Meni Adler and Michael Elhadad (BGU) for helpful comments and discussion.</S><S sid ="97" ssid = "29">Thus our proposed model is a proper model assigning probability mass to all (7r  L) pairs  where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S><S sid ="130" ssid = "8">To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and malformed7 were removed.</S><S sid ="149" ssid = "27">We use a patched version of BitPar allowing for direct input of probabilities instead of counts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'154'", "'194'", "'97'", "'130'", "'149'"]
'154'
'194'
'97'
'130'
'149'
['154', '194', '97', '130', '149']
parsed_discourse_facet ['method_citation']
<S sid ="5" ssid = "1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  tokens) that constitute the unanalyzed surface forms (utterances).</S><S sid ="76" ssid = "8">Furthermore  some of the arcs represent lexemes not present in the input tokens (e.g. h/DT  fl/POS)  however these are parts of valid analyses of the token (cf. super-segmental morphology section 2).</S><S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="108" ssid = "40">Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p * (s  p)) > 0  while other segments have never been observed as a lexical event before.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'76'", "'180'", "'108'", "'133'"]
'5'
'76'
'180'
'108'
'133'
['5', '76', '180', '108', '133']
parsed_discourse_facet ['results_citation']
<S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="48" ssid = "6">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S><S sid ="181" ssid = "19">Many morphological decisions are based on long distance dependencies  and when the global syntactic evidence disagrees with evidence based on local linear context  the two models compete with one another  despite the fact that the PCFG takes also local context into account.</S><S sid ="95" ssid = "27">A compatible view is presented by Charniak et al. (1996) who consider the kind of probabilities a generative parser should get from a PoS tagger  and concludes that these should be P(w|t) and nothing fancier.3 In our setting  therefore  the Lattice is not used to induce a probability distribution on a linear context  but rather  it is used as a common-denominator of state-indexation of all segmentations possibilities of a surface form.</S><S sid ="54" ssid = "1">A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'48'", "'181'", "'95'", "'54'"]
'80'
'48'
'181'
'95'
'54'
['80', '48', '181', '95', '54']
parsed_discourse_facet ['implication_citation']
<S sid ="164" ssid = "2">In addition we report for each model its performance on goldsegmented input (GS) to indicate the upper bound 11Overt definiteness errors may be seen as a wrong feature rather than as wrong constituent and it is by now an accepted standard to report accuracy with and without such errors. for the grammars performance on the parsing task.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S><S sid ="22" ssid = "1">Segmental morphology Hebrew consists of seven particles m(from) f(when/who/that) h(the) w(and) k(like) l(to) and b(in). which may never appear in isolation and must always attach as prefixes to the following open-class category item we refer to as stem.</S><S sid ="126" ssid = "4">When a comparison against previous results requires additional pre-processing  we state it explicitly to allow for the reader to replicate the reported results.</S><S sid ="173" ssid = "11">The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter () which is tuned separately for each of the tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'164'", "'133'", "'22'", "'126'", "'173'"]
'164'
'133'
'22'
'126'
'173'
['164', '133', '22', '126', '173']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P04-1036_sweta.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="82" ssid="11">We also calculate the WSD accuracy that would be obtained on SemCor, when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["82'"]
82'
['82']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="20">We briefly summarise the two measures here; for a more detailed summary see (Patwardhan et al., 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="172" ssid="20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="1">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="165" ssid="13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus, whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S>
original cit marker offset is 0
new cit marker offset is 0



["165'"]
165'
['165']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["171'"]
171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["89'"]
89'
['89']
parsed_discourse_facet ['method_citation']
<S sid="172" ssid="20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="12">Additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["189'"]
189'
['189']
parsed_discourse_facet ['method_citation']
<S sid="87" ssid="16">Again, the automatic ranking outperforms this by a large margin.</S>
original cit marker offset is 0
new cit marker offset is 0



["87'"]
87'
['87']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["89'"]
89'
['89']
parsed_discourse_facet ['method_citation']
 <S sid="137" ssid="14">Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["137'"]
137'
['137']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="19">We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline, but because of space limitations give results for the two which perform the best.</S>
original cit marker offset is 0
new cit marker offset is 0



["63'"]
63'
['63']
parsed_discourse_facet ['method_citation']
<S sid="159" ssid="7">Magnini and Cavagli`a (2000) have identified WordNet word senses with particular domains, and this has proven useful for high precision WSD (Magnini et al., 2001); indeed in section 5 we used these domain labels for evaluation.</S>
original cit marker offset is 0
new cit marker offset is 0



["159'"]
159'
['159']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="98" ssid = "27">This seems intuitive given our expected relative usage of these senses in modern British English.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="27" ssid = "20">We use WordNet as our sense inventory for this work.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'98'", "'124'", "'5'", "'27'", "'65'"]
'98'
'124'
'5'
'27'
'65'
['98', '124', '5', '27', '65']
parsed_discourse_facet ['implication_citation']
<S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="109" ssid = "7">We generated a thesaurus entry for all polysemous nouns in WordNet as described in section 2.1 above.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S><S sid ="132" ssid = "9">The FINANCE corpus consists of 117734 documents (about 32.5 million words).</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'109'", "'95'", "'55'", "'132'"]
'124'
'109'
'95'
'55'
'132'
['124', '109', '95', '55', '132']
parsed_discourse_facet ['implication_citation']
<S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="32" ssid = "25">We describe some related work in section 6 and conclude in section 7. are therefore investigating a method of automatically ranking WordNet senses from raw text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'137'", "'5'", "'110'", "'32'"]
'44'
'137'
'5'
'110'
'32'
['44', '137', '5', '110', '32']
parsed_discourse_facet ['results_citation']
<S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="96" ssid = "25">Another example where the ranking is intuitive  is soil.</S><S sid ="33" ssid = "26">Many researchers are developing thesauruses from automatically parsed data.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'95'", "'96'", "'33'", "'44'"]
'124'
'95'
'96'
'33'
'44'
['124', '95', '96', '33', '44']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "5">To disambiguate senses a system should take context into account.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="67" ssid = "23">Each 2We use this version of WordNet since it allows us to map information to WordNets of other languages more accurately.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'124'", "'153'", "'147'", "'67'"]
'107'
'124'
'153'
'147'
'67'
['107', '124', '153', '147', '67']
parsed_discourse_facet ['method_citation']
<S sid ="94" ssid = "23">This seems quite reasonable given the nearest neighbours: tube  cable  wire  tank  hole  cylinder  fitting  tap  cistern  plate....</S><S sid ="162" ssid = "10">It only requires raw text from the given domain and because of this it can easily be applied to a new domain  or sense inventory  given sufficient text.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="99" ssid = "28">Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'", "'162'", "'60'", "'95'", "'99'"]
'94'
'162'
'60'
'95'
'99'
['94', '162', '60', '95', '99']
parsed_discourse_facet ['method_citation']
<S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="101" ssid = "30">Thus  if we used the sense ranking as a heuristic for an all nouns task we would expect to get precision in the region of 60%.</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'165'", "'101'", "'169'", "'108'", "'44'"]
'165'
'101'
'169'
'108'
'44'
['165', '101', '169', '108', '44']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="127" ssid = "4">We chose the domains of SPORTS and FINANCE since there is sufficient material for these domains in this publically available corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'60'", "'169'", "'91'", "'127'"]
'136'
'60'
'169'
'91'
'127'
['136', '60', '169', '91', '127']
parsed_discourse_facet ['method_citation']
<S sid ="193" ssid = "2">This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.</S><S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="151" ssid = "28">This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="167" ssid = "15">In this work the lists of neighbours are themselves clustered to bring out the various senses of the word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'193'", "'136'", "'151'", "'123'", "'167'"]
'193'
'136'
'151'
'123'
'167'
['193', '136', '151', '123', '167']
parsed_discourse_facet ['method_citation']
<S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="63" ssid = "19">We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline  but because of space limitations give results for the two which perform the best.</S>
original cit marker offset is 0
new cit marker offset is 0



["'173'", "'5'", "'108'", "'95'", "'63'"]
'173'
'5'
'108'
'95'
'63'
['173', '5', '108', '95', '63']
parsed_discourse_facet ['implication_citation']
<S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="76" ssid = "5">The jcn measure uses corpus data for the calculation of IC.</S><S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'184'", "'110'", "'60'", "'76'", "'55'"]
'184'
'110'
'60'
'76'
'55'
['184', '110', '60', '76', '55']
parsed_discourse_facet ['method_citation']
<S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="97" ssid = "26">The first ranked sense according to SemCor is the filth  stain: state of being unclean sense whereas the automatic ranking lists dirt  ground  earth as the first sense  which is the second ranked sense according to SemCor.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'123'", "'169'", "'147'", "'97'", "'8'"]
'123'
'169'
'147'
'97'
'8'
['123', '169', '147', '97', '8']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="28" ssid = "21">The paper is structured as follows.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of nearest neighbours.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'28'", "'124'", "'60'", "'34'"]
'61'
'28'
'124'
'60'
'34'
['61', '28', '124', '60', '34']
parsed_discourse_facet ['method_citation']
<S sid ="163" ssid = "11">Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="9" ssid = "2">This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).</S><S sid ="0" ssid = "0">Finding Predominant Word Senses in Untagged Text</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'124'", "'9'", "'0'", "'142'"]
'163'
'124'
'9'
'0'
'142'
['163', '124', '9', '0', '142']
parsed_discourse_facet ['results_citation']
<S sid ="138" ssid = "15">The SFC contains an economy label and a sports label.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of nearest neighbours.</S><S sid ="22" ssid = "15">More importantly  when working within a specific domain one would wish to tune the first sense heuristic to the domain at hand.</S><S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="92" ssid = "21">For example  in WordNet the first listed sense ofpipe is tobacco pipe  and this is ranked joint first according to the Brown files in SemCor with the second sense tube made of metal or plastic used to carry water  oil or gas etc....</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'", "'34'", "'22'", "'184'", "'92'"]
'138'
'34'
'22'
'184'
'92'
['138', '34', '22', '184', '92']
parsed_discourse_facet ['implication_citation']
<S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="185" ssid = "8">In the future  we will perform a large scale evaluation on domain specific corpora.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'173'", "'95'", "'185'", "'65'"]
'108'
'173'
'95'
'185'
'65'
['108', '173', '95', '185', '65']
parsed_discourse_facet ['method_citation']
<S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="151" ssid = "28">This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'173'", "'110'", "'151'", "'95'", "'5'"]
'173'
'110'
'151'
'95'
'5'
['173', '110', '151', '95', '5']
parsed_discourse_facet ['results_citation']
parsing: input/ref/Task1/P87-1015_sweta.csv
<S sid="205" ssid="11">Independence of paths at this level reflects context freeness of rewriting and suggests why they can be recognized efficiently.</S>
original cit marker offset is 0
new cit marker offset is 0



["205'"]
205'
['205']
parsed_discourse_facet ['method_citation']
<S sid="229" ssid="35">In addition, the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars, and the hierarchy defined by Weir).</S>
original cit marker offset is 0
new cit marker offset is 0



["229'"]
229'
['229']
parsed_discourse_facet ['method_citation']
<S sid="146" ssid="31">Since every CFL is known to be semilinear (Parikh, 1966), in order to show semilinearity of some language, we need only show the existence of a letter equivalent CFL Our definition of LCFRS's insists that the composition operations are linear and nonerasing.</S>
original cit marker offset is 0
new cit marker offset is 0



["146'"]
146'
['146']
parsed_discourse_facet ['method_citation']
<S sid="201" ssid="7">It is interesting to note, however, that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG, FUG and IG's.</S>
original cit marker offset is 0
new cit marker offset is 0



["201'"]
201'
['201']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
 <S sid="222" ssid="28">However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



["222'"]
222'
['222']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="7">Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.</S>
    <S sid="23" ssid="8">He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["22'", "'23'"]
22'
'23'
['22', '23']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="221" ssid="27">Members of LCFRS whose operations have this property can be translated into the ILFP notation (Rounds, 1985).</S>
original cit marker offset is 0
new cit marker offset is 0



["221'"]
221'
['221']
parsed_discourse_facet ['method_citation']
<S sid="54" ssid="39">An IG can be viewed as a CFG in which each nonterminal is associated with a stack.</S>
original cit marker offset is 0
new cit marker offset is 0



["54'"]
54'
['54']
parsed_discourse_facet ['method_citation']
<S sid="128" ssid="13">As in the case of the derivation trees of CFG's, nodes are labeled by a member of some finite set of symbols (perhaps only implicit in the grammar as in TAG's) used to denote derived structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["128'"]
128'
['128']
parsed_discourse_facet ['method_citation']
<S sid="217" ssid="23">In considering the recognition of these languages, we were forced to be more specific regarding the relationship between the structures derived by these formalisms and the substrings they span.</S>
original cit marker offset is 0
new cit marker offset is 0



[";217'"]
;217'
[';217']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="1">From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="1">From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="214" ssid="20">LCFRS's share several properties possessed by the class of mildly context-sensitive formalisms discussed by Joshi (1983/85).</S>
original cit marker offset is 0
new cit marker offset is 0



["214'"]
214'
['214']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="20">TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish, for cross serial dependencies in Dutch subordinate clauses, and for the nestings of paired English complementizers.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P87-1015.csv
<S sid ="191" ssid = "76">In addition to the tapes required to store the indices  M requires one work tape for splitting the substrings.</S><S sid ="206" ssid = "12">As suggested in Section 4.3.2  a derivation with independent paths can be divided into subcomputations with limited sharing of information.</S><S sid ="186" ssid = "71">To do this  the x's and y's are stored in the next 2ni + 2n2 tapes  and M goes to a universal state.</S><S sid ="104" ssid = "10">Pumping t2 will change only one branch and leave the other branch unaffected.</S><S sid ="192" ssid = "77">Thus  the ATM has no more than 6km&quot; + 1 work tapes  where km&quot; is the maximum number of substrings spanned by a derived structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'191'", "'206'", "'186'", "'104'", "'192'"]
'191'
'206'
'186'
'104'
'192'
['191', '206', '186', '104', '192']
parsed_discourse_facet ['implication_citation']
<S sid ="84" ssid = "69">((fii  Q2  Pa)    (01  i32  03)   e) (02 e) Oa  en) The path complexity of the tee set generated by a MCTAG is not necessarily context-free.</S><S sid ="54" ssid = "39">An IG can be viewed as a CFG in which each nonterminal is associated with a stack.</S><S sid ="101" ssid = "7">The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently.</S><S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="71" ssid = "56">0n0'i'i0'2&quot;bin242bn I n = 711 + n2 } On the other hand  no linguistic use is made of this general form of composition and Steedman (personal communication) and Steedman (1986) argues that a more limited definition of composition is more natural.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'54'", "'101'", "'59'", "'71'"]
'84'
'54'
'101'
'59'
'71'
['84', '54', '101', '59', '71']
parsed_discourse_facet ['implication_citation']
<S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="44" ssid = "29">The edge from the root to the subtree for the derivation of 7i is labeled by the address ni.</S><S sid ="136" ssid = "21">These two restrictions impose the constraint that the result of composing any two structures should be a structure whose &quot;size&quot; is the sum of its constituents plus some constant For example  the operation 4  discussed in the case of CFG's (in Section 4.1) adds the constant equal to the sum of the length of the strings VI  un+r Since we are considering formalisms with arbitrary structures it is difficult to precisely specify all of the restrictions on the composition operations that we believe would appropriately generalize the concatenation operation for the particular structures used by the formalism.</S><S sid ="28" ssid = "13">A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'44'", "'136'", "'28'", "'153'"]
'151'
'44'
'136'
'28'
'153'
['151', '44', '136', '28', '153']
parsed_discourse_facet ['results_citation']
<S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="134" ssid = "19">These systems are similar to those described by Pollard (1984) as Generalized Context-Free Grammars (GCFG's).</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="112" ssid = "18">.t The path set of tree sets at level k +1 have the complexity of the string language of level k. The independence of paths in a tree set appears to be an important property.</S><S sid ="20" ssid = "5">As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'134'", "'153'", "'112'", "'20'"]
'59'
'134'
'153'
'112'
'20'
['59', '134', '153', '112', '20']
parsed_discourse_facet ['method_citation']
<S sid ="106" ssid = "12">We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths.</S><S sid ="54" ssid = "39">An IG can be viewed as a CFG in which each nonterminal is associated with a stack.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="101" ssid = "7">The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently.</S><S sid ="182" ssid = "67">Since each zi is a contiguous substring of the input (say ai )  and no two substrings overlap  we can represent zi by the pair of integers (i2  i2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'54'", "'143'", "'101'", "'182'"]
'106'
'54'
'143'
'101'
'182'
['106', '54', '143', '101', '182']
parsed_discourse_facet ['method_citation']
<S sid ="55" ssid = "40">Each production can push or pop symbols on the stack as can be seen in the following productions that generate tree of the form shown in Figure 4b.</S><S sid ="117" ssid = "2">Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.</S><S sid ="200" ssid = "6">The importance of this property becomes clear in contrasting theories underlying GPSG (Gazdar  Klein  Pulluna  and Sag  1985)  and GB (as described by Berwick  1984) with those underlying LFG and FUG.</S><S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="106" ssid = "12">We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'117'", "'200'", "'59'", "'106'"]
'55'
'117'
'200'
'59'
'106'
['55', '117', '200', '59', '106']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "17">When 7' is adjoined at ?I in the tree 7 we obtain a tree v&quot;.</S><S sid ="121" ssid = "6">First  any grammar must involve a finite number of elementary structures  composed using a finite number of composition operations.</S><S sid ="19" ssid = "4">It can be easily shown from Thatcher's result that the path set of every local set is a regular set.</S><S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="138" ssid = "23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'121'", "'19'", "'151'", "'138'"]
'32'
'121'
'19'
'151'
'138'
['32', '121', '19', '151', '138']
parsed_discourse_facet ['method_citation']
<S sid ="188" ssid = "73">Thus  for example  one successor process will be have M to be in the existential state qa with the indices encoding xi     xn  in the first 2n i tapes.</S><S sid ="118" ssid = "3">In the remainder of the paper  we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S><S sid ="89" ssid = "74">Hence  trees shown in Figure 8 can not be generated by any MCTAG (but can be generated by an IG) because the number of pairs of dependent paths grows with n. Since the derivation tees of TAG's  MCTAG's  and HG's are local sets  the choice of the structure used at each point in a derivation in these systems does not depend on the context at that point within the derivation.</S><S sid ="96" ssid = "2">A tree set may be said to have dependencies between paths if some &quot;appropriate&quot; subset can be shown to have dependent paths as defined above.</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'188'", "'118'", "'89'", "'96'", "'227'"]
'188'
'118'
'89'
'96'
'227'
['188', '118', '89', '96', '227']
parsed_discourse_facet ['method_citation']
<S sid ="84" ssid = "69">((fii  Q2  Pa)    (01  i32  03)   e) (02 e) Oa  en) The path complexity of the tee set generated by a MCTAG is not necessarily context-free.</S><S sid ="98" ssid = "4">Thatcher (1973) describes a tee pumping lemma for recognizable sets related to the string pumping lemma for regular sets.</S><S sid ="12" ssid = "10">We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.</S><S sid ="179" ssid = "64">It can be seen that M performs a top-down recognition of the input al ... nin logspace.</S><S sid ="185" ssid = "70">Each spawned process must check if xi     xn  and   yn  can be derived from B and C  respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'98'", "'12'", "'179'", "'185'"]
'84'
'98'
'12'
'179'
'185'
['84', '98', '12', '179', '185']
parsed_discourse_facet ['method_citation']
<S sid ="207" ssid = "13">We outlined the definition of a family of constrained grammatical formalisms  called Linear Context-Free Rewriting Systems.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="26" ssid = "11">The productions of HG's are very similar to those of CFG's except that the operation used must be made explicit.</S><S sid ="178" ssid = "63">We define an ATM  M  recognizing a language generated by a grammar  G  having the properties discussed in Section 43.</S><S sid ="155" ssid = "40">CFG's  TAG's  MCTAG's and HG's are all members of this class since they satisfy these restrictions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'153'", "'26'", "'178'", "'155'"]
'207'
'153'
'26'
'178'
'155'
['207', '153', '26', '178', '155']
parsed_discourse_facet ['implication_citation']
<S sid ="38" ssid = "23">Thus  the derivation trees for TAG's have the same structure as local sets.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="165" ssid = "50">This class of formalisms have the properties that their derivation trees are local sets  and manipulate objects  using a finite number of composition operations that use a finite number of symbols.</S><S sid ="62" ssid = "47">TAG's can be shown to be equivalent to this restricted system.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'153'", "'165'", "'62'", "'143'"]
'38'
'153'
'165'
'62'
'143'
['38', '153', '165', '62', '143']
parsed_discourse_facet ['method_citation']
<S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="132" ssid = "17">In TAG's the elementary tree and addresses where adjunction takes place are used to instantiate the operation.</S><S sid ="101" ssid = "7">The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently.</S><S sid ="96" ssid = "2">A tree set may be said to have dependencies between paths if some &quot;appropriate&quot; subset can be shown to have dependent paths as defined above.</S><S sid ="94" ssid = "79">The semilinearity of Tree Adjoining Languages (TAL's)  MCTAL's  and Head Languages (HL's) can be proved using this property  with suitable restrictions on the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'132'", "'101'", "'96'", "'94'"]
'143'
'132'
'101'
'96'
'94'
['143', '132', '101', '96', '94']
parsed_discourse_facet ['method_citation']
<S sid ="89" ssid = "74">Hence  trees shown in Figure 8 can not be generated by any MCTAG (but can be generated by an IG) because the number of pairs of dependent paths grows with n. Since the derivation tees of TAG's  MCTAG's  and HG's are local sets  the choice of the structure used at each point in a derivation in these systems does not depend on the context at that point within the derivation.</S><S sid ="164" ssid = "49">Although embedding this version of LCFRS's in the framework of ILFP developed by Rounds (1985) is straightforward  our motivation was to capture properties shared by a family of grammatical systems and generalize them defining a class of related formalisms.</S><S sid ="17" ssid = "2">We define the path set of a tree 1 as the set of strings that label a path from the root to frontier of 7.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="26" ssid = "11">The productions of HG's are very similar to those of CFG's except that the operation used must be made explicit.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'164'", "'17'", "'143'", "'26'"]
'89'
'164'
'17'
'143'
'26'
['89', '164', '17', '143', '26']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">For example  Gazdar (1985) discusses the applicability of Indexed Grammars (IG's) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar (LFG) and Government and Bindings grammars (GB).</S><S sid ="91" ssid = "76">We characterize a class of formalisms that have this property in Section 4.</S><S sid ="68" ssid = "53">This kind of dependency arises from the use of the composition operation to compose two arbitrarily large categories.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="189" ssid = "74">For rules p : A fpo such that fp is constant function  giving an elementary structure  fp is defined such that fp() = (Si ... xi() where each z is a constant string.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'91'", "'68'", "'143'", "'189'"]
'6'
'91'
'68'
'143'
'189'
['6', '91', '68', '143', '189']
parsed_discourse_facet ['results_citation']
<S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="90" ssid = "75">Thus  as in CFG's  at any point in the derivation  the set of structures that can be applied is determined only by a finite set of rules encapsulated by the grammar.</S><S sid ="185" ssid = "70">Each spawned process must check if xi     xn  and   yn  can be derived from B and C  respectively.</S><S sid ="141" ssid = "26">Thus  the length of any string in L is a linear combination of the length of strings in some fixed finite subset of L  and thus L is said to have the constant growth property.</S><S sid ="158" ssid = "43">We can represent any derived tree of a TAG by the two substrings that appear in its frontier  and then define how the adjunction operation concatenates the substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'90'", "'185'", "'141'", "'158'"]
'151'
'90'
'185'
'141'
'158'
['151', '90', '185', '141', '158']
parsed_discourse_facet ['implication_citation']
<S sid ="156" ssid = "41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S><S sid ="106" ssid = "12">We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths.</S><S sid ="32" ssid = "17">When 7' is adjoined at ?I in the tree 7 we obtain a tree v&quot;.</S><S sid ="12" ssid = "10">We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.</S><S sid ="55" ssid = "40">Each production can push or pop symbols on the stack as can be seen in the following productions that generate tree of the form shown in Figure 4b.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'", "'106'", "'32'", "'12'", "'55'"]
'156'
'106'
'32'
'12'
'55'
['156', '106', '32', '12', '55']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/D10-1044_sweta.csv
<S sid="4" ssid="1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S>
original cit marker offset is 0
new cit marker offset is 0



["4'"]
4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="1">We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="4">For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components (word-alignment model, language model, translation model, etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["7'"]
7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="14">Linear weights are difficult to incorporate into the standard MERT procedure because they are &#8220;hidden&#8221; within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn, 2007), we circumvent this problem by choosing weights to optimize corpus loglikelihood, which is roughly speaking the training criterion used by the LM and TM themselves.</S>
original cit marker offset is 0
new cit marker offset is 0



["50'"]
50'
['50']
parsed_discourse_facet ['method_citation']
<S sid="152" ssid="9">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["152'"]
152'
['152']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
 <S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="97" ssid="1">We carried out translation experiments in two different settings.</S>
original cit marker offset is 0
new cit marker offset is 0



["97'"]
97'
['97']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="12">Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="10">Finally, we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="141" ssid="10">Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L&#168;u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L&#168;u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["141'"]
141'
['141']
parsed_discourse_facet ['method_citation']
 <S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="37" ssid="1">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S>
original cit marker offset is 0
new cit marker offset is 0



["37'"]
37'
['37']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D10-1044.csv
<S sid ="32" ssid = "29">This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.</S><S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="101" ssid = "5">The second setting uses the news-related subcorpora for the NIST09 MT Chinese to English evaluation8 as IN  and the remaining NIST parallel Chinese/English corpora (UN  Hong Kong Laws  and Hong Kong Hansard) as OUT.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'2'", "'114'", "'101'", "'104'"]
'32'
'2'
'114'
'101'
'104'
['32', '2', '114', '101', '104']
parsed_discourse_facet ['implication_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="138" ssid = "7">However  for multinomial models like our LMs and TMs  there is a one to one correspondence between instances and features  eg the correspondence between a phrase pair (s  t) and its conditional multinomial probability p(s1t).</S><S sid ="144" ssid = "1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S><S sid ="133" ssid = "2">It is difficult to directly compare the Matsoukas et al results with ours  since our out-of-domain corpus is homogeneous; given heterogeneous training data  however  it would be trivial to include Matsoukas-style identity features in our instance-weighting model.</S><S sid ="99" ssid = "3">The dev and test sets were randomly chosen from the EMEA corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'138'", "'144'", "'133'", "'99'"]
'67'
'138'
'144'
'133'
'99'
['67', '138', '144', '133', '99']
parsed_discourse_facet ['implication_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="142" ssid = "11">There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).</S><S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'142'", "'8'", "'2'", "'38'"]
'67'
'142'
'8'
'2'
'38'
['67', '142', '8', '2', '38']
parsed_discourse_facet ['results_citation']
<S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="32" ssid = "29">This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.</S><S sid ="83" ssid = "20">We have not yet tried this.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daumes (2007) work that splits each feature into domain-specific and general copies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'32'", "'83'", "'78'", "'136'"]
'8'
'32'
'83'
'78'
'136'
['8', '32', '83', '78', '136']
parsed_discourse_facet ['method_citation']
<S sid ="133" ssid = "2">It is difficult to directly compare the Matsoukas et al results with ours  since our out-of-domain corpus is homogeneous; given heterogeneous training data  however  it would be trivial to include Matsoukas-style identity features in our instance-weighting model.</S><S sid ="118" ssid = "22">Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.</S><S sid ="5" ssid = "2">Even when there is training data available in the domain of interest  there is often additional data from other domains that could in principle be used to improve performance.</S><S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'118'", "'5'", "'1'", "'10'"]
'133'
'118'
'5'
'1'
'10'
['133', '118', '5', '1', '10']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="5" ssid = "2">Even when there is training data available in the domain of interest  there is often additional data from other domains that could in principle be used to improve performance.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daumes (2007) work that splits each feature into domain-specific and general copies.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S><S sid ="14" ssid = "11">There is a fairly large body of work on SMT adaptation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'5'", "'136'", "'104'", "'14'"]
'10'
'5'
'136'
'104'
'14'
['10', '5', '136', '104', '14']
parsed_discourse_facet ['method_citation']
<S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="95" ssid = "32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="125" ssid = "29">Somewhat surprisingly  there do not appear to be large systematic differences between linear and MAP combinations.</S><S sid ="20" ssid = "17">Daume (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'", "'95'", "'78'", "'125'", "'20'"]
'64'
'95'
'78'
'125'
'20'
['64', '95', '78', '125', '20']
parsed_discourse_facet ['method_citation']
<S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="36" ssid = "33">Section 5 covers relevant previous work on SMT adaptation  and section 6 concludes.</S><S sid ="19" ssid = "16">The idea of distinguishing between general and domain-specific examples is due to Daume and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.</S><S sid ="79" ssid = "16">This combination generalizes (2) and (3): we use either at = a to obtain a fixed-weight linear combination  or at = cI(t)/(cI(t) + 0) to obtain a MAP combination.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'36'", "'19'", "'79'", "'114'"]
'151'
'36'
'19'
'79'
'114'
['151', '36', '19', '79', '114']
parsed_discourse_facet ['method_citation']
<S sid ="118" ssid = "22">Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.</S><S sid ="50" ssid = "14">Linear weights are difficult to incorporate into the standard MERT procedure because they are hidden within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.</S><S sid ="88" ssid = "25">We have not explored this strategy.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S><S sid ="19" ssid = "16">The idea of distinguishing between general and domain-specific examples is due to Daume and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'", "'50'", "'88'", "'104'", "'19'"]
'118'
'50'
'88'
'104'
'19'
['118', '50', '88', '104', '19']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "15">For the LM  adaptive weights are set as follows: where  is a weight vector containing an element i for each domain (just IN and OUT in our case)  pi are the corresponding domain-specific models  and p(w  h) is an empirical distribution from a targetlanguage training corpuswe used the IN dev set for this.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the flattened variant described in section 3.2.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="118" ssid = "22">Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'114'", "'127'", "'67'", "'118'"]
'51'
'114'
'127'
'67'
'118'
['51', '114', '127', '67', '118']
parsed_discourse_facet ['implication_citation']
<S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="96" ssid = "33">We used it to score all phrase pairs in the OUT table  in order to provide a feature for the instance-weighting model.</S><S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="106" ssid = "10">The corpora for both settings are summarized in table 1.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'96'", "'128'", "'106'", "'111'"]
'1'
'96'
'128'
'106'
'111'
['1', '96', '128', '106', '111']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "7">Its success depends on the two domains being relatively close  and on the OUT corpus not being so large as to overwhelm the contribution of IN.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="41" ssid = "5">We do not adapt the alignment procedure for generating the phrase table from which the TM distributions are derived.</S><S sid ="60" ssid = "24">The matching sentence pairs are then added to the IN corpus  and the system is re-trained.</S><S sid ="0" ssid = "0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'64'", "'41'", "'60'", "'0'"]
'43'
'64'
'41'
'60'
'0'
['43', '64', '41', '60', '0']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="15" ssid = "12">We introduce several new ideas.</S><S sid ="71" ssid = "8">Finally  we incorporate the instance-weighting model into a general linear combination  and learn weights and mixing parameters simultaneously. where c(s  t) is a modified count for pair (s  t) in OUT  u(s|t) is a prior distribution  and y is a prior weight.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'67'", "'15'", "'71'", "'104'"]
'8'
'67'
'15'
'71'
'104'
['8', '67', '15', '71', '104']
parsed_discourse_facet ['method_citation']
<S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S><S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="43" ssid = "7">Its success depends on the two domains being relatively close  and on the OUT corpus not being so large as to overwhelm the contribution of IN.</S><S sid ="135" ssid = "4">Finally  we note that Jiangs instance-weighting framework is broader than we have presented above  encompassing among other possibilities the use of unlabelled IN data  which is applicable to SMT settings where source-only IN corpora are available.</S><S sid ="146" ssid = "3">The features are weighted within a logistic model to give an overall weight that is applied to the phrase pairs frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'", "'151'", "'43'", "'135'", "'146'"]
'104'
'151'
'43'
'135'
'146'
['104', '151', '43', '135', '146']
parsed_discourse_facet ['results_citation']
<S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="83" ssid = "20">We have not yet tried this.</S><S sid ="28" ssid = "25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'114'", "'83'", "'28'", "'17'"]
'16'
'114'
'83'
'28'
'17'
['16', '114', '83', '28', '17']
parsed_discourse_facet ['implication_citation']
<S sid ="89" ssid = "26">We used 22 features for the logistic weighting model  divided into two groups: one intended to reflect the degree to which a phrase pair belongs to general language  and one intended to capture similarity to the IN domain.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="110" ssid = "14">Je voudrais preciser  a` ladresse du commissaire Liikanen  quil nest pas aise de recourir aux tribunaux nationaux.</S><S sid ="109" ssid = "13"> I would also like to point out to commissioner Liikanen that it is not easy to take a matter to a national court.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'78'", "'110'", "'109'", "'67'"]
'89'
'78'
'110'
'109'
'67'
['89', '78', '110', '109', '67']
parsed_discourse_facet ['method_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="36" ssid = "33">Section 5 covers relevant previous work on SMT adaptation  and section 6 concludes.</S><S sid ="131" ssid = "35">The general-language features have a slight advantage over the similarity features  and both are better than the SVM feature.</S><S sid ="47" ssid = "11">Apart from MERT difficulties  a conceptual problem with log-linear combination is that it multiplies feature probabilities  essentially forcing different features to agree on high-scoring candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'64'", "'36'", "'131'", "'47'"]
'67'
'64'
'36'
'131'
'47'
['67', '64', '36', '131', '47']
parsed_discourse_facet ['results_citation']
<S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daumes (2007) work that splits each feature into domain-specific and general copies.</S><S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'136'", "'8'", "'11'", "'111'"]
'114'
'136'
'8'
'11'
'111'
['114', '136', '8', '11', '111']
parsed_discourse_facet ['aim_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S><S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="92" ssid = "29">6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.</S><S sid ="95" ssid = "32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'111'", "'128'", "'92'", "'95'"]
'67'
'111'
'128'
'92'
'95'
['67', '111', '128', '92', '95']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P08-1028_swastika.csv
<S sid="21" ssid="17">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="23">Our results show that the multiplicative models are superior and correlate significantly with behavioral data.</S>
original cit marker offset is 0
new cit marker offset is 0



['27']
27
['27']
parsed_discourse_facet ['result_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['189']
189
['189']
parsed_discourse_facet ['aim_citation']
<S sid="53" ssid="1">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.</S>
original cit marker offset is 0
new cit marker offset is 0



['53']
53
['53']
parsed_discourse_facet ['method_citation']
<S sid="76" ssid="24">The models considered so far assume that components do not &#8216;interfere&#8217; with each other, i.e., that It is also possible to re-introduce the dependence on K into the model of vector composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['76']
76
['76']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['57']
57
['57']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['57']
57
['57']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="60" ssid="8">To derive specific models from this general framework requires the identification of appropriate constraints to narrow the space of functions being considered.</S>
original cit marker offset is 0
new cit marker offset is 0



['60']
60
['60']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="73" ssid="21">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware, since semantically important constituents can participate more actively in the composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['73']
73
['73']
parsed_discourse_facet ['result_citation']
<S sid="99" ssid="12">In order to establish an independent measure of sentence similarity, we assembled a set of experimental materials and elicited similarity ratings from human subjects.</S>
original cit marker offset is 0
new cit marker offset is 0



['99']
99
['99']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['189']
189
['189']
parsed_discourse_facet ['aim_citation']
<S sid="191" ssid="3">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S>
original cit marker offset is 0
new cit marker offset is 0



['191']
191
['191']
parsed_discourse_facet ['result_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P08-1028.csv
<S sid ="200" ssid = "12">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid ="136" ssid = "49">We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine-grained semantic distinctions (see Table 1).</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants similarity judgments with those of the models.6 We used Spearmans p for our correlation analyses.</S><S sid ="135" ssid = "48">The average inter-subject agreement5 was  = 0.40.</S><S sid ="149" ssid = "62">Our composition models have no additional parameters beyond the semantic space just described  with three exceptions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'200'", "'136'", "'164'", "'135'", "'149'"]
'200'
'136'
'164'
'135'
'149'
['200', '136', '164', '135', '149']
parsed_discourse_facet ['implication_citation']
<S sid ="107" ssid = "20">Landmarks were taken from WordNet (Fellbaum  1998).</S><S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="45" ssid = "18">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid ="192" ssid = "4">We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'63'", "'42'", "'45'", "'192'"]
'107'
'63'
'42'
'45'
'192'
['107', '63', '42', '45', '192']
parsed_discourse_facet ['implication_citation']
<S sid ="30" ssid = "3">For the hierarchical structure of natural language this binding problem becomes particularly acute.</S><S sid ="78" ssid = "26">These vectors are not arbitrary and ideally they must exhibit some relation to the words of the construction under consideration.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants similarity judgments with those of the models.6 We used Spearmans p for our correlation analyses.</S><S sid ="171" ssid = "5">For comparison  we also show the human ratings for these items (UpperBound).</S><S sid ="176" ssid = "10">The multiplicative and combined models yield means closer to the human ratings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'78'", "'164'", "'171'", "'176'"]
'30'
'78'
'164'
'171'
'176'
['30', '78', '164', '171', '176']
parsed_discourse_facet ['results_citation']
<S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S><S sid ="85" ssid = "33">One potential drawback of multiplicative models is the effect of components with value zero.</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'", "'103'", "'85'", "'101'", "'63'"]
'42'
'103'
'85'
'101'
'63'
['42', '103', '85', '101', '63']
parsed_discourse_facet ['method_citation']
<S sid ="97" ssid = "10">Moreover by using a minimal structure we factor out inessential degrees of freedom and are able to assess the merits of different models on an equal footing.</S><S sid ="126" ssid = "39">49 unpaid volunteers completed the experiment  all native speakers of English.</S><S sid ="192" ssid = "4">We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials.</S><S sid ="120" ssid = "33">Examples of our items are given in Table 1.</S><S sid ="165" ssid = "78">Again  better models should correlate better with the experimental data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'126'", "'192'", "'120'", "'165'"]
'97'
'126'
'192'
'120'
'165'
['97', '126', '192', '120', '165']
parsed_discourse_facet ['method_citation']
<S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="45" ssid = "18">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="68" ssid = "16">Although the composition model in (5) is commonly used in the literature  from a linguistic perspective  the model in (6) is more appealing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'", "'121'", "'45'", "'116'", "'68'"]
'42'
'121'
'45'
'116'
'68'
['42', '121', '45', '116', '68']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "13">In the following we describe our data collection procedure and give details on how our composition models were constructed and evaluated.</S><S sid ="123" ssid = "36">Sentence pairs were presented serially in random order.</S><S sid ="112" ssid = "25">The stimuli were administered to four separate groups; each group saw one set of 100 sentences.</S><S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'123'", "'112'", "'25'", "'63'"]
'100'
'123'
'112'
'25'
'63'
['100', '123', '112', '25', '63']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S><S sid ="85" ssid = "33">One potential drawback of multiplicative models is the effect of components with value zero.</S><S sid ="145" ssid = "58">The latter were the most common context words (excluding a stop list of function words).</S><S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'85'", "'145'", "'121'", "'103'"]
'4'
'85'
'145'
'121'
'103'
['4', '85', '145', '121', '103']
parsed_discourse_facet ['method_citation']
<S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="45" ssid = "18">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid ="95" ssid = "8">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'45'", "'95'", "'150'", "'42'"]
'25'
'45'
'95'
'150'
'42'
['25', '45', '95', '150', '42']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "49">We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine-grained semantic distinctions (see Table 1).</S><S sid ="132" ssid = "45">We also measured how well humans agree in their ratings.</S><S sid ="158" ssid = "71">The m neighbors most similar to the predicate  and the k of m neighbors closest to its argument.</S><S sid ="200" ssid = "12">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid ="17" ssid = "13">It was not the sales manager who hit the bottle that day  but the office worker with the serious drinking problem. b.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'132'", "'158'", "'200'", "'17'"]
'136'
'132'
'158'
'200'
'17'
['136', '132', '158', '200', '17']
parsed_discourse_facet ['implication_citation']
<S sid ="95" ssid = "8">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="190" ssid = "2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid ="46" ssid = "19">However  since it is order independent  it cannot capture meaning differences that are modulated by differences in syntactic structure.</S><S sid ="29" ssid = "2">While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'150'", "'190'", "'46'", "'29'"]
'95'
'150'
'190'
'46'
'29'
['95', '150', '190', '46', '29']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="156" ssid = "69">This yielded a weighted sum consisting of 95% verb  0% noun and 5% of their multiplicative combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'103'", "'116'", "'150'", "'156'"]
'34'
'103'
'116'
'150'
'156'
['34', '103', '116', '150', '156']
parsed_discourse_facet ['method_citation']
<S sid ="176" ssid = "10">The multiplicative and combined models yield means closer to the human ratings.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="157" ssid = "70">Finally  Kintschs (2001) additive model has two extra parameters.</S><S sid ="88" ssid = "1">We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).</S><S sid ="38" ssid = "11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'", "'42'", "'157'", "'88'", "'38'"]
'176'
'42'
'157'
'88'
'38'
['176', '42', '157', '88', '38']
parsed_discourse_facet ['method_citation']
<S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="114" ssid = "27">For each reference verb  the subjects responses were entered into a contingency table  whose rows corresponded to nouns and columns to each possible answer (i.e.  one of the two landmarks).</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'116'", "'150'", "'59'", "'114'"]
'63'
'116'
'150'
'59'
'114'
['63', '116', '150', '59', '114']
parsed_discourse_facet ['results_citation']
<S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="64" ssid = "12">Now  if we assume that p lies in the same space as u and v  avoiding the issues of dimensionality associated with tensor products  and that f is a linear function  for simplicity  of the cartesian product of u and v  then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast  if we assume that f is a linear function of the tensor product of u and v  then we obtain multiplicative models: where C is a tensor of rank 3  which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'", "'121'", "'116'", "'64'", "'103'"]
'42'
'121'
'116'
'64'
'103'
['42', '121', '116', '64', '103']
parsed_discourse_facet ['implication_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="77" ssid = "25">For additive models  a natural way to achieve this is to include further vectors into the summation.</S><S sid ="70" ssid = "18">Instead  it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v  and vice versa.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="85" ssid = "33">One potential drawback of multiplicative models is the effect of components with value zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'77'", "'70'", "'59'", "'85'"]
'194'
'77'
'70'
'59'
'85'
['194', '77', '70', '59', '85']
parsed_discourse_facet ['method_citation']
<S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S><S sid ="130" ssid = "43">As we can see sentences with high similarity landmarks are perceived as more similar to the reference sentence.</S><S sid ="82" ssid = "30">In contrast to the simple additive model  this extended model is sensitive to syntactic structure  since n is chosen from among the neighbors of the predicate  distinguishing it from the argument.</S><S sid ="73" ssid = "21">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware  since semantically important constituents can participate more actively in the composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'121'", "'103'", "'130'", "'82'", "'73'"]
'121'
'103'
'130'
'82'
'73'
['121', '103', '130', '82', '73']
parsed_discourse_facet ['results_citation']



P08-1028
P10-1021
0
result_citation
['method_citation']
parsing: input/ref/Task1/P11-1060_aakansha.csv
<S sid="11" ssid="7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'"]
'2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="5">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'"]
'112'
['112']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="11">Although a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">However, we still model the logical form (now as a latent variable) to capture the complexities of language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="12">It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="21">The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="88" ssid="64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets, a restrictor A and a nuclear scope B.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'"]
'88'
['88']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S sid ="57" ssid = "33">But consider Figure 4: (a) is headed by borders  but states needs to be extracted; in (b)  the quantifier no is syntactically dominated by the head verb borders but needs to take wider scope.</S><S sid ="39" ssid = "15">Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.</S><S sid ="60" ssid = "36">Then higher up in the tree  we invoke it with an execute relation Xi to create the desired semantic scope.2 This mark-execute construct acts non-locally  so to maintain compositionality  we must augment the denotation d = JzKw to include any information about the marked nodes in z that can be accessed by an execute relation later on.</S><S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="167" ssid = "52">In DCS  we start with lexical triggers  which are 6 Conclusion more basic than CCG lexical entries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'", "'39'", "'60'", "'22'", "'167'"]
'57'
'39'
'60'
'22'
'167'
['57', '39', '60', '22', '167']
parsed_discourse_facet ['implication_citation']
<S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="71" ssid = "47">Let z be a DCS tree.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="120" ssid = "5">GEO has 48 non-value predicates and JOBS has 26.</S><S sid ="85" ssid = "61">Extraction allows us to return the set of consistent values of a marked non-root node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'71'", "'141'", "'120'", "'85'"]
'15'
'71'
'141'
'120'
'85'
['15', '71', '141', '120', '85']
parsed_discourse_facet ['implication_citation']
<S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S><S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parents denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="39" ssid = "15">Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.</S><S sid ="147" ssid = "32">However  training on just these examples is enough to improve the parameters  and this 29% increases to 66% and then to 95% over the next few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'51'", "'3'", "'39'", "'147'"]
'46'
'51'
'3'
'39'
'147'
['46', '51', '3', '39', '147']
parsed_discourse_facet ['results_citation']
<S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parents denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid ="84" ssid = "60">There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.</S><S sid ="170" ssid = "55">Our features as soft preferences.</S><S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'84'", "'170'", "'46'", "'98'"]
'51'
'84'
'170'
'46'
'98'
['51', '84', '170', '46', '98']
parsed_discourse_facet ['method_citation']
<S sid ="122" ssid = "7">For JOBS  if we use the standard Jobs database  close to half the ys are empty  which makes it uninteresting.</S><S sid ="94" ssid = "70">We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.</S><S sid ="102" ssid = "78">As a running example  consider x = city that is in California and z = hcity; 11:hloc; 21:hCAiii  where city triggers city and California triggers CA.</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S><S sid ="95" ssid = "71">Our first question is: given notation of the DCS tree before execution is an utterance x  what trees z  Z are permissible?</S>
original cit marker offset is 0
new cit marker offset is 0



["'122'", "'94'", "'102'", "'98'", "'95'"]
'122'
'94'
'102'
'98'
'95'
['122', '94', '102', '98', '95']
parsed_discourse_facet ['method_citation']
<S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="84" ssid = "60">There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'", "'3'", "'6'", "'84'", "'162'"]
'22'
'3'
'6'
'84'
'162'
['22', '3', '6', '84', '162']
parsed_discourse_facet ['method_citation']
<S sid ="13" ssid = "9">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x  y)  which is much cheaper to obtain than (x  z) pairs.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="29" ssid = "5">Let P be a set of predicates (e.g.  state  count  P)  which are just symbols.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="172" ssid = "57">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms  we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus  and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'134'", "'29'", "'77'", "'172'"]
'13'
'134'
'29'
'77'
'172'
['13', '134', '29', '77', '172']
parsed_discourse_facet ['method_citation']
<S sid ="96" ssid = "72">To California cities)  and it also allows us to underspecify L. In particular  our L will not include verbs or prepositions; rather  we rely on the predicates corresponding to those words to be triggered by traces.</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S><S sid ="94" ssid = "70">We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.</S><S sid ="124" ssid = "9">For each data predicate p (e.g.  language)  we add each possible tuple (e.g.  (job37  Java)) to w(p) independently with probability 0.8.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S>
original cit marker offset is 0
new cit marker offset is 0



["'96'", "'88'", "'94'", "'124'", "'162'"]
'96'
'88'
'94'
'124'
'162'
['96', '88', '94', '124', '162']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "84">However  in order to learn  we need to sum over {z  ZL(x) : JzKw = y}  and unfortunately  the additional constraint JzKw = y does not factorize.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="84" ssid = "60">There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'3'", "'84'", "'134'", "'46'"]
'108'
'3'
'84'
'134'
'46'
['108', '3', '84', '134', '46']
parsed_discourse_facet ['method_citation']
<S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'", "'98'", "'77'", "'162'", "'88'"]
'22'
'98'
'77'
'162'
'88'
['22', '98', '77', '162', '88']
parsed_discourse_facet ['implication_citation']
<S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'134'", "'3'", "'141'", "'98'"]
'12'
'134'
'3'
'141'
'98'
['12', '134', '3', '141', '98']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="29" ssid = "5">Let P be a set of predicates (e.g.  state  count  P)  which are just symbols.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="129" ssid = "14">We also define an augmented lexicon L+ which includes a prototype word x for each predicate appearing in (iii) above (e.g.  (large  size))  which cancels the predicates triggered by xs POS tag.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'29'", "'77'", "'129'", "'8'"]
'3'
'29'
'77'
'129'
'8'
['3', '29', '77', '129', '8']
parsed_discourse_facet ['method_citation']
<S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S><S sid ="142" ssid = "27">This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.</S><S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="144" ssid = "29">Intuitions How is our system learning?</S><S sid ="112" ssid = "88">Our learning algorithm alternates between (i) using the current parameters  to generate the K-best set ZL (x) for each training example x  and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'142'", "'10'", "'144'", "'112'"]
'46'
'142'
'10'
'144'
'112'
['46', '142', '10', '144', '112']
parsed_discourse_facet ['method_citation']
<S sid ="133" ssid = "18">Table 2 shows that our system using lexical triggers L (henceforth  DCS) outperforms SEMRESP (78.9% over 73.2%).</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="163" ssid = "48">Feedback from the context; for example  the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x  y)  which means (Schuler  2003) and semantic parsing (Popescu et borders looks right for the first argument and left al.  2003; Clarke et al.  2010).</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'134'", "'163'", "'141'", "'10'"]
'133'
'134'
'163'
'141'
'10'
['133', '134', '163', '141', '10']
parsed_discourse_facet ['results_citation']
<S sid ="86" ssid = "62">Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  (i  )]{1 = }.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="113" ssid = "89">Formally  let O(  ') be the objective function O() with ZL(x) ZL I(x).</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'162'", "'77'", "'3'", "'113'"]
'86'
'162'
'77'
'3'
'113'
['86', '162', '77', '3', '113']
parsed_discourse_facet ['implication_citation']
<S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="55" ssid = "31">The tree structure still enables us to compute denotations efficiently based on (1) and (2).</S><S sid ="80" ssid = "56">The full definition of join is as follows: Aggregate The aggregate operation takes a denotation and forms a set out of the tuples in the first column for each setting of the rest of the columns: Now we turn to the mark (M) and execute (Xi) operations  which handles the divergence between syntactic and semantic scope.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'134'", "'3'", "'55'", "'80'"]
'10'
'134'
'3'
'55'
'80'
['10', '134', '3', '55', '80']
parsed_discourse_facet ['method_citation']
<S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="50" ssid = "26">For example  consider the phrase number of major cities  and suppose that number corresponds to the count predicate.</S><S sid ="113" ssid = "89">Formally  let O(  ') be the objective function O() with ZL(x) ZL I(x).</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'", "'134'", "'154'", "'50'", "'113'"]
'22'
'134'
'154'
'50'
'113'
['22', '134', '154', '50', '113']
parsed_discourse_facet ['results_citation']
<S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="126" ssid = "11">During development  we further held out a random 30% of the training sets for validation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'134'", "'3'", "'12'", "'126'"]
'6'
'134'
'3'
'12'
'126'
['6', '134', '3', '12', '126']
parsed_discourse_facet ['aim_citation']
<S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S><S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="172" ssid = "57">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms  we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus  and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'132'", "'9'", "'10'", "'172'", "'134'"]
'132'
'9'
'10'
'172'
'134'
['132', '9', '10', '172', '134']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W06-3114_swastika.csv
<S sid="47" ssid="13">Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data.</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
    <S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="37">Our suspicion is that BLEU is very sensitive to jargon, to selecting exactly the right words, and not synonyms that human judges may appreciate as equally good.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
<S sid="145" ssid="38">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S>
original cit marker offset is 0
new cit marker offset is 0



['145']
145
['145']
parsed_discourse_facet ['result_citation']
    <S sid="103" ssid="19">Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['103']
103
['103']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="16">Following this method, we repeatedly &#8212; say, 1000 times &#8212; sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.</S>
original cit marker offset is 0
new cit marker offset is 0



['50']
50
['50']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="7">We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.</S>
original cit marker offset is 0
new cit marker offset is 0



['68']
68
['68']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="8">The way judgements are collected, human judges tend to use the scores to rank systems against each other.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['result_citation']
<S sid="145" ssid="38">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S>
original cit marker offset is 0
new cit marker offset is 0



['145']
145
['145']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="22" ssid = "15">The text type are editorials instead of speech transcripts.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S><S sid ="11" ssid = "4">To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.</S><S sid ="167" ssid = "60">One annotator suggested that this was the case for as much as 10% of our test sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'22'", "'155'", "'11'", "'167'"]
'59'
'22'
'155'
'11'
'167'
['59', '22', '155', '11', '167']
parsed_discourse_facet ['implication_citation']
<S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="34" ssid = "27">For more on the participating systems  please refer to the respective system description in the proceedings of the workshop.</S><S sid ="89" ssid = "5">In words  the judgements are normalized  so that the average normalized judgement per judge is 3.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S><S sid ="160" ssid = "53">Annotators suggested that long sentences are almost impossible to judge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'34'", "'89'", "'155'", "'160'"]
'136'
'34'
'89'
'155'
'160'
['136', '34', '89', '155', '160']
parsed_discourse_facet ['implication_citation']
<S sid ="27" ssid = "20">Microsofts approach uses dependency trees  others use hierarchical phrase models.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="115" ssid = "8">Often  two systems can not be distinguished with a confidence of over 95%  so there are ranked the same.</S><S sid ="15" ssid = "8">Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.</S><S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'", "'1'", "'115'", "'15'", "'59'"]
'27'
'1'
'115'
'15'
'59'
['27', '1', '115', '15', '59']
parsed_discourse_facet ['results_citation']
<S sid ="140" ssid = "33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid ="22" ssid = "15">The text type are editorials instead of speech transcripts.</S><S sid ="26" ssid = "19">Most of these groups follow a phrase-based statistical approach to machine translation.</S><S sid ="82" ssid = "21">This decreases the statistical significance of our results compared to those studies.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'22'", "'26'", "'82'", "'136'"]
'140'
'22'
'26'
'82'
'136'
['140', '22', '26', '82', '136']
parsed_discourse_facet ['method_citation']
<S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="131" ssid = "24">The manual scores are averages over the raw unnormalized scores.</S><S sid ="119" ssid = "12">There may be occasionally a system clearly at the top or at the bottom  but most systems are so close that it is hard to distinguish them.</S><S sid ="44" ssid = "10">We computed BLEU scores for each submission with a single reference translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'84'", "'131'", "'119'", "'44'"]
'125'
'84'
'131'
'119'
'44'
['125', '84', '131', '119', '44']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "12">We aligned the texts at a sentence level across all four languages  resulting in 1064 sentence per language.</S><S sid ="63" ssid = "2">Many human evaluation metrics have been proposed.</S><S sid ="135" ssid = "28">The differences in difficulty are better reflected in the BLEU scores than in the raw un-normalized manual judgements.</S><S sid ="11" ssid = "4">To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'63'", "'135'", "'11'", "'1'"]
'19'
'63'
'135'
'11'
'1'
['19', '63', '135', '11', '1']
parsed_discourse_facet ['method_citation']
<S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="51" ssid = "17">When dropping the top and bottom 2.5% the remaining BLEU scores define the range of the confidence interval.</S><S sid ="146" ssid = "39">This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.</S><S sid ="139" ssid = "32">Given the closeness of most systems and the wide over-lapping confidence intervals it is hard to make strong statements about the correlation between human judgements and automatic scoring methods such as BLEU.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'51'", "'146'", "'139'", "'1'"]
'163'
'51'
'146'
'139'
'1'
['163', '51', '146', '139', '1']
parsed_discourse_facet ['method_citation']
<S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="67" ssid = "6">Participants and other volunteers contributed about 180 hours of labor in the manual evaluation.</S><S sid ="26" ssid = "19">Most of these groups follow a phrase-based statistical approach to machine translation.</S><S sid ="55" ssid = "21">If one system is better in 95% of the sample sets  we conclude that its higher BLEU score is statistically significantly better.</S><S sid ="89" ssid = "5">In words  the judgements are normalized  so that the average normalized judgement per judge is 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'67'", "'26'", "'55'", "'89'"]
'163'
'67'
'26'
'55'
'89'
['163', '67', '26', '55', '89']
parsed_discourse_facet ['method_citation']
<S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="121" ssid = "14">For the automatic scoring method BLEU  we can distinguish three quarters of the systems.</S><S sid ="161" ssid = "54">Since all long sentence translation are somewhat muddled  even a contrastive evaluation between systems was difficult.</S><S sid ="30" ssid = "23">The other half was replaced by other participants  so we ended up with roughly the same number.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'136'", "'121'", "'161'", "'30'"]
'84'
'136'
'121'
'161'
'30'
['84', '136', '121', '161', '30']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="50" ssid = "16">Following this method  we repeatedly  say  1000 times  sample sets of sentences from the output of each system  measure their BLEU score  and use these 1000 BLEU scores as basis for estimating a confidence interval.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="83" ssid = "22">The number of judgements is additionally fragmented by our breakup of sentences into in-domain and out-of-domain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'163'", "'50'", "'84'", "'83'"]
'136'
'163'
'50'
'84'
'83'
['136', '163', '50', '84', '83']
parsed_discourse_facet ['implication_citation']
<S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="119" ssid = "12">There may be occasionally a system clearly at the top or at the bottom  but most systems are so close that it is hard to distinguish them.</S><S sid ="82" ssid = "21">This decreases the statistical significance of our results compared to those studies.</S><S sid ="43" ssid = "9">At the very least  we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.</S><S sid ="10" ssid = "3">Figure 1 provides some statistics about this corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'119'", "'82'", "'43'", "'10'"]
'59'
'119'
'82'
'43'
'10'
['59', '119', '82', '43', '10']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "8">We settled on contrastive evaluations of 5 system outputs for a single test sentence.</S><S sid ="99" ssid = "15">Systems that generally do worse than others will receive a negative one.</S><S sid ="112" ssid = "5">The normalization on a per-judge basis gave very similar ranking  only slightly less consistent with the ranking from the pairwise comparisons.</S><S sid ="62" ssid = "1">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems  they are only a imperfect substitute for human assessment of translation quality  or as the acronym BLEU puts it  a bilingual evaluation understudy.</S><S sid ="98" ssid = "14">Systems that generally do better than others will receive a positive average normalizedjudgement per sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'99'", "'112'", "'62'", "'98'"]
'69'
'99'
'112'
'62'
'98'
['69', '99', '112', '62', '98']
parsed_discourse_facet ['method_citation']
<S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="110" ssid = "3">In the graphs  system scores are indicated by a point  the confidence intervals by shaded areas around the point.</S><S sid ="146" ssid = "39">This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.</S><S sid ="90" ssid = "6">Another way to view the judgements is that they are less quality judgements of machine translation systems per se  but rankings of machine translation systems.</S><S sid ="33" ssid = "26">While building a machine translation system is a serious undertaking  in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'110'", "'146'", "'90'", "'33'"]
'125'
'110'
'146'
'90'
'33'
['125', '110', '146', '90', '33']
parsed_discourse_facet ['method_citation']
<S sid ="98" ssid = "14">Systems that generally do better than others will receive a positive average normalizedjudgement per sentence.</S><S sid ="148" ssid = "41">The best answer to this is: many research labs have very competitive systems whose performance is hard to tell apart.</S><S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S><S sid ="37" ssid = "3">It rewards matches of n-gram sequences  but measures only at most indirectly overall grammatical coherence.</S><S sid ="102" ssid = "18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems  we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'98'", "'148'", "'6'", "'37'", "'102'"]
'98'
'148'
'6'
'37'
'102'
['98', '148', '6', '37', '102']
parsed_discourse_facet ['results_citation']
<S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="177" ssid = "1">This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency  Contract No.</S><S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="3" ssid = "1">was done by the participants.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'177'", "'59'", "'1'", "'3'"]
'125'
'177'
'59'
'1'
'3'
['125', '177', '59', '1', '3']
parsed_discourse_facet ['implication_citation']
<S sid ="90" ssid = "6">Another way to view the judgements is that they are less quality judgements of machine translation systems per se  but rankings of machine translation systems.</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="135" ssid = "28">The differences in difficulty are better reflected in the BLEU scores than in the raw un-normalized manual judgements.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'0'", "'125'", "'126'", "'135'"]
'90'
'0'
'125'
'126'
'135'
['90', '0', '125', '126', '135']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "17">When dropping the top and bottom 2.5% the remaining BLEU scores define the range of the confidence interval.</S><S sid ="39" ssid = "5">However  a recent study (Callison-Burch et al.  2006)  pointed out that this correlation may not always be strong.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S><S sid ="26" ssid = "19">Most of these groups follow a phrase-based statistical approach to machine translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'39'", "'1'", "'162'", "'26'"]
'51'
'39'
'1'
'162'
'26'
['51', '39', '1', '162', '26']
parsed_discourse_facet ['results_citation']
<S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="112" ssid = "5">The normalization on a per-judge basis gave very similar ranking  only slightly less consistent with the ranking from the pairwise comparisons.</S><S sid ="174" ssid = "5">The manual evaluation of scoring translation on a graded scale from 15 seems to be very hard to perform.</S><S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'112'", "'174'", "'59'", "'125'"]
'136'
'112'
'174'
'59'
'125'
['136', '112', '174', '59', '125']
parsed_discourse_facet ['aim_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W99-0623_sweta.csv
 <S sid="144" ssid="6">Combining multiple highly-accurate independent parsers yields promising results.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
 <S sid="125" ssid="54">The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["125'"]
125'
['125']
parsed_discourse_facet ['method_citation']
<S sid="125" ssid="54">The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["125'"]
125'
['125']
parsed_discourse_facet ['method_citation']
    <S sid="48" ssid="34">&#8226; Similarly, when the na&#239;ve Bayes classifier is configured such that the constituents require estimated probabilities strictly larger than 0.5 to be accepted, there is not enough probability mass remaining on crossing brackets for them to be included in the hypothesis.</S>
original cit marker offset is 0
new cit marker offset is 0



["48'"]
48'
['48']
parsed_discourse_facet ['method_citation']
 <S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="63">As seen by the drop in average individual parser performance baseline, the introduced parser does not perform very well.</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
    <S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="13" ssid="9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993).</S>
original cit marker offset is 0
new cit marker offset is 0



["13'"]
13'
['13']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="37">From this we see that a finer-grained model for parser combination, at least for the features we have examined, will not give us any additional power.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="98" ssid="27">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["98'"]
98'
['98']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="13">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'"]
27'
['27']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="9">For our experiments we also report the mean of precision and recall, which we denote by (P + R)I2 and F-measure.</S>
    <S sid="81" ssid="10">F-measure is the harmonic mean of precision and recall, 2PR/(P + R).</S>
    <S sid="82" ssid="11">It is closer to the smaller value of precision and recall when there is a large skew in their values.</S>
original cit marker offset is 0
new cit marker offset is 0



["80'", "'81'", "'82'"]
80'
'81'
'82'
['80', '81', '82']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="35">In general, the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["49'"]
49'
['49']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="7">Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["11'"]
11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="98" ssid="27">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["98'"]
98'
['98']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0623.csv
<S sid ="34" ssid = "20">Mi(c) is a binary function returning t when parser i (from among the k parsers) suggests constituent c should be in the parse.</S><S sid ="106" ssid = "35">In each figure the upper graph shows the isolated constituent precision and the bottom graph shows the corresponding number of hypothesized constituents.</S><S sid ="66" ssid = "52">Each decision determines the inclusion or exclusion of a candidate constituent.</S><S sid ="65" ssid = "51">We model each parse as the decisions made to create it  and model those decisions as independent events.</S><S sid ="2" ssid = "2">Two general approaches are presented and two combination techniques are described for each approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'106'", "'66'", "'65'", "'2'"]
'34'
'106'
'66'
'65'
'2'
['34', '106', '66', '65', '2']
Error in Discourse Facet
<S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'15'", "'130'", "'57'", "'50'"]
'18'
'15'
'130'
'57'
'50'
['18', '15', '130', '57', '50']
Error in Discourse Facet
<S sid ="122" ssid = "51">All of these systems were run on data that was not seen during their development.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="24" ssid = "10">We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="107" ssid = "36">Again we notice that the isolated constituent precision is larger than 0.5 only in those partitions that contain very few samples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'122'", "'11'", "'24'", "'61'", "'107'"]
'122'
'11'
'24'
'61'
'107'
['122', '11', '24', '61', '107']
Error in Discourse Facet
<S sid ="92" ssid = "21">While we cannot prove there are no such useful features on which one should condition trust  we can give some insight into why the features we explored offered no gain.</S><S sid ="111" ssid = "40">The first row represents the average accuracy of the three parsers we combine.</S><S sid ="104" ssid = "33">In the cases where isolated constituent precision is larger than 0.5 the affected portion of the hypotheses is negligible.</S><S sid ="42" ssid = "28">Call the crossing constituents A and B.</S><S sid ="117" ssid = "46">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'111'", "'104'", "'42'", "'117'"]
'92'
'111'
'104'
'42'
'117'
['92', '111', '104', '42', '117']
Error in Discourse Facet
<S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="39" ssid = "25">There are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.</S><S sid ="63" ssid = "49">The probabilistic version of this procedure is straightforward: We once again assume independence among our various member parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'139'", "'11'", "'39'", "'63'"]
'15'
'139'
'11'
'39'
'63'
['15', '139', '11', '39', '63']
Error in Discourse Facet
<S sid ="39" ssid = "25">There are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.</S><S sid ="86" ssid = "15">Finally we show the combining techniques degrade very little when a poor parser is added to the set.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a nave Bayes classifier to determine which constituents to include in the parse.</S><S sid ="17" ssid = "3">The substructures that are unanimously hypothesized by the parsers should be preserved after combination  and the combination technique should not foolishly create substructures for which there is no supporting evidence.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'86'", "'27'", "'17'", "'61'"]
'39'
'86'
'27'
'17'
'61'
['39', '86', '27', '17', '61']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="24" ssid = "10">We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'72'", "'21'", "'24'", "'139'"]
'112'
'72'
'21'
'24'
'139'
['112', '72', '21', '24', '139']
Error in Discourse Facet
<S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S><S sid ="96" ssid = "25">We call such a constituent an isolated constituent.</S><S sid ="132" ssid = "61">The results of this experiment can be seen in Table 5.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and nave Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="125" ssid = "54">The constituent voting and nave Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'139'", "'96'", "'132'", "'38'", "'125'"]
'139'
'96'
'132'
'38'
'125'
['139', '96', '132', '38', '125']
Error in Discourse Facet
<S sid ="43" ssid = "29">A receives a votes  and B receives b votes.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="84" ssid = "13">The first shows how constituent features and context do not help in deciding which parser to trust.</S><S sid ="93" ssid = "22">Because we are working with only three parsers  the only situation in which context will help us is when it can indicate we should choose to believe a single parser that disagrees with the majority hypothesis instead of the majority hypothesis itself.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'15'", "'52'", "'84'", "'93'"]
'43'
'15'
'52'
'84'
'93'
['43', '15', '52', '84', '93']
Error in Discourse Facet
<S sid ="101" ssid = "30">We show the results of three of the experiments we conducted to measure isolated constituent precision under various partitioning schemes.</S><S sid ="126" ssid = "55">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a nave Bayes classifier to determine which constituents to include in the parse.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="43" ssid = "29">A receives a votes  and B receives b votes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'126'", "'27'", "'52'", "'43'"]
'101'
'126'
'27'
'52'
'43'
['101', '126', '27', '52', '43']
Error in Discourse Facet
<S sid ="126" ssid = "55">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S><S sid ="142" ssid = "4">Both of the switching techniques  as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.</S><S sid ="111" ssid = "40">The first row represents the average accuracy of the three parsers we combine.</S><S sid ="43" ssid = "29">A receives a votes  and B receives b votes.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'142'", "'111'", "'43'", "'52'"]
'126'
'142'
'111'
'43'
'52'
['126', '142', '111', '43', '52']
Error in Discourse Facet
<S sid ="101" ssid = "30">We show the results of three of the experiments we conducted to measure isolated constituent precision under various partitioning schemes.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="106" ssid = "35">In each figure the upper graph shows the isolated constituent precision and the bottom graph shows the corresponding number of hypothesized constituents.</S><S sid ="22" ssid = "8">If enough parsers suggest that a particular constituent belongs in the parse  we include it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'50'", "'15'", "'106'", "'22'"]
'101'
'50'
'15'
'106'
'22'
['101', '50', '15', '106', '22']
Error in Discourse Facet
<S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S><S sid ="111" ssid = "40">The first row represents the average accuracy of the three parsers we combine.</S><S sid ="30" ssid = "16">This is equivalent to the assumption used in probability estimation for nave Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.</S><S sid ="28" ssid = "14">The development of a nave Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a nave Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'111'", "'30'", "'28'", "'27'"]
'50'
'111'
'30'
'28'
'27'
['50', '111', '30', '28', '27']
Error in Discourse Facet
<S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="140" ssid = "2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S><S sid ="131" ssid = "60">It was then tested on section 22 of the Treebank in conjunction with the other parsers.</S><S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="34" ssid = "20">Mi(c) is a binary function returning t when parser i (from among the k parsers) suggests constituent c should be in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'140'", "'131'", "'112'", "'34'"]
'61'
'140'
'131'
'112'
'34'
['61', '140', '131', '112', '34']
Error in Discourse Facet
<S sid ="87" ssid = "16">It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S><S sid ="37" ssid = "23">Here NO counts the number of hypothesized constituents in the development set that match the binary predicate specified as an argument.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="100" ssid = "29">When this metric is less than 0.5  we expect to incur more errors' than we will remove by adding those constituents to the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'", "'57'", "'37'", "'11'", "'100'"]
'87'
'57'
'37'
'11'
'100'
['87', '57', '37', '11', '100']
Error in Discourse Facet
<S sid ="83" ssid = "12">We performed three experiments to evaluate our techniques.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="108" ssid = "37">From this we see that a finer-grained model for parser combination  at least for the features we have examined  will not give us any additional power.</S><S sid ="107" ssid = "36">Again we notice that the isolated constituent precision is larger than 0.5 only in those partitions that contain very few samples.</S><S sid ="118" ssid = "47">The maximum precision oracle is an upper bound on the possible gain we can achieve by parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'11'", "'108'", "'107'", "'118'"]
'83'
'11'
'108'
'107'
'118'
['83', '11', '108', '107', '118']
Error in Discourse Facet
<S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="30" ssid = "16">This is equivalent to the assumption used in probability estimation for nave Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="113" ssid = "42">The next two rows are results of oracle experiments.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'30'", "'52'", "'113'", "'50'"]
'18'
'30'
'52'
'113'
'50'
['18', '30', '52', '113', '50']
Error in Discourse Facet
IGNORE THIS: key error 1
parsing: input/ref/Task1/P87-1015_swastika.csv
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="149" ssid="34">We can obtain a letter equivalent CFL defined by a CFG in which the for each rule as above, we have the production A &#8212;* A1 Anup where tk (up) = cp.</S>
original cit marker offset is 0
new cit marker offset is 0



['149']
149
['149']
parsed_discourse_facet ['method_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
    <S sid="2" ssid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['2']
2
['2']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="222" ssid="28">However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



['222']
222
['222']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['???']
???
['???']
parsed_discourse_facet ['aim_citation']
<S sid="207" ssid="13">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['207']
207
['207']
parsed_discourse_facet ['result_citation']
<S sid="207" ssid="13">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['207']
207
['207']
parsed_discourse_facet ['result_citation']
<S sid="19" ssid="4">It can be easily shown from Thatcher's result that the path set of every local set is a regular set.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/P87-1015.csv
<S sid ="191" ssid = "76">In addition to the tapes required to store the indices  M requires one work tape for splitting the substrings.</S><S sid ="206" ssid = "12">As suggested in Section 4.3.2  a derivation with independent paths can be divided into subcomputations with limited sharing of information.</S><S sid ="186" ssid = "71">To do this  the x's and y's are stored in the next 2ni + 2n2 tapes  and M goes to a universal state.</S><S sid ="104" ssid = "10">Pumping t2 will change only one branch and leave the other branch unaffected.</S><S sid ="192" ssid = "77">Thus  the ATM has no more than 6km&quot; + 1 work tapes  where km&quot; is the maximum number of substrings spanned by a derived structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'191'", "'206'", "'186'", "'104'", "'192'"]
'191'
'206'
'186'
'104'
'192'
['191', '206', '186', '104', '192']
parsed_discourse_facet ['implication_citation']
<S sid ="84" ssid = "69">((fii  Q2  Pa)    (01  i32  03)   e) (02 e) Oa  en) The path complexity of the tee set generated by a MCTAG is not necessarily context-free.</S><S sid ="54" ssid = "39">An IG can be viewed as a CFG in which each nonterminal is associated with a stack.</S><S sid ="101" ssid = "7">The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently.</S><S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="71" ssid = "56">0n0'i'i0'2&quot;bin242bn I n = 711 + n2 } On the other hand  no linguistic use is made of this general form of composition and Steedman (personal communication) and Steedman (1986) argues that a more limited definition of composition is more natural.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'54'", "'101'", "'59'", "'71'"]
'84'
'54'
'101'
'59'
'71'
['84', '54', '101', '59', '71']
parsed_discourse_facet ['implication_citation']
<S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="44" ssid = "29">The edge from the root to the subtree for the derivation of 7i is labeled by the address ni.</S><S sid ="136" ssid = "21">These two restrictions impose the constraint that the result of composing any two structures should be a structure whose &quot;size&quot; is the sum of its constituents plus some constant For example  the operation 4  discussed in the case of CFG's (in Section 4.1) adds the constant equal to the sum of the length of the strings VI  un+r Since we are considering formalisms with arbitrary structures it is difficult to precisely specify all of the restrictions on the composition operations that we believe would appropriately generalize the concatenation operation for the particular structures used by the formalism.</S><S sid ="28" ssid = "13">A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'44'", "'136'", "'28'", "'153'"]
'151'
'44'
'136'
'28'
'153'
['151', '44', '136', '28', '153']
parsed_discourse_facet ['results_citation']
<S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="134" ssid = "19">These systems are similar to those described by Pollard (1984) as Generalized Context-Free Grammars (GCFG's).</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="112" ssid = "18">.t The path set of tree sets at level k +1 have the complexity of the string language of level k. The independence of paths in a tree set appears to be an important property.</S><S sid ="20" ssid = "5">As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'134'", "'153'", "'112'", "'20'"]
'59'
'134'
'153'
'112'
'20'
['59', '134', '153', '112', '20']
parsed_discourse_facet ['method_citation']
<S sid ="106" ssid = "12">We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths.</S><S sid ="54" ssid = "39">An IG can be viewed as a CFG in which each nonterminal is associated with a stack.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="101" ssid = "7">The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently.</S><S sid ="182" ssid = "67">Since each zi is a contiguous substring of the input (say ai )  and no two substrings overlap  we can represent zi by the pair of integers (i2  i2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'54'", "'143'", "'101'", "'182'"]
'106'
'54'
'143'
'101'
'182'
['106', '54', '143', '101', '182']
parsed_discourse_facet ['method_citation']
<S sid ="55" ssid = "40">Each production can push or pop symbols on the stack as can be seen in the following productions that generate tree of the form shown in Figure 4b.</S><S sid ="117" ssid = "2">Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.</S><S sid ="200" ssid = "6">The importance of this property becomes clear in contrasting theories underlying GPSG (Gazdar  Klein  Pulluna  and Sag  1985)  and GB (as described by Berwick  1984) with those underlying LFG and FUG.</S><S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="106" ssid = "12">We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'117'", "'200'", "'59'", "'106'"]
'55'
'117'
'200'
'59'
'106'
['55', '117', '200', '59', '106']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "17">When 7' is adjoined at ?I in the tree 7 we obtain a tree v&quot;.</S><S sid ="121" ssid = "6">First  any grammar must involve a finite number of elementary structures  composed using a finite number of composition operations.</S><S sid ="19" ssid = "4">It can be easily shown from Thatcher's result that the path set of every local set is a regular set.</S><S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="138" ssid = "23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'121'", "'19'", "'151'", "'138'"]
'32'
'121'
'19'
'151'
'138'
['32', '121', '19', '151', '138']
parsed_discourse_facet ['method_citation']
<S sid ="188" ssid = "73">Thus  for example  one successor process will be have M to be in the existential state qa with the indices encoding xi     xn  in the first 2n i tapes.</S><S sid ="118" ssid = "3">In the remainder of the paper  we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S><S sid ="89" ssid = "74">Hence  trees shown in Figure 8 can not be generated by any MCTAG (but can be generated by an IG) because the number of pairs of dependent paths grows with n. Since the derivation tees of TAG's  MCTAG's  and HG's are local sets  the choice of the structure used at each point in a derivation in these systems does not depend on the context at that point within the derivation.</S><S sid ="96" ssid = "2">A tree set may be said to have dependencies between paths if some &quot;appropriate&quot; subset can be shown to have dependent paths as defined above.</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'188'", "'118'", "'89'", "'96'", "'227'"]
'188'
'118'
'89'
'96'
'227'
['188', '118', '89', '96', '227']
parsed_discourse_facet ['method_citation']
<S sid ="84" ssid = "69">((fii  Q2  Pa)    (01  i32  03)   e) (02 e) Oa  en) The path complexity of the tee set generated by a MCTAG is not necessarily context-free.</S><S sid ="98" ssid = "4">Thatcher (1973) describes a tee pumping lemma for recognizable sets related to the string pumping lemma for regular sets.</S><S sid ="12" ssid = "10">We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.</S><S sid ="179" ssid = "64">It can be seen that M performs a top-down recognition of the input al ... nin logspace.</S><S sid ="185" ssid = "70">Each spawned process must check if xi     xn  and   yn  can be derived from B and C  respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'98'", "'12'", "'179'", "'185'"]
'84'
'98'
'12'
'179'
'185'
['84', '98', '12', '179', '185']
parsed_discourse_facet ['method_citation']
<S sid ="207" ssid = "13">We outlined the definition of a family of constrained grammatical formalisms  called Linear Context-Free Rewriting Systems.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="26" ssid = "11">The productions of HG's are very similar to those of CFG's except that the operation used must be made explicit.</S><S sid ="178" ssid = "63">We define an ATM  M  recognizing a language generated by a grammar  G  having the properties discussed in Section 43.</S><S sid ="155" ssid = "40">CFG's  TAG's  MCTAG's and HG's are all members of this class since they satisfy these restrictions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'153'", "'26'", "'178'", "'155'"]
'207'
'153'
'26'
'178'
'155'
['207', '153', '26', '178', '155']
parsed_discourse_facet ['implication_citation']
<S sid ="38" ssid = "23">Thus  the derivation trees for TAG's have the same structure as local sets.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="165" ssid = "50">This class of formalisms have the properties that their derivation trees are local sets  and manipulate objects  using a finite number of composition operations that use a finite number of symbols.</S><S sid ="62" ssid = "47">TAG's can be shown to be equivalent to this restricted system.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'153'", "'165'", "'62'", "'143'"]
'38'
'153'
'165'
'62'
'143'
['38', '153', '165', '62', '143']
parsed_discourse_facet ['method_citation']
<S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="132" ssid = "17">In TAG's the elementary tree and addresses where adjunction takes place are used to instantiate the operation.</S><S sid ="101" ssid = "7">The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently.</S><S sid ="96" ssid = "2">A tree set may be said to have dependencies between paths if some &quot;appropriate&quot; subset can be shown to have dependent paths as defined above.</S><S sid ="94" ssid = "79">The semilinearity of Tree Adjoining Languages (TAL's)  MCTAL's  and Head Languages (HL's) can be proved using this property  with suitable restrictions on the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'132'", "'101'", "'96'", "'94'"]
'143'
'132'
'101'
'96'
'94'
['143', '132', '101', '96', '94']
parsed_discourse_facet ['method_citation']
<S sid ="89" ssid = "74">Hence  trees shown in Figure 8 can not be generated by any MCTAG (but can be generated by an IG) because the number of pairs of dependent paths grows with n. Since the derivation tees of TAG's  MCTAG's  and HG's are local sets  the choice of the structure used at each point in a derivation in these systems does not depend on the context at that point within the derivation.</S><S sid ="164" ssid = "49">Although embedding this version of LCFRS's in the framework of ILFP developed by Rounds (1985) is straightforward  our motivation was to capture properties shared by a family of grammatical systems and generalize them defining a class of related formalisms.</S><S sid ="17" ssid = "2">We define the path set of a tree 1 as the set of strings that label a path from the root to frontier of 7.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="26" ssid = "11">The productions of HG's are very similar to those of CFG's except that the operation used must be made explicit.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'164'", "'17'", "'143'", "'26'"]
'89'
'164'
'17'
'143'
'26'
['89', '164', '17', '143', '26']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">For example  Gazdar (1985) discusses the applicability of Indexed Grammars (IG's) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar (LFG) and Government and Bindings grammars (GB).</S><S sid ="91" ssid = "76">We characterize a class of formalisms that have this property in Section 4.</S><S sid ="68" ssid = "53">This kind of dependency arises from the use of the composition operation to compose two arbitrarily large categories.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="189" ssid = "74">For rules p : A fpo such that fp is constant function  giving an elementary structure  fp is defined such that fp() = (Si ... xi() where each z is a constant string.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'91'", "'68'", "'143'", "'189'"]
'6'
'91'
'68'
'143'
'189'
['6', '91', '68', '143', '189']
parsed_discourse_facet ['results_citation']
<S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="90" ssid = "75">Thus  as in CFG's  at any point in the derivation  the set of structures that can be applied is determined only by a finite set of rules encapsulated by the grammar.</S><S sid ="185" ssid = "70">Each spawned process must check if xi     xn  and   yn  can be derived from B and C  respectively.</S><S sid ="141" ssid = "26">Thus  the length of any string in L is a linear combination of the length of strings in some fixed finite subset of L  and thus L is said to have the constant growth property.</S><S sid ="158" ssid = "43">We can represent any derived tree of a TAG by the two substrings that appear in its frontier  and then define how the adjunction operation concatenates the substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'90'", "'185'", "'141'", "'158'"]
'151'
'90'
'185'
'141'
'158'
['151', '90', '185', '141', '158']
parsed_discourse_facet ['implication_citation']
<S sid ="156" ssid = "41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S><S sid ="106" ssid = "12">We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths.</S><S sid ="32" ssid = "17">When 7' is adjoined at ?I in the tree 7 we obtain a tree v&quot;.</S><S sid ="12" ssid = "10">We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.</S><S sid ="55" ssid = "40">Each production can push or pop symbols on the stack as can be seen in the following productions that generate tree of the form shown in Figure 4b.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'", "'106'", "'32'", "'12'", "'55'"]
'156'
'106'
'32'
'12'
'55'
['156', '106', '32', '12', '55']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P04-1036_swastika.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="24">We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.</S>
original cit marker offset is 0
new cit marker offset is 0



['68']
68
['68']
parsed_discourse_facet ['result_citation']
<S sid="83" ssid="12">The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.</S>
original cit marker offset is 0
new cit marker offset is 0



['83']
83
['83']
parsed_discourse_facet ['result_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
<S sid="101" ssid="30">Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.</S>
original cit marker offset is 0
new cit marker offset is 0



['101']
101
['101']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="83" ssid="12">The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.</S>
original cit marker offset is 0
new cit marker offset is 0



['83']
83
['83']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="46" ssid="2">This provides the nearest neighbours to each target word, along with the distributional similarity score between the target word and its neighbour.</S>
original cit marker offset is 0
new cit marker offset is 0



['46']
46
['46']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents, rather than a lexical sample task, where the target words are manually determined and the results will depend on the skew of the words in the sample.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
<S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['method_citation']
<S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="98" ssid = "27">This seems intuitive given our expected relative usage of these senses in modern British English.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="27" ssid = "20">We use WordNet as our sense inventory for this work.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'98'", "'124'", "'5'", "'27'", "'65'"]
'98'
'124'
'5'
'27'
'65'
['98', '124', '5', '27', '65']
parsed_discourse_facet ['implication_citation']
<S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="109" ssid = "7">We generated a thesaurus entry for all polysemous nouns in WordNet as described in section 2.1 above.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S><S sid ="132" ssid = "9">The FINANCE corpus consists of 117734 documents (about 32.5 million words).</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'109'", "'95'", "'55'", "'132'"]
'124'
'109'
'95'
'55'
'132'
['124', '109', '95', '55', '132']
parsed_discourse_facet ['implication_citation']
<S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="32" ssid = "25">We describe some related work in section 6 and conclude in section 7. are therefore investigating a method of automatically ranking WordNet senses from raw text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'137'", "'5'", "'110'", "'32'"]
'44'
'137'
'5'
'110'
'32'
['44', '137', '5', '110', '32']
parsed_discourse_facet ['results_citation']
<S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="96" ssid = "25">Another example where the ranking is intuitive  is soil.</S><S sid ="33" ssid = "26">Many researchers are developing thesauruses from automatically parsed data.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'95'", "'96'", "'33'", "'44'"]
'124'
'95'
'96'
'33'
'44'
['124', '95', '96', '33', '44']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "5">To disambiguate senses a system should take context into account.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="67" ssid = "23">Each 2We use this version of WordNet since it allows us to map information to WordNets of other languages more accurately.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'124'", "'153'", "'147'", "'67'"]
'107'
'124'
'153'
'147'
'67'
['107', '124', '153', '147', '67']
parsed_discourse_facet ['method_citation']
<S sid ="94" ssid = "23">This seems quite reasonable given the nearest neighbours: tube  cable  wire  tank  hole  cylinder  fitting  tap  cistern  plate....</S><S sid ="162" ssid = "10">It only requires raw text from the given domain and because of this it can easily be applied to a new domain  or sense inventory  given sufficient text.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="99" ssid = "28">Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'", "'162'", "'60'", "'95'", "'99'"]
'94'
'162'
'60'
'95'
'99'
['94', '162', '60', '95', '99']
parsed_discourse_facet ['method_citation']
<S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="101" ssid = "30">Thus  if we used the sense ranking as a heuristic for an all nouns task we would expect to get precision in the region of 60%.</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'165'", "'101'", "'169'", "'108'", "'44'"]
'165'
'101'
'169'
'108'
'44'
['165', '101', '169', '108', '44']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="127" ssid = "4">We chose the domains of SPORTS and FINANCE since there is sufficient material for these domains in this publically available corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'60'", "'169'", "'91'", "'127'"]
'136'
'60'
'169'
'91'
'127'
['136', '60', '169', '91', '127']
parsed_discourse_facet ['method_citation']
<S sid ="193" ssid = "2">This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.</S><S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="151" ssid = "28">This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="167" ssid = "15">In this work the lists of neighbours are themselves clustered to bring out the various senses of the word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'193'", "'136'", "'151'", "'123'", "'167'"]
'193'
'136'
'151'
'123'
'167'
['193', '136', '151', '123', '167']
parsed_discourse_facet ['method_citation']
<S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="63" ssid = "19">We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline  but because of space limitations give results for the two which perform the best.</S>
original cit marker offset is 0
new cit marker offset is 0



["'173'", "'5'", "'108'", "'95'", "'63'"]
'173'
'5'
'108'
'95'
'63'
['173', '5', '108', '95', '63']
parsed_discourse_facet ['implication_citation']
<S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="76" ssid = "5">The jcn measure uses corpus data for the calculation of IC.</S><S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'184'", "'110'", "'60'", "'76'", "'55'"]
'184'
'110'
'60'
'76'
'55'
['184', '110', '60', '76', '55']
parsed_discourse_facet ['method_citation']
<S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="97" ssid = "26">The first ranked sense according to SemCor is the filth  stain: state of being unclean sense whereas the automatic ranking lists dirt  ground  earth as the first sense  which is the second ranked sense according to SemCor.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'123'", "'169'", "'147'", "'97'", "'8'"]
'123'
'169'
'147'
'97'
'8'
['123', '169', '147', '97', '8']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="28" ssid = "21">The paper is structured as follows.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of nearest neighbours.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'28'", "'124'", "'60'", "'34'"]
'61'
'28'
'124'
'60'
'34'
['61', '28', '124', '60', '34']
parsed_discourse_facet ['method_citation']
<S sid ="163" ssid = "11">Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="9" ssid = "2">This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).</S><S sid ="0" ssid = "0">Finding Predominant Word Senses in Untagged Text</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'124'", "'9'", "'0'", "'142'"]
'163'
'124'
'9'
'0'
'142'
['163', '124', '9', '0', '142']
parsed_discourse_facet ['results_citation']
<S sid ="138" ssid = "15">The SFC contains an economy label and a sports label.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of nearest neighbours.</S><S sid ="22" ssid = "15">More importantly  when working within a specific domain one would wish to tune the first sense heuristic to the domain at hand.</S><S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="92" ssid = "21">For example  in WordNet the first listed sense ofpipe is tobacco pipe  and this is ranked joint first according to the Brown files in SemCor with the second sense tube made of metal or plastic used to carry water  oil or gas etc....</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'", "'34'", "'22'", "'184'", "'92'"]
'138'
'34'
'22'
'184'
'92'
['138', '34', '22', '184', '92']
parsed_discourse_facet ['implication_citation']
<S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="185" ssid = "8">In the future  we will perform a large scale evaluation on domain specific corpora.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'173'", "'95'", "'185'", "'65'"]
'108'
'173'
'95'
'185'
'65'
['108', '173', '95', '185', '65']
parsed_discourse_facet ['method_citation']
<S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="151" ssid = "28">This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'173'", "'110'", "'151'", "'95'", "'5'"]
'173'
'110'
'151'
'95'
'5'
['173', '110', '151', '95', '5']
parsed_discourse_facet ['results_citation']
parsing: input/ref/Task1/W06-2932_vardha.csv
    <S sid="19" ssid="1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="5">However, in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="76" ssid="14">For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'"]
'76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="54" ssid="2">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'"]
'54'
['54']
parsed_discourse_facet ['method_citation']
    <S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="58" ssid="6">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'"]
'58'
['58']
parsed_discourse_facet ['method_citation']
    <S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'"]
'22'
['22']
parsed_discourse_facet ['method_citation']
    <S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
    <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'"]
'64'
['64']
parsed_discourse_facet ['method_citation']
    <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'"]
'43'
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S><S sid ="6" ssid = "2">With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'cuk  1988).</S><S sid ="71" ssid = "9">Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S>
original cit marker offset is nan
new cit marker offset is 0



["'64'", "'6'", "'7'", "'71'", "'2'"]
'64'
'6'
'7'
'71'
'2'
['64', '6', '7', '71', '2']
parsed_discourse_facet ['implication_citation']
<S sid ="71" ssid = "9">Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'cuk  1988).</S><S sid ="78" ssid = "16">Even with this improvement  the labeling of verb dependents remains the highest source of error.</S><S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'71'", "'20'", "'7'", "'78'", "'61'"]
'71'
'20'
'7'
'78'
'61'
['71', '20', '7', '78', '61']
parsed_discourse_facet ['implication_citation']
<S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="92" ssid = "14">For example  in the test sentence Lo que decia Mae West de si misma podriamos decirlo tambien los hombres:...  decias head is given as decirlo  although the main verbs of relative clauses are normally dependent on what the relative modifies  in this case the article Lo.</S><S sid ="28" ssid = "10">Consider a proposed dependency of a dependent xj on the head xi  each with morphological features Mj and Mi respectively.</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'92'", "'28'", "'88'", "'10'"]
'20'
'92'
'28'
'88'
'10'
['20', '92', '28', '88', '10']
parsed_discourse_facet ['results_citation']
<S sid ="13" ssid = "9">We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajic et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; Bohmova et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S><S sid ="30" ssid = "12">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'36'", "'64'", "'30'", "'72'"]
'13'
'36'
'64'
'30'
'72'
['13', '36', '64', '30', '72']
parsed_discourse_facet ['method_citation']
<S sid ="109" ssid = "6">The current system simply includes all morphological bi-gram features.</S><S sid ="60" ssid = "8">Furthermore  these results show that a twostage system can achieve a relatively high performance.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S><S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="101" ssid = "23">For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'60'", "'107'", "'86'", "'101'"]
'109'
'60'
'107'
'86'
'101'
['109', '60', '107', '86', '101']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="101" ssid = "23">For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.</S><S sid ="100" ssid = "22">The fact that Arabic has only 1500 training instances might also be problematic.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'72'", "'101'", "'100'", "'64'"]
'86'
'72'
'101'
'100'
'64'
['86', '72', '101', '100', '64']
parsed_discourse_facet ['method_citation']
<S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S><S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216220  New York City  June 2006. c2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="30" ssid = "12">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.</S><S sid ="82" ssid = "4">These weaknesses are not surprising  since these decisions encode the more global aspects of sentence structure: arrangement of clauses and adverbial dependents in multi-clause sentences  and prepositional phrase attachment.</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'66'", "'18'", "'30'", "'82'", "'41'"]
'66'
'18'
'30'
'82'
'41'
['66', '18', '30', '82', '41']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="51" ssid = "20">Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.</S><S sid ="91" ssid = "13">In doing this preliminary analysis  we noticed some inconsistencies in the reference dependency structures.</S><S sid ="60" ssid = "8">Furthermore  these results show that a twostage system can achieve a relatively high performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'72'", "'51'", "'91'", "'60'"]
'107'
'72'
'51'
'91'
'60'
['107', '72', '51', '91', '60']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="90" ssid = "12">We need to look more carefully at verb features that may be useful here  in particular features that distinguish finite and non-finite forms.</S><S sid ="69" ssid = "7">However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).</S><S sid ="71" ssid = "9">Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'72'", "'90'", "'69'", "'71'"]
'86'
'72'
'90'
'69'
'71'
['86', '72', '90', '69', '71']
parsed_discourse_facet ['method_citation']
<S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216220  New York City  June 2006. c2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="93" ssid = "15">A quick look at unlabeled attachment accuracies indicate that errors in Arabic parsing are the most common across all languages: prepositions (62%)  conjunctions (69%) and to a lesser extent verbs (73%).</S><S sid ="100" ssid = "22">The fact that Arabic has only 1500 training instances might also be problematic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'", "'18'", "'86'", "'93'", "'100'"]
'72'
'18'
'86'
'93'
'100'
['72', '18', '86', '93', '100']
parsed_discourse_facet ['implication_citation']
<S sid ="109" ssid = "6">The current system simply includes all morphological bi-gram features.</S><S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S><S sid ="0" ssid = "0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S sid ="48" ssid = "17">Is this the left/rightmost dependent for the head?</S><S sid ="93" ssid = "15">A quick look at unlabeled attachment accuracies indicate that errors in Arabic parsing are the most common across all languages: prepositions (62%)  conjunctions (69%) and to a lesser extent verbs (73%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'61'", "'0'", "'48'", "'93'"]
'109'
'61'
'0'
'48'
'93'
['109', '61', '0', '48', '93']
parsed_discourse_facet ['method_citation']
<S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S><S sid ="82" ssid = "4">These weaknesses are not surprising  since these decisions encode the more global aspects of sentence structure: arrangement of clauses and adverbial dependents in multi-clause sentences  and prepositional phrase attachment.</S><S sid ="9" ssid = "5">Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.</S><S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S><S sid ="100" ssid = "22">The fact that Arabic has only 1500 training instances might also be problematic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'66'", "'82'", "'9'", "'61'", "'100'"]
'66'
'82'
'9'
'61'
'100'
['66', '82', '9', '61', '100']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="69" ssid = "7">However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).</S><S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216220  New York City  June 2006. c2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'2'", "'69'", "'18'", "'66'"]
'32'
'2'
'69'
'18'
'66'
['32', '2', '69', '18', '66']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W06-2932_sweta.csv
 <S sid="5" ssid="1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="5">However, in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["36'"]
36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="9">In fact, for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="76" ssid="14">For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["76'"]
76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="45" ssid="14">Furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["45'"]
45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="3">First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["106'"]
106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="58" ssid="6">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["58'"]
58'
['58']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
 <S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="2">Ideally one would like to make all parsing and labeling decisions jointly so that the shared knowledge of both decisions will help resolve any ambiguities.</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
 <S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["43'"]
43'
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S><S sid ="6" ssid = "2">With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'cuk  1988).</S><S sid ="71" ssid = "9">Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S>
original cit marker offset is nan
new cit marker offset is 0



["'64'", "'6'", "'7'", "'71'", "'2'"]
'64'
'6'
'7'
'71'
'2'
['64', '6', '7', '71', '2']
parsed_discourse_facet ['implication_citation']
<S sid ="71" ssid = "9">Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'cuk  1988).</S><S sid ="78" ssid = "16">Even with this improvement  the labeling of verb dependents remains the highest source of error.</S><S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'71'", "'20'", "'7'", "'78'", "'61'"]
'71'
'20'
'7'
'78'
'61'
['71', '20', '7', '78', '61']
parsed_discourse_facet ['implication_citation']
<S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="92" ssid = "14">For example  in the test sentence Lo que decia Mae West de si misma podriamos decirlo tambien los hombres:...  decias head is given as decirlo  although the main verbs of relative clauses are normally dependent on what the relative modifies  in this case the article Lo.</S><S sid ="28" ssid = "10">Consider a proposed dependency of a dependent xj on the head xi  each with morphological features Mj and Mi respectively.</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'92'", "'28'", "'88'", "'10'"]
'20'
'92'
'28'
'88'
'10'
['20', '92', '28', '88', '10']
parsed_discourse_facet ['results_citation']
<S sid ="13" ssid = "9">We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajic et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; Bohmova et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S><S sid ="30" ssid = "12">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'36'", "'64'", "'30'", "'72'"]
'13'
'36'
'64'
'30'
'72'
['13', '36', '64', '30', '72']
parsed_discourse_facet ['method_citation']
<S sid ="109" ssid = "6">The current system simply includes all morphological bi-gram features.</S><S sid ="60" ssid = "8">Furthermore  these results show that a twostage system can achieve a relatively high performance.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S><S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="101" ssid = "23">For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'60'", "'107'", "'86'", "'101'"]
'109'
'60'
'107'
'86'
'101'
['109', '60', '107', '86', '101']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="101" ssid = "23">For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.</S><S sid ="100" ssid = "22">The fact that Arabic has only 1500 training instances might also be problematic.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'72'", "'101'", "'100'", "'64'"]
'86'
'72'
'101'
'100'
'64'
['86', '72', '101', '100', '64']
parsed_discourse_facet ['method_citation']
<S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S><S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216220  New York City  June 2006. c2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="30" ssid = "12">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.</S><S sid ="82" ssid = "4">These weaknesses are not surprising  since these decisions encode the more global aspects of sentence structure: arrangement of clauses and adverbial dependents in multi-clause sentences  and prepositional phrase attachment.</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'66'", "'18'", "'30'", "'82'", "'41'"]
'66'
'18'
'30'
'82'
'41'
['66', '18', '30', '82', '41']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="51" ssid = "20">Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.</S><S sid ="91" ssid = "13">In doing this preliminary analysis  we noticed some inconsistencies in the reference dependency structures.</S><S sid ="60" ssid = "8">Furthermore  these results show that a twostage system can achieve a relatively high performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'72'", "'51'", "'91'", "'60'"]
'107'
'72'
'51'
'91'
'60'
['107', '72', '51', '91', '60']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="90" ssid = "12">We need to look more carefully at verb features that may be useful here  in particular features that distinguish finite and non-finite forms.</S><S sid ="69" ssid = "7">However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).</S><S sid ="71" ssid = "9">Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'72'", "'90'", "'69'", "'71'"]
'86'
'72'
'90'
'69'
'71'
['86', '72', '90', '69', '71']
parsed_discourse_facet ['method_citation']
<S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216220  New York City  June 2006. c2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="93" ssid = "15">A quick look at unlabeled attachment accuracies indicate that errors in Arabic parsing are the most common across all languages: prepositions (62%)  conjunctions (69%) and to a lesser extent verbs (73%).</S><S sid ="100" ssid = "22">The fact that Arabic has only 1500 training instances might also be problematic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'", "'18'", "'86'", "'93'", "'100'"]
'72'
'18'
'86'
'93'
'100'
['72', '18', '86', '93', '100']
parsed_discourse_facet ['implication_citation']
<S sid ="109" ssid = "6">The current system simply includes all morphological bi-gram features.</S><S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S><S sid ="0" ssid = "0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S sid ="48" ssid = "17">Is this the left/rightmost dependent for the head?</S><S sid ="93" ssid = "15">A quick look at unlabeled attachment accuracies indicate that errors in Arabic parsing are the most common across all languages: prepositions (62%)  conjunctions (69%) and to a lesser extent verbs (73%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'61'", "'0'", "'48'", "'93'"]
'109'
'61'
'0'
'48'
'93'
['109', '61', '0', '48', '93']
parsed_discourse_facet ['method_citation']
<S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S><S sid ="82" ssid = "4">These weaknesses are not surprising  since these decisions encode the more global aspects of sentence structure: arrangement of clauses and adverbial dependents in multi-clause sentences  and prepositional phrase attachment.</S><S sid ="9" ssid = "5">Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.</S><S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S><S sid ="100" ssid = "22">The fact that Arabic has only 1500 training instances might also be problematic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'66'", "'82'", "'9'", "'61'", "'100'"]
'66'
'82'
'9'
'61'
'100'
['66', '82', '9', '61', '100']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="69" ssid = "7">However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).</S><S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216220  New York City  June 2006. c2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'2'", "'69'", "'18'", "'66'"]
'32'
'2'
'69'
'18'
'66'
['32', '2', '69', '18', '66']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
parsing: input/ref/Task1/P04-1036_aakansha.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="34">In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
    <S sid="181" ssid="4">This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'181'"]
'180'
'181'
['180', '181']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="4">To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'"]
'48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="169" ssid="17">This method obtains precision of 61% and recall 51%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'"]
'169'
['169']
parsed_discourse_facet ['result_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="115" ssid="13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
    <S sid="181" ssid="4">This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'181'"]
'180'
'181'
['180', '181']
parsed_discourse_facet ['result_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'"]
'180'
['180']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="34">In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="98" ssid = "27">This seems intuitive given our expected relative usage of these senses in modern British English.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="27" ssid = "20">We use WordNet as our sense inventory for this work.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'98'", "'124'", "'5'", "'27'", "'65'"]
'98'
'124'
'5'
'27'
'65'
['98', '124', '5', '27', '65']
parsed_discourse_facet ['implication_citation']
<S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="109" ssid = "7">We generated a thesaurus entry for all polysemous nouns in WordNet as described in section 2.1 above.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S><S sid ="132" ssid = "9">The FINANCE corpus consists of 117734 documents (about 32.5 million words).</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'109'", "'95'", "'55'", "'132'"]
'124'
'109'
'95'
'55'
'132'
['124', '109', '95', '55', '132']
parsed_discourse_facet ['implication_citation']
<S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="32" ssid = "25">We describe some related work in section 6 and conclude in section 7. are therefore investigating a method of automatically ranking WordNet senses from raw text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'137'", "'5'", "'110'", "'32'"]
'44'
'137'
'5'
'110'
'32'
['44', '137', '5', '110', '32']
parsed_discourse_facet ['results_citation']
<S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="96" ssid = "25">Another example where the ranking is intuitive  is soil.</S><S sid ="33" ssid = "26">Many researchers are developing thesauruses from automatically parsed data.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'95'", "'96'", "'33'", "'44'"]
'124'
'95'
'96'
'33'
'44'
['124', '95', '96', '33', '44']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "5">To disambiguate senses a system should take context into account.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="67" ssid = "23">Each 2We use this version of WordNet since it allows us to map information to WordNets of other languages more accurately.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'124'", "'153'", "'147'", "'67'"]
'107'
'124'
'153'
'147'
'67'
['107', '124', '153', '147', '67']
parsed_discourse_facet ['method_citation']
<S sid ="94" ssid = "23">This seems quite reasonable given the nearest neighbours: tube  cable  wire  tank  hole  cylinder  fitting  tap  cistern  plate....</S><S sid ="162" ssid = "10">It only requires raw text from the given domain and because of this it can easily be applied to a new domain  or sense inventory  given sufficient text.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="99" ssid = "28">Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'", "'162'", "'60'", "'95'", "'99'"]
'94'
'162'
'60'
'95'
'99'
['94', '162', '60', '95', '99']
parsed_discourse_facet ['method_citation']
<S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="101" ssid = "30">Thus  if we used the sense ranking as a heuristic for an all nouns task we would expect to get precision in the region of 60%.</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'165'", "'101'", "'169'", "'108'", "'44'"]
'165'
'101'
'169'
'108'
'44'
['165', '101', '169', '108', '44']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="127" ssid = "4">We chose the domains of SPORTS and FINANCE since there is sufficient material for these domains in this publically available corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'60'", "'169'", "'91'", "'127'"]
'136'
'60'
'169'
'91'
'127'
['136', '60', '169', '91', '127']
parsed_discourse_facet ['method_citation']
<S sid ="193" ssid = "2">This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.</S><S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="151" ssid = "28">This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="167" ssid = "15">In this work the lists of neighbours are themselves clustered to bring out the various senses of the word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'193'", "'136'", "'151'", "'123'", "'167'"]
'193'
'136'
'151'
'123'
'167'
['193', '136', '151', '123', '167']
parsed_discourse_facet ['method_citation']
<S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="63" ssid = "19">We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline  but because of space limitations give results for the two which perform the best.</S>
original cit marker offset is 0
new cit marker offset is 0



["'173'", "'5'", "'108'", "'95'", "'63'"]
'173'
'5'
'108'
'95'
'63'
['173', '5', '108', '95', '63']
parsed_discourse_facet ['implication_citation']
<S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="76" ssid = "5">The jcn measure uses corpus data for the calculation of IC.</S><S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'184'", "'110'", "'60'", "'76'", "'55'"]
'184'
'110'
'60'
'76'
'55'
['184', '110', '60', '76', '55']
parsed_discourse_facet ['method_citation']
<S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="97" ssid = "26">The first ranked sense according to SemCor is the filth  stain: state of being unclean sense whereas the automatic ranking lists dirt  ground  earth as the first sense  which is the second ranked sense according to SemCor.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'123'", "'169'", "'147'", "'97'", "'8'"]
'123'
'169'
'147'
'97'
'8'
['123', '169', '147', '97', '8']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="28" ssid = "21">The paper is structured as follows.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of nearest neighbours.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'28'", "'124'", "'60'", "'34'"]
'61'
'28'
'124'
'60'
'34'
['61', '28', '124', '60', '34']
parsed_discourse_facet ['method_citation']
<S sid ="163" ssid = "11">Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="9" ssid = "2">This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).</S><S sid ="0" ssid = "0">Finding Predominant Word Senses in Untagged Text</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'124'", "'9'", "'0'", "'142'"]
'163'
'124'
'9'
'0'
'142'
['163', '124', '9', '0', '142']
parsed_discourse_facet ['results_citation']
<S sid ="138" ssid = "15">The SFC contains an economy label and a sports label.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of nearest neighbours.</S><S sid ="22" ssid = "15">More importantly  when working within a specific domain one would wish to tune the first sense heuristic to the domain at hand.</S><S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="92" ssid = "21">For example  in WordNet the first listed sense ofpipe is tobacco pipe  and this is ranked joint first according to the Brown files in SemCor with the second sense tube made of metal or plastic used to carry water  oil or gas etc....</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'", "'34'", "'22'", "'184'", "'92'"]
'138'
'34'
'22'
'184'
'92'
['138', '34', '22', '184', '92']
parsed_discourse_facet ['implication_citation']
<S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="185" ssid = "8">In the future  we will perform a large scale evaluation on domain specific corpora.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'173'", "'95'", "'185'", "'65'"]
'108'
'173'
'95'
'185'
'65'
['108', '173', '95', '185', '65']
parsed_discourse_facet ['method_citation']
<S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="151" ssid = "28">This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'173'", "'110'", "'151'", "'95'", "'5'"]
'173'
'110'
'151'
'95'
'5'
['173', '110', '151', '95', '5']
parsed_discourse_facet ['results_citation']
parsing: input/ref/Task1/P04-1036_vardha.csv
    <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
    <S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="66" ssid="22">It uses the glosses of semantically related (according to WordNet) senses too. jcn (Jiang and Conrath, 1997) This score uses corpus data to populate classes (synsets) in the WordNet hierarchy with frequency counts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'66'"]
'66'
['66']
parsed_discourse_facet ['method_citation']
 <S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="101" ssid="30">Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'"]
'101'
['101']
parsed_discourse_facet ['method_citation']
 <S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
 <S sid="115" ssid="13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
  <S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'"]
'89'
['89']
parsed_discourse_facet ['method_citation']
 <S sid="137" ssid="14">Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'"]
'137'
['137']
parsed_discourse_facet ['method_citation']
75 ssid="4">We generated a thesaurus entry for all polysemous nouns which occurred in SemCor with a frequency 2, and in the BNC with a frequency 10 in the grammatical relations listed in section 2.1 above.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'"]
'75'
['75']
Error in Reference Offset
 <S sid="155" ssid="3">A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
  <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
 <S sid="155" ssid="3">A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="24">We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'"]
'68'
['68']
parsed_discourse_facet ['method_citation']
    <S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'"]
'13'
['13']
parsed_discourse_facet ['method_citation']
  <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="98" ssid = "27">This seems intuitive given our expected relative usage of these senses in modern British English.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="27" ssid = "20">We use WordNet as our sense inventory for this work.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'98'", "'124'", "'5'", "'27'", "'65'"]
'98'
'124'
'5'
'27'
'65'
['98', '124', '5', '27', '65']
parsed_discourse_facet ['implication_citation']
<S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="109" ssid = "7">We generated a thesaurus entry for all polysemous nouns in WordNet as described in section 2.1 above.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S><S sid ="132" ssid = "9">The FINANCE corpus consists of 117734 documents (about 32.5 million words).</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'109'", "'95'", "'55'", "'132'"]
'124'
'109'
'95'
'55'
'132'
['124', '109', '95', '55', '132']
parsed_discourse_facet ['implication_citation']
<S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="32" ssid = "25">We describe some related work in section 6 and conclude in section 7. are therefore investigating a method of automatically ranking WordNet senses from raw text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'137'", "'5'", "'110'", "'32'"]
'44'
'137'
'5'
'110'
'32'
['44', '137', '5', '110', '32']
parsed_discourse_facet ['results_citation']
<S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="96" ssid = "25">Another example where the ranking is intuitive  is soil.</S><S sid ="33" ssid = "26">Many researchers are developing thesauruses from automatically parsed data.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'95'", "'96'", "'33'", "'44'"]
'124'
'95'
'96'
'33'
'44'
['124', '95', '96', '33', '44']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "5">To disambiguate senses a system should take context into account.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="67" ssid = "23">Each 2We use this version of WordNet since it allows us to map information to WordNets of other languages more accurately.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'124'", "'153'", "'147'", "'67'"]
'107'
'124'
'153'
'147'
'67'
['107', '124', '153', '147', '67']
parsed_discourse_facet ['method_citation']
<S sid ="94" ssid = "23">This seems quite reasonable given the nearest neighbours: tube  cable  wire  tank  hole  cylinder  fitting  tap  cistern  plate....</S><S sid ="162" ssid = "10">It only requires raw text from the given domain and because of this it can easily be applied to a new domain  or sense inventory  given sufficient text.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="99" ssid = "28">Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'", "'162'", "'60'", "'95'", "'99'"]
'94'
'162'
'60'
'95'
'99'
['94', '162', '60', '95', '99']
parsed_discourse_facet ['method_citation']
<S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="101" ssid = "30">Thus  if we used the sense ranking as a heuristic for an all nouns task we would expect to get precision in the region of 60%.</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'165'", "'101'", "'169'", "'108'", "'44'"]
'165'
'101'
'169'
'108'
'44'
['165', '101', '169', '108', '44']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="127" ssid = "4">We chose the domains of SPORTS and FINANCE since there is sufficient material for these domains in this publically available corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'60'", "'169'", "'91'", "'127'"]
'136'
'60'
'169'
'91'
'127'
['136', '60', '169', '91', '127']
parsed_discourse_facet ['method_citation']
<S sid ="193" ssid = "2">This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.</S><S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="151" ssid = "28">This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="167" ssid = "15">In this work the lists of neighbours are themselves clustered to bring out the various senses of the word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'193'", "'136'", "'151'", "'123'", "'167'"]
'193'
'136'
'151'
'123'
'167'
['193', '136', '151', '123', '167']
parsed_discourse_facet ['method_citation']
<S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="63" ssid = "19">We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline  but because of space limitations give results for the two which perform the best.</S>
original cit marker offset is 0
new cit marker offset is 0



["'173'", "'5'", "'108'", "'95'", "'63'"]
'173'
'5'
'108'
'95'
'63'
['173', '5', '108', '95', '63']
parsed_discourse_facet ['implication_citation']
<S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="76" ssid = "5">The jcn measure uses corpus data for the calculation of IC.</S><S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'184'", "'110'", "'60'", "'76'", "'55'"]
'184'
'110'
'60'
'76'
'55'
['184', '110', '60', '76', '55']
parsed_discourse_facet ['method_citation']
<S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="97" ssid = "26">The first ranked sense according to SemCor is the filth  stain: state of being unclean sense whereas the automatic ranking lists dirt  ground  earth as the first sense  which is the second ranked sense according to SemCor.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'123'", "'169'", "'147'", "'97'", "'8'"]
'123'
'169'
'147'
'97'
'8'
['123', '169', '147', '97', '8']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="28" ssid = "21">The paper is structured as follows.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of nearest neighbours.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'28'", "'124'", "'60'", "'34'"]
'61'
'28'
'124'
'60'
'34'
['61', '28', '124', '60', '34']
parsed_discourse_facet ['method_citation']
<S sid ="163" ssid = "11">Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="9" ssid = "2">This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).</S><S sid ="0" ssid = "0">Finding Predominant Word Senses in Untagged Text</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'124'", "'9'", "'0'", "'142'"]
'163'
'124'
'9'
'0'
'142'
['163', '124', '9', '0', '142']
parsed_discourse_facet ['results_citation']
<S sid ="138" ssid = "15">The SFC contains an economy label and a sports label.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of nearest neighbours.</S><S sid ="22" ssid = "15">More importantly  when working within a specific domain one would wish to tune the first sense heuristic to the domain at hand.</S><S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="92" ssid = "21">For example  in WordNet the first listed sense ofpipe is tobacco pipe  and this is ranked joint first according to the Brown files in SemCor with the second sense tube made of metal or plastic used to carry water  oil or gas etc....</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'", "'34'", "'22'", "'184'", "'92'"]
'138'
'34'
'22'
'184'
'92'
['138', '34', '22', '184', '92']
parsed_discourse_facet ['implication_citation']
<S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="185" ssid = "8">In the future  we will perform a large scale evaluation on domain specific corpora.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'173'", "'95'", "'185'", "'65'"]
'108'
'173'
'95'
'185'
'65'
['108', '173', '95', '185', '65']
parsed_discourse_facet ['method_citation']
<S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="151" ssid = "28">This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'173'", "'110'", "'151'", "'95'", "'5'"]
'173'
'110'
'151'
'95'
'5'
['173', '110', '151', '95', '5']
parsed_discourse_facet ['results_citation']
parsing: input/ref/Task1/W99-0623_swastika.csv
<S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['aim_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['result_citation']
<S sid="91" ssid="20">Features and context were initially introduced into the models, but they refused to offer any gains in performance.</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



['139']
139
['139']
parsed_discourse_facet ['result_citation']
<S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['method_citation']
<S sid="103" ssid="32">The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.</S>
original cit marker offset is 0
new cit marker offset is 0



['103']
103
['103']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



['139']
139
['139']
parsed_discourse_facet ['result_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
<S sid="140" ssid="2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
<S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['result_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/W99-0623.csv
<S sid ="34" ssid = "20">Mi(c) is a binary function returning t when parser i (from among the k parsers) suggests constituent c should be in the parse.</S><S sid ="106" ssid = "35">In each figure the upper graph shows the isolated constituent precision and the bottom graph shows the corresponding number of hypothesized constituents.</S><S sid ="66" ssid = "52">Each decision determines the inclusion or exclusion of a candidate constituent.</S><S sid ="65" ssid = "51">We model each parse as the decisions made to create it  and model those decisions as independent events.</S><S sid ="2" ssid = "2">Two general approaches are presented and two combination techniques are described for each approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'106'", "'66'", "'65'", "'2'"]
'34'
'106'
'66'
'65'
'2'
['34', '106', '66', '65', '2']
Error in Discourse Facet
<S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'15'", "'130'", "'57'", "'50'"]
'18'
'15'
'130'
'57'
'50'
['18', '15', '130', '57', '50']
Error in Discourse Facet
<S sid ="122" ssid = "51">All of these systems were run on data that was not seen during their development.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="24" ssid = "10">We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="107" ssid = "36">Again we notice that the isolated constituent precision is larger than 0.5 only in those partitions that contain very few samples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'122'", "'11'", "'24'", "'61'", "'107'"]
'122'
'11'
'24'
'61'
'107'
['122', '11', '24', '61', '107']
Error in Discourse Facet
<S sid ="92" ssid = "21">While we cannot prove there are no such useful features on which one should condition trust  we can give some insight into why the features we explored offered no gain.</S><S sid ="111" ssid = "40">The first row represents the average accuracy of the three parsers we combine.</S><S sid ="104" ssid = "33">In the cases where isolated constituent precision is larger than 0.5 the affected portion of the hypotheses is negligible.</S><S sid ="42" ssid = "28">Call the crossing constituents A and B.</S><S sid ="117" ssid = "46">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'111'", "'104'", "'42'", "'117'"]
'92'
'111'
'104'
'42'
'117'
['92', '111', '104', '42', '117']
Error in Discourse Facet
<S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="39" ssid = "25">There are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.</S><S sid ="63" ssid = "49">The probabilistic version of this procedure is straightforward: We once again assume independence among our various member parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'139'", "'11'", "'39'", "'63'"]
'15'
'139'
'11'
'39'
'63'
['15', '139', '11', '39', '63']
Error in Discourse Facet
<S sid ="39" ssid = "25">There are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.</S><S sid ="86" ssid = "15">Finally we show the combining techniques degrade very little when a poor parser is added to the set.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a nave Bayes classifier to determine which constituents to include in the parse.</S><S sid ="17" ssid = "3">The substructures that are unanimously hypothesized by the parsers should be preserved after combination  and the combination technique should not foolishly create substructures for which there is no supporting evidence.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'86'", "'27'", "'17'", "'61'"]
'39'
'86'
'27'
'17'
'61'
['39', '86', '27', '17', '61']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="24" ssid = "10">We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'72'", "'21'", "'24'", "'139'"]
'112'
'72'
'21'
'24'
'139'
['112', '72', '21', '24', '139']
Error in Discourse Facet
<S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S><S sid ="96" ssid = "25">We call such a constituent an isolated constituent.</S><S sid ="132" ssid = "61">The results of this experiment can be seen in Table 5.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and nave Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="125" ssid = "54">The constituent voting and nave Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'139'", "'96'", "'132'", "'38'", "'125'"]
'139'
'96'
'132'
'38'
'125'
['139', '96', '132', '38', '125']
Error in Discourse Facet
<S sid ="43" ssid = "29">A receives a votes  and B receives b votes.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="84" ssid = "13">The first shows how constituent features and context do not help in deciding which parser to trust.</S><S sid ="93" ssid = "22">Because we are working with only three parsers  the only situation in which context will help us is when it can indicate we should choose to believe a single parser that disagrees with the majority hypothesis instead of the majority hypothesis itself.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'15'", "'52'", "'84'", "'93'"]
'43'
'15'
'52'
'84'
'93'
['43', '15', '52', '84', '93']
Error in Discourse Facet
<S sid ="101" ssid = "30">We show the results of three of the experiments we conducted to measure isolated constituent precision under various partitioning schemes.</S><S sid ="126" ssid = "55">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a nave Bayes classifier to determine which constituents to include in the parse.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="43" ssid = "29">A receives a votes  and B receives b votes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'126'", "'27'", "'52'", "'43'"]
'101'
'126'
'27'
'52'
'43'
['101', '126', '27', '52', '43']
Error in Discourse Facet
<S sid ="126" ssid = "55">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S><S sid ="142" ssid = "4">Both of the switching techniques  as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.</S><S sid ="111" ssid = "40">The first row represents the average accuracy of the three parsers we combine.</S><S sid ="43" ssid = "29">A receives a votes  and B receives b votes.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'142'", "'111'", "'43'", "'52'"]
'126'
'142'
'111'
'43'
'52'
['126', '142', '111', '43', '52']
Error in Discourse Facet
<S sid ="101" ssid = "30">We show the results of three of the experiments we conducted to measure isolated constituent precision under various partitioning schemes.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="106" ssid = "35">In each figure the upper graph shows the isolated constituent precision and the bottom graph shows the corresponding number of hypothesized constituents.</S><S sid ="22" ssid = "8">If enough parsers suggest that a particular constituent belongs in the parse  we include it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'50'", "'15'", "'106'", "'22'"]
'101'
'50'
'15'
'106'
'22'
['101', '50', '15', '106', '22']
Error in Discourse Facet
<S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S><S sid ="111" ssid = "40">The first row represents the average accuracy of the three parsers we combine.</S><S sid ="30" ssid = "16">This is equivalent to the assumption used in probability estimation for nave Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.</S><S sid ="28" ssid = "14">The development of a nave Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a nave Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'111'", "'30'", "'28'", "'27'"]
'50'
'111'
'30'
'28'
'27'
['50', '111', '30', '28', '27']
Error in Discourse Facet
<S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="140" ssid = "2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S><S sid ="131" ssid = "60">It was then tested on section 22 of the Treebank in conjunction with the other parsers.</S><S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="34" ssid = "20">Mi(c) is a binary function returning t when parser i (from among the k parsers) suggests constituent c should be in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'140'", "'131'", "'112'", "'34'"]
'61'
'140'
'131'
'112'
'34'
['61', '140', '131', '112', '34']
Error in Discourse Facet
<S sid ="87" ssid = "16">It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S><S sid ="37" ssid = "23">Here NO counts the number of hypothesized constituents in the development set that match the binary predicate specified as an argument.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="100" ssid = "29">When this metric is less than 0.5  we expect to incur more errors' than we will remove by adding those constituents to the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'", "'57'", "'37'", "'11'", "'100'"]
'87'
'57'
'37'
'11'
'100'
['87', '57', '37', '11', '100']
Error in Discourse Facet
<S sid ="83" ssid = "12">We performed three experiments to evaluate our techniques.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="108" ssid = "37">From this we see that a finer-grained model for parser combination  at least for the features we have examined  will not give us any additional power.</S><S sid ="107" ssid = "36">Again we notice that the isolated constituent precision is larger than 0.5 only in those partitions that contain very few samples.</S><S sid ="118" ssid = "47">The maximum precision oracle is an upper bound on the possible gain we can achieve by parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'11'", "'108'", "'107'", "'118'"]
'83'
'11'
'108'
'107'
'118'
['83', '11', '108', '107', '118']
Error in Discourse Facet
<S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="30" ssid = "16">This is equivalent to the assumption used in probability estimation for nave Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="113" ssid = "42">The next two rows are results of oracle experiments.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'30'", "'52'", "'113'", "'50'"]
'18'
'30'
'52'
'113'
'50'
['18', '30', '52', '113', '50']
Error in Discourse Facet
parsing: input/ref/Task1/W06-2932_swastika.csv
<S sid="86" ssid="8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S>
original cit marker offset is 
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="54" ssid="2">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



['54']
54
['54']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



['12']
12
['12']
parsed_discourse_facet ['method_citation']
    <S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



['41']
41
['41']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="6">Its power lies in the ability to define a rich set of features over parsing decisions, as well as surface level features relative to these decisions.</S>
original cit marker offset is 0
new cit marker offset is 0



['24']
24
['24']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



['12']
12
['12']
parsed_discourse_facet ['method_citation']
<S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



['43']
43
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S><S sid ="6" ssid = "2">With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'cuk  1988).</S><S sid ="71" ssid = "9">Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S>
original cit marker offset is nan
new cit marker offset is 0



["'64'", "'6'", "'7'", "'71'", "'2'"]
'64'
'6'
'7'
'71'
'2'
['64', '6', '7', '71', '2']
parsed_discourse_facet ['implication_citation']
<S sid ="71" ssid = "9">Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'cuk  1988).</S><S sid ="78" ssid = "16">Even with this improvement  the labeling of verb dependents remains the highest source of error.</S><S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'71'", "'20'", "'7'", "'78'", "'61'"]
'71'
'20'
'7'
'78'
'61'
['71', '20', '7', '78', '61']
parsed_discourse_facet ['implication_citation']
<S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="92" ssid = "14">For example  in the test sentence Lo que decia Mae West de si misma podriamos decirlo tambien los hombres:...  decias head is given as decirlo  although the main verbs of relative clauses are normally dependent on what the relative modifies  in this case the article Lo.</S><S sid ="28" ssid = "10">Consider a proposed dependency of a dependent xj on the head xi  each with morphological features Mj and Mi respectively.</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'92'", "'28'", "'88'", "'10'"]
'20'
'92'
'28'
'88'
'10'
['20', '92', '28', '88', '10']
parsed_discourse_facet ['results_citation']
<S sid ="13" ssid = "9">We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajic et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; Bohmova et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S><S sid ="30" ssid = "12">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'36'", "'64'", "'30'", "'72'"]
'13'
'36'
'64'
'30'
'72'
['13', '36', '64', '30', '72']
parsed_discourse_facet ['method_citation']
<S sid ="109" ssid = "6">The current system simply includes all morphological bi-gram features.</S><S sid ="60" ssid = "8">Furthermore  these results show that a twostage system can achieve a relatively high performance.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S><S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="101" ssid = "23">For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'60'", "'107'", "'86'", "'101'"]
'109'
'60'
'107'
'86'
'101'
['109', '60', '107', '86', '101']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="101" ssid = "23">For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.</S><S sid ="100" ssid = "22">The fact that Arabic has only 1500 training instances might also be problematic.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'72'", "'101'", "'100'", "'64'"]
'86'
'72'
'101'
'100'
'64'
['86', '72', '101', '100', '64']
parsed_discourse_facet ['method_citation']
<S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S><S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216220  New York City  June 2006. c2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="30" ssid = "12">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.</S><S sid ="82" ssid = "4">These weaknesses are not surprising  since these decisions encode the more global aspects of sentence structure: arrangement of clauses and adverbial dependents in multi-clause sentences  and prepositional phrase attachment.</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'66'", "'18'", "'30'", "'82'", "'41'"]
'66'
'18'
'30'
'82'
'41'
['66', '18', '30', '82', '41']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="51" ssid = "20">Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.</S><S sid ="91" ssid = "13">In doing this preliminary analysis  we noticed some inconsistencies in the reference dependency structures.</S><S sid ="60" ssid = "8">Furthermore  these results show that a twostage system can achieve a relatively high performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'72'", "'51'", "'91'", "'60'"]
'107'
'72'
'51'
'91'
'60'
['107', '72', '51', '91', '60']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="90" ssid = "12">We need to look more carefully at verb features that may be useful here  in particular features that distinguish finite and non-finite forms.</S><S sid ="69" ssid = "7">However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).</S><S sid ="71" ssid = "9">Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'72'", "'90'", "'69'", "'71'"]
'86'
'72'
'90'
'69'
'71'
['86', '72', '90', '69', '71']
parsed_discourse_facet ['method_citation']
<S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216220  New York City  June 2006. c2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="93" ssid = "15">A quick look at unlabeled attachment accuracies indicate that errors in Arabic parsing are the most common across all languages: prepositions (62%)  conjunctions (69%) and to a lesser extent verbs (73%).</S><S sid ="100" ssid = "22">The fact that Arabic has only 1500 training instances might also be problematic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'", "'18'", "'86'", "'93'", "'100'"]
'72'
'18'
'86'
'93'
'100'
['72', '18', '86', '93', '100']
parsed_discourse_facet ['implication_citation']
<S sid ="109" ssid = "6">The current system simply includes all morphological bi-gram features.</S><S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S><S sid ="0" ssid = "0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S sid ="48" ssid = "17">Is this the left/rightmost dependent for the head?</S><S sid ="93" ssid = "15">A quick look at unlabeled attachment accuracies indicate that errors in Arabic parsing are the most common across all languages: prepositions (62%)  conjunctions (69%) and to a lesser extent verbs (73%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'61'", "'0'", "'48'", "'93'"]
'109'
'61'
'0'
'48'
'93'
['109', '61', '0', '48', '93']
parsed_discourse_facet ['method_citation']
<S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S><S sid ="82" ssid = "4">These weaknesses are not surprising  since these decisions encode the more global aspects of sentence structure: arrangement of clauses and adverbial dependents in multi-clause sentences  and prepositional phrase attachment.</S><S sid ="9" ssid = "5">Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.</S><S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S><S sid ="100" ssid = "22">The fact that Arabic has only 1500 training instances might also be problematic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'66'", "'82'", "'9'", "'61'", "'100'"]
'66'
'82'
'9'
'61'
'100'
['66', '82', '9', '61', '100']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="69" ssid = "7">However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).</S><S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216220  New York City  June 2006. c2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'2'", "'69'", "'18'", "'66'"]
'32'
'2'
'69'
'18'
'66'
['32', '2', '69', '18', '66']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/J01-2004_sweta.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["372'"]
372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="28">The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.</S>
    <S sid="41" ssid="29">There will also be a brief review of previous work using syntactic information for language modeling, before we introduce our model in Section 4.</S>
    <S sid="42" ssid="30">Three parse trees: (a) a complete parse tree; (b) a complete parse tree with an explicit stop symbol; and (c) a partial parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["40'", "'41'", "'42'"]
40'
'41'
'42'
['40', '41', '42']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="13">A parser that is not left to right, but which has rooted derivations, e.g., a headfirst parser, will be able to calculate generative joint probabilities for entire strings; however, it will not be able to calculate probabilities for each word conditioned on previously generated words, unless each derivation generates the words in the string in exactly the same order.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="364" ssid="120">We follow Chelba (2000) in dealing with this problem: for parsing purposes, we use the Penn Treebank tokenization; for interpolation with the provided trigram model, and for evaluation, the lattice tokenization is used.</S>
original cit marker offset is 0
new cit marker offset is 0



["364'"]
364'
['364']
parsed_discourse_facet ['method_citation']
<S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



["302'"]
302'
['302']
parsed_discourse_facet ['method_citation']
 <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="231" ssid="135">Since we do not know the POS for the word, we must sum the LAP for all POS For a PCFG G, a stack S = Ao An$ (which we will write AN and a look-ahead terminal item wi, we define the look-ahead probability as follows: We recursively estimate this with two empirically observed conditional probabilities for every nonterminal A,: 13(A, w,a) and P(A, c).</S>
original cit marker offset is 0
new cit marker offset is 0



["231'"]
231'
['231']
parsed_discourse_facet ['method_citation']
<S sid="297" ssid="53">The differences between a k-best and a beam-search parser (not to mention the use of dynamic programming) make a running time difference unsurprising.</S>
original cit marker offset is 0
new cit marker offset is 0



["297'"]
297'
['297']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="37">Statistically based heuristic best-first or beam-search strategies (Caraballo and Charniak 1998; Charniak, Goldwater, and Johnson 1998; Goodman 1997) have yielded an enormous improvement in the quality and speed of parsers, even without any guarantee that the parse returned is, in fact, that with the maximum likelihood for the probability model.</S>
original cit marker offset is 0
new cit marker offset is 0



["133'"]
133'
['133']
parsed_discourse_facet ['method_citation']
<S sid="291" ssid="47">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["291'"]
291'
['291']
parsed_discourse_facet ['method_citation']
<S sid="355" ssid="111">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.</S>
original cit marker offset is 0
new cit marker offset is 0



["355'"]
355'
['355']
parsed_discourse_facet ['method_citation']
<S sid="59" ssid="17">A PCFG is a CFG with a probability assigned to each rule; specifically, each righthand side has a probability given the left-hand side of the rule.</S>
original cit marker offset is 0
new cit marker offset is 0



["59'"]
59'
['59']
parsed_discourse_facet ['method_citation']
<S sid="100" ssid="4">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="12">Another approach that uses syntactic structure for language modeling has been to use a shift-reduce parser to &amp;quot;surface&amp;quot; c-commanding phrasal headwords or part-of-speech (POS) tags from arbitrarily far back in the prefix string, for use in a trigram-like model.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S sid ="321" ssid = "77">In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.</S><S sid ="387" ssid = "143">Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.</S><S sid ="315" ssid = "71">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;the sum of the probabilities over the vocabulary is less than one.</S><S sid ="344" ssid = "100">However  when we interpolate with the trigram  we see that the additional improvement is greater than the one they experienced.</S><S sid ="403" ssid = "16">Perhaps some compromise between the fully connected structures and extreme underspecification will yield an efficiency improvement.</S>
original cit marker offset is 0
new cit marker offset is 0



["'321'", "'387'", "'315'", "'344'", "'403'"]
'321'
'387'
'315'
'344'
'403'
['321', '387', '315', '344', '403']
parsed_discourse_facet ['implication_citation']
<S sid ="380" ssid = "136">Future work will include more substantial word recognition experiments.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S><S sid ="344" ssid = "100">However  when we interpolate with the trigram  we see that the additional improvement is greater than the one they experienced.</S><S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S>
original cit marker offset is 0
new cit marker offset is 0



["'380'", "'280'", "'391'", "'344'", "'336'"]
'380'
'280'
'391'
'344'
'336'
['380', '280', '391', '344', '336']
parsed_discourse_facet ['implication_citation']
<S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="338" ssid = "94">Table 4 compares the perplexity of our model with Chelba and Jelinek (1998a  1998b) on the same training and testing corpora.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="326" ssid = "82">We obtained the training and testing corpora from them (which we will denote C&J corpus)  and also created intermediate corpora  upon which only the first two modifications were carried out (which we will denote no punct).</S><S sid ="343" ssid = "99">Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'</S>
original cit marker offset is 0
new cit marker offset is 0



["'349'", "'338'", "'280'", "'326'", "'343'"]
'349'
'338'
'280'
'326'
'343'
['349', '338', '280', '326', '343']
parsed_discourse_facet ['results_citation']
<S sid ="372" ssid = "128">The small size of our training data  as well as the fact that we are rescoring n-best lists  rather than working directly on lattices  makes comparison with the other models not particularly informative.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="309" ssid = "65">Let Ht be the priority queue H  before any processing has begun with word w  in the look-ahead.</S><S sid ="266" ssid = "22">From this set of measures  we will also include the crossing bracket scores: average crossing brackets (CB)  percentage of sentences with no crossing brackets (0 CB)  and the percentage of sentences with two crossing brackets or fewer (< 2 CB).</S><S sid ="268" ssid = "24">This is an incremental parser with a pruning strategy and no backtracking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'", "'301'", "'309'", "'266'", "'268'"]
'372'
'301'
'309'
'266'
'268'
['372', '301', '309', '266', '268']
parsed_discourse_facet ['method_citation']
<S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="355" ssid = "111">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task  we performed a very small preliminary experiment on n-best lists.</S><S sid ="340" ssid = "96">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid ="354" ssid = "110">Interestingly  at the word where the failure occurred  the sum of the probabilities was 0.9301.</S>
original cit marker offset is 0
new cit marker offset is 0



["'336'", "'301'", "'355'", "'340'", "'354'"]
'336'
'301'
'355'
'340'
'354'
['336', '301', '355', '340', '354']
parsed_discourse_facet ['method_citation']
<S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="361" ssid = "117">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S><S sid ="319" ssid = "75">The hope  however  is that the improved parsing model provided by our conditional probability model will cause the distribution over structures to be more peaked  thus enabling us to capture more of the total probability mass  and making this a fairly snug upper bound on the perplexity.</S><S sid ="355" ssid = "111">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task  we performed a very small preliminary experiment on n-best lists.</S>
original cit marker offset is 0
new cit marker offset is 0



["'301'", "'349'", "'361'", "'319'", "'355'"]
'301'
'349'
'361'
'319'
'355'
['301', '349', '361', '319', '355']
parsed_discourse_facet ['method_citation']
<S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="324" ssid = "80">Their modifications included: (i) removing orthographic cues to structure (e.g.  punctuation); (ii) replacing all numbers with the single token N; and (iii) closing the vocabulary at 10 000  replacing all other words with the UNK token.</S><S sid ="343" ssid = "99">Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'</S><S sid ="387" ssid = "143">Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.</S><S sid ="346" ssid = "102">These results are particularly remarkable  given that we did not build our model as a language model per se  but rather as a parsing model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'349'", "'324'", "'343'", "'387'", "'346'"]
'349'
'324'
'343'
'387'
'346'
['349', '324', '343', '387', '346']
parsed_discourse_facet ['method_citation']
<S sid ="315" ssid = "71">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;the sum of the probabilities over the vocabulary is less than one.</S><S sid ="377" ssid = "133">The point of this small experiment was to see if our parsing model could provide useful information even in the case that recognition errors occur  as opposed to the (generally) fully grammatical strings upon which the perplexity results were obtained.</S><S sid ="288" ssid = "44">Interestingly  conditioning all POS expansions on two c-commanding heads made no difference in accuracy compared to conditioning only leftmost POS expansions on a single c-commanding head; but it did improve the efficiency.</S><S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="382" ssid = "138">The base beam factor that we have used to this point is 10'  which is quite wide.</S>
original cit marker offset is 0
new cit marker offset is 0



["'315'", "'377'", "'288'", "'358'", "'382'"]
'315'
'377'
'288'
'358'
'382'
['315', '377', '288', '358', '382']
parsed_discourse_facet ['method_citation']
<S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="382" ssid = "138">The base beam factor that we have used to this point is 10'  which is quite wide.</S><S sid ="283" ssid = "39">Unlike the Roark and Johnson parser  however  our coverage did not substantially drop as the amount of conditioning information increased  and in some cases  coverage improved slightly.</S><S sid ="380" ssid = "136">Future work will include more substantial word recognition experiments.</S><S sid ="268" ssid = "24">This is an incremental parser with a pruning strategy and no backtracking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'301'", "'382'", "'283'", "'380'", "'268'"]
'301'
'382'
'283'
'380'
'268'
['301', '382', '283', '380', '268']
parsed_discourse_facet ['method_citation']
<S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S><S sid ="321" ssid = "77">In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.</S><S sid ="262" ssid = "18">Recall is the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD.</S>
original cit marker offset is 0
new cit marker offset is 0



["'336'", "'280'", "'391'", "'321'", "'262'"]
'336'
'280'
'391'
'321'
'262'
['336', '280', '391', '321', '262']
parsed_discourse_facet ['implication_citation']
<S sid ="315" ssid = "71">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;the sum of the probabilities over the vocabulary is less than one.</S><S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S><S sid ="295" ssid = "51">Our observed times look polynomial  which is to be expected given our pruning strategy: the denser the competitors within a narrow probability range of the best analysis  the more time will be spent working on these competitors; and the farther along in the sentence  the more chance for ambiguities that can lead to such a situation.</S><S sid ="382" ssid = "138">The base beam factor that we have used to this point is 10'  which is quite wide.</S><S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'315'", "'336'", "'295'", "'382'", "'391'"]
'315'
'336'
'295'
'382'
'391'
['315', '336', '295', '382', '391']
parsed_discourse_facet ['method_citation']
<S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="258" ssid = "14">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S><S sid ="372" ssid = "128">The small size of our training data  as well as the fact that we are rescoring n-best lists  rather than working directly on lattices  makes comparison with the other models not particularly informative.</S><S sid ="298" ssid = "54">What is perhaps surprising is that the difference is not greater.</S><S sid ="270" ssid = "26">In such a case  the parser fails to return a complete parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'301'", "'258'", "'372'", "'298'", "'270'"]
'301'
'258'
'372'
'298'
'270'
['301', '258', '372', '298', '270']
parsed_discourse_facet ['method_citation']
<S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S><S sid ="398" ssid = "11">By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="354" ssid = "110">Interestingly  at the word where the failure occurred  the sum of the probabilities was 0.9301.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'391'", "'398'", "'301'", "'354'", "'404'"]
'391'
'398'
'301'
'354'
'404'
['391', '398', '301', '354', '404']
parsed_discourse_facet ['method_citation']
<S sid ="402" ssid = "15">In fact  left-corner parsing can be simulated by a top-down parser by transforming the grammar  as was done in Roark and Johnson (1999)  and so an approach very similar to the one outlined here could be used in that case.</S><S sid ="398" ssid = "11">By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="371" ssid = "127">For our model and the Treebank trigram model  the LM weight that resulted in the lowest error rates is given.</S><S sid ="343" ssid = "99">Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'</S>
original cit marker offset is 0
new cit marker offset is 0



["'402'", "'398'", "'280'", "'371'", "'343'"]
'402'
'398'
'280'
'371'
'343'
['402', '398', '280', '371', '343']
parsed_discourse_facet ['results_citation']
<S sid ="398" ssid = "11">By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="361" ssid = "117">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S><S sid ="258" ssid = "14">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S>
original cit marker offset is 0
new cit marker offset is 0



["'398'", "'301'", "'390'", "'361'", "'258'"]
'398'
'301'
'390'
'361'
'258'
['398', '301', '390', '361', '258']
parsed_discourse_facet ['implication_citation']
<S sid ="258" ssid = "14">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S><S sid ="262" ssid = "18">Recall is the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD.</S><S sid ="361" ssid = "117">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S><S sid ="339" ssid = "95">We built an interpolated trigram model to serve as a baseline (as they did)  and also interpolated our model's perplexity with the trigram  using the same mixing coefficient as they did in their trials (taking 36 percent of the estimate from the trigram).'</S><S sid ="324" ssid = "80">Their modifications included: (i) removing orthographic cues to structure (e.g.  punctuation); (ii) replacing all numbers with the single token N; and (iii) closing the vocabulary at 10 000  replacing all other words with the UNK token.</S>
original cit marker offset is 0
new cit marker offset is 0



["'258'", "'262'", "'361'", "'339'", "'324'"]
'258'
'262'
'361'
'339'
'324'
['258', '262', '361', '339', '324']
parsed_discourse_facet ['method_citation']
<S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="257" ssid = "13">A constituent for evaluation purposes consists of a label (e.g.  NP) and a span (beginning and ending word positions).</S><S sid ="340" ssid = "96">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="387" ssid = "143">Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.</S>
original cit marker offset is 0
new cit marker offset is 0



["'349'", "'257'", "'340'", "'301'", "'387'"]
'349'
'257'
'340'
'301'
'387'
['349', '257', '340', '301', '387']
parsed_discourse_facet ['results_citation']
parsing: input/ref/Task1/P05-1013_vardha.csv
 <S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
  <S sid="9" ssid="5">This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
  <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
    <S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
 <S sid="62" ssid="1">In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al., 2004) and English (Nivre and Scholz, 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="23" ssid="19">By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
    <S sid="96" ssid="7">With respect to exact match, the improvement is even more noticeable, which shows quite clearly that even if non-projective dependencies are rare on the token level, they are nevertheless important for getting the global syntactic structure correct.</S>
original cit marker offset is 0
new cit marker offset is 0



["'96'"]
'96'
['96']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
  <S sid="40" ssid="11">Even this may be nondeterministic, in case the graph contains several non-projective arcs whose lifts interact, but we use the following algorithm to construct a minimal projective transformation D0 = (W, A0) of a (nonprojective) dependency graph D = (W, A): The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right).</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
  <S sid="7" ssid="3">From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="14" ssid="10">While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
 <S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'"]
'49'
['49']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P05-1013.csv
<S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S><S sid ="53" ssid = "24">In the third and final scheme  denoted Path  we keep the extra infor2Note that this is a baseline for the parsing experiment only (Experiment 2).</S><S sid ="37" ssid = "8">Here we use a slightly different notion of lift  applying to individual arcs and moving their head upwards one step at a time: Intuitively  lifting an arc makes the word wk dependent on the head wi of its original head wj (which is unique in a well-formed dependency graph)  unless wj is a root in which case the operation is undefined (but then wj * wk is necessarily projective if the dependency graph is well-formed).</S><S sid ="83" ssid = "10">It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'53'", "'37'", "'83'", "'75'"]
'51'
'53'
'37'
'83'
'75'
['51', '53', '37', '83', '75']
parsed_discourse_facet ['implication_citation']
<S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S><S sid ="38" ssid = "9">Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'38'", "'78'", "'34'", "'51'"]
'75'
'38'
'78'
'34'
'51'
['75', '38', '78', '34', '51']
parsed_discourse_facet ['implication_citation']
<S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S><S sid ="23" ssid = "19">By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S><S sid ="110" ssid = "2">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="25" ssid = "21">The rest of the paper is structured as follows.</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'70'", "'23'", "'110'", "'25'", "'11'"]
'70'
'23'
'110'
'25'
'11'
['70', '23', '110', '25', '11']
parsed_discourse_facet ['results_citation']
<S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="77" ssid = "4">The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'24'", "'95'", "'77'", "'11'"]
'34'
'24'
'95'
'77'
'11'
['34', '24', '95', '77', '11']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "14">Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.</S><S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="18" ssid = "14">In addition  there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington  1990; Kahane et al.  1998; Duchier and Debusmann  2001; Holan et al.  2001; Hellwig  2003).</S><S sid ="103" ssid = "14">On the other hand  given that all schemes have similar parsing accuracy overall  this means that the Path scheme is the least likely to introduce errors on projective arcs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'65'", "'14'", "'18'", "'103'"]
'43'
'65'
'14'
'18'
'103'
['43', '65', '14', '18', '103']
parsed_discourse_facet ['method_citation']
<S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S><S sid ="42" ssid = "13">Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="55" ssid = "26">As can be seen from the last column in Table 1  both Head and Head+Path may theoretically lead to a quadratic increase in the number of distinct arc labels (Head+Path being worse than Head only by a constant factor)  while the increase is only linear in the case of Path.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'42'", "'14'", "'55'", "'104'"]
'75'
'42'
'14'
'55'
'104'
['75', '42', '14', '55', '104']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="107" ssid = "18">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="66" ssid = "5">The choice between different actions is in general nondeterministic  and the parser relies on a memorybased classifier  trained on treebank data  to predict the next action based on features of the current parser configuration.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'107'", "'14'", "'95'", "'66'"]
'11'
'107'
'14'
'95'
'66'
['11', '107', '14', '95', '66']
parsed_discourse_facet ['method_citation']
<S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S><S sid ="12" ssid = "8">Prague Dependency Treebank (Hajic et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'70'", "'81'", "'12'", "'78'", "'89'"]
'70'
'81'
'12'
'78'
'89'
['70', '81', '12', '78', '89']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Prague Dependency Treebank (Hajic et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S><S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="5" ssid = "1">It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order  where discontinuous syntactic constructions are more common than in languages like English (Melcuk  1988; Covington  1990).</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S><S sid ="66" ssid = "5">The choice between different actions is in general nondeterministic  and the parser relies on a memorybased classifier  trained on treebank data  to predict the next action based on features of the current parser configuration.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'65'", "'5'", "'71'", "'66'"]
'12'
'65'
'5'
'71'
'66'
['12', '65', '5', '71', '66']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S><S sid ="82" ssid = "9">The last four columns in Table 3 show the distribution of nonprojective arcs with respect to the number of lifts required.</S><S sid ="86" ssid = "13">As expected  the most informative encoding  Head+Path  gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S><S sid ="84" ssid = "11">In the second part of the experiment  we applied the inverse transformation based on breadth-first search under the three different encoding schemes.</S><S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'82'", "'86'", "'84'", "'70'"]
'51'
'82'
'86'
'84'
'70'
['51', '82', '86', '84', '70']
parsed_discourse_facet ['implication_citation']
<S sid ="85" ssid = "12">The results are given in Table 4.</S><S sid ="33" ssid = "4">If (wi  r  wj) E A  we say that wi is the head of wj and wj a dependent of wi.</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="43" ssid = "14">Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.</S><S sid ="44" ssid = "15">Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'", "'33'", "'60'", "'43'", "'44'"]
'85'
'33'
'60'
'43'
'44'
['85', '33', '60', '43', '44']
parsed_discourse_facet ['method_citation']
<S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="25" ssid = "21">The rest of the paper is structured as follows.</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'14'", "'25'", "'60'", "'75'"]
'65'
'14'
'25'
'60'
'75'
['65', '14', '25', '60', '75']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="26" ssid = "22">In section 2 we introduce the graph transformation techniques used to projectivize and deprojectivize dependency graphs  and in section 3 we describe the data-driven dependency parser that is the core of our system.</S><S sid ="61" ssid = "32">Before we turn to the evaluation  however  we need to introduce the data-driven dependency parser used in the latter experiments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'104'", "'21'", "'26'", "'61'"]
'11'
'104'
'21'
'26'
'61'
['11', '104', '21', '26', '61']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="43" ssid = "14">Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S><S sid ="63" ssid = "2">The parser builds dependency graphs by traversing the input from left to right  using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'11'", "'43'", "'81'", "'63'"]
'34'
'11'
'43'
'81'
'63'
['34', '11', '43', '81', '63']
parsed_discourse_facet ['results_citation']
<S sid ="86" ssid = "13">As expected  the most informative encoding  Head+Path  gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S><S sid ="57" ssid = "28">In approaching this problem  a variety of different methods are conceivable  including a more or less sophisticated use of machine learning.</S><S sid ="73" ssid = "12">More details on the memory-based prediction can be found in Nivre et al. (2004) and Nivre and Scholz (2004).</S><S sid ="61" ssid = "32">Before we turn to the evaluation  however  we need to introduce the data-driven dependency parser used in the latter experiments.</S><S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'57'", "'73'", "'61'", "'34'"]
'86'
'57'
'73'
'61'
'34'
['86', '57', '73', '61', '34']
parsed_discourse_facet ['implication_citation']
<S sid ="83" ssid = "10">It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="101" ssid = "12">However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.</S><S sid ="25" ssid = "21">The rest of the paper is structured as follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'106'", "'78'", "'101'", "'25'"]
'83'
'106'
'78'
'101'
'25'
['83', '106', '78', '101', '25']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="77" ssid = "4">The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'24'", "'95'", "'77'", "'11'"]
'34'
'24'
'95'
'77'
'11'
['34', '24', '95', '77', '11']
parsed_discourse_facet ['results_citation']
<S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="92" ssid = "3">Evaluation metrics used are Attachment Score (AS)  i.e. the proportion of tokens that are attached to the correct head  and Exact Match (EM)  i.e. the proportion of sentences for which the dependency graph exactly matches the gold standard.</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'65'", "'92'", "'60'", "'71'"]
'95'
'65'
'92'
'60'
'71'
['95', '65', '92', '60', '71']
parsed_discourse_facet ['aim_citation']
<S sid ="67" ssid = "6">Table 2 shows the features used in the current version of the parser.</S><S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S><S sid ="82" ssid = "9">The last four columns in Table 3 show the distribution of nonprojective arcs with respect to the number of lifts required.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'70'", "'82'", "'14'", "'81'"]
'67'
'70'
'82'
'14'
'81'
['67', '70', '82', '14', '81']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S><S sid ="63" ssid = "2">The parser builds dependency graphs by traversing the input from left to right  using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'11'", "'34'", "'75'", "'63'"]
'51'
'11'
'34'
'75'
'63'
['51', '11', '34', '75', '63']
parsed_discourse_facet ['aim_citation']
parsing: input/ref/Task1/W99-0613_aakansha.csv
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="30">Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'"]
'137'
['137']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="12">2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="4">The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="12">But we will show that the use of unlabeled data can drastically reduce the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
    <S sid="237" ssid="4">The numbers falling into the location, person, organization categories were 186, 289 and 402 respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'", "'237'"]
'236'
'237'
['236', '237']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
    <S sid="10" ssid="4">The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'10'"]
'9'
'10'
['9', '10']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S sid="139" ssid="6">This section describes AdaBoost, which is the basis for the CoBoost algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'39'"]
'137'
'39'
['137', '39']
parsed_discourse_facet ['method_citation']
<S sid="26" ssid="20">We present two algorithms.</S>
    <S sid="27" ssid="21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'27'"]
'26'
'27'
['26', '27']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="12">But we will show that the use of unlabeled data can drastically reduce the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="2">Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S>
    <S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'9'"]
'8'
'9'
['8', '9']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="125" ssid = "58">It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'178'", "'125'", "'41'"]
'256'
'198'
'178'
'125'
'41'
['256', '198', '178', '125', '41']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'97'", "'70'", "'198'"]
'256'
'178'
'97'
'70'
'198'
['256', '178', '97', '70', '198']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="128" ssid = "61">At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'128'", "'198'", "'178'", "'70'"]
'256'
'128'
'198'
'178'
'70'
['256', '128', '198', '178', '70']
parsed_discourse_facet ['results_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'97'", "'70'", "'198'", "'178'"]
'256'
'97'
'70'
'198'
'178'
['256', '97', '70', '198', '178']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="70" ssid = "3">.</S><S sid ="220" ssid = "87">(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'198'", "'70'", "'220'"]
'256'
'178'
'198'
'70'
'220'
['256', '178', '198', '70', '220']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'178'", "'70'", "'236'"]
'256'
'198'
'178'
'70'
'236'
['256', '198', '178', '70', '236']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="70" ssid = "3">.</S><S sid ="128" ssid = "61">At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'70'", "'128'", "'97'"]
'256'
'198'
'70'
'128'
'97'
['256', '198', '70', '128', '97']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S><S sid ="220" ssid = "87">(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'70'", "'220'", "'198'"]
'256'
'178'
'70'
'220'
'198'
['256', '178', '70', '220', '198']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'236'", "'41'", "'97'"]
'256'
'198'
'236'
'41'
'97'
['256', '198', '236', '41', '97']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="70" ssid = "3">.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'97'", "'236'", "'70'"]
'256'
'178'
'97'
'236'
'70'
['256', '178', '97', '236', '70']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="125" ssid = "58">It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'198'", "'125'", "'41'"]
'256'
'178'
'198'
'125'
'41'
['256', '178', '198', '125', '41']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'97'", "'178'", "'198'", "'236'"]
'256'
'97'
'178'
'198'
'236'
['256', '97', '178', '198', '236']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'70'", "'198'", "'178'", "'236'"]
'256'
'70'
'198'
'178'
'236'
['256', '70', '198', '178', '236']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P08-1043_swastika.csv
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['result_citation']
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="37">3An English sentence with ambiguous PoS assignment can be trivially represented as a lattice similar to our own, where every pair of consecutive nodes correspond to a word, and every possible PoS assignment for this word is a connecting arc.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="163" ssid="1">The accuracy results for segmentation, tagging and parsing using our different models and our standard data split are summarized in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



['163']
163
['163']
parsed_discourse_facet ['result_citation']
    <S sid="100" ssid="32">This means that the rules in our grammar are of two kinds: (a) syntactic rules relating nonterminals to a sequence of non-terminals and/or PoS tags, and (b) lexical rules relating PoS tags to lattice arcs (lexemes).</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['method_citation']
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['result_citation']
<S sid="188" ssid="2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



['188']
188
['188']
parsed_discourse_facet ['result_citation']
<S sid="86" ssid="18">A morphological analyzer M : W&#8212;* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S>
original cit marker offset is 0
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['result_citation']
<S sid="97" ssid="29">Thus our proposed model is a proper model assigning probability mass to all (7r, L) pairs, where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S>
original cit marker offset is 0
new cit marker offset is 0



['97']
97
['97']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S sid ="54" ssid = "1">A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.</S><S sid ="89" ssid = "21">Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree  s.t.</S><S sid ="179" ssid = "17">On the surface  our model may seem as a special case of Cohen and Smith in which  = 0.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the  hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="148" ssid = "26">Removing the leaves from the resulting tree yields a parse for L under G  with the desired probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'89'", "'179'", "'183'", "'148'"]
'54'
'89'
'179'
'183'
'148'
['54', '89', '179', '183', '148']
parsed_discourse_facet ['implication_citation']
<S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="33" ssid = "12">The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="169" ssid = "7">Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.</S><S sid ="188" ssid = "2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'33'", "'156'", "'169'", "'188'"]
'80'
'33'
'156'
'169'
'188'
['80', '33', '156', '169', '188']
parsed_discourse_facet ['implication_citation']
<S sid ="130" ssid = "8">To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and malformed7 were removed.</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="48" ssid = "6">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S><S sid ="176" ssid = "14">Furthermore  the combination of pruning and vertical markovization of the grammar outperforms the Oracle results reported by Cohen and Smith.</S><S sid ="97" ssid = "29">Thus our proposed model is a proper model assigning probability mass to all (7r  L) pairs  where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'80'", "'48'", "'176'", "'97'"]
'130'
'80'
'48'
'176'
'97'
['130', '80', '48', '176', '97']
parsed_discourse_facet ['results_citation']
<S sid ="49" ssid = "7">Tsarfaty and Simaan (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S><S sid ="163" ssid = "1">The accuracy results for segmentation  tagging and parsing using our different models and our standard data split are summarized in Table 1.</S><S sid ="71" ssid = "3">This is by now a fairly standard representation for multiple morphological segmentation of Hebrew utterances (Adler  2001; Bar-Haim et al.  2005; Smith et al.  2005; Cohen and Smith  2007; Adler  2007).</S><S sid ="33" ssid = "12">The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).</S><S sid ="148" ssid = "26">Removing the leaves from the resulting tree yields a parse for L under G  with the desired probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'163'", "'71'", "'33'", "'148'"]
'49'
'163'
'71'
'33'
'148'
['49', '163', '71', '33', '148']
parsed_discourse_facet ['method_citation']
<S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="45" ssid = "3">Morphological disambiguators that consider a token in context (an utterance) and propose the most likely morphological analysis of an utterance (including segmentation) were presented by Bar-Haim et al. (2005)  Adler and Elhadad (2006)  Shacham and Wintner (2007)  and achieved good results (the best segmentation result so far is around 98%).</S><S sid ="157" ssid = "35">SEGTok(noH) is the segmentation accuracy ignoring mistakes involving the implicit definite article h.11 To evaluate our performance on the tagging task we report CPOS and FPOS corresponding to coarse- and fine-grained PoS tagging results (F1) measure.</S><S sid ="169" ssid = "7">Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'156'", "'45'", "'157'", "'169'"]
'180'
'156'
'45'
'157'
'169'
['180', '156', '45', '157', '169']
parsed_discourse_facet ['method_citation']
<S sid ="169" ssid = "7">Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="161" ssid = "39">We report the F1 value of both measures.</S><S sid ="67" ssid = "14">Hence  we take the probability of the event fmnh analyzed as REL VB to be This means that we generate f and mnh independently depending on their corresponding PoS tags  and the context (as well as the syntactic relation between the two) is modeled via the derivation resulting in a sequence REL VB spanning the form fmnh. based on linear context.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'", "'156'", "'80'", "'161'", "'67'"]
'169'
'156'
'80'
'161'
'67'
['169', '156', '80', '161', '67']
parsed_discourse_facet ['method_citation']
<S sid ="88" ssid = "20">M(wi) = Li).</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="36" ssid = "15">Furthermore  the systematic way in which particles are prefixed to one another and onto an open-class category gives rise to a distinct sort of morphological ambiguity: space-delimited tokens may be ambiguous between several different segmentation possibilities.</S><S sid ="26" ssid = "5">The relativizer f(that) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.</S><S sid ="73" ssid = "5">We use double-circles to indicate the space-delimited token boundaries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'156'", "'36'", "'26'", "'73'"]
'88'
'156'
'36'
'26'
'73'
['88', '156', '36', '26', '73']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="38" ssid = "17">The form mnh itself can be read as at least three different verbs (counted  appointed  was appointed)  a noun (a portion)  and a possessed noun (her kind).</S><S sid ="108" ssid = "40">Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p * (s  p)) > 0  while other segments have never been observed as a lexical event before.</S><S sid ="49" ssid = "7">Tsarfaty and Simaan (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S><S sid ="158" ssid = "36">Evaluating parsing results in our joint framework  as argued by Tsarfaty (2006)  is not trivial under the joint disambiguation task  as the hypothesized yield need not coincide with the correct one.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'38'", "'108'", "'49'", "'158'"]
'80'
'38'
'108'
'49'
'158'
['80', '38', '108', '49', '158']
parsed_discourse_facet ['method_citation']
<S sid ="182" ssid = "20">In addition  as the CRF and PCFG look at similar sorts of information from within two inherently different models  they are far from independent and optimizing their product is meaningless.</S><S sid ="101" ssid = "33">The possible analyses of a surface token pose constraints on the analyses of specific segments.</S><S sid ="163" ssid = "1">The accuracy results for segmentation  tagging and parsing using our different models and our standard data split are summarized in Table 1.</S><S sid ="58" ssid = "5">Such tag sequences are often treated as complex tags (e.g.</S><S sid ="44" ssid = "2">Such analyzers propose multiple segmentation possibilities and their corresponding analyses for a token in isolation but have no means to determine the most likely ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["'182'", "'101'", "'163'", "'58'", "'44'"]
'182'
'101'
'163'
'58'
'44'
['182', '101', '163', '58', '44']
parsed_discourse_facet ['method_citation']
<S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="195" ssid = "9">We further thank Khalil Simaan (ILLCUvA) for his careful advise concerning the formal details of the proposal.</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="98" ssid = "30">The Grammar Our parser looks for the most likely tree spanning a single path through the lattice of which the yield is a sequence of lexemes.</S><S sid ="191" ssid = "5">In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'195'", "'80'", "'98'", "'191'"]
'180'
'195'
'80'
'98'
'191'
['180', '195', '80', '98', '191']
parsed_discourse_facet ['implication_citation']
<S sid ="120" ssid = "52">From now on all lattice arcs are tagged segments and the assignment of probability P(p * (s  p)) to lattice arcs proceeds as usual.4 A rather pathological case is when our lexical heuristics prune away all segmentation possibilities and we remain with an empty lattice.</S><S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="158" ssid = "36">Evaluating parsing results in our joint framework  as argued by Tsarfaty (2006)  is not trivial under the joint disambiguation task  as the hypothesized yield need not coincide with the correct one.</S><S sid ="157" ssid = "35">SEGTok(noH) is the segmentation accuracy ignoring mistakes involving the implicit definite article h.11 To evaluate our performance on the tagging task we report CPOS and FPOS corresponding to coarse- and fine-grained PoS tagging results (F1) measure.</S><S sid ="49" ssid = "7">Tsarfaty and Simaan (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'", "'180'", "'158'", "'157'", "'49'"]
'120'
'180'
'158'
'157'
'49'
['120', '180', '158', '157', '49']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="26" ssid = "5">The relativizer f(that) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.</S><S sid ="54" ssid = "1">A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.</S><S sid ="130" ssid = "8">To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and malformed7 were removed.</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'26'", "'54'", "'130'", "'80'"]
'14'
'26'
'54'
'130'
'80'
['14', '26', '54', '130', '80']
parsed_discourse_facet ['method_citation']
<S sid ="154" ssid = "32">For all grammars  we use fine-grained PoS tags indicating various morphological features annotated therein.</S><S sid ="194" ssid = "8">Acknowledgments We thank Meni Adler and Michael Elhadad (BGU) for helpful comments and discussion.</S><S sid ="97" ssid = "29">Thus our proposed model is a proper model assigning probability mass to all (7r  L) pairs  where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S><S sid ="130" ssid = "8">To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and malformed7 were removed.</S><S sid ="149" ssid = "27">We use a patched version of BitPar allowing for direct input of probabilities instead of counts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'154'", "'194'", "'97'", "'130'", "'149'"]
'154'
'194'
'97'
'130'
'149'
['154', '194', '97', '130', '149']
parsed_discourse_facet ['method_citation']
<S sid ="5" ssid = "1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  tokens) that constitute the unanalyzed surface forms (utterances).</S><S sid ="76" ssid = "8">Furthermore  some of the arcs represent lexemes not present in the input tokens (e.g. h/DT  fl/POS)  however these are parts of valid analyses of the token (cf. super-segmental morphology section 2).</S><S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="108" ssid = "40">Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p * (s  p)) > 0  while other segments have never been observed as a lexical event before.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'76'", "'180'", "'108'", "'133'"]
'5'
'76'
'180'
'108'
'133'
['5', '76', '180', '108', '133']
parsed_discourse_facet ['results_citation']
<S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="48" ssid = "6">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S><S sid ="181" ssid = "19">Many morphological decisions are based on long distance dependencies  and when the global syntactic evidence disagrees with evidence based on local linear context  the two models compete with one another  despite the fact that the PCFG takes also local context into account.</S><S sid ="95" ssid = "27">A compatible view is presented by Charniak et al. (1996) who consider the kind of probabilities a generative parser should get from a PoS tagger  and concludes that these should be P(w|t) and nothing fancier.3 In our setting  therefore  the Lattice is not used to induce a probability distribution on a linear context  but rather  it is used as a common-denominator of state-indexation of all segmentations possibilities of a surface form.</S><S sid ="54" ssid = "1">A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'48'", "'181'", "'95'", "'54'"]
'80'
'48'
'181'
'95'
'54'
['80', '48', '181', '95', '54']
parsed_discourse_facet ['implication_citation']
<S sid ="164" ssid = "2">In addition we report for each model its performance on goldsegmented input (GS) to indicate the upper bound 11Overt definiteness errors may be seen as a wrong feature rather than as wrong constituent and it is by now an accepted standard to report accuracy with and without such errors. for the grammars performance on the parsing task.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S><S sid ="22" ssid = "1">Segmental morphology Hebrew consists of seven particles m(from) f(when/who/that) h(the) w(and) k(like) l(to) and b(in). which may never appear in isolation and must always attach as prefixes to the following open-class category item we refer to as stem.</S><S sid ="126" ssid = "4">When a comparison against previous results requires additional pre-processing  we state it explicitly to allow for the reader to replicate the reported results.</S><S sid ="173" ssid = "11">The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter () which is tuned separately for each of the tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'164'", "'133'", "'22'", "'126'", "'173'"]
'164'
'133'
'22'
'126'
'173'
['164', '133', '22', '126', '173']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W11-2123_vardha.csv
    <S sid="12" ssid="7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
 <S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
 <S sid="131" ssid="3">Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N &#8722; 1 preceding words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'"]
'131'
['131']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="16">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="16">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="12">Compared with SRILM, IRSTLM adds several features: lower memory consumption, a binary file format with memory mapping, caching to increase speed, and quantization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'"]
'108'
['108']
parsed_discourse_facet ['method_citation']
    <S sid="129" ssid="1">In addition to the optimizations specific to each datastructure described in Section 2, we implement several general optimizations for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'129'"]
'129'
['129']
parsed_discourse_facet ['method_citation']
    <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
    <S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
    <S sid="263" ssid="5">Quantization can be improved by jointly encoding probability and backoff.</S>
original cit marker offset is 0
new cit marker offset is 0



["'263'"]
'263'
['263']
parsed_discourse_facet ['method_citation']
    <S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
    <S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="17">If the context wnf will never extend to the right (i.e. wnf v is not present in the model for all words v) then no subsequent query will match the full context.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'"]
'145'
['145']
parsed_discourse_facet ['method_citation']
 <S sid="182" ssid="1">This section measures performance on shared tasks in order of increasing complexity: sparse lookups, evaluating perplexity of a large file, and translation with Moses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'182'"]
'182'
['182']
parsed_discourse_facet ['method_citation']
    <S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
    <S sid="12" ssid="7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
    <S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="12">We have modified Moses (Koehn et al., 2007) to keep our state with hypotheses; to conserve memory, phrases do not keep state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
 <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
 <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W11-2123.csv
<S sid ="270" ssid = "12">This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S>
original cit marker offset is 0
new cit marker offset is 0



["'270'", "'276'", "'265'", "'286'", "'284'"]
'270'
'276'
'265'
'286'
'284'
['270', '276', '265', '286', '284']
parsed_discourse_facet ['implication_citation']
<S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S>
original cit marker offset is 0
new cit marker offset is 0



["'262'", "'280'", "'265'", "'277'", "'284'"]
'262'
'280'
'265'
'277'
'284'
['262', '280', '265', '277', '284']
parsed_discourse_facet ['implication_citation']
<S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="285" ssid = "6">Juri Ganitkevitch answered questions about Joshua.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'265'", "'284'", "'278'", "'285'", "'276'"]
'265'
'284'
'278'
'285'
'276'
['265', '284', '278', '285', '276']
parsed_discourse_facet ['results_citation']
<S sid ="260" ssid = "2">For speed  we plan to implement the direct-mapped cache from BerkeleyLM.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>
original cit marker offset is 0
new cit marker offset is 0



["'260'", "'262'", "'280'", "'283'", "'266'"]
'260'
'262'
'280'
'283'
'266'
['260', '262', '280', '283', '266']
parsed_discourse_facet ['method_citation']
<S sid ="272" ssid = "14">Generalizing state minimization  the model could also provide explicit bounds on probability for both backward and forward extension.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="268" ssid = "10">For example  syntactic decoders (Koehn et al.  2007; Dyer et al.  2010; Li et al.  2009) perform dynamic programming parametrized by both backward- and forward-looking state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'272'", "'265'", "'283'", "'286'", "'268'"]
'272'
'265'
'283'
'286'
'268'
['272', '265', '283', '286', '268']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="264" ssid = "6">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'284'", "'265'", "'264'", "'267'"]
'256'
'284'
'265'
'264'
'267'
['256', '284', '265', '264', '267']
parsed_discourse_facet ['method_citation']
<S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S>
original cit marker offset is 0
new cit marker offset is 0



["'283'", "'284'", "'280'", "'265'", "'275'"]
'283'
'284'
'280'
'265'
'275'
['283', '284', '280', '265', '275']
parsed_discourse_facet ['method_citation']
<S sid ="261" ssid = "3">Much could be done to further reduce memory consumption.</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'261'", "'277'", "'286'", "'284'", "'283'"]
'261'
'277'
'286'
'284'
'283'
['261', '277', '286', '284', '283']
parsed_discourse_facet ['method_citation']
<S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S><S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'286'", "'280'", "'267'", "'279'", "'283'"]
'286'
'280'
'267'
'279'
'283'
['286', '280', '267', '279', '283']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'287'", "'266'", "'262'", "'265'"]
'256'
'287'
'266'
'262'
'265'
['256', '287', '266', '262', '265']
parsed_discourse_facet ['implication_citation']
<S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="264" ssid = "6">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'262'", "'278'", "'277'", "'265'", "'264'"]
'262'
'278'
'277'
'265'
'264'
['262', '278', '277', '265', '264']
parsed_discourse_facet ['method_citation']
<S sid ="261" ssid = "3">Much could be done to further reduce memory consumption.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S><S sid ="269" ssid = "11">If they knew that the first four words in a hypergraph node would never extend to the left and form a 5-gram  then three or even fewer words could be kept in the backward state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'261'", "'265'", "'266'", "'267'", "'269'"]
'261'
'265'
'266'
'267'
'269'
['261', '265', '266', '267', '269']
parsed_discourse_facet ['method_citation']
<S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'287'", "'266'", "'286'", "'262'", "'274'"]
'287'
'266'
'286'
'262'
'274'
['287', '266', '286', '262', '274']
parsed_discourse_facet ['method_citation']
<S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'286'", "'256'", "'265'", "'284'", "'276'"]
'286'
'256'
'265'
'284'
'276'
['286', '256', '265', '284', '276']
parsed_discourse_facet ['results_citation']
<S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S><S sid ="270" ssid = "12">This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.</S>
original cit marker offset is 0
new cit marker offset is 0



["'278'", "'262'", "'265'", "'275'", "'270'"]
'278'
'262'
'265'
'275'
'270'
['278', '262', '265', '275', '270']
parsed_discourse_facet ['implication_citation']
<S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'280'", "'262'", "'265'", "'279'", "'256'"]
'280'
'262'
'265'
'279'
'256'
['280', '262', '265', '279', '256']
parsed_discourse_facet ['method_citation']
<S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'279'", "'256'", "'262'", "'265'", "'276'"]
'279'
'256'
'262'
'265'
'276'
['279', '256', '262', '265', '276']
parsed_discourse_facet ['results_citation']
<S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="264" ssid = "6">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'277'", "'286'", "'287'", "'264'"]
'256'
'277'
'286'
'287'
'264'
['256', '277', '286', '287', '264']
parsed_discourse_facet ['aim_citation']
<S sid ="263" ssid = "5">Quantization can be improved by jointly encoding probability and backoff.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="258" ssid = "77">However  the point of RandLM is to scale to even larger data  compensating for this loss in quality.</S>
original cit marker offset is 0
new cit marker offset is 0



["'263'", "'265'", "'287'", "'283'", "'258'"]
'263'
'265'
'287'
'283'
'258'
['263', '265', '287', '283', '258']
parsed_discourse_facet ['method_citation']
<S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'265'", "'280'", "'256'", "'278'", "'267'"]
'265'
'280'
'256'
'278'
'267'
['265', '280', '256', '278', '267']
parsed_discourse_facet ['aim_citation']





input/ref/Task1/W99-0613_swastika.csv
input/res/Task1/W99-0613.csv
parsing: input/ref/Task1/W99-0613_swastika.csv
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
<S sid="159" ssid="26">To prevent this we &amp;quot;smooth&amp;quot; the confidence by adding a small value, e, to both W+ and W_, giving at = Plugging the value of at from Equ.</S>
original cit marker offset is 0
new cit marker offset is 0



['159']
159
['159']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S>
original cit marker offset is 0
new cit marker offset is 0



['137']
137
['137']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['method_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
 <S sid="250" ssid="1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S>
original cit marker offset is 0
new cit marker offset is 0



['250']
250
['250']
parsed_discourse_facet ['result_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
    <S sid="36" ssid="30">Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.</S>
original cit marker offset is 0
new cit marker offset is 0



['36']
36
['36']
parsed_discourse_facet ['result_citation']
<S sid="29" ssid="23">Unfortunately, Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective function.</S>
original cit marker offset is 0
new cit marker offset is 0



['29']
29
['29']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
    <S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['method_citation']
    <S sid="95" ssid="28">(Specifically, the limit n starts at 5 and increases by 5 at each iteration.)</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['method_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="125" ssid = "58">It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'178'", "'125'", "'41'"]
'256'
'198'
'178'
'125'
'41'
['256', '198', '178', '125', '41']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'97'", "'70'", "'198'"]
'256'
'178'
'97'
'70'
'198'
['256', '178', '97', '70', '198']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="128" ssid = "61">At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'128'", "'198'", "'178'", "'70'"]
'256'
'128'
'198'
'178'
'70'
['256', '128', '198', '178', '70']
parsed_discourse_facet ['results_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'97'", "'70'", "'198'", "'178'"]
'256'
'97'
'70'
'198'
'178'
['256', '97', '70', '198', '178']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="70" ssid = "3">.</S><S sid ="220" ssid = "87">(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'198'", "'70'", "'220'"]
'256'
'178'
'198'
'70'
'220'
['256', '178', '198', '70', '220']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'178'", "'70'", "'236'"]
'256'
'198'
'178'
'70'
'236'
['256', '198', '178', '70', '236']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="70" ssid = "3">.</S><S sid ="128" ssid = "61">At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'70'", "'128'", "'97'"]
'256'
'198'
'70'
'128'
'97'
['256', '198', '70', '128', '97']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S><S sid ="220" ssid = "87">(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'70'", "'220'", "'198'"]
'256'
'178'
'70'
'220'
'198'
['256', '178', '70', '220', '198']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'236'", "'41'", "'97'"]
'256'
'198'
'236'
'41'
'97'
['256', '198', '236', '41', '97']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="70" ssid = "3">.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'97'", "'236'", "'70'"]
'256'
'178'
'97'
'236'
'70'
['256', '178', '97', '236', '70']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="125" ssid = "58">It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'198'", "'125'", "'41'"]
'256'
'178'
'198'
'125'
'41'
['256', '178', '198', '125', '41']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'97'", "'178'", "'198'", "'236'"]
'256'
'97'
'178'
'198'
'236'
['256', '97', '178', '198', '236']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'70'", "'198'", "'178'", "'236'"]
'256'
'70'
'198'
'178'
'236'
['256', '70', '198', '178', '236']
parsed_discourse_facet ['method_citation']
['The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.']
['At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.', '.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00189', '(95%-conf.int.', '0.00189', '-', '0.00189)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00355', '(95%-conf.int.', '0.00355', '-', '0.00355)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:528', 'P:36', 'F:1']
["Unfortunately, Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective function."]
['We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', '.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:120', 'F:0']
['Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.']
['(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).', '.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:55', 'F:0']
['Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.']
['We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', '.']
['system', 'ROUGE-S*', 'Average_R:', '0.00690', '(95%-conf.int.', '0.00690', '-', '0.00690)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.01224', '(95%-conf.int.', '0.01224', '-', '0.01224)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:55', 'F:3']
['Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.']
['At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', '.']
['system', 'ROUGE-S*', 'Average_R:', '0.00150', '(95%-conf.int.', '0.00150', '-', '0.00150)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00277', '(95%-conf.int.', '0.00277', '-', '0.00277)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:55', 'F:1']
['This paper discusses the use of unlabeled examples for the problem of named entity classification.']
['(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).', '.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:28', 'F:0']
['To prevent this we &quot;smooth&quot; the confidence by adding a small value, e, to both W+ and W_, giving at = Plugging the value of at from Equ.']
['.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00645', '(95%-conf.int.', '0.00645', '-', '0.00645)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01176', '(95%-conf.int.', '0.01176', '-', '0.01176)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:45', 'F:3']
['(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.']
['We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00122', '(95%-conf.int.', '0.00122', '-', '0.00122)']
['system', 'ROUGE-S*', 'Average_P:', '0.00159', '(95%-conf.int.', '0.00159', '-', '0.00159)']
['system', 'ROUGE-S*', 'Average_F:', '0.00138', '(95%-conf.int.', '0.00138', '-', '0.00138)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:630', 'F:1']
['Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.']
['We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', '.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:55', 'F:0']
0.0187522220139 0.00199555553338 0.00352222218309





input/ref/Task1/E03-1005_swastika.csv
input/res/Task1/E03-1005.csv
parsing: input/ref/Task1/E03-1005_swastika.csv
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
    <S sid="41" ssid="38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



['41']
41
['41']
parsed_discourse_facet ['aim_citation']
<S sid="80" ssid="32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S>
original cit marker offset is 0
new cit marker offset is 0



['80']
80
['80']
parsed_discourse_facet ['result_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



['143']
143
['143']
parsed_discourse_facet ['method_citation']
<S sid="146" ssid="11">This paper also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic depen dencies (as e.g. in Collins 1999 and Charniak 2000).</S>
original cit marker offset is 0
new cit marker offset is 0



['146']
146
['146']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
<S sid="100" ssid="3">In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['result_citation']
    <S sid="30" ssid="27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S>
original cit marker offset is 0
new cit marker offset is 0



['30']
30
['30']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
    <S sid="30" ssid="27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S>
original cit marker offset is 0
new cit marker offset is 0



['30']
30
['30']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="95" ssid = "47">By using these PCFG-reductions we can thus parse with all subtrees in polynomial time.</S><S sid ="25" ssid = "22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="87" ssid = "39">Yet  his grammar contains more than 5 million subtrees and processing times of over 200 seconds per WSJ sentence are reported (Bod 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'95'", "'25'", "'22'", "'87'"]
'130'
'95'
'25'
'22'
'87'
['130', '95', '25', '22', '87']
parsed_discourse_facet ['implication_citation']
<S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S><S sid ="58" ssid = "10">The notation A@k denotes the node at address k where A is the nonterminal labeling that node.</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="112" ssid = "15">Moreover  if n gets large  SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'", "'58'", "'40'", "'79'", "'112'"]
'85'
'58'
'40'
'79'
'112'
['85', '58', '40', '79', '112']
parsed_discourse_facet ['implication_citation']
<S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="25" ssid = "22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="105" ssid = "8">The derivation with the smallest sum  or highest rank  is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP  its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'25'", "'40'", "'108'", "'105'"]
'130'
'25'
'40'
'108'
'105'
['130', '25', '40', '108', '105']
parsed_discourse_facet ['results_citation']
<S sid ="30" ssid = "27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S><S sid ="139" ssid = "4">But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP  the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.</S><S sid ="32" ssid = "29">However  ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'139'", "'32'", "'22'", "'40'"]
'30'
'139'
'32'
'22'
'40'
['30', '139', '32', '22', '40']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S><S sid ="112" ssid = "15">Moreover  if n gets large  SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="71" ssid = "23">For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.</S><S sid ="139" ssid = "4">But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP  the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'112'", "'79'", "'71'", "'139'"]
'80'
'112'
'79'
'71'
'139'
['80', '112', '79', '71', '139']
parsed_discourse_facet ['method_citation']
<S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="92" ssid = "44">Tested on the OVIS corpus  Bonnema et al. 's proposal obtains results that are comparable to Sima'an (1999) -- see Bonnema et al.</S><S sid ="51" ssid = "3">That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S sid ="101" ssid = "4">We will refer to this model as Simplicity-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'82'", "'140'", "'92'", "'51'", "'101'"]
'82'
'140'
'92'
'51'
'101'
['82', '140', '92', '51', '101']
parsed_discourse_facet ['method_citation']
<S sid ="94" ssid = "46">This paper presents the first published results with this estimator on the WSJ.</S><S sid ="42" ssid = "39">We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.</S><S sid ="12" ssid = "9">This approach has now gained wide usage  as exemplified by the work of Collins (1996  1999)  Charniak (1996  1997)  Johnson (1998)  Chiang (2000)  and many others.</S><S sid ="51" ssid = "3">That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'", "'42'", "'12'", "'51'", "'45'"]
'94'
'42'
'12'
'51'
'45'
['94', '42', '12', '51', '45']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "4">We will refer to this model as Simplicity-DOP.</S><S sid ="51" ssid = "3">That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S sid ="139" ssid = "4">But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP  the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.</S><S sid ="67" ssid = "19">We can create a subtree by choosing any possible left subtree and any possible right subtree.</S><S sid ="126" ssid = "7">We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores  as these are commonly used to rank parsing systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'51'", "'139'", "'67'", "'126'"]
'101'
'51'
'139'
'67'
'126'
['101', '51', '139', '67', '126']
parsed_discourse_facet ['method_citation']
<S sid ="111" ssid = "14">Note that for n=1  SL-DOP is equal to Likelihood-DOP  since there is only one most probable tree to select from  and LS-DOP is equal to Simplicity-DOP  since there is only one simplest tree to select from.</S><S sid ="0" ssid = "0">An Efficient Implementation of a New DOP Model</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="71" ssid = "23">For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'111'", "'0'", "'82'", "'71'", "'140'"]
'111'
'0'
'82'
'71'
'140'
['111', '0', '82', '71', '140']
parsed_discourse_facet ['method_citation']
<S sid ="112" ssid = "15">Moreover  if n gets large  SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.</S><S sid ="145" ssid = "10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S><S sid ="106" ssid = "9">What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.</S><S sid ="101" ssid = "4">We will refer to this model as Simplicity-DOP.</S><S sid ="68" ssid = "20">Thus  there are aj= (bk+ 1)(ci + 1) possible subtrees headed by A @j. Goodman then gives a simple small PCFG with the following property: for every subtree in the training corpus headed by A  the grammar will generate an isomorphic subderivation with probability 1/a.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'145'", "'106'", "'101'", "'68'"]
'112'
'145'
'106'
'101'
'68'
['112', '145', '106', '101', '68']
parsed_discourse_facet ['implication_citation']
<S sid ="58" ssid = "10">The notation A@k denotes the node at address k where A is the nonterminal labeling that node.</S><S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S><S sid ="38" ssid = "35">Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task  the parsing time was reported to be over 200 seconds per sentence (Bod 2003).</S><S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="106" ssid = "9">What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'28'", "'38'", "'13'", "'106'"]
'58'
'28'
'38'
'13'
'106'
['58', '28', '38', '13', '106']
parsed_discourse_facet ['method_citation']
<S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="29" ssid = "26">Johnson (1998b  2002) showed that DOP1's subtree estimation method is statistically biased and inconsistent.</S><S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'", "'40'", "'39'", "'29'", "'130'"]
'28'
'40'
'39'
'29'
'130'
['28', '40', '39', '29', '130']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S><S sid ="44" ssid = "41">But while Bod's estimator obtains state-of-the-art results on the WSJ  comparable to Charniak (2000) and Collins (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'22'", "'45'", "'44'", "'82'"]
'39'
'22'
'45'
'44'
'82'
['39', '22', '45', '44', '82']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="0" ssid = "0">An Efficient Implementation of a New DOP Model</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'22'", "'0'", "'79'", "'15'"]
'108'
'22'
'0'
'79'
'15'
['108', '22', '0', '79', '15']
parsed_discourse_facet ['results_citation']
<S sid ="25" ssid = "22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S><S sid ="114" ssid = "17">Note that Goodman's PCFG-reduction method summarized in Section 2 applies not only to Likelihood-DOP but also to Simplicity-DOP.</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="89" ssid = "41">Let a be the number of times nonterminals of type A occur in the training data.</S><S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'114'", "'82'", "'89'", "'28'"]
'25'
'114'
'82'
'89'
'28'
['25', '114', '82', '89', '28']
parsed_discourse_facet ['implication_citation']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.', "Tested on the OVIS corpus  Bonnema et al. 's proposal obtains results that are comparable to Sima'an (1999) -- see Bonnema et al.", "That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.", 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'We will refer to this model as Simplicity-DOP.']
['system', 'ROUGE-S*', 'Average_R:', '0.02532', '(95%-conf.int.', '0.02532', '-', '0.02532)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.04938', '(95%-conf.int.', '0.04938', '-', '0.04938)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:78', 'F:78']
['The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.']
["Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).", "While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).", 'By using these PCFG-reductions we can thus parse with all subtrees in polynomial time.', 'Yet  his grammar contains more than 5 million subtrees and processing times of over 200 seconds per WSJ sentence are reported (Bod 2003).', 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).']
['system', 'ROUGE-S*', 'Average_R:', '0.00185', '(95%-conf.int.', '0.00185', '-', '0.00185)']
['system', 'ROUGE-S*', 'Average_P:', '0.02000', '(95%-conf.int.', '0.02000', '-', '0.02000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00339', '(95%-conf.int.', '0.00339', '-', '0.00339)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3240', 'P:300', 'F:6']
['Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.']
['Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.', "Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.", 'In the second part of this paper  we extend our experiments with a new notion of the best parse tree.', "But while Bod's estimator obtains state-of-the-art results on the WSJ  comparable to Charniak (2000) and Collins (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).", 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).']
['system', 'ROUGE-S*', 'Average_R:', '0.00093', '(95%-conf.int.', '0.00093', '-', '0.00093)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00181', '(95%-conf.int.', '0.00181', '-', '0.00181)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3240', 'P:78', 'F:3']
['While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.']
['However  ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.', 'But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP  the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.', 'Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.', "Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.", 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).']
['system', 'ROUGE-S*', 'Average_R:', '0.01407', '(95%-conf.int.', '0.01407', '-', '0.01407)']
['system', 'ROUGE-S*', 'Average_P:', '0.09402', '(95%-conf.int.', '0.09402', '-', '0.09402)']
['system', 'ROUGE-S*', 'Average_F:', '0.02447', '(95%-conf.int.', '0.02447', '-', '0.02447)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:351', 'F:33']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['An Efficient Implementation of a New DOP Model', 'The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.', 'However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', "While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.", 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:78', 'F:0']
['Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.']
['Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.', "Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).", 'Let a be the number of times nonterminals of type A occur in the training data.', "While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.", "Note that Goodman's PCFG-reduction method summarized in Section 2 applies not only to Likelihood-DOP but also to Simplicity-DOP."]
['system', 'ROUGE-S*', 'Average_R:', '0.00443', '(95%-conf.int.', '0.00443', '-', '0.00443)']
['system', 'ROUGE-S*', 'Average_P:', '0.17949', '(95%-conf.int.', '0.17949', '-', '0.17949)']
['system', 'ROUGE-S*', 'Average_F:', '0.00865', '(95%-conf.int.', '0.00865', '-', '0.00865)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:78', 'F:14']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['Thus  there are aj= (bk+ 1)(ci + 1) possible subtrees headed by A @j. Goodman then gives a simple small PCFG with the following property: for every subtree in the training corpus headed by A  the grammar will generate an isomorphic subderivation with probability 1/a.', 'This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.', 'What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.', 'We will refer to this model as Simplicity-DOP.', 'Moreover  if n gets large  SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00136', '(95%-conf.int.', '0.00136', '-', '0.00136)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00262', '(95%-conf.int.', '0.00262', '-', '0.00262)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:78', 'F:3']
['DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).']
["Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).", 'The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.', "While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).", "Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.", 'The derivation with the smallest sum  or highest rank  is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP  its results are still rather impressive for such a simple model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00547', '(95%-conf.int.', '0.00547', '-', '0.00547)']
['system', 'ROUGE-S*', 'Average_P:', '0.57778', '(95%-conf.int.', '0.57778', '-', '0.57778)']
['system', 'ROUGE-S*', 'Average_F:', '0.01084', '(95%-conf.int.', '0.01084', '-', '0.01084)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4753', 'P:45', 'F:26']
['The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.']
['Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.', 'An Efficient Implementation of a New DOP Model', 'Note that for n=1  SL-DOP is equal to Likelihood-DOP  since there is only one most probable tree to select from  and LS-DOP is equal to Simplicity-DOP  since there is only one simplest tree to select from.', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.']
['system', 'ROUGE-S*', 'Average_R:', '0.01243', '(95%-conf.int.', '0.01243', '-', '0.01243)']
['system', 'ROUGE-S*', 'Average_P:', '0.07333', '(95%-conf.int.', '0.07333', '-', '0.07333)']
['system', 'ROUGE-S*', 'Average_F:', '0.02126', '(95%-conf.int.', '0.02126', '-', '0.02126)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:300', 'F:22']
0.22461555306 0.00731777769647 0.0136022220711





input/ref/Task1/A97-1014_swastika.csv
input/res/Task1/A97-1014.csv
parsing: input/ref/Task1/A97-1014_swastika.csv
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



['151']
151
['151']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
<S sid="47" ssid="37">Argument structure can be represented in terms of unordered trees (with crossing branches).</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



['167']
167
['167']
parsed_discourse_facet ['method_citation']
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



['160']
160
['160']
parsed_discourse_facet ['method_citation']
<S sid="127" ssid="8">As the need for certain functionalities becomes obvious with growing annotation experience, we have decided to implement the tool in two stages.</S>
original cit marker offset is 0
new cit marker offset is 0



['127']
127
['127']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



['39']
39
['39']
parsed_discourse_facet ['method_citation']
<S sid="72" ssid="17">In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.</S>
original cit marker offset is 0
new cit marker offset is 0



['72']
72
['72']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/A97-1014.csv
<S sid ="73" ssid = "18">While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.</S><S sid ="115" ssid = "28">3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here  where C stands for coordination  and VP for the actual category.</S><S sid ="135" ssid = "16">Figure 1 shows a screen dump of the tool.</S><S sid ="39" ssid = "29">Consider the German sentence (1) daran wird ihn Anna erkennen  &di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies  marked by co-references between traces and the corresponding nodes.</S><S sid ="84" ssid = "29">Consider (qui verbs where the subject of the infinitival VP is not realised syntactically  but co-referent with the subject or object. of the matrix equi verb: (3) er bat mich zu kommen he asked me to come (mich is the understood subject. of kommt n).</S>
original cit marker offset is 0
new cit marker offset is 0



["'73'", "'115'", "'135'", "'39'", "'84'"]
'73'
'115'
'135'
'39'
'84'
['73', '115', '135', '39', '84']
parsed_discourse_facet ['implication_citation']
<S sid ="110" ssid = "23">A similar convention ha.s been adopted for constructions in which scope ambiguities have syntactic effects but a one-to-one correspondence between scope and attachment. does not seem reasonable  cf. focus particles such as only or also.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="30" ssid = "20">Thus the context-free constituent backbone plays a pivotal role in the annotation scheme.</S><S sid ="113" ssid = "26">Since a precise structural description of non-constituent coordination would require a. rich inventory of incomplete phrase types  we have agreed on a sort of unde.rspe.cified representations: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'", "'80'", "'30'", "'113'", "'56'"]
'110'
'80'
'30'
'113'
'56'
['110', '80', '30', '113', '56']
parsed_discourse_facet ['implication_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S><S sid ="78" ssid = "23">Morphological information: Another set of labels represents morphological information.</S><S sid ="8" ssid = "5">A formalism complying with these requirements is described in section 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'56'", "'60'", "'78'", "'8'"]
'80'
'56'
'60'
'78'
'8'
['80', '56', '60', '78', '8']
parsed_discourse_facet ['results_citation']
<S sid ="0" ssid = "0">An Annotation Scheme for Free Word Order Languages</S><S sid ="18" ssid = "8">(Marcus et al.  1994).</S><S sid ="151" ssid = "32">For evaluation  the already annotated sentences were divided into two disjoint sets  one for training (90% of the corpus)  the other one for testing (10%).</S><S sid ="49" ssid = "39">Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.</S><S sid ="114" ssid = "27">Fig.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'18'", "'151'", "'49'", "'114'"]
'0'
'18'
'151'
'49'
'114'
['0', '18', '151', '49', '114']
parsed_discourse_facet ['method_citation']
<S sid ="65" ssid = "10">The tree resembles traditional constituent structures.</S><S sid ="62" ssid = "7">2.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="77" ssid = "22">Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'62'", "'80'", "'77'", "'5'"]
'65'
'62'
'80'
'77'
'5'
['65', '62', '80', '77', '5']
parsed_discourse_facet ['method_citation']
<S sid ="49" ssid = "39">Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="118" ssid = "31">An explicit coordinating conjunction need not be present.</S><S sid ="28" ssid = "18">(Bies et al.  1995)  (Sampson  1995)).</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'56'", "'118'", "'28'", "'80'"]
'49'
'56'
'118'
'28'
'80'
['49', '56', '118', '28', '80']
parsed_discourse_facet ['method_citation']
<S sid ="65" ssid = "10">The tree resembles traditional constituent structures.</S><S sid ="108" ssid = "21">However  full or partial disambiguation takes place in context  and the annotators do not consider unrealistic readings.</S><S sid ="96" ssid = "9">In (5)  either a lexical nominalisation rule for the adjective Gliickliche is stipulated  or the existence of an empty nominal head.</S><S sid ="86" ssid = "31">We call such additional edges secondary links and represent them as dotted lines  see fig.</S><S sid ="135" ssid = "16">Figure 1 shows a screen dump of the tool.</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'108'", "'96'", "'86'", "'135'"]
'65'
'108'
'96'
'86'
'135'
['65', '108', '96', '86', '135']
parsed_discourse_facet ['method_citation']
<S sid ="115" ssid = "28">3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here  where C stands for coordination  and VP for the actual category.</S><S sid ="77" ssid = "22">Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.</S><S sid ="109" ssid = "22">In addition  we have adopted a simple convention for those cases in which context information is insufficient  for total disambiguation: the highest possible attachment  site is chosen.</S><S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'77'", "'109'", "'60'", "'21'"]
'115'
'77'
'109'
'60'
'21'
['115', '77', '109', '60', '21']
parsed_discourse_facet ['method_citation']
<S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="73" ssid = "18">While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.</S><S sid ="86" ssid = "31">We call such additional edges secondary links and represent them as dotted lines  see fig.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S><S sid ="77" ssid = "22">Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'73'", "'86'", "'21'", "'77'"]
'56'
'73'
'86'
'21'
'77'
['56', '73', '86', '21', '77']
parsed_discourse_facet ['method_citation']
<S sid ="138" ssid = "19">This allows easy modification if needed.</S><S sid ="125" ssid = "6">The tool should also permit a convenient handling of node and edge labels.</S><S sid ="89" ssid = "2">However  some other standard analysts turn out to be problematic  mainly due to the partial  idealised character of competence grammars  which often marginalise or ignore such important. phenomena. as 'deficient' (e.g. headless) constructions  appositions  temporal expressions  etc.</S><S sid ="49" ssid = "39">Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.</S><S sid ="110" ssid = "23">A similar convention ha.s been adopted for constructions in which scope ambiguities have syntactic effects but a one-to-one correspondence between scope and attachment. does not seem reasonable  cf. focus particles such as only or also.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'", "'125'", "'89'", "'49'", "'110'"]
'138'
'125'
'89'
'49'
'110'
['138', '125', '89', '49', '110']
parsed_discourse_facet ['implication_citation']
<S sid ="147" ssid = "28">(Cutting et al.  1992) and (Feldweg  1995)).</S><S sid ="7" ssid = "4">On the basis of these considerations  we formulate several additional requirements.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="144" ssid = "25">We distinguish five degrees of automation: So far  about 1100 sentences of our corpus have been annotated.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'7'", "'80'", "'144'", "'21'"]
'147'
'7'
'80'
'144'
'21'
['147', '7', '80', '144', '21']
parsed_discourse_facet ['method_citation']
<S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S><S sid ="73" ssid = "18">While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.</S><S sid ="96" ssid = "9">In (5)  either a lexical nominalisation rule for the adjective Gliickliche is stipulated  or the existence of an empty nominal head.</S><S sid ="46" ssid = "36">These approaches provide an adequate explanation for several issues problematic for phrase-structure grammars (clause union  extraposition  diverse second-position phenomena).</S><S sid ="33" ssid = "23">(Lehmann et al.  1996)  (Marcus et al.  1994)  (Sampson  1995)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'73'", "'96'", "'46'", "'33'"]
'60'
'73'
'96'
'46'
'33'
['60', '73', '96', '46', '33']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "13">As the annotation scheme does not distinguish different bar levels or any similar intermediate categories  only a small set of node labels is needed (currently 16 tags  S  NP  AP ...).</S><S sid ="51" ssid = "41">This requirement speaks against the traditional sort of dependency trees  in which heads a re represented as non-terminal nodes  cf.</S><S sid ="109" ssid = "22">In addition  we have adopted a simple convention for those cases in which context information is insufficient  for total disambiguation: the highest possible attachment  site is chosen.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'51'", "'109'", "'11'", "'21'"]
'68'
'51'
'109'
'11'
'21'
['68', '51', '109', '11', '21']
parsed_discourse_facet ['method_citation']
<S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="138" ssid = "19">This allows easy modification if needed.</S><S sid ="115" ssid = "28">3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here  where C stands for coordination  and VP for the actual category.</S><S sid ="112" ssid = "25">A problem for the rudimentary argument. structure representations is the use of incomplete structures in natural language  i.e. phenomena such as coordination and ellipsis.</S><S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'138'", "'115'", "'112'", "'60'"]
'56'
'138'
'115'
'112'
'60'
['56', '138', '115', '112', '60']
parsed_discourse_facet ['results_citation']
['In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.']
['Separable verb prefixes are labeled SVP.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", '(Bies et al.  1995)  (Sampson  1995)).', 'An explicit coordinating conjunction need not be present.', 'Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:91', 'F:0']
['We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.']
['However  full or partial disambiguation takes place in context  and the annotators do not consider unrealistic readings.', 'We call such additional edges secondary links and represent them as dotted lines  see fig.', 'The tree resembles traditional constituent structures.', 'Figure 1 shows a screen dump of the tool.', 'In (5)  either a lexical nominalisation rule for the adjective Gliickliche is stipulated  or the existence of an empty nominal head.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:55', 'F:0']
['These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.']
['Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.', 'We call such additional edges secondary links and represent them as dotted lines  see fig.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.', 'Disambiguation is based on human processing skills (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:21', 'F:0']
['In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.']
['Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.', 'These approaches provide an adequate explanation for several issues problematic for phrase-structure grammars (clause union  extraposition  diverse second-position phenomena).', 'While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.', '(Lehmann et al.  1996)  (Marcus et al.  1994)  (Sampson  1995)).', 'In (5)  either a lexical nominalisation rule for the adjective Gliickliche is stipulated  or the existence of an empty nominal head.']
['system', 'ROUGE-S*', 'Average_R:', '0.00058', '(95%-conf.int.', '0.00058', '-', '0.00058)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00114', '(95%-conf.int.', '0.00114', '-', '0.00114)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:36', 'F:1']
["The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction."]
['As the annotation scheme does not distinguish different bar levels or any similar intermediate categories  only a small set of node labels is needed (currently 16 tags  S  NP  AP ...).', 'Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.', 'In addition  we have adopted a simple convention for those cases in which context information is insufficient  for total disambiguation: the highest possible attachment  site is chosen.', 'This requirement speaks against the traditional sort of dependency trees  in which heads a re represented as non-terminal nodes  cf.', 'Disambiguation is based on human processing skills (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:66', 'F:0']
["The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction."]
['This allows easy modification if needed.', "3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here  where C stands for coordination  and VP for the actual category.", "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.', 'A problem for the rudimentary argument. structure representations is the use of incomplete structures in natural language  i.e. phenomena such as coordination and ellipsis.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:66', 'F:0']
['For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).']
['Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.', 'A formalism complying with these requirements is described in section 3.', 'Separable verb prefixes are labeled SVP.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'Morphological information: Another set of labels represents morphological information.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:55', 'F:0']
["Consider the German sentence (1) daran wird ihn Anna erkennen, &di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes."]
['(Cutting et al.  1992) and (Feldweg  1995)).', 'Separable verb prefixes are labeled SVP.', 'We distinguish five degrees of automation: So far  about 1100 sentences of our corpus have been annotated.', 'On the basis of these considerations  we formulate several additional requirements.', 'Disambiguation is based on human processing skills (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:406', 'F:0']
['As the need for certain functionalities becomes obvious with growing annotation experience, we have decided to implement the tool in two stages.']
['A similar convention ha.s been adopted for constructions in which scope ambiguities have syntactic effects but a one-to-one correspondence between scope and attachment. does not seem reasonable  cf. focus particles such as only or also.', 'This allows easy modification if needed.', "However  some other standard analysts turn out to be problematic  mainly due to the partial  idealised character of competence grammars  which often marginalise or ignore such important. phenomena. as 'deficient' (e.g. headless) constructions  appositions  temporal expressions  etc.", 'The tool should also permit a convenient handling of node and edge labels.', 'Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:36', 'F:0']
['We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.']
["We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'Separable verb prefixes are labeled SVP.', 'Thus the context-free constituent backbone plays a pivotal role in the annotation scheme.', 'A similar convention ha.s been adopted for constructions in which scope ambiguities have syntactic effects but a one-to-one correspondence between scope and attachment. does not seem reasonable  cf. focus particles such as only or also.', 'Since a precise structural description of non-constituent coordination would require a. rich inventory of incomplete phrase types  we have agreed on a sort of unde.rspe.cified representations: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:55', 'F:0']
['Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.']
['For evaluation  the already annotated sentences were divided into two disjoint sets  one for training (90% of the corpus)  the other one for testing (10%).', 'An Annotation Scheme for Free Word Order Languages', 'Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.', 'Fig.', '(Marcus et al.  1994).']
['system', 'ROUGE-S*', 'Average_R:', '0.00476', '(95%-conf.int.', '0.00476', '-', '0.00476)']
['system', 'ROUGE-S*', 'Average_P:', '0.02857', '(95%-conf.int.', '0.02857', '-', '0.02857)']
['system', 'ROUGE-S*', 'Average_F:', '0.00816', '(95%-conf.int.', '0.00816', '-', '0.00816)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:105', 'F:3']
0.00512272722616 0.000485454541041 0.000845454537769





input/ref/Task1/A00-2030_sweta.csv
input/res/Task1/A00-2030.csv
parsing: input/ref/Task1/A00-2030_sweta.csv
<S sid="18" ssid="1">Almost all approaches to information extraction &#8212; even at the sentence level &#8212; are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="100" ssid="5">Given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a While our focus throughout the project was on TE and TR, we became curious about how well the model did at part-of-speech tagging, syntactic parsing, and at name finding.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="15">Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
 <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="1">Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.<
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="102" ssid = "7">The results are summarized in Table 2.</S><S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'79'", "'82'", "'102'", "'56'"]
'68'
'79'
'82'
'102'
'56'
['68', '79', '82', '102', '56']
parsed_discourse_facet ['implication_citation']
<S sid ="65" ssid = "6">We illustrate the generation process by walking through a few of the steps of the parse shown in Figure 3.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="9" ssid = "7">Manually creating sourcespecific training data for syntax was not required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'96'", "'23'", "'88'", "'9'"]
'65'
'96'
'23'
'88'
'9'
['65', '96', '23', '88', '9']
parsed_discourse_facet ['implication_citation']
<S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'68'", "'36'", "'6'", "'96'"]
'88'
'68'
'36'
'6'
'96'
['88', '68', '36', '6', '96']
parsed_discourse_facet ['results_citation']
<S sid ="60" ssid = "1">In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'62'", "'13'", "'36'", "'105'"]
'60'
'62'
'13'
'36'
'105'
['60', '62', '13', '36', '105']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="47" ssid = "7">It soon became painfully obvious that this task could not be performed in the available time.</S><S sid ="73" ssid = "14">We now briefly summarize the probability structure of the model.</S><S sid ="97" ssid = "2">The evaluation results are summarized in Table 1.</S><S sid ="80" ssid = "2">However  because these estimates are too sparse to be relied upon  we use interpolated estimates consisting of mixtures of successively lowerorder estimates (as in Placeway et al. 1993).</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'47'", "'73'", "'97'", "'80'"]
'38'
'47'
'73'
'97'
'80'
['38', '47', '73', '97', '80']
parsed_discourse_facet ['method_citation']
<S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'", "'15'", "'45'", "'88'", "'56'"]
'79'
'15'
'45'
'88'
'56'
['79', '15', '45', '88', '56']
parsed_discourse_facet ['method_citation']
<S sid ="49" ssid = "9">By necessity  we adopted the strategy of hand marking only the semantics.</S><S sid ="70" ssid = "11">Post-modifier constituents for the PER/NP.</S><S sid ="69" ssid = "10">8.</S><S sid ="95" ssid = "14">The semantics  that is  the entities and relations  can then be directly extracted from these sentential trees.</S><S sid ="111" ssid = "3">The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies  either expressed or implied  of the Defense Advanced Research Projects Agency or the United States Government.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'70'", "'69'", "'95'", "'111'"]
'49'
'70'
'69'
'95'
'111'
['49', '70', '69', '95', '111']
parsed_discourse_facet ['method_citation']
<S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="4" ssid = "2">Yet  relatively few have embedded one of these algorithms in a task.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="67" ssid = "8">We pick up the derivation just after the topmost S and its head word  said  have been produced.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'79'", "'4'", "'82'", "'67'"]
'23'
'79'
'4'
'82'
'67'
['23', '79', '4', '82', '67']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "3">However  pipelined architectures suffer from a serious disadvantage: errors accumulate as they propagate through the pipeline.</S><S sid ="31" ssid = "14">If the single generalized model could then be extended to semantic analysis  all necessary sentence level processing would be contained in that model.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="84" ssid = "3">Although mathematically the model predicts tree elements in a top-down fashion  we search the space bottom-up using a chartbased search.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'31'", "'0'", "'38'", "'84'"]
'20'
'31'
'0'
'38'
'84'
['20', '31', '0', '38', '84']
parsed_discourse_facet ['method_citation']
<S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'38'", "'107'", "'104'", "'23'"]
'88'
'38'
'107'
'104'
'23'
['88', '38', '107', '104', '23']
parsed_discourse_facet ['implication_citation']
<S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="74" ssid = "15">The categories for head constituents  cl are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c _1  and the head word of their parent  wp.</S><S sid ="69" ssid = "10">8.</S><S sid ="111" ssid = "3">The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies  either expressed or implied  of the Defense Advanced Research Projects Agency or the United States Government.</S>
original cit marker offset is 0
new cit marker offset is 0



["'82'", "'45'", "'74'", "'69'", "'111'"]
'82'
'45'
'74'
'69'
'111'
['82', '45', '74', '69', '111']
parsed_discourse_facet ['method_citation']
<S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="102" ssid = "7">The results are summarized in Table 2.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'", "'102'", "'82'", "'62'", "'68'"]
'45'
'102'
'82'
'62'
'68'
['45', '102', '82', '62', '68']
parsed_discourse_facet ['method_citation']
<S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="97" ssid = "2">The evaluation results are summarized in Table 1.</S><S sid ="75" ssid = "16">Separate probabilities are maintained for left (pre) and right (post) modifiers: Part-of-speech tags  t    for modifiers are predicted based on the modifier  cm  the partof-speech tag of the head word  th  and the head word itself  wh: Head words  w for modifiers are predicted based on the modifier  cm  the part-of-speech tag of the modifier word   t the part-ofspeech tag of the head word   th  and the head word itself  wh: lAwmicm tm th wh)  e.g.</S><S sid ="95" ssid = "14">The semantics  that is  the entities and relations  can then be directly extracted from these sentential trees.</S><S sid ="59" ssid = "5">These labels serve to form a continuous chain between the relation and its argument.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'", "'97'", "'75'", "'95'", "'59'"]
'45'
'97'
'75'
'95'
'59'
['45', '97', '75', '95', '59']
parsed_discourse_facet ['method_citation']
<S sid ="76" ssid = "17">Finally  word features  fm  for modifiers are predicted based on the modifier  cm  the partof-speech tag of the modifier word   t the part-of-speech tag of the head word th  the head word itself  wh  and whether or not the modifier head word  w is known or unknown.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="112" ssid = "4">We thank Michael Collins of the University of Pennsylvania for his valuable suggestions.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'", "'36'", "'112'", "'101'", "'5'"]
'76'
'36'
'112'
'101'
'5'
['76', '36', '112', '101', '5']
parsed_discourse_facet ['results_citation']
<S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'68'", "'13'", "'38'", "'23'"]
'56'
'68'
'13'
'38'
'23'
['56', '68', '13', '38', '23']
parsed_discourse_facet ['implication_citation']
<S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'62'", "'36'", "'79'", "'107'"]
'51'
'62'
'36'
'79'
'107'
['51', '62', '36', '79', '107']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="49" ssid = "9">By necessity  we adopted the strategy of hand marking only the semantics.</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="39" ssid = "7">For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'49'", "'38'", "'5'", "'39'"]
'6'
'49'
'38'
'5'
'39'
['6', '49', '38', '5', '39']
parsed_discourse_facet ['results_citation']
<S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="31" ssid = "14">If the single generalized model could then be extended to semantic analysis  all necessary sentence level processing would be contained in that model.</S><S sid ="83" ssid = "2">More precisely  it must find the most likely augmented parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'57'", "'88'", "'31'", "'83'"]
'23'
'57'
'88'
'31'
'83'
['23', '57', '88', '31', '83']
parsed_discourse_facet ['aim_citation']
<S sid ="95" ssid = "14">The semantics  that is  the entities and relations  can then be directly extracted from these sentential trees.</S><S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="34" ssid = "2">In these trees  the standard TREEBANK structures are augmented to convey semantic information  that is  entities and relations.</S><S sid ="4" ssid = "2">Yet  relatively few have embedded one of these algorithms in a task.</S><S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'45'", "'34'", "'4'", "'6'"]
'95'
'45'
'34'
'4'
'6'
['95', '45', '34', '4', '6']
parsed_discourse_facet ['method_citation']
['We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.']
['The next steps are to generate in order: In this case  there are none.', 'An integrated model can limit the propagation of errors by making all decisions jointly.', 'For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.', 'For example  in the phrase &quot;Lt. Cmdr.', 'Other labels indicate relations among entities.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:406', 'P:78', 'F:0']
['In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.']
['Other labels indicate relations among entities.', 'By necessity  we adopted the strategy of hand marking only the semantics.', 'Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.', 'For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.', 'In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.02459', '(95%-conf.int.', '0.02459', '-', '0.02459)']
['system', 'ROUGE-S*', 'Average_P:', '0.23684', '(95%-conf.int.', '0.23684', '-', '0.23684)']
['system', 'ROUGE-S*', 'Average_F:', '0.04455', '(95%-conf.int.', '0.04455', '-', '0.04455)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:190', 'F:45']
['We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction']
['More precisely  it must find the most likely augmented parse tree.', 'If the single generalized model could then be extended to semantic analysis  all necessary sentence level processing would be contained in that model.', 'David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.', 'For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).', 'An integrated model can limit the propagation of errors by making all decisions jointly.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:78', 'F:0']
['Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.']
['It soon became painfully obvious that this task could not be performed in the available time.', 'However  because these estimates are too sparse to be relied upon  we use interpolated estimates consisting of mixtures of successively lowerorder estimates (as in Placeway et al. 1993).', 'Other labels indicate relations among entities.', 'We now briefly summarize the probability structure of the model.', 'The evaluation results are summarized in Table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:78', 'F:0']
['The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.']
['For example  in the phrase &quot;Lt. Cmdr.', 'For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).', 'The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.', 'For each location  one must also give its type (city  province  county  body of water  etc.).', 'Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00101', '(95%-conf.int.', '0.00101', '-', '0.00101)']
['system', 'ROUGE-S*', 'Average_P:', '0.00654', '(95%-conf.int.', '0.00654', '-', '0.00654)']
['system', 'ROUGE-S*', 'Average_F:', '0.00175', '(95%-conf.int.', '0.00175', '-', '0.00175)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:153', 'F:1']
['We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.']
['8.', 'The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies  either expressed or implied  of the Defense Advanced Research Projects Agency or the United States Government.', 'Post-modifier constituents for the PER/NP.', u'The semantics \u2014 that is  the entities and relations \u2014 can then be directly extracted from these sentential trees.', 'By necessity  we adopted the strategy of hand marking only the semantics.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:78', 'F:0']
['In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.']
['A Novel Use of Statistical Parsing to Extract Information from Text', 'If the single generalized model could then be extended to semantic analysis  all necessary sentence level processing would be contained in that model.', 'Other labels indicate relations among entities.', 'However  pipelined architectures suffer from a serious disadvantage: errors accumulate as they propagate through the pipeline.', 'Although mathematically the model predicts tree elements in a top-down fashion  we search the space bottom-up using a chartbased search.']
['system', 'ROUGE-S*', 'Average_R:', '0.00540', '(95%-conf.int.', '0.00540', '-', '0.00540)']
['system', 'ROUGE-S*', 'Average_P:', '0.02105', '(95%-conf.int.', '0.02105', '-', '0.02105)']
['system', 'ROUGE-S*', 'Average_F:', '0.00859', '(95%-conf.int.', '0.00859', '-', '0.00859)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:190', 'F:4']
['The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.']
['For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).', 'Other labels indicate relations among entities.', 'The semantic training corpus was produced by students according to a simple set of guidelines.', 'We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.', 'An integrated model can limit the propagation of errors by making all decisions jointly.']
['system', 'ROUGE-S*', 'Average_R:', '0.00193', '(95%-conf.int.', '0.00193', '-', '0.00193)']
['system', 'ROUGE-S*', 'Average_P:', '0.01307', '(95%-conf.int.', '0.01307', '-', '0.01307)']
['system', 'ROUGE-S*', 'Average_F:', '0.00337', '(95%-conf.int.', '0.00337', '-', '0.00337)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:153', 'F:2']
['In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.']
['In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.', 'The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.', u'The semantics \u2014 that is  the entities and relations \u2014 can then be directly extracted from these sentential trees.', 'Yet  relatively few have embedded one of these algorithms in a task.', 'In these trees  the standard TREEBANK structures are augmented to convey semantic information  that is  entities and relations.']
['system', 'ROUGE-S*', 'Average_R:', '0.06091', '(95%-conf.int.', '0.06091', '-', '0.06091)']
['system', 'ROUGE-S*', 'Average_P:', '0.28947', '(95%-conf.int.', '0.28947', '-', '0.28947)']
['system', 'ROUGE-S*', 'Average_F:', '0.10064', '(95%-conf.int.', '0.10064', '-', '0.10064)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:190', 'F:55']
['In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.']
['The results are summarized in Table 2.', 'Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.', 'For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.', 'The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.', 'The next steps are to generate in order: In this case  there are none.']
['system', 'ROUGE-S*', 'Average_R:', '0.00189', '(95%-conf.int.', '0.00189', '-', '0.00189)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00349', '(95%-conf.int.', '0.00349', '-', '0.00349)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:528', 'P:45', 'F:1']
['We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.']
['For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.', 'To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.', 'The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.', 'Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.', 'The semantic training corpus was produced by students according to a simple set of guidelines.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:78', 'F:0']
['In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.']
[u'Finally  word features  fm  for modifiers are predicted based on the modifier  cm  the partof-speech tag of the modifier word   t\u201e\u201e the part-of-speech tag of the head word th  the head word itself  wh  and whether or not the modifier head word  w\u201e\u201e is known or unknown.', 'We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.', 'Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.', 'The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.', 'We thank Michael Collins of the University of Pennsylvania for his valuable suggestions.']
['system', 'ROUGE-S*', 'Average_R:', '0.00032', '(95%-conf.int.', '0.00032', '-', '0.00032)']
['system', 'ROUGE-S*', 'Average_P:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00062', '(95%-conf.int.', '0.00062', '-', '0.00062)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:120', 'F:1']
['Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.']
['These labels serve to form a continuous chain between the relation and its argument.', u'Separate probabilities are maintained for left (pre) and right (post) modifiers: Part-of-speech tags  t    for modifiers are predicted based on the modifier  cm  the partof-speech tag of the head word  th  and the head word itself  wh: Head words  w\u201e\u201e for modifiers are predicted based on the modifier  cm  the part-of-speech tag of the modifier word   t\u201e\u201e the part-ofspeech tag of the head word   th  and the head word itself  wh: lAwmicm tm th wh)  e.g.', 'The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.', u'The semantics \u2014 that is  the entities and relations \u2014 can then be directly extracted from these sentential trees.', 'The evaluation results are summarized in Table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2701', 'P:253', 'F:0']
0.0459630765695 0.00738846148163 0.0125392306728





input/ref/Task1/P08-1028_aakansha.csv
input/res/Task1/P08-1028.csv
parsing: input/ref/Task1/P08-1028_aakansha.csv
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="185" ssid="19">The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.</S>
    <S sid="186" ssid="20">The combined model is best overall with &#961; = 0.19.</S>
    <S sid="187" ssid="21">However, the difference between the two models is not statistically significant.</S>
original cit marker offset is 0
new cit marker offset is 0



["'185'", "'186'"]
'185'
'186'
['185', '186']
parsed_discourse_facet ['result_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="21">The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'"]
'48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="53" ssid="1">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.</S><S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'53'", "'57'"]
'53'
'57'
['53', '57']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="16">Although the composition model in (5) is commonly used in the literature, from a linguistic perspective, the model in (6) is more appealing.</S>
    <S sid="69" ssid="17">Simply adding the vectors u and v lumps their contents together rather than allowing the content of one vector to pick out the relevant content of the other.</S>
    <S sid="70" ssid="18">Instead, it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v, and vice versa.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'69'", "'70'"]
'68'
'69'
'70'
['68', '69', '70']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
    <S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'", "'25'"]
'24'
'25'
['24', '25']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="12">Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'"]
'64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="176" ssid="10">The multiplicative and combined models yield means closer to the human ratings.</S>
    <S sid="177" ssid="11">The difference between High and Low similarity values estimated by these models are statistically significant (p &lt; 0.01 using the Wilcoxon rank sum test).</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'", "'177'"]
'176'
'177'
['176', '177']
parsed_discourse_facet ['method_citation']
<S sid="191" ssid="3">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'191'"]
'191'
['191']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1028.csv
<S sid ="200" ssid = "12">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid ="136" ssid = "49">We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine-grained semantic distinctions (see Table 1).</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants similarity judgments with those of the models.6 We used Spearmans p for our correlation analyses.</S><S sid ="135" ssid = "48">The average inter-subject agreement5 was  = 0.40.</S><S sid ="149" ssid = "62">Our composition models have no additional parameters beyond the semantic space just described  with three exceptions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'200'", "'136'", "'164'", "'135'", "'149'"]
'200'
'136'
'164'
'135'
'149'
['200', '136', '164', '135', '149']
parsed_discourse_facet ['implication_citation']
<S sid ="107" ssid = "20">Landmarks were taken from WordNet (Fellbaum  1998).</S><S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="45" ssid = "18">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid ="192" ssid = "4">We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'63'", "'42'", "'45'", "'192'"]
'107'
'63'
'42'
'45'
'192'
['107', '63', '42', '45', '192']
parsed_discourse_facet ['implication_citation']
<S sid ="30" ssid = "3">For the hierarchical structure of natural language this binding problem becomes particularly acute.</S><S sid ="78" ssid = "26">These vectors are not arbitrary and ideally they must exhibit some relation to the words of the construction under consideration.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants similarity judgments with those of the models.6 We used Spearmans p for our correlation analyses.</S><S sid ="171" ssid = "5">For comparison  we also show the human ratings for these items (UpperBound).</S><S sid ="176" ssid = "10">The multiplicative and combined models yield means closer to the human ratings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'78'", "'164'", "'171'", "'176'"]
'30'
'78'
'164'
'171'
'176'
['30', '78', '164', '171', '176']
parsed_discourse_facet ['results_citation']
<S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S><S sid ="85" ssid = "33">One potential drawback of multiplicative models is the effect of components with value zero.</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'", "'103'", "'85'", "'101'", "'63'"]
'42'
'103'
'85'
'101'
'63'
['42', '103', '85', '101', '63']
parsed_discourse_facet ['method_citation']
<S sid ="97" ssid = "10">Moreover by using a minimal structure we factor out inessential degrees of freedom and are able to assess the merits of different models on an equal footing.</S><S sid ="126" ssid = "39">49 unpaid volunteers completed the experiment  all native speakers of English.</S><S sid ="192" ssid = "4">We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials.</S><S sid ="120" ssid = "33">Examples of our items are given in Table 1.</S><S sid ="165" ssid = "78">Again  better models should correlate better with the experimental data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'126'", "'192'", "'120'", "'165'"]
'97'
'126'
'192'
'120'
'165'
['97', '126', '192', '120', '165']
parsed_discourse_facet ['method_citation']
<S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="45" ssid = "18">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="68" ssid = "16">Although the composition model in (5) is commonly used in the literature  from a linguistic perspective  the model in (6) is more appealing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'", "'121'", "'45'", "'116'", "'68'"]
'42'
'121'
'45'
'116'
'68'
['42', '121', '45', '116', '68']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "13">In the following we describe our data collection procedure and give details on how our composition models were constructed and evaluated.</S><S sid ="123" ssid = "36">Sentence pairs were presented serially in random order.</S><S sid ="112" ssid = "25">The stimuli were administered to four separate groups; each group saw one set of 100 sentences.</S><S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'123'", "'112'", "'25'", "'63'"]
'100'
'123'
'112'
'25'
'63'
['100', '123', '112', '25', '63']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S><S sid ="85" ssid = "33">One potential drawback of multiplicative models is the effect of components with value zero.</S><S sid ="145" ssid = "58">The latter were the most common context words (excluding a stop list of function words).</S><S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'85'", "'145'", "'121'", "'103'"]
'4'
'85'
'145'
'121'
'103'
['4', '85', '145', '121', '103']
parsed_discourse_facet ['method_citation']
<S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="45" ssid = "18">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid ="95" ssid = "8">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'45'", "'95'", "'150'", "'42'"]
'25'
'45'
'95'
'150'
'42'
['25', '45', '95', '150', '42']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "49">We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine-grained semantic distinctions (see Table 1).</S><S sid ="132" ssid = "45">We also measured how well humans agree in their ratings.</S><S sid ="158" ssid = "71">The m neighbors most similar to the predicate  and the k of m neighbors closest to its argument.</S><S sid ="200" ssid = "12">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid ="17" ssid = "13">It was not the sales manager who hit the bottle that day  but the office worker with the serious drinking problem. b.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'132'", "'158'", "'200'", "'17'"]
'136'
'132'
'158'
'200'
'17'
['136', '132', '158', '200', '17']
parsed_discourse_facet ['implication_citation']
<S sid ="95" ssid = "8">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="190" ssid = "2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid ="46" ssid = "19">However  since it is order independent  it cannot capture meaning differences that are modulated by differences in syntactic structure.</S><S sid ="29" ssid = "2">While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'150'", "'190'", "'46'", "'29'"]
'95'
'150'
'190'
'46'
'29'
['95', '150', '190', '46', '29']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="156" ssid = "69">This yielded a weighted sum consisting of 95% verb  0% noun and 5% of their multiplicative combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'103'", "'116'", "'150'", "'156'"]
'34'
'103'
'116'
'150'
'156'
['34', '103', '116', '150', '156']
parsed_discourse_facet ['method_citation']
<S sid ="176" ssid = "10">The multiplicative and combined models yield means closer to the human ratings.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="157" ssid = "70">Finally  Kintschs (2001) additive model has two extra parameters.</S><S sid ="88" ssid = "1">We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).</S><S sid ="38" ssid = "11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'", "'42'", "'157'", "'88'", "'38'"]
'176'
'42'
'157'
'88'
'38'
['176', '42', '157', '88', '38']
parsed_discourse_facet ['method_citation']
<S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="114" ssid = "27">For each reference verb  the subjects responses were entered into a contingency table  whose rows corresponded to nouns and columns to each possible answer (i.e.  one of the two landmarks).</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'116'", "'150'", "'59'", "'114'"]
'63'
'116'
'150'
'59'
'114'
['63', '116', '150', '59', '114']
parsed_discourse_facet ['results_citation']
<S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="64" ssid = "12">Now  if we assume that p lies in the same space as u and v  avoiding the issues of dimensionality associated with tensor products  and that f is a linear function  for simplicity  of the cartesian product of u and v  then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast  if we assume that f is a linear function of the tensor product of u and v  then we obtain multiplicative models: where C is a tensor of rank 3  which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'", "'121'", "'116'", "'64'", "'103'"]
'42'
'121'
'116'
'64'
'103'
['42', '121', '116', '64', '103']
parsed_discourse_facet ['implication_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="77" ssid = "25">For additive models  a natural way to achieve this is to include further vectors into the summation.</S><S sid ="70" ssid = "18">Instead  it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v  and vice versa.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="85" ssid = "33">One potential drawback of multiplicative models is the effect of components with value zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'77'", "'70'", "'59'", "'85'"]
'194'
'77'
'70'
'59'
'85'
['194', '77', '70', '59', '85']
parsed_discourse_facet ['method_citation']
<S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S><S sid ="130" ssid = "43">As we can see sentences with high similarity landmarks are perceived as more similar to the reference sentence.</S><S sid ="82" ssid = "30">In contrast to the simple additive model  this extended model is sensitive to syntactic structure  since n is chosen from among the neighbors of the predicate  distinguishing it from the argument.</S><S sid ="73" ssid = "21">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware  since semantically important constituents can participate more actively in the composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'121'", "'103'", "'130'", "'82'", "'73'"]
'121'
'103'
'130'
'82'
'73'
['121', '103', '130', '82', '73']
parsed_discourse_facet ['results_citation']
['Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
[u'A more scrupulous evaluation requires directly correlating all the individual participants\u2019 similarity judgments with those of the models.6 We used Spearman\u2019s p for our correlation analyses.', 'The applications of the framework discussed here are many and varied both for cognitive science and NLP.', 'We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine-grained semantic distinctions (see Table 1).', u'The average inter-subject agreement5 was \u03c1 = 0.40.', 'Our composition models have no additional parameters beyond the semantic space just described  with three exceptions.']
['system', 'ROUGE-S*', 'Average_R:', '0.00621', '(95%-conf.int.', '0.00621', '-', '0.00621)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01135', '(95%-conf.int.', '0.01135', '-', '0.01135)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:105', 'F:7']
['The combined model is best overall with &#961; = 0.19.', 'However, the difference between the two models is not statistically significant.', 'The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.']
['This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.', 'All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).', 'Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.', 'This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.', 'One potential drawback of multiplicative models is the effect of components with value zero.']
['system', 'ROUGE-S*', 'Average_R:', '0.00773', '(95%-conf.int.', '0.00773', '-', '0.00773)']
['system', 'ROUGE-S*', 'Average_P:', '0.05229', '(95%-conf.int.', '0.05229', '-', '0.05229)']
['system', 'ROUGE-S*', 'Average_F:', '0.01347', '(95%-conf.int.', '0.01347', '-', '0.01347)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:153', 'F:8']
['Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
['Examples of our items are given in Table 1.', 'Again  better models should correlate better with the experimental data.', '49 unpaid volunteers completed the experiment  all native speakers of English.', 'Moreover by using a minimal structure we factor out inessential degrees of freedom and are able to assess the merits of different models on an equal footing.', 'We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials.']
['system', 'ROUGE-S*', 'Average_R:', '0.00150', '(95%-conf.int.', '0.00150', '-', '0.00150)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00259', '(95%-conf.int.', '0.00259', '-', '0.00259)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:105', 'F:1']
['Although the composition model in (5) is commonly used in the literature, from a linguistic perspective, the model in (6) is more appealing.', 'Simply adding the vectors u and v lumps their contents together rather than allowing the content of one vector to pick out the relevant content of the other.', 'Instead, it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v, and vice versa.']
['We present a general framework for vector-based composition which allows us to consider different classes of models.', 'Vector addition does not increase the dimensionality of the resulting vector.', 'Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.', 'This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.', 'First  the additive model in (7) weighs differentially the contribution of the two constituents.']
['system', 'ROUGE-S*', 'Average_R:', '0.02020', '(95%-conf.int.', '0.02020', '-', '0.02020)']
['system', 'ROUGE-S*', 'Average_P:', '0.04926', '(95%-conf.int.', '0.04926', '-', '0.04926)']
['system', 'ROUGE-S*', 'Average_F:', '0.02865', '(95%-conf.int.', '0.02865', '-', '0.02865)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:406', 'F:20']
['In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.']
['In contrast to the simple additive model  this extended model is sensitive to syntactic structure  since n is chosen from among the neighbors of the predicate  distinguishing it from the argument.', 'Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).', 'As we can see sentences with high similarity landmarks are perceived as more similar to the reference sentence.', 'Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware  since semantically important constituents can participate more actively in the composition.', 'All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).']
['system', 'ROUGE-S*', 'Average_R:', '0.00152', '(95%-conf.int.', '0.00152', '-', '0.00152)']
['system', 'ROUGE-S*', 'Average_P:', '0.08889', '(95%-conf.int.', '0.08889', '-', '0.08889)']
['system', 'ROUGE-S*', 'Average_F:', '0.00299', '(95%-conf.int.', '0.00299', '-', '0.00299)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:45', 'F:4']
['In this paper we presented a general framework for vector-based semantic composition.', 'We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['However  since it is order independent  it cannot capture meaning differences that are modulated by differences in syntactic structure.', 'First  the additive model in (7) weighs differentially the contribution of the two constituents.', 'Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.', 'We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.', 'While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.']
['system', 'ROUGE-S*', 'Average_R:', '0.03571', '(95%-conf.int.', '0.03571', '-', '0.03571)']
['system', 'ROUGE-S*', 'Average_P:', '0.40441', '(95%-conf.int.', '0.40441', '-', '0.40441)']
['system', 'ROUGE-S*', 'Average_F:', '0.06563', '(95%-conf.int.', '0.06563', '-', '0.06563)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:136', 'F:55']
['The difference between High and Low similarity values estimated by these models are statistically significant (p &lt; 0.01 using the Wilcoxon rank sum test).', 'The multiplicative and combined models yield means closer to the human ratings.']
[u'For each reference verb  the subjects\u2019 responses were entered into a contingency table  whose rows corresponded to nouns and columns to each possible answer (i.e.  one of the two landmarks).', 'First  the additive model in (7) weighs differentially the contribution of the two constituents.', 'This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.', u'We used Fisher\u2019s exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.', 'It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.']
['system', 'ROUGE-S*', 'Average_R:', '0.00519', '(95%-conf.int.', '0.00519', '-', '0.00519)']
['system', 'ROUGE-S*', 'Average_P:', '0.02899', '(95%-conf.int.', '0.02899', '-', '0.02899)']
['system', 'ROUGE-S*', 'Average_F:', '0.00881', '(95%-conf.int.', '0.00881', '-', '0.00881)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:276', 'F:8']
['The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.']
['This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.', 'We present a general framework for vector-based composition which allows us to consider different classes of models.', 'In the following we describe our data collection procedure and give details on how our composition models were constructed and evaluated.', 'Sentence pairs were presented serially in random order.', 'The stimuli were administered to four separate groups; each group saw one set of 100 sentences.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:21', 'F:0']
0.0875037489062 0.00975749987803 0.0166862497914





input/ref/Task1/P05-1013_swastika.csv
input/res/Task1/P05-1013.csv
parsing: input/ref/Task1/P05-1013_swastika.csv
<S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



['49']
49
['49']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="6">In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
<S sid="38" ssid="9">Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="6">In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
Error in Reference Offset
S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
Error in Reference Offset
    <S sid="86" ssid="13">As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
 <S sid="51" ssid="22">In the second scheme, Head+Path, we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p&#8595;.</S>
original cit marker offset is 0
new cit marker offset is 0



['51']
51
['51']
parsed_discourse_facet ['method_citation']
<S sid="99" ssid="10">This may seem surprising, given the experiments reported in section 4, but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift, where the encoding of path information is often redundant.</S>
original cit marker offset is 0
new cit marker offset is 0



['99']
99
['99']
parsed_discourse_facet ['result_citation']
<S sid="7" ssid="3">From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



['2']
2
['2']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/P05-1013.csv
<S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S><S sid ="53" ssid = "24">In the third and final scheme  denoted Path  we keep the extra infor2Note that this is a baseline for the parsing experiment only (Experiment 2).</S><S sid ="37" ssid = "8">Here we use a slightly different notion of lift  applying to individual arcs and moving their head upwards one step at a time: Intuitively  lifting an arc makes the word wk dependent on the head wi of its original head wj (which is unique in a well-formed dependency graph)  unless wj is a root in which case the operation is undefined (but then wj * wk is necessarily projective if the dependency graph is well-formed).</S><S sid ="83" ssid = "10">It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'53'", "'37'", "'83'", "'75'"]
'51'
'53'
'37'
'83'
'75'
['51', '53', '37', '83', '75']
parsed_discourse_facet ['implication_citation']
<S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S><S sid ="38" ssid = "9">Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'38'", "'78'", "'34'", "'51'"]
'75'
'38'
'78'
'34'
'51'
['75', '38', '78', '34', '51']
parsed_discourse_facet ['implication_citation']
<S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S><S sid ="23" ssid = "19">By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S><S sid ="110" ssid = "2">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="25" ssid = "21">The rest of the paper is structured as follows.</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'70'", "'23'", "'110'", "'25'", "'11'"]
'70'
'23'
'110'
'25'
'11'
['70', '23', '110', '25', '11']
parsed_discourse_facet ['results_citation']
<S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="77" ssid = "4">The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'24'", "'95'", "'77'", "'11'"]
'34'
'24'
'95'
'77'
'11'
['34', '24', '95', '77', '11']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "14">Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.</S><S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="18" ssid = "14">In addition  there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington  1990; Kahane et al.  1998; Duchier and Debusmann  2001; Holan et al.  2001; Hellwig  2003).</S><S sid ="103" ssid = "14">On the other hand  given that all schemes have similar parsing accuracy overall  this means that the Path scheme is the least likely to introduce errors on projective arcs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'65'", "'14'", "'18'", "'103'"]
'43'
'65'
'14'
'18'
'103'
['43', '65', '14', '18', '103']
parsed_discourse_facet ['method_citation']
<S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S><S sid ="42" ssid = "13">Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="55" ssid = "26">As can be seen from the last column in Table 1  both Head and Head+Path may theoretically lead to a quadratic increase in the number of distinct arc labels (Head+Path being worse than Head only by a constant factor)  while the increase is only linear in the case of Path.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'42'", "'14'", "'55'", "'104'"]
'75'
'42'
'14'
'55'
'104'
['75', '42', '14', '55', '104']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="107" ssid = "18">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="66" ssid = "5">The choice between different actions is in general nondeterministic  and the parser relies on a memorybased classifier  trained on treebank data  to predict the next action based on features of the current parser configuration.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'107'", "'14'", "'95'", "'66'"]
'11'
'107'
'14'
'95'
'66'
['11', '107', '14', '95', '66']
parsed_discourse_facet ['method_citation']
<S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S><S sid ="12" ssid = "8">Prague Dependency Treebank (Hajic et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'70'", "'81'", "'12'", "'78'", "'89'"]
'70'
'81'
'12'
'78'
'89'
['70', '81', '12', '78', '89']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Prague Dependency Treebank (Hajic et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S><S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="5" ssid = "1">It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order  where discontinuous syntactic constructions are more common than in languages like English (Melcuk  1988; Covington  1990).</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S><S sid ="66" ssid = "5">The choice between different actions is in general nondeterministic  and the parser relies on a memorybased classifier  trained on treebank data  to predict the next action based on features of the current parser configuration.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'65'", "'5'", "'71'", "'66'"]
'12'
'65'
'5'
'71'
'66'
['12', '65', '5', '71', '66']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S><S sid ="82" ssid = "9">The last four columns in Table 3 show the distribution of nonprojective arcs with respect to the number of lifts required.</S><S sid ="86" ssid = "13">As expected  the most informative encoding  Head+Path  gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S><S sid ="84" ssid = "11">In the second part of the experiment  we applied the inverse transformation based on breadth-first search under the three different encoding schemes.</S><S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'82'", "'86'", "'84'", "'70'"]
'51'
'82'
'86'
'84'
'70'
['51', '82', '86', '84', '70']
parsed_discourse_facet ['implication_citation']
<S sid ="85" ssid = "12">The results are given in Table 4.</S><S sid ="33" ssid = "4">If (wi  r  wj) E A  we say that wi is the head of wj and wj a dependent of wi.</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="43" ssid = "14">Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.</S><S sid ="44" ssid = "15">Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'", "'33'", "'60'", "'43'", "'44'"]
'85'
'33'
'60'
'43'
'44'
['85', '33', '60', '43', '44']
parsed_discourse_facet ['method_citation']
<S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="25" ssid = "21">The rest of the paper is structured as follows.</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'14'", "'25'", "'60'", "'75'"]
'65'
'14'
'25'
'60'
'75'
['65', '14', '25', '60', '75']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="26" ssid = "22">In section 2 we introduce the graph transformation techniques used to projectivize and deprojectivize dependency graphs  and in section 3 we describe the data-driven dependency parser that is the core of our system.</S><S sid ="61" ssid = "32">Before we turn to the evaluation  however  we need to introduce the data-driven dependency parser used in the latter experiments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'104'", "'21'", "'26'", "'61'"]
'11'
'104'
'21'
'26'
'61'
['11', '104', '21', '26', '61']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="43" ssid = "14">Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S><S sid ="63" ssid = "2">The parser builds dependency graphs by traversing the input from left to right  using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'11'", "'43'", "'81'", "'63'"]
'34'
'11'
'43'
'81'
'63'
['34', '11', '43', '81', '63']
parsed_discourse_facet ['results_citation']
<S sid ="86" ssid = "13">As expected  the most informative encoding  Head+Path  gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S><S sid ="57" ssid = "28">In approaching this problem  a variety of different methods are conceivable  including a more or less sophisticated use of machine learning.</S><S sid ="73" ssid = "12">More details on the memory-based prediction can be found in Nivre et al. (2004) and Nivre and Scholz (2004).</S><S sid ="61" ssid = "32">Before we turn to the evaluation  however  we need to introduce the data-driven dependency parser used in the latter experiments.</S><S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'57'", "'73'", "'61'", "'34'"]
'86'
'57'
'73'
'61'
'34'
['86', '57', '73', '61', '34']
parsed_discourse_facet ['implication_citation']
<S sid ="83" ssid = "10">It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="101" ssid = "12">However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.</S><S sid ="25" ssid = "21">The rest of the paper is structured as follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'106'", "'78'", "'101'", "'25'"]
'83'
'106'
'78'
'101'
'25'
['83', '106', '78', '101', '25']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="77" ssid = "4">The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'24'", "'95'", "'77'", "'11'"]
'34'
'24'
'95'
'77'
'11'
['34', '24', '95', '77', '11']
parsed_discourse_facet ['results_citation']
<S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="92" ssid = "3">Evaluation metrics used are Attachment Score (AS)  i.e. the proportion of tokens that are attached to the correct head  and Exact Match (EM)  i.e. the proportion of sentences for which the dependency graph exactly matches the gold standard.</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'65'", "'92'", "'60'", "'71'"]
'95'
'65'
'92'
'60'
'71'
['95', '65', '92', '60', '71']
parsed_discourse_facet ['aim_citation']
<S sid ="67" ssid = "6">Table 2 shows the features used in the current version of the parser.</S><S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S><S sid ="82" ssid = "9">The last four columns in Table 3 show the distribution of nonprojective arcs with respect to the number of lifts required.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'70'", "'82'", "'14'", "'81'"]
'67'
'70'
'82'
'14'
'81'
['67', '70', '82', '14', '81']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S><S sid ="63" ssid = "2">The parser builds dependency graphs by traversing the input from left to right  using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'11'", "'34'", "'75'", "'63'"]
'51'
'11'
'34'
'75'
'63'
['51', '11', '34', '75', '63']
parsed_discourse_facet ['aim_citation']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['The choice between different actions is in general nondeterministic  and the parser relies on a memorybased classifier  trained on treebank data  to predict the next action based on features of the current parser configuration.', u'Prague Dependency Treebank (Haji\u02c7c et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.', u'It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order  where discontinuous syntactic constructions are more common than in languages like English (Mel\u2019\u02c7cuk  1988; Covington  1990).', 'More details on the parsing algorithm can be found in Nivre (2003).', 'For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.']
['system', 'ROUGE-S*', 'Average_R:', '0.00261', '(95%-conf.int.', '0.00261', '-', '0.00261)']
['system', 'ROUGE-S*', 'Average_P:', '0.09524', '(95%-conf.int.', '0.09524', '-', '0.09524)']
['system', 'ROUGE-S*', 'Average_F:', '0.00509', '(95%-conf.int.', '0.00509', '-', '0.00509)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3828', 'P:105', 'F:10']
['In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.']
['We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).', 'The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).', 'This is in contrast to dependency treebanks  e.g.', 'The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.', u'In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi \u2014*\u2217 wj for the reflexive and transitive closure of the (unlabeled) arc relation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00044', '(95%-conf.int.', '0.00044', '-', '0.00044)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00086', '(95%-conf.int.', '0.00086', '-', '0.00086)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:45', 'F:1']
['In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.']
[u'Prague Dependency Treebank (Haji\xcb\u2021c et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.', 'However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.', 'The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.', 'The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.', 'Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.']
['system', 'ROUGE-S*', 'Average_R:', '0.00687', '(95%-conf.int.', '0.00687', '-', '0.00687)']
['system', 'ROUGE-S*', 'Average_P:', '0.28889', '(95%-conf.int.', '0.28889', '-', '0.28889)']
['system', 'ROUGE-S*', 'Average_F:', '0.01343', '(95%-conf.int.', '0.01343', '-', '0.01343)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:45', 'F:13']
['From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.']
['In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.', 'Evaluation metrics used are Attachment Score (AS)  i.e. the proportion of tokens that are attached to the correct head  and Exact Match (EM)  i.e. the proportion of sentences for which the dependency graph exactly matches the gold standard.', 'The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.', 'More details on the parsing algorithm can be found in Nivre (2003).', 'For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.']
['system', 'ROUGE-S*', 'Average_R:', '0.00190', '(95%-conf.int.', '0.00190', '-', '0.00190)']
['system', 'ROUGE-S*', 'Average_P:', '0.03676', '(95%-conf.int.', '0.03676', '-', '0.03676)']
['system', 'ROUGE-S*', 'Average_F:', '0.00362', '(95%-conf.int.', '0.00362', '-', '0.00362)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:136', 'F:5']
no Reference Text in gold P05-1013
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.']
['This is in contrast to dependency treebanks  e.g.', u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'The parser builds dependency graphs by traversing the input from left to right  using a stack to store tokens that are not yet complete with respect to their dependents.', u'In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p\u2193.', u'In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi \u2014*\u2217 wj for the reflexive and transitive closure of the (unlabeled) arc relation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00136', '(95%-conf.int.', '0.00136', '-', '0.00136)']
['system', 'ROUGE-S*', 'Average_P:', '0.02857', '(95%-conf.int.', '0.02857', '-', '0.02857)']
['system', 'ROUGE-S*', 'Average_F:', '0.00259', '(95%-conf.int.', '0.00259', '-', '0.00259)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:105', 'F:3']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['In addition  there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington  1990; Kahane et al.  1998; Duchier and Debusmann  2001; Holan et al.  2001; Hellwig  2003).', 'On the other hand  given that all schemes have similar parsing accuracy overall  this means that the Path scheme is the least likely to introduce errors on projective arcs.', 'Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.', u'While the proportion of sentences containing non-projective dependencies is often 15\u201325%  the total proportion of non-projective arcs is normally only 1\u20132%.', 'More details on the parsing algorithm can be found in Nivre (2003).']
['system', 'ROUGE-S*', 'Average_R:', '0.00820', '(95%-conf.int.', '0.00820', '-', '0.00820)']
['system', 'ROUGE-S*', 'Average_P:', '0.14286', '(95%-conf.int.', '0.14286', '-', '0.14286)']
['system', 'ROUGE-S*', 'Average_F:', '0.01550', '(95%-conf.int.', '0.01550', '-', '0.01550)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:105', 'F:15']
['The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.']
['This is in contrast to dependency treebanks  e.g.', 'The rest of the paper is structured as follows.', 'By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.', 'The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.', 'Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.']
['system', 'ROUGE-S*', 'Average_R:', '0.00665', '(95%-conf.int.', '0.00665', '-', '0.00665)']
['system', 'ROUGE-S*', 'Average_P:', '0.12500', '(95%-conf.int.', '0.12500', '-', '0.12500)']
['system', 'ROUGE-S*', 'Average_F:', '0.01263', '(95%-conf.int.', '0.01263', '-', '0.01263)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:136', 'F:17']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
[u'In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi \u2014*\u2217 wj for the reflexive and transitive closure of the (unlabeled) arc relation.', u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.', u'In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p\u2193.', 'The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.']
['system', 'ROUGE-S*', 'Average_R:', '0.00140', '(95%-conf.int.', '0.00140', '-', '0.00140)']
['system', 'ROUGE-S*', 'Average_P:', '0.03810', '(95%-conf.int.', '0.03810', '-', '0.03810)']
['system', 'ROUGE-S*', 'Average_F:', '0.00271', '(95%-conf.int.', '0.00271', '-', '0.00271)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:105', 'F:4']
['In the second scheme, Head+Path, we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p&#8595;.']
['It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.', 'However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.', 'However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).', 'The rest of the paper is structured as follows.', 'The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.']
['system', 'ROUGE-S*', 'Average_R:', '0.00050', '(95%-conf.int.', '0.00050', '-', '0.00050)']
['system', 'ROUGE-S*', 'Average_P:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00094', '(95%-conf.int.', '0.00094', '-', '0.00094)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:120', 'F:1']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['This is in contrast to dependency treebanks  e.g.', 'The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.', 'Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.', 'The choice between different actions is in general nondeterministic  and the parser relies on a memorybased classifier  trained on treebank data  to predict the next action based on features of the current parser configuration.', u'While the proportion of sentences containing non-projective dependencies is often 15\u201325%  the total proportion of non-projective arcs is normally only 1\u20132%.']
['system', 'ROUGE-S*', 'Average_R:', '0.00974', '(95%-conf.int.', '0.00974', '-', '0.00974)']
['system', 'ROUGE-S*', 'Average_P:', '0.28571', '(95%-conf.int.', '0.28571', '-', '0.28571)']
['system', 'ROUGE-S*', 'Average_F:', '0.01883', '(95%-conf.int.', '0.01883', '-', '0.01883)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:105', 'F:30']
['The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.']
[u'Here we use a slightly different notion of lift  applying to individual arcs and moving their head upwards one step at a time: Intuitively  lifting an arc makes the word wk dependent on the head wi of its original head wj (which is unique in a well-formed dependency graph)  unless wj is a root in which case the operation is undefined (but then wj \u2014* wk is necessarily projective if the dependency graph is well-formed).', u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', u'In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p\u2193.', 'In the third and final scheme  denoted Path  we keep the extra infor2Note that this is a baseline for the parsing experiment only (Experiment 2).', 'It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.']
['system', 'ROUGE-S*', 'Average_R:', '0.03048', '(95%-conf.int.', '0.03048', '-', '0.03048)']
['system', 'ROUGE-S*', 'Average_P:', '0.21188', '(95%-conf.int.', '0.21188', '-', '0.21188)']
['system', 'ROUGE-S*', 'Average_F:', '0.05329', '(95%-conf.int.', '0.05329', '-', '0.05329)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5151', 'P:741', 'F:157']
['This may seem surprising, given the experiments reported in section 4, but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift, where the encoding of path information is often redundant.']
['We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).', 'The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).', 'This is in contrast to dependency treebanks  e.g.', 'The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.', u'In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi \u2014*\u2217 wj for the reflexive and transitive closure of the (unlabeled) arc relation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00044', '(95%-conf.int.', '0.00044', '-', '0.00044)']
['system', 'ROUGE-S*', 'Average_P:', '0.00654', '(95%-conf.int.', '0.00654', '-', '0.00654)']
['system', 'ROUGE-S*', 'Average_F:', '0.00082', '(95%-conf.int.', '0.00082', '-', '0.00082)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:153', 'F:1']
['Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.']
['As can be seen from the last column in Table 1  both Head and Head+Path may theoretically lead to a quadratic increase in the number of distinct arc labels (Head+Path being worse than Head only by a constant factor)  while the increase is only linear in the case of Path.', u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.', u'While the proportion of sentences containing non-projective dependencies is often 15\u201325%  the total proportion of non-projective arcs is normally only 1\u20132%.', 'The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.00144', '(95%-conf.int.', '0.00144', '-', '0.00144)']
['system', 'ROUGE-S*', 'Average_P:', '0.08889', '(95%-conf.int.', '0.08889', '-', '0.08889)']
['system', 'ROUGE-S*', 'Average_F:', '0.00284', '(95%-conf.int.', '0.00284', '-', '0.00284)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:45', 'F:4']
no Reference Text in gold P05-1013
0.10607615303 0.00554076918815 0.0102423076135





input/ref/Task1/A00-2018_vardha.csv
input/res/Task1/A00-2018.csv
parsing: input/ref/Task1/A00-2018_vardha.csv
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'91'"]
'91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="8">In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'"]
'39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="162" ssid="53">Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'"]
'162'
['162']
parsed_discourse_facet ['method_citation']
 <S sid="87" ssid="56">In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17].</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'"]
'87'
['87']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'91'"]
'91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="176" ssid="3">That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'"]
'176'
['176']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['method_citation']
 <S sid="101" ssid="12">In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning).</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'"]
'101'
['101']
parsed_discourse_facet ['method_citation']
<S sid="155" ssid="46">For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
  <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="162" ssid="53">Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'"]
'162'
['162']
parsed_discourse_facet ['method_citation']
 <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
 <S sid="79" ssid="48">We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
    <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
    <S sid="175" ssid="2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["'175'"]
'175'
['175']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2018.csv
<S sid ="114" ssid = "5">That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.</S><S sid ="159" ssid = "50">Something very much like this is done in [15].</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="165" ssid = "56">Note that we also tried including this information using a standard deleted-interpolation model.</S><S sid ="163" ssid = "54">Next we add the less obvious conditioning events noted in our previous discussion of the final model  grandparent label lg and left sibling label /b.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'159'", "'2'", "'165'", "'163'"]
'114'
'159'
'2'
'165'
'163'
['114', '159', '2', '165', '163']
parsed_discourse_facet ['implication_citation']
<S sid ="114" ssid = "5">That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.</S><S sid ="3" ssid = "3">The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.</S><S sid ="99" ssid = "10">Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S><S sid ="129" ssid = "20">It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments)  it does not guess the pre-terminal before guessing the lexical head  and it uses a tree-bank grammar rather than a Markov grammar.</S><S sid ="92" ssid = "3">For runs with the generative model based upon Markov grammar statistics  the first pass uses the same statistics  but conditioned only on standard PCFG information.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'3'", "'99'", "'129'", "'92'"]
'114'
'3'
'99'
'129'
'92'
['114', '3', '99', '129', '92']
parsed_discourse_facet ['implication_citation']
<S sid ="132" ssid = "23">Between the Old model and the Best model  Figure 2 gives precision/recall measurements for several different versions of our parser.</S><S sid ="69" ssid = "38">For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.</S><S sid ="184" ssid = "11">The first is the slight  but important  improvement achieved by using this model over conventional deleted interpolation  as indicated in Figure 2.</S><S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="85" ssid = "54">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models  this simplifies the model significantly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'132'", "'69'", "'184'", "'157'", "'85'"]
'132'
'69'
'184'
'157'
'85'
['132', '69', '184', '157', '85']
parsed_discourse_facet ['results_citation']
<S sid ="137" ssid = "28">However  Collins in [10] does not stress the decision to guess the head's pre-terminal first  and it might be lost on the casual reader.</S><S sid ="51" ssid = "20">Second  and this is a point we have not yet mentioned  the features used in these models need have no particular independence of one another.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="58" ssid = "27">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid ="150" ssid = "41">The second modification is the explicit marking of noun and verb-phrase coordination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'51'", "'2'", "'58'", "'150'"]
'137'
'51'
'2'
'58'
'150'
['137', '51', '2', '58', '150']
parsed_discourse_facet ['method_citation']
<S sid ="49" ssid = "18">First  as already implicit in our discussion  factoring the probability computation into a sequence of values corresponding to various &quot;features&quot; suggests that the probability model should be easily changeable  just change the set of features used.</S><S sid ="4" ssid = "4">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c  information outside c that our probability model deems important in determining the probability in question.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="165" ssid = "56">Note that we also tried including this information using a standard deleted-interpolation model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'4'", "'13'", "'2'", "'165'"]
'49'
'4'
'13'
'2'
'165'
['49', '4', '13', '2', '165']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "3">Also  remember that H is a pla ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.</S><S sid ="11" ssid = "7">What fundamentally distinguishes probabilistic generative parsers is how they compute p(r)  and it is to that topic we turn next.</S><S sid ="96" ssid = "7">As noted above  the probability model uses five smoothed probability distributions  one each for Li  M Ri t  and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7.</S><S sid ="99" ssid = "10">Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S><S sid ="19" ssid = "8">The method we use follows that of [10].</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'11'", "'96'", "'99'", "'19'"]
'34'
'11'
'96'
'99'
'19'
['34', '11', '96', '99', '19']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.</S><S sid ="185" ssid = "12">We expect that as we experiment with other  more semantic conditioning information  the importance of this aspect of the model will increase.</S><S sid ="80" ssid = "49">(Our experience is that rather than requiring 50 or so iterations  three suffice.)</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="144" ssid = "35">This quantity is a relatively intuitive one (as  for example  it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it  in effect  as the unsmoothed probability upon which all smoothing of p(h) is based.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'185'", "'80'", "'54'", "'144'"]
'3'
'185'
'80'
'54'
'144'
['3', '185', '80', '54', '144']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "16">The intuitive idea is that each factor gi is larger than one if the feature in question makes the probability more likely  one if the feature has no effect  and smaller than one if it makes the probability less likely.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S><S sid ="85" ssid = "54">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models  this simplifies the model significantly.</S><S sid ="22" ssid = "11">For us the non-terminal symbols are those of the tree-bank  augmented by the symbols aux and auxg  which have been assigned deterministically to certain auxiliary verbs such as &quot;have&quot; or &quot;having&quot;.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'2'", "'41'", "'85'", "'22'"]
'47'
'2'
'41'
'85'
'22'
['47', '2', '41', '85', '22']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="55" ssid = "24">This method is known as &quot;deleted interpolation&quot; smoothing.</S><S sid ="66" ssid = "35">In many cases this is clearly warranted.</S><S sid ="4" ssid = "4">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid ="121" ssid = "12">Without these enhancements Char97 performs at the 86.6% level for sentences of length < 40.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'55'", "'66'", "'4'", "'121'"]
'157'
'55'
'66'
'4'
'121'
['157', '55', '66', '4', '121']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="0" ssid = "0">A Maximum-Entropy-Inspired Parser *</S><S sid ="37" ssid = "6">Rather  we concentrate on the aspects of these models that most directly influenced the model presented here.</S><S sid ="21" ssid = "10">(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'157'", "'0'", "'37'", "'21'"]
'2'
'157'
'0'
'37'
'21'
['2', '157', '0', '37', '21']
parsed_discourse_facet ['implication_citation']
<S sid ="48" ssid = "17">Maximum-entropy models have two benefits for a parser builder.</S><S sid ="21" ssid = "10">(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="177" ssid = "4">The results reported here disprove this conjecture.</S><S sid ="159" ssid = "50">Something very much like this is done in [15].</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'21'", "'54'", "'177'", "'159'"]
'48'
'21'
'54'
'177'
'159'
['48', '21', '54', '177', '159']
parsed_discourse_facet ['method_citation']
<S sid ="35" ssid = "4">In the past few years the maximum entropy  or log-linear  approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing [1 17].</S><S sid ="121" ssid = "12">Without these enhancements Char97 performs at the 86.6% level for sentences of length < 40.</S><S sid ="69" ssid = "38">For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.</S><S sid ="138" ssid = "29">Indeed  it was lost on the present author until he went back after the fact and found it there.</S><S sid ="114" ssid = "5">That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'", "'121'", "'69'", "'138'", "'114'"]
'35'
'121'
'69'
'138'
'114'
['35', '121', '69', '138', '114']
parsed_discourse_facet ['method_citation']
<S sid ="169" ssid = "60">Up to this point all the models considered in this section are tree-bank grammar models.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="134" ssid = "25">As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.</S><S sid ="99" ssid = "10">Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S><S sid ="155" ssid = "46">For example  in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'", "'125'", "'134'", "'99'", "'155'"]
'169'
'125'
'134'
'99'
'155'
['169', '125', '134', '99', '155']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid ="34" ssid = "3">Also  remember that H is a pla ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.</S><S sid ="165" ssid = "56">Note that we also tried including this information using a standard deleted-interpolation model.</S><S sid ="69" ssid = "38">For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.</S><S sid ="0" ssid = "0">A Maximum-Entropy-Inspired Parser *</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'34'", "'165'", "'69'", "'0'"]
'7'
'34'
'165'
'69'
'0'
['7', '34', '165', '69', '0']
parsed_discourse_facet ['results_citation']
<S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S><S sid ="37" ssid = "6">Rather  we concentrate on the aspects of these models that most directly influenced the model presented here.</S><S sid ="169" ssid = "60">Up to this point all the models considered in this section are tree-bank grammar models.</S><S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'", "'37'", "'169'", "'7'", "'125'"]
'41'
'37'
'169'
'7'
'125'
['41', '37', '169', '7', '125']
parsed_discourse_facet ['implication_citation']
<S sid ="88" ssid = "57">While we could have smoothed in the same fashion  we choose instead to use standard deleted interpolation.</S><S sid ="124" ssid = "15">We note here that this corpus is somewhat more difficult than the &quot;official&quot; test corpus.</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="21" ssid = "10">(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)</S><S sid ="63" ssid = "32">As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'124'", "'54'", "'21'", "'63'"]
'88'
'124'
'54'
'21'
'63'
['88', '124', '54', '21', '63']
parsed_discourse_facet ['method_citation']
['We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.']
['Something very much like this is done in [15].', 'That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.', u'Next we add the less obvious conditioning events noted in our previous discussion of the final model \xe2\u20ac\u201d grandparent label lg and left sibling label /b.', 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].', 'Note that we also tried including this information using a standard deleted-interpolation model.']
['system', 'ROUGE-S*', 'Average_R:', '0.03987', '(95%-conf.int.', '0.03987', '-', '0.03987)']
['system', 'ROUGE-S*', 'Average_P:', '0.03805', '(95%-conf.int.', '0.03805', '-', '0.03805)']
['system', 'ROUGE-S*', 'Average_F:', '0.03894', '(95%-conf.int.', '0.03894', '-', '0.03894)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:946', 'F:36']
['In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.']
['Second  and this is a point we have not yet mentioned  the features used in these models need have no particular independence of one another.', 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].', "However  Collins in [10] does not stress the decision to guess the head's pre-terminal first  and it might be lost on the casual reader.", 'Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.', 'The second modification is the explicit marking of noun and verb-phrase coordination.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:55', 'F:0']
["In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17]."]
['This quantity is a relatively intuitive one (as  for example  it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it  in effect  as the unsmoothed probability upon which all smoothing of p(h) is based.', "The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).", 'The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.', '(Our experience is that rather than requiring 50 or so iterations  three suffice.)', 'We expect that as we experiment with other  more semantic conditioning information  the importance of this aspect of the model will increase.']
['system', 'ROUGE-S*', 'Average_R:', '0.00235', '(95%-conf.int.', '0.00235', '-', '0.00235)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00451', '(95%-conf.int.', '0.00451', '-', '0.00451)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:55', 'F:3']
['In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.']
['For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.', 'Rather  we concentrate on the aspects of these models that most directly influenced the model presented here.', 'Up to this point all the models considered in this section are tree-bank grammar models.', 'Following [5 10]  our parser is based upon a probabilistic generative model.', "For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise."]
['system', 'ROUGE-S*', 'Average_R:', '0.00067', '(95%-conf.int.', '0.00067', '-', '0.00067)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00131', '(95%-conf.int.', '0.00131', '-', '0.00131)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:45', 'F:1']
['Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.']
['What fundamentally distinguishes probabilistic generative parsers is how they compute p(r)  and it is to that topic we turn next.', 'Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.', 'As noted above  the probability model uses five smoothed probability distributions  one each for Li  M Ri t  and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7.', 'The method we use follows that of [10].', 'Also  remember that H is a pla ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:28', 'F:0']
['As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.']
['Between the Old model and the Best model  Figure 2 gives precision/recall measurements for several different versions of our parser.', 'As partition-function calculation is typically the major on-line computational problem for maximum-entropy models  this simplifies the model significantly.', 'Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.', 'The first is the slight  but important  improvement achieved by using this model over conventional deleted interpolation  as indicated in Figure 2.', "For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label."]
['system', 'ROUGE-S*', 'Average_R:', '0.01131', '(95%-conf.int.', '0.01131', '-', '0.01131)']
['system', 'ROUGE-S*', 'Average_P:', '0.06494', '(95%-conf.int.', '0.06494', '-', '0.06494)']
['system', 'ROUGE-S*', 'Average_F:', '0.01927', '(95%-conf.int.', '0.01927', '-', '0.01927)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:231', 'F:15']
['That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.']
['This method is known as &quot;deleted interpolation&quot; smoothing.', 'Without these enhancements Char97 performs at the 86.6% level for sentences of length ', 'In many cases this is clearly warranted.', 'Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.', "We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head."]
['system', 'ROUGE-S*', 'Average_R:', '0.00106', '(95%-conf.int.', '0.00106', '-', '0.00106)']
['system', 'ROUGE-S*', 'Average_P:', '0.00476', '(95%-conf.int.', '0.00476', '-', '0.00476)']
['system', 'ROUGE-S*', 'Average_F:', '0.00173', '(95%-conf.int.', '0.00173', '-', '0.00173)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:210', 'F:1']
['We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.']
['Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.', 'That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.', 'The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.', 'For runs with the generative model based upon Markov grammar statistics  the first pass uses the same statistics  but conditioned only on standard PCFG information.', 'It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments)  it does not guess the pre-terminal before guessing the lexical head  and it uses a tree-bank grammar rather than a Markov grammar.']
['system', 'ROUGE-S*', 'Average_R:', '0.03154', '(95%-conf.int.', '0.03154', '-', '0.03154)']
['system', 'ROUGE-S*', 'Average_P:', '0.07822', '(95%-conf.int.', '0.07822', '-', '0.07822)']
['system', 'ROUGE-S*', 'Average_F:', '0.04496', '(95%-conf.int.', '0.04496', '-', '0.04496)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:946', 'F:74']
['This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].']
["The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).", 'While we could have smoothed in the same fashion  we choose instead to use standard deleted interpolation.', "As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).", 'We note here that this corpus is somewhat more difficult than the &quot;official&quot; test corpus.', '(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:78', 'F:0']
['Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.']
['Without these enhancements Char97 performs at the 86.6% level for sentences of length ', 'In the past few years the maximum entropy  or log-linear  approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing [1 17].']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:276', 'P:28', 'F:0']
['We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.']
['Note that we also tried including this information using a standard deleted-interpolation model.', "For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.", 'A Maximum-Entropy-Inspired Parser *', 'Following [5 10]  our parser is based upon a probabilistic generative model.', 'Also  remember that H is a pla ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.']
['system', 'ROUGE-S*', 'Average_R:', '0.00168', '(95%-conf.int.', '0.00168', '-', '0.00168)']
['system', 'ROUGE-S*', 'Average_P:', '0.00476', '(95%-conf.int.', '0.00476', '-', '0.00476)']
['system', 'ROUGE-S*', 'Average_F:', '0.00248', '(95%-conf.int.', '0.00248', '-', '0.00248)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:210', 'F:1']
0.0243181815971 0.00804363629051 0.0102909089974





input/ref/Task1/P11-1060_swastika.csv
input/res/Task1/P11-1060.csv
parsing: input/ref/Task1/P11-1060_swastika.csv
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['aim_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['aim_citation']
    <S sid="148" ssid="33">This bootstrapping behavior occurs naturally: The &#8220;easy&#8221; examples are processed first, where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
    <S sid="13" ssid="9">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['result_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="47" ssid="23">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
S sid="44" ssid="20">The recurrence is as follows: At each node, we compute the set of tuples v consistent with the predicate at that node (v &#8712; w(p)), and S(x)}, where a set of pairs S is treated as a set-valued function S(x) = {y : (x, y) &#8712; S} with domain S1 = {x : (x, y) &#8712; S}.</S>
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
Error in Reference Offset
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



['106']
106
['106']
parsed_discourse_facet ['aim_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
    <S sid="172" ssid="57">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms, we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus, and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S>
original cit marker offset is 0
new cit marker offset is 0



['172']
172
['172']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



['171']
171
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S sid ="57" ssid = "33">But consider Figure 4: (a) is headed by borders  but states needs to be extracted; in (b)  the quantifier no is syntactically dominated by the head verb borders but needs to take wider scope.</S><S sid ="39" ssid = "15">Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.</S><S sid ="60" ssid = "36">Then higher up in the tree  we invoke it with an execute relation Xi to create the desired semantic scope.2 This mark-execute construct acts non-locally  so to maintain compositionality  we must augment the denotation d = JzKw to include any information about the marked nodes in z that can be accessed by an execute relation later on.</S><S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="167" ssid = "52">In DCS  we start with lexical triggers  which are 6 Conclusion more basic than CCG lexical entries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'", "'39'", "'60'", "'22'", "'167'"]
'57'
'39'
'60'
'22'
'167'
['57', '39', '60', '22', '167']
parsed_discourse_facet ['implication_citation']
<S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="71" ssid = "47">Let z be a DCS tree.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="120" ssid = "5">GEO has 48 non-value predicates and JOBS has 26.</S><S sid ="85" ssid = "61">Extraction allows us to return the set of consistent values of a marked non-root node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'71'", "'141'", "'120'", "'85'"]
'15'
'71'
'141'
'120'
'85'
['15', '71', '141', '120', '85']
parsed_discourse_facet ['implication_citation']
<S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S><S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parents denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="39" ssid = "15">Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.</S><S sid ="147" ssid = "32">However  training on just these examples is enough to improve the parameters  and this 29% increases to 66% and then to 95% over the next few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'51'", "'3'", "'39'", "'147'"]
'46'
'51'
'3'
'39'
'147'
['46', '51', '3', '39', '147']
parsed_discourse_facet ['results_citation']
<S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parents denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid ="84" ssid = "60">There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.</S><S sid ="170" ssid = "55">Our features as soft preferences.</S><S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'84'", "'170'", "'46'", "'98'"]
'51'
'84'
'170'
'46'
'98'
['51', '84', '170', '46', '98']
parsed_discourse_facet ['method_citation']
<S sid ="122" ssid = "7">For JOBS  if we use the standard Jobs database  close to half the ys are empty  which makes it uninteresting.</S><S sid ="94" ssid = "70">We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.</S><S sid ="102" ssid = "78">As a running example  consider x = city that is in California and z = hcity; 11:hloc; 21:hCAiii  where city triggers city and California triggers CA.</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S><S sid ="95" ssid = "71">Our first question is: given notation of the DCS tree before execution is an utterance x  what trees z  Z are permissible?</S>
original cit marker offset is 0
new cit marker offset is 0



["'122'", "'94'", "'102'", "'98'", "'95'"]
'122'
'94'
'102'
'98'
'95'
['122', '94', '102', '98', '95']
parsed_discourse_facet ['method_citation']
<S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="84" ssid = "60">There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'", "'3'", "'6'", "'84'", "'162'"]
'22'
'3'
'6'
'84'
'162'
['22', '3', '6', '84', '162']
parsed_discourse_facet ['method_citation']
<S sid ="13" ssid = "9">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x  y)  which is much cheaper to obtain than (x  z) pairs.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="29" ssid = "5">Let P be a set of predicates (e.g.  state  count  P)  which are just symbols.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="172" ssid = "57">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms  we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus  and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'134'", "'29'", "'77'", "'172'"]
'13'
'134'
'29'
'77'
'172'
['13', '134', '29', '77', '172']
parsed_discourse_facet ['method_citation']
<S sid ="96" ssid = "72">To California cities)  and it also allows us to underspecify L. In particular  our L will not include verbs or prepositions; rather  we rely on the predicates corresponding to those words to be triggered by traces.</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S><S sid ="94" ssid = "70">We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.</S><S sid ="124" ssid = "9">For each data predicate p (e.g.  language)  we add each possible tuple (e.g.  (job37  Java)) to w(p) independently with probability 0.8.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S>
original cit marker offset is 0
new cit marker offset is 0



["'96'", "'88'", "'94'", "'124'", "'162'"]
'96'
'88'
'94'
'124'
'162'
['96', '88', '94', '124', '162']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "84">However  in order to learn  we need to sum over {z  ZL(x) : JzKw = y}  and unfortunately  the additional constraint JzKw = y does not factorize.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="84" ssid = "60">There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'3'", "'84'", "'134'", "'46'"]
'108'
'3'
'84'
'134'
'46'
['108', '3', '84', '134', '46']
parsed_discourse_facet ['method_citation']
<S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'", "'98'", "'77'", "'162'", "'88'"]
'22'
'98'
'77'
'162'
'88'
['22', '98', '77', '162', '88']
parsed_discourse_facet ['implication_citation']
<S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'134'", "'3'", "'141'", "'98'"]
'12'
'134'
'3'
'141'
'98'
['12', '134', '3', '141', '98']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="29" ssid = "5">Let P be a set of predicates (e.g.  state  count  P)  which are just symbols.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="129" ssid = "14">We also define an augmented lexicon L+ which includes a prototype word x for each predicate appearing in (iii) above (e.g.  (large  size))  which cancels the predicates triggered by xs POS tag.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'29'", "'77'", "'129'", "'8'"]
'3'
'29'
'77'
'129'
'8'
['3', '29', '77', '129', '8']
parsed_discourse_facet ['method_citation']
<S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S><S sid ="142" ssid = "27">This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.</S><S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="144" ssid = "29">Intuitions How is our system learning?</S><S sid ="112" ssid = "88">Our learning algorithm alternates between (i) using the current parameters  to generate the K-best set ZL (x) for each training example x  and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'142'", "'10'", "'144'", "'112'"]
'46'
'142'
'10'
'144'
'112'
['46', '142', '10', '144', '112']
parsed_discourse_facet ['method_citation']
<S sid ="133" ssid = "18">Table 2 shows that our system using lexical triggers L (henceforth  DCS) outperforms SEMRESP (78.9% over 73.2%).</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="163" ssid = "48">Feedback from the context; for example  the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x  y)  which means (Schuler  2003) and semantic parsing (Popescu et borders looks right for the first argument and left al.  2003; Clarke et al.  2010).</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'134'", "'163'", "'141'", "'10'"]
'133'
'134'
'163'
'141'
'10'
['133', '134', '163', '141', '10']
parsed_discourse_facet ['results_citation']
<S sid ="86" ssid = "62">Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  (i  )]{1 = }.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="113" ssid = "89">Formally  let O(  ') be the objective function O() with ZL(x) ZL I(x).</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'162'", "'77'", "'3'", "'113'"]
'86'
'162'
'77'
'3'
'113'
['86', '162', '77', '3', '113']
parsed_discourse_facet ['implication_citation']
<S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="55" ssid = "31">The tree structure still enables us to compute denotations efficiently based on (1) and (2).</S><S sid ="80" ssid = "56">The full definition of join is as follows: Aggregate The aggregate operation takes a denotation and forms a set out of the tuples in the first column for each setting of the rest of the columns: Now we turn to the mark (M) and execute (Xi) operations  which handles the divergence between syntactic and semantic scope.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'134'", "'3'", "'55'", "'80'"]
'10'
'134'
'3'
'55'
'80'
['10', '134', '3', '55', '80']
parsed_discourse_facet ['method_citation']
<S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="50" ssid = "26">For example  consider the phrase number of major cities  and suppose that number corresponds to the count predicate.</S><S sid ="113" ssid = "89">Formally  let O(  ') be the objective function O() with ZL(x) ZL I(x).</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'", "'134'", "'154'", "'50'", "'113'"]
'22'
'134'
'154'
'50'
'113'
['22', '134', '154', '50', '113']
parsed_discourse_facet ['results_citation']
<S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="126" ssid = "11">During development  we further held out a random 30% of the training sets for validation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'134'", "'3'", "'12'", "'126'"]
'6'
'134'
'3'
'12'
'126'
['6', '134', '3', '12', '126']
parsed_discourse_facet ['aim_citation']
<S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S><S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="172" ssid = "57">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms  we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus  and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'132'", "'9'", "'10'", "'172'", "'134'"]
'132'
'9'
'10'
'172'
'134'
['132', '9', '10', '172', '134']
parsed_discourse_facet ['method_citation']
['Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.']
['Table 2 shows that our system using lexical triggers L (henceforth  DCS) outperforms SEMRESP (78.9% over 73.2%).', 'However  we still model the logical form (now as a latent variable) to capture the complexities of language.', 'Feedback from the context; for example  the lexical entry for borders world has been used to guide both syntactic parsing is S\\NP/NP : Ay.Ax.border(x  y)  which means (Schuler  2003) and semantic parsing (Popescu et borders looks right for the first argument and left al.  2003; Clarke et al.  2010).', 'Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.', 'In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:300', 'F:0']
['The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['As a running example  consider x = city that is in California and z = hcity; 11:hloc; 21:hCAiii  where city triggers city and California triggers CA.', 'The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.', u'For JOBS  if we use the standard Jobs database  close to half the y\u2019s are empty  which makes it uninteresting.', u'Our first question is: given notation of the DCS tree before execution is an utterance x  what trees z \u2208 Z are permissible?', 'We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:105', 'F:0']
['Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.']
[u"Join The join of two denotations d and d' with respect to components j and j' (\xe2\u02c6\u2014 means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.", 'The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.', 'Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.', 'The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.', 'The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.']
['system', 'ROUGE-S*', 'Average_R:', '0.00606', '(95%-conf.int.', '0.00606', '-', '0.00606)']
['system', 'ROUGE-S*', 'Average_P:', '0.04333', '(95%-conf.int.', '0.04333', '-', '0.04333)']
['system', 'ROUGE-S*', 'Average_F:', '0.01063', '(95%-conf.int.', '0.01063', '-', '0.01063)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:300', 'F:13']
['This bootstrapping behavior occurs naturally: The &#8220;easy&#8221; examples are processed first, where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.']
['Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.', 'For each data predicate p (e.g.  language)  we add each possible tuple (e.g.  (job37  Java)) to w(p) independently with probability 0.8.', 'We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.', 'The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.', 'To California cities)  and it also allows us to underspecify L. In particular  our L will not include verbs or prepositions; rather  we rely on the predicates corresponding to those words to be triggered by traces.']
['system', 'ROUGE-S*', 'Average_R:', '0.00202', '(95%-conf.int.', '0.00202', '-', '0.00202)']
['system', 'ROUGE-S*', 'Average_P:', '0.01429', '(95%-conf.int.', '0.01429', '-', '0.01429)']
['system', 'ROUGE-S*', 'Average_F:', '0.00354', '(95%-conf.int.', '0.00354', '-', '0.00354)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:210', 'F:3']
['This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.']
['As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.', 'Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.', 'In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.', 'Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms  we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus  and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.', 'However  we still model the logical form (now as a latent variable) to capture the complexities of language.']
['system', 'ROUGE-S*', 'Average_R:', '0.00316', '(95%-conf.int.', '0.00316', '-', '0.00316)']
['system', 'ROUGE-S*', 'Average_P:', '0.05263', '(95%-conf.int.', '0.05263', '-', '0.05263)']
['system', 'ROUGE-S*', 'Average_F:', '0.00597', '(95%-conf.int.', '0.00597', '-', '0.00597)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:190', 'F:10']
['The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.', 'In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.', 'In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.', 'During development  we further held out a random 30% of the training sets for validation.', 'Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).']
['system', 'ROUGE-S*', 'Average_R:', '0.01098', '(95%-conf.int.', '0.01098', '-', '0.01098)']
['system', 'ROUGE-S*', 'Average_P:', '0.13333', '(95%-conf.int.', '0.13333', '-', '0.13333)']
['system', 'ROUGE-S*', 'Average_F:', '0.02029', '(95%-conf.int.', '0.02029', '-', '0.02029)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:105', 'F:14']
['We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs.']
[u"Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z \u2208 Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the child\u2019s denotation (t \u2208 JciKw).", u'However  in order to learn  we need to sum over {z \u2208 ZL(x) : JzKw = y}  and unfortunately  the additional constraint JzKw = y does not factorize.', 'In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.', 'There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.', 'In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00053', '(95%-conf.int.', '0.00053', '-', '0.00053)']
['system', 'ROUGE-S*', 'Average_P:', '0.01515', '(95%-conf.int.', '0.01515', '-', '0.01515)']
['system', 'ROUGE-S*', 'Average_F:', '0.00102', '(95%-conf.int.', '0.00102', '-', '0.00102)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:66', 'F:1']
['The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['The tree structure still enables us to compute denotations efficiently based on (1) and (2).', 'However  we still model the logical form (now as a latent variable) to capture the complexities of language.', 'In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.', 'The full definition of join is as follows: Aggregate The aggregate operation takes a denotation and forms a set out of the tuples in the first column for each setting of the rest of the columns: Now we turn to the mark (M) and execute (Xi) operations  which handles the divergence between syntactic and semantic scope.', 'In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00606', '(95%-conf.int.', '0.00606', '-', '0.00606)']
['system', 'ROUGE-S*', 'Average_P:', '0.12381', '(95%-conf.int.', '0.12381', '-', '0.12381)']
['system', 'ROUGE-S*', 'Average_F:', '0.01156', '(95%-conf.int.', '0.01156', '-', '0.01156)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:105', 'F:13']
['Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.']
['Then higher up in the tree  we invoke it with an execute relation Xi to create the desired semantic scope.2 This mark-execute construct acts non-locally  so to maintain compositionality  we must augment the denotation d = JzKw to include any information about the marked nodes in z that can be accessed by an execute relation later on.', 'But consider Figure 4: (a) is headed by borders  but states needs to be extracted; in (b)  the quantifier no is syntactically dominated by the head verb borders but needs to take wider scope.', 'In DCS  we start with lexical triggers  which are 6 Conclusion more basic than CCG lexical entries.', 'Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.', 'The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.']
['system', 'ROUGE-S*', 'Average_R:', '0.00133', '(95%-conf.int.', '0.00133', '-', '0.00133)']
['system', 'ROUGE-S*', 'Average_P:', '0.01333', '(95%-conf.int.', '0.01333', '-', '0.01333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00242', '(95%-conf.int.', '0.00242', '-', '0.00242)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:300', 'F:4']
['Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms, we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus, and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.']
['For example  consider the phrase number of major cities  and suppose that number corresponds to the count predicate.', u"Formally  let \u02dcO(\u03b8  \u03b8') be the objective function O(\u03b8) with ZL(x) \u02dcZL \u03b8I(x).", 'The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).', 'The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.', 'In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00075', '(95%-conf.int.', '0.00075', '-', '0.00075)']
['system', 'ROUGE-S*', 'Average_P:', '0.00159', '(95%-conf.int.', '0.00159', '-', '0.00159)']
['system', 'ROUGE-S*', 'Average_F:', '0.00102', '(95%-conf.int.', '0.00102', '-', '0.00102)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:630', 'F:1']
['We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.']
['In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.', 'The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.', 'There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.', 'The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.', 'Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).']
['system', 'ROUGE-S*', 'Average_R:', '0.00805', '(95%-conf.int.', '0.00805', '-', '0.00805)']
['system', 'ROUGE-S*', 'Average_P:', '0.11696', '(95%-conf.int.', '0.11696', '-', '0.11696)']
['system', 'ROUGE-S*', 'Average_F:', '0.01506', '(95%-conf.int.', '0.01506', '-', '0.01506)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:171', 'F:20']
0.0467654541203 0.00353999996782 0.00650090903181





input/ref/Task1/P08-1028_sweta.csv
input/res/Task1/P08-1028.csv
parsing: input/ref/Task1/P08-1028_sweta.csv
<S sid="65" ssid="13">So, if we assume that only the ith components of u and v contribute to the ith component of p, that these components are not dependent on i, and that the function is symmetric with regard to the interchange of u and v, we obtain a simpler instantiation of an additive model: Analogously, under the same assumptions, we obtain the following simpler multiplicative model: only the ith components of u and v contribute to the ith component of p. Another class of models can be derived by relaxing this constraint.</S>
original cit marker offset is 0
new cit marker offset is 0



["65'"]
65'
['65']
parsed_discourse_facet ['method_citation']
 <S sid="75" ssid="23">An extreme form of this differential in the contribution of constituents is where one of the vectors, say u, contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic, however it can serve as a simple baseline against which to compare more sophisticated models.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["42'"]
42'
['42']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="17">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="195" ssid="7">The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately.</S>
original cit marker offset is 0
new cit marker offset is 0



["195'"]
195'
['195']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["57'"]
57'
['57']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["190'"]
190'
['190']
parsed_discourse_facet ['method_citation']
 <S sid="29" ssid="2">While neural networks can readily represent single distinct objects, in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>
original cit marker offset is 0
new cit marker offset is 0



["29'"]
29'
['29']
parsed_discourse_facet ['method_citation']
    <S sid="38" ssid="11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="12">Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="163" ssid="76">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["163'"]
163'
['163']
parsed_discourse_facet ['method_citation']
<S sid="185" ssid="19">The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.</S>
original cit marker offset is 0
new cit marker offset is 0



["185'"]
185'
['185']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["190'"]
190'
['190']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1028.csv
<S sid ="200" ssid = "12">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid ="136" ssid = "49">We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine-grained semantic distinctions (see Table 1).</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants similarity judgments with those of the models.6 We used Spearmans p for our correlation analyses.</S><S sid ="135" ssid = "48">The average inter-subject agreement5 was  = 0.40.</S><S sid ="149" ssid = "62">Our composition models have no additional parameters beyond the semantic space just described  with three exceptions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'200'", "'136'", "'164'", "'135'", "'149'"]
'200'
'136'
'164'
'135'
'149'
['200', '136', '164', '135', '149']
parsed_discourse_facet ['implication_citation']
<S sid ="107" ssid = "20">Landmarks were taken from WordNet (Fellbaum  1998).</S><S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="45" ssid = "18">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid ="192" ssid = "4">We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'63'", "'42'", "'45'", "'192'"]
'107'
'63'
'42'
'45'
'192'
['107', '63', '42', '45', '192']
parsed_discourse_facet ['implication_citation']
<S sid ="30" ssid = "3">For the hierarchical structure of natural language this binding problem becomes particularly acute.</S><S sid ="78" ssid = "26">These vectors are not arbitrary and ideally they must exhibit some relation to the words of the construction under consideration.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants similarity judgments with those of the models.6 We used Spearmans p for our correlation analyses.</S><S sid ="171" ssid = "5">For comparison  we also show the human ratings for these items (UpperBound).</S><S sid ="176" ssid = "10">The multiplicative and combined models yield means closer to the human ratings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'78'", "'164'", "'171'", "'176'"]
'30'
'78'
'164'
'171'
'176'
['30', '78', '164', '171', '176']
parsed_discourse_facet ['results_citation']
<S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S><S sid ="85" ssid = "33">One potential drawback of multiplicative models is the effect of components with value zero.</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'", "'103'", "'85'", "'101'", "'63'"]
'42'
'103'
'85'
'101'
'63'
['42', '103', '85', '101', '63']
parsed_discourse_facet ['method_citation']
<S sid ="97" ssid = "10">Moreover by using a minimal structure we factor out inessential degrees of freedom and are able to assess the merits of different models on an equal footing.</S><S sid ="126" ssid = "39">49 unpaid volunteers completed the experiment  all native speakers of English.</S><S sid ="192" ssid = "4">We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials.</S><S sid ="120" ssid = "33">Examples of our items are given in Table 1.</S><S sid ="165" ssid = "78">Again  better models should correlate better with the experimental data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'126'", "'192'", "'120'", "'165'"]
'97'
'126'
'192'
'120'
'165'
['97', '126', '192', '120', '165']
parsed_discourse_facet ['method_citation']
<S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="45" ssid = "18">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="68" ssid = "16">Although the composition model in (5) is commonly used in the literature  from a linguistic perspective  the model in (6) is more appealing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'", "'121'", "'45'", "'116'", "'68'"]
'42'
'121'
'45'
'116'
'68'
['42', '121', '45', '116', '68']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "13">In the following we describe our data collection procedure and give details on how our composition models were constructed and evaluated.</S><S sid ="123" ssid = "36">Sentence pairs were presented serially in random order.</S><S sid ="112" ssid = "25">The stimuli were administered to four separate groups; each group saw one set of 100 sentences.</S><S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'123'", "'112'", "'25'", "'63'"]
'100'
'123'
'112'
'25'
'63'
['100', '123', '112', '25', '63']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S><S sid ="85" ssid = "33">One potential drawback of multiplicative models is the effect of components with value zero.</S><S sid ="145" ssid = "58">The latter were the most common context words (excluding a stop list of function words).</S><S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'85'", "'145'", "'121'", "'103'"]
'4'
'85'
'145'
'121'
'103'
['4', '85', '145', '121', '103']
parsed_discourse_facet ['method_citation']
<S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="45" ssid = "18">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid ="95" ssid = "8">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'45'", "'95'", "'150'", "'42'"]
'25'
'45'
'95'
'150'
'42'
['25', '45', '95', '150', '42']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "49">We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine-grained semantic distinctions (see Table 1).</S><S sid ="132" ssid = "45">We also measured how well humans agree in their ratings.</S><S sid ="158" ssid = "71">The m neighbors most similar to the predicate  and the k of m neighbors closest to its argument.</S><S sid ="200" ssid = "12">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid ="17" ssid = "13">It was not the sales manager who hit the bottle that day  but the office worker with the serious drinking problem. b.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'132'", "'158'", "'200'", "'17'"]
'136'
'132'
'158'
'200'
'17'
['136', '132', '158', '200', '17']
parsed_discourse_facet ['implication_citation']
<S sid ="95" ssid = "8">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="190" ssid = "2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid ="46" ssid = "19">However  since it is order independent  it cannot capture meaning differences that are modulated by differences in syntactic structure.</S><S sid ="29" ssid = "2">While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'150'", "'190'", "'46'", "'29'"]
'95'
'150'
'190'
'46'
'29'
['95', '150', '190', '46', '29']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="156" ssid = "69">This yielded a weighted sum consisting of 95% verb  0% noun and 5% of their multiplicative combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'103'", "'116'", "'150'", "'156'"]
'34'
'103'
'116'
'150'
'156'
['34', '103', '116', '150', '156']
parsed_discourse_facet ['method_citation']
<S sid ="176" ssid = "10">The multiplicative and combined models yield means closer to the human ratings.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="157" ssid = "70">Finally  Kintschs (2001) additive model has two extra parameters.</S><S sid ="88" ssid = "1">We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).</S><S sid ="38" ssid = "11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'", "'42'", "'157'", "'88'", "'38'"]
'176'
'42'
'157'
'88'
'38'
['176', '42', '157', '88', '38']
parsed_discourse_facet ['method_citation']
<S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="114" ssid = "27">For each reference verb  the subjects responses were entered into a contingency table  whose rows corresponded to nouns and columns to each possible answer (i.e.  one of the two landmarks).</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'116'", "'150'", "'59'", "'114'"]
'63'
'116'
'150'
'59'
'114'
['63', '116', '150', '59', '114']
parsed_discourse_facet ['results_citation']
<S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="64" ssid = "12">Now  if we assume that p lies in the same space as u and v  avoiding the issues of dimensionality associated with tensor products  and that f is a linear function  for simplicity  of the cartesian product of u and v  then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast  if we assume that f is a linear function of the tensor product of u and v  then we obtain multiplicative models: where C is a tensor of rank 3  which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'", "'121'", "'116'", "'64'", "'103'"]
'42'
'121'
'116'
'64'
'103'
['42', '121', '116', '64', '103']
parsed_discourse_facet ['implication_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="77" ssid = "25">For additive models  a natural way to achieve this is to include further vectors into the summation.</S><S sid ="70" ssid = "18">Instead  it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v  and vice versa.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="85" ssid = "33">One potential drawback of multiplicative models is the effect of components with value zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'77'", "'70'", "'59'", "'85'"]
'194'
'77'
'70'
'59'
'85'
['194', '77', '70', '59', '85']
parsed_discourse_facet ['method_citation']
<S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S><S sid ="130" ssid = "43">As we can see sentences with high similarity landmarks are perceived as more similar to the reference sentence.</S><S sid ="82" ssid = "30">In contrast to the simple additive model  this extended model is sensitive to syntactic structure  since n is chosen from among the neighbors of the predicate  distinguishing it from the argument.</S><S sid ="73" ssid = "21">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware  since semantically important constituents can participate more actively in the composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'121'", "'103'", "'130'", "'82'", "'73'"]
'121'
'103'
'130'
'82'
'73'
['121', '103', '130', '82', '73']
parsed_discourse_facet ['results_citation']
['An extreme form of this differential in the contribution of constituents is where one of the vectors, say u, contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic, however it can serve as a simple baseline against which to compare more sophisticated models.']
[u'A more scrupulous evaluation requires directly correlating all the individual participants\u2019 similarity judgments with those of the models.6 We used Spearman\u2019s p for our correlation analyses.', 'The applications of the framework discussed here are many and varied both for cognitive science and NLP.', 'We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine-grained semantic distinctions (see Table 1).', u'The average inter-subject agreement5 was \u03c1 = 0.40.', 'Our composition models have no additional parameters beyond the semantic space just described  with three exceptions.']
['system', 'ROUGE-S*', 'Average_R:', '0.00089', '(95%-conf.int.', '0.00089', '-', '0.00089)']
['system', 'ROUGE-S*', 'Average_P:', '0.00585', '(95%-conf.int.', '0.00585', '-', '0.00585)']
['system', 'ROUGE-S*', 'Average_F:', '0.00154', '(95%-conf.int.', '0.00154', '-', '0.00154)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:171', 'F:1']
['The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately.']
['This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.', 'All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).', 'Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.', 'This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.', 'One potential drawback of multiplicative models is the effect of components with value zero.']
['system', 'ROUGE-S*', 'Average_R:', '0.00290', '(95%-conf.int.', '0.00290', '-', '0.00290)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00539', '(95%-conf.int.', '0.00539', '-', '0.00539)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:78', 'F:3']
['We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).']
[u'For each reference verb  the subjects\u2019 responses were entered into a contingency table  whose rows corresponded to nouns and columns to each possible answer (i.e.  one of the two landmarks).', 'First  the additive model in (7) weighs differentially the contribution of the two constituents.', 'This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.', u'We used Fisher\u2019s exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.', 'It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:55', 'F:0']
['We present a general framework for vector-based composition which allows us to consider different classes of models.']
['Examples of our items are given in Table 1.', 'Again  better models should correlate better with the experimental data.', '49 unpaid volunteers completed the experiment  all native speakers of English.', 'Moreover by using a minimal structure we factor out inessential degrees of freedom and are able to assess the merits of different models on an equal footing.', 'We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:28', 'F:0']
['We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['We present a general framework for vector-based composition which allows us to consider different classes of models.', 'Vector addition does not increase the dimensionality of the resulting vector.', 'Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.', 'This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.', 'First  the additive model in (7) weighs differentially the contribution of the two constituents.']
['system', 'ROUGE-S*', 'Average_R:', '0.00808', '(95%-conf.int.', '0.00808', '-', '0.00808)']
['system', 'ROUGE-S*', 'Average_P:', '0.22222', '(95%-conf.int.', '0.22222', '-', '0.22222)']
['system', 'ROUGE-S*', 'Average_F:', '0.01559', '(95%-conf.int.', '0.01559', '-', '0.01559)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:36', 'F:8']
['We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['In contrast to the simple additive model  this extended model is sensitive to syntactic structure  since n is chosen from among the neighbors of the predicate  distinguishing it from the argument.', 'Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).', 'As we can see sentences with high similarity landmarks are perceived as more similar to the reference sentence.', 'Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware  since semantically important constituents can participate more actively in the composition.', 'All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).']
['system', 'ROUGE-S*', 'Average_R:', '0.00038', '(95%-conf.int.', '0.00038', '-', '0.00038)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00075', '(95%-conf.int.', '0.00075', '-', '0.00075)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:36', 'F:1']
['The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.']
['However  since it is order independent  it cannot capture meaning differences that are modulated by differences in syntactic structure.', 'First  the additive model in (7) weighs differentially the contribution of the two constituents.', 'Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.', 'We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.', 'While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.']
['system', 'ROUGE-S*', 'Average_R:', '0.00065', '(95%-conf.int.', '0.00065', '-', '0.00065)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00125', '(95%-conf.int.', '0.00125', '-', '0.00125)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:55', 'F:1']
['This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.']
['Vector addition does not increase the dimensionality of the resulting vector.', 'This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.', 'This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.', 'Landmarks were taken from WordNet (Fellbaum  1998).', 'We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials.']
['system', 'ROUGE-S*', 'Average_R:', '0.06757', '(95%-conf.int.', '0.06757', '-', '0.06757)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.12658', '(95%-conf.int.', '0.12658', '-', '0.12658)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:45', 'F:45']
['Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.']
['This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.', 'We present a general framework for vector-based composition which allows us to consider different classes of models.', 'In the following we describe our data collection procedure and give details on how our composition models were constructed and evaluated.', 'Sentence pairs were presented serially in random order.', 'The stimuli were administered to four separate groups; each group saw one set of 100 sentences.']
['system', 'ROUGE-S*', 'Average_R:', '0.00810', '(95%-conf.int.', '0.00810', '-', '0.00810)']
['system', 'ROUGE-S*', 'Average_P:', '0.04412', '(95%-conf.int.', '0.04412', '-', '0.04412)']
['system', 'ROUGE-S*', 'Average_F:', '0.01368', '(95%-conf.int.', '0.01368', '-', '0.01368)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:136', 'F:6']
0.15073444277 0.00984111100177 0.0183088886855





input/ref/Task1/D09-1092_vardha.csv
input/res/Task1/D09-1092.csv
parsing: input/ref/Task1/D09-1092_vardha.csv
<S sid="17" ssid="13">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
 <S sid="20" ssid="16">We also explore how the characteristics of different languages affect topic model performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
    <S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
   <S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'"]
'55'
['55']
parsed_discourse_facet ['method_citation']
 sid="9" ssid="5">In this paper, we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
Error in Reference Offset
<S sid="118" ssid="67">We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
 <S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'"]
'55'
['55']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="51">In contrast, PLTM assigns a significant number of tokens to almost all 800 topics, in very similar proportions in both languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'"]
'102'
['102']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="4">This is unlike LDA, in which each document is assumed to have its own document-specific distribution over topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'"]
'38'
['38']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="105">We use both Jensen-Shannon divergence and cosine distance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'"]
'156'
['156']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="2">Each tuple is a set of documents that are loosely equivalent to each other, but written in different languages, e.g., corresponding Wikipedia articles in French, English and German.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
  <S sid="170" ssid="4">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
    <S sid="29" ssid="5">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
  <S sid="170" ssid="4">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="168" ssid="2">However, the growth of the web, and in particular Wikipedia, has made comparable text corpora &#8211; documents that are topically similar but are not direct translations of one another &#8211; considerably more abundant than true parallel corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'168'"]
'168'
['168']
parsed_discourse_facet ['method_citation']
    <S sid="29" ssid="5">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
 <S sid="110" ssid="59">Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'"]
'110'
['110']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="176" ssid = "10">We dropped all articles in non-English languages that did not link to an English article.</S><S sid ="145" ssid = "94">For T = 800  the top English and Spanish words in 448 topics were exact translations of one another.</S><S sid ="155" ssid = "104">Finally  for each pair of languages (query and target) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'18'", "'176'", "'145'", "'155'"]
'147'
'18'
'176'
'145'
'155'
['147', '18', '176', '145', '155']
parsed_discourse_facet ['implication_citation']
<S sid ="63" ssid = "12">Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.</S><S sid ="156" ssid = "105">We use both Jensen-Shannon divergence and cosine distance.</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="64" ssid = "13">The first topic contains words relating to the European Central Bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'156'", "'32'", "'23'", "'64'"]
'63'
'156'
'32'
'23'
'64'
['63', '156', '32', '23', '64']
parsed_discourse_facet ['implication_citation']
<S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S><S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="182" ssid = "16">As with EuroParl  we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.</S><S sid ="190" ssid = "24">Meanwhile  interest in actors and actresses (center) is consistent across all languages.</S><S sid ="104" ssid = "53">This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'30'", "'182'", "'190'", "'104'"]
'38'
'30'
'182'
'190'
'104'
['38', '30', '182', '190', '104']
parsed_discourse_facet ['results_citation']
<S sid ="164" ssid = "113">Results vary by language.</S><S sid ="160" ssid = "109">It is important to note that the length of documents matters.</S><S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of objective. The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S><S sid ="104" ssid = "53">This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.</S><S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of 1  .</S>
original cit marker offset is 0
new cit marker offset is 0



["'164'", "'160'", "'69'", "'104'", "'43'"]
'164'
'160'
'69'
'104'
'43'
['164', '160', '69', '104', '43']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S><S sid ="172" ssid = "6">Second  because comparable texts may not use exactly the same topics  it becomes crucially important to be able to characterize differences in topic prevalence at the document level (do different languages have different perspectives on the same article?) and at the language-wide level (which topics do particular languages focus on?).</S><S sid ="63" ssid = "12">Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'23'", "'38'", "'172'", "'63'"]
'32'
'23'
'38'
'172'
'63'
['32', '23', '38', '172', '63']
parsed_discourse_facet ['method_citation']
<S sid ="135" ssid = "84">We do not consider multi-word terms.</S><S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of objective. The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S><S sid ="175" ssid = "9">We preprocessed the data by removing tables  references  images and info-boxes.</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="176" ssid = "10">We dropped all articles in non-English languages that did not link to an English article.</S>
original cit marker offset is 0
new cit marker offset is 0



["'135'", "'69'", "'175'", "'32'", "'176'"]
'135'
'69'
'175'
'32'
'176'
['135', '69', '175', '32', '176']
parsed_discourse_facet ['method_citation']
<S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="97" ssid = "46">Rather  these results are intended as a quantitative analysis of the difference between the two models.</S><S sid ="40" ssid = "6">Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter  and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl  P(wl  |zl l) = 11n lwl |zl .</S><S sid ="21" ssid = "17">The second corpus  Wikipedia articles in twelve languages  contains sets of documents that are not translations of one another  but are very likely to be about similar concepts.</S><S sid ="75" ssid = "24">We compute histograms of these maximum topic probabilities for T  {50 100  200  400  800}.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'97'", "'40'", "'21'", "'75'"]
'147'
'97'
'40'
'21'
'75'
['147', '97', '40', '21', '75']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="184" ssid = "18">Interestingly  we find that almost all languages in our corpus  including several pairs that have historically been in conflict  show average JS divergences of between approximately 0.08 and 0.12 for T = 400  consistent with our findings for EuroParl translations.</S><S sid ="87" ssid = "36">The probability of previously unseen held-out document tuples given these estimates can then be computed.</S><S sid ="145" ssid = "94">For T = 800  the top English and Spanish words in 448 topics were exact translations of one another.</S><S sid ="178" ssid = "12">For efficiency  we truncated each article to the nearest word after 1000 characters and dropped the 50 most common word types in each language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'184'", "'87'", "'145'", "'178'"]
'32'
'184'
'87'
'145'
'178'
['32', '184', '87', '145', '178']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="48" ssid = "14">.</S><S sid ="124" ssid = "73">At p = 0.01 (1% glue documents)  German and French both include words relating to Russia  while the English and Italian word distributions appear locally consistent but unrelated to Russia.</S><S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'48'", "'124'", "'1'", "'38'"]
'30'
'48'
'124'
'1'
'38'
['30', '48', '124', '1', '38']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of objective. The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="9" ssid = "5">In this paper  we present the polylingual topic model (PLTM).</S><S sid ="40" ssid = "6">Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter  and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl  P(wl  |zl l) = 11n lwl |zl .</S><S sid ="186" ssid = "20">Although we find that if Wikipedia contains an article on a particular subject in some language  the article will tend to be topically similar to the articles about that subject in other languages  we also find that across the whole collection different languages emphasize topics to different extents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'23'", "'9'", "'40'", "'186'"]
'69'
'23'
'9'
'40'
'186'
['69', '23', '9', '40', '186']
parsed_discourse_facet ['implication_citation']
<S sid ="46" ssid = "12">  L and m from P(1  ...   L  m  |W'  ) or by evaluating a point estimate.</S><S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="72" ssid = "21">To answer this question  we compute the posterior probability of each topic in each tuple under the trained model.</S><S sid ="28" ssid = "4">We take a simpler approach that is more suitable for topically similar document tuples (where documents are not direct translations of one another) in more than two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'30'", "'32'", "'72'", "'28'"]
'46'
'30'
'32'
'72'
'28'
['46', '30', '32', '72', '28']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual synsets by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="45" ssid = "11">.</S><S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="67" ssid = "16">(Interestingly  all languages except Greek and Finnish use closely related words for youth or young in a separate topic.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'131'", "'45'", "'59'", "'67'"]
'30'
'131'
'45'
'59'
'67'
['30', '131', '45', '59', '67']
parsed_discourse_facet ['method_citation']
<S sid ="155" ssid = "104">Finally  for each pair of languages (query and target) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S><S sid ="29" ssid = "5">A recent extended abstract  developed concurrently by Ni et al. (Ni et al.  2009)  discusses a multilingual topic model similar to the one presented here.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="173" ssid = "7">We downloaded XML copies of all Wikipedia articles in twelve different languages: Welsh  German  Greek  English  Farsi  Finnish  French  Hebrew  Italian  Polish  Russian and Turkish.</S><S sid ="103" ssid = "52">PLTM topics therefore have a higher granularity  i.e.  they are more specific.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'", "'29'", "'19'", "'173'", "'103'"]
'155'
'29'
'19'
'173'
'103'
['155', '29', '19', '173', '103']
parsed_discourse_facet ['method_citation']
<S sid ="135" ssid = "84">We do not consider multi-word terms.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual synsets by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="89" ssid = "38">Analytically calculating the probability of a set of held-out document tuples given 1  ...   L and m is intractable  due to the summation over an exponential number of topic assignments for these held-out documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'135'", "'131'", "'147'", "'1'", "'89'"]
'135'
'131'
'147'
'1'
'89'
['135', '131', '147', '1', '89']
parsed_discourse_facet ['results_citation']
<S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of 1  .</S><S sid ="136" ssid = "85">We expect that simple analysis of topic assignments for sequential words would yield such collocations  but we leave this for future work.</S><S sid ="156" ssid = "105">We use both Jensen-Shannon divergence and cosine distance.</S><S sid ="45" ssid = "11">.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'136'", "'156'", "'45'", "'23'"]
'43'
'136'
'156'
'45'
'23'
['43', '136', '156', '45', '23']
parsed_discourse_facet ['implication_citation']
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual synsets by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="84" ssid = "33">As the number of topics increases  greater variability in topic distributions causes divergence to increase.</S><S sid ="63" ssid = "12">Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.</S><S sid ="175" ssid = "9">We preprocessed the data by removing tables  references  images and info-boxes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'147'", "'84'", "'63'", "'175'"]
'131'
'147'
'84'
'63'
'175'
['131', '147', '84', '63', '175']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of 1  .</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="175" ssid = "9">We preprocessed the data by removing tables  references  images and info-boxes.</S><S sid ="53" ssid = "2">In this case  we can be confident that the topic distribution is genuinely shared across all languages.</S><S sid ="193" ssid = "2">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'32'", "'175'", "'53'", "'193'"]
'43'
'32'
'175'
'53'
'193'
['43', '32', '175', '53', '193']
parsed_discourse_facet ['results_citation']
<S sid ="97" ssid = "46">Rather  these results are intended as a quantitative analysis of the difference between the two models.</S><S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="135" ssid = "84">We do not consider multi-word terms.</S><S sid ="103" ssid = "52">PLTM topics therefore have a higher granularity  i.e.  they are more specific.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'52'", "'30'", "'135'", "'103'"]
'97'
'52'
'30'
'135'
'103'
['97', '52', '30', '135', '103']
parsed_discourse_facet ['aim_citation']
<S sid ="14" ssid = "10">Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.</S><S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="190" ssid = "24">Meanwhile  interest in actors and actresses (center) is consistent across all languages.</S><S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'59'", "'190'", "'1'", "'31'"]
'14'
'59'
'190'
'1'
'31'
['14', '59', '190', '1', '31']
parsed_discourse_facet ['method_citation']
['We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).']
['Results vary by language.', u'English and the Romance languages use only singular and plural versions of \u201cobjective.\u201d The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.', 'It is important to note that the length of documents matters.', u'These tasks can either be accomplished by averaging over samples of \u03a61  .', 'This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00097', '(95%-conf.int.', '0.00097', '-', '0.00097)']
['system', 'ROUGE-S*', 'Average_P:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00173', '(95%-conf.int.', '0.00173', '-', '0.00173)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:120', 'F:1']
['We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.']
['As with EuroParl  we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.', 'This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.', 'Meanwhile  interest in actors and actresses (center) is consistent across all languages.', 'This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.', 'However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:78', 'F:0']
['Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.']
['Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.', 'They also provide little analysis of the differences between polylingual and single-language topic models.', 'The remaining collection consists of over 121 million words.', 'Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.', 'Meanwhile  interest in actors and actresses (center) is consistent across all languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:153', 'F:0']
['We also explore how the characteristics of different languages affect topic model performance.']
['Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.', 'The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.', 'Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.', 'The first topic contains words relating to the European Central Bank.', 'We use both Jensen-Shannon divergence and cosine distance.']
['system', 'ROUGE-S*', 'Average_R:', '0.00327', '(95%-conf.int.', '0.00327', '-', '0.00327)']
['system', 'ROUGE-S*', 'Average_P:', '0.19048', '(95%-conf.int.', '0.19048', '-', '0.19048)']
['system', 'ROUGE-S*', 'Average_F:', '0.00642', '(95%-conf.int.', '0.00642', '-', '0.00642)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:21', 'F:4']
['We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.']
['These aligned document pairs could then be fed into standard machine translation systems as training data.', u'We compute histograms of these maximum topic probabilities for T \u2208 {50 100  200  400  800}.', u'Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter \u03b1 and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl \u223c P(wl  |zl \u03a6l) = 11n \u03c6lwl |zl .', 'Rather  these results are intended as a quantitative analysis of the difference between the two models.', 'The second corpus  Wikipedia articles in twelve languages  contains sets of documents that are not translations of one another  but are very likely to be about similar concepts.']
['system', 'ROUGE-S*', 'Average_R:', '0.00421', '(95%-conf.int.', '0.00421', '-', '0.00421)']
['system', 'ROUGE-S*', 'Average_P:', '0.13187', '(95%-conf.int.', '0.13187', '-', '0.13187)']
['system', 'ROUGE-S*', 'Average_F:', '0.00816', '(95%-conf.int.', '0.00816', '-', '0.00816)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:91', 'F:12']
['The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.']
['Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.', 'Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.', 'This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.', 'Second  because comparable texts may not use exactly the same topics  it becomes crucially important to be able to characterize differences in topic prevalence at the document level (do different languages have different perspectives on the same article?) and at the language-wide level (which topics do particular languages focus on?).', 'The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00047', '(95%-conf.int.', '0.00047', '-', '0.00047)']
['system', 'ROUGE-S*', 'Average_P:', '0.00526', '(95%-conf.int.', '0.00526', '-', '0.00526)']
['system', 'ROUGE-S*', 'Average_F:', '0.00086', '(95%-conf.int.', '0.00086', '-', '0.00086)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:190', 'F:1']
['First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.']
['Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.', 'These aligned document pairs could then be fed into standard machine translation systems as training data.', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', u'Analytically calculating the probability of a set of held-out document tuples given \u03a61  ...   \u03a6L and \u03b1m is intractable  due to the summation over an exponential number of topic assignments for these held-out documents.', 'We do not consider multi-word terms.']
['system', 'ROUGE-S*', 'Average_R:', '0.00701', '(95%-conf.int.', '0.00701', '-', '0.00701)']
['system', 'ROUGE-S*', 'Average_P:', '0.13187', '(95%-conf.int.', '0.13187', '-', '0.13187)']
['system', 'ROUGE-S*', 'Average_F:', '0.01332', '(95%-conf.int.', '0.01332', '-', '0.01332)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:91', 'F:12']
['A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.']
['.', 'The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.', 'We use both Jensen-Shannon divergence and cosine distance.', u'These tasks can either be accomplished by averaging over samples of \xce\xa61  .', 'We expect that simple analysis of topic assignments for sequential words would yield such collocations  but we leave this for future work.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:120', 'F:0']
['We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.']
['These aligned document pairs could then be fed into standard machine translation systems as training data.', u'Finally  for each pair of languages (\u201cquery\u201d and \u201ctarget\u201d) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.', 'For T = 800  the top English and Spanish words in 448 topics were exact translations of one another.', 'We dropped all articles in non-English languages that did not link to an English article.', 'In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.']
['system', 'ROUGE-S*', 'Average_R:', '0.00269', '(95%-conf.int.', '0.00269', '-', '0.00269)']
['system', 'ROUGE-S*', 'Average_P:', '0.07273', '(95%-conf.int.', '0.07273', '-', '0.07273)']
['system', 'ROUGE-S*', 'Average_F:', '0.00519', '(95%-conf.int.', '0.00519', '-', '0.00519)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:55', 'F:4']
['However, the growth of the web, and in particular Wikipedia, has made comparable text corpora &#8211; documents that are topically similar but are not direct translations of one another &#8211; considerably more abundant than true parallel corpora.']
['Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.', 'We preprocessed the data by removing tables  references  images and info-boxes.', 'We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.', u'These tasks can either be accomplished by averaging over samples of \xce\xa61  .', 'In this case  we can be confident that the topic distribution is genuinely shared across all languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00097', '(95%-conf.int.', '0.00097', '-', '0.00097)']
['system', 'ROUGE-S*', 'Average_P:', '0.00585', '(95%-conf.int.', '0.00585', '-', '0.00585)']
['system', 'ROUGE-S*', 'Average_F:', '0.00166', '(95%-conf.int.', '0.00166', '-', '0.00166)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:171', 'F:1']
0.0546389994536 0.00195899998041 0.00373399996266





input/ref/Task1/P08-1102_sweta.csv
input/res/Task1/P08-1102.csv
parsing: input/ref/Task1/P08-1102_sweta.csv
<S sid="21" ssid="17">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="24">A subsequence of boundary-POS labelling result indicates a word with POS t only if the boundary tag sequence composed of its boundary part conforms to s or bm*e style, and all POS tags in its POS part equal to t. For example, a tag sequence b NN m NN e NN represents a threecharacter word with POS tag NN.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="43" ssid="15">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["43'"]
43'
['43']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["92'"]
92'
['92']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="5">To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&amp;T).</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="5">In following subsections, we describe the feature templates and the perceptron training algorithm.</S>
    <S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["33'", "'34'"]
33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="4">By maintaining a stack of size N at each position i of the sequence, we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S>
original cit marker offset is 0
new cit marker offset is 0



["79'"]
79'
['79']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="7">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["96'"]
96'
['96']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="16" ssid="12">Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="7">To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S sid ="120" ssid = "31">Without the perceptron  the cascaded model (if we can still call it cascaded) performs poorly on both segmentation and Joint S&T.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'", "'58'", "'136'", "'31'", "'134'"]
'120'
'58'
'136'
'31'
'134'
['120', '58', '136', '31', '134']
parsed_discourse_facet ['implication_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="117" ssid = "28">Table 4 shows experiments results.</S><S sid ="93" ssid = "4">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid ="18" ssid = "14">In this architecture  knowledge sources that are intractable to incorporate into the perceptron  can be easily incorporated into the outside linear model.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'117'", "'93'", "'18'", "'136'"]
'68'
'117'
'93'
'18'
'136'
['68', '117', '93', '18', '136']
parsed_discourse_facet ['implication_citation']
<S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x  X to outputs y  Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="78" ssid = "3">Given a Chinese character sequence C1:n  the decoding procedure can proceed in a left-right fashion with a dynamic programming approach.</S><S sid ="79" ssid = "4">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="46" ssid = "18">Following Collins  we use a function GEN(x) generating all candidate results of an input x   a representation 4) mapping each training example (x  y)  X  Y to a feature vector 4)(x  y)  Rd  and a parameter vector   Rd corresponding to the feature vector. d means the dimension of the vector space  it equals to the amount of features in the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'", "'78'", "'79'", "'7'", "'46'"]
'45'
'78'
'79'
'7'
'46'
['45', '78', '79', '7', '46']
parsed_discourse_facet ['results_citation']
<S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="65" ssid = "16">As shown in Figure 1  the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.</S><S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="95" ssid = "6">For convenience of comparing with others  we focus only on the close test  which means that any extra resource is forbidden except the designated training corpus.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'65'", "'101'", "'95'", "'58'"]
'133'
'65'
'101'
'95'
'58'
['133', '65', '101', '95', '58']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="95" ssid = "6">For convenience of comparing with others  we focus only on the close test  which means that any extra resource is forbidden except the designated training corpus.</S><S sid ="61" ssid = "12">In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'47'", "'95'", "'61'", "'136'"]
'101'
'47'
'95'
'61'
'136'
['101', '47', '95', '61', '136']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called averaged parameters (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="71" ssid = "22">Using W = w1:m to denote the word sequence  T = t1:m to denote the corresponding POS sequence  P (T |W) to denote the probability that W is labelled as T  and P(W|T) to denote the probability that T generates W  we can define the cooccurrence model as follows: wt and tw denote the corresponding weights of the two components.</S><S sid ="92" ssid = "3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.</S><S sid ="82" ssid = "7">In addition  we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'49'", "'71'", "'92'", "'82'"]
'68'
'49'
'71'
'92'
'82'
['68', '49', '71', '92', '82']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "10">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid ="40" ssid = "12">Templates in the column below are expanded from the upper ones.</S><S sid ="43" ssid = "15">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S><S sid ="25" ssid = "21">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'58'", "'40'", "'43'", "'25'"]
'38'
'58'
'40'
'43'
'25'
['38', '58', '40', '43', '25']
parsed_discourse_facet ['method_citation']
<S sid ="93" ssid = "4">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid ="57" ssid = "8">It has a two-layer architecture  with a perceptron as the core and another linear model as the outside-layer.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="40" ssid = "12">Templates in the column below are expanded from the upper ones.</S><S sid ="17" ssid = "13">We will describe it in detail in Section 4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'93'", "'57'", "'136'", "'40'", "'17'"]
'93'
'57'
'136'
'40'
'17'
['93', '57', '136', '40', '17']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="33" ssid = "5">In following subsections  we describe the feature templates and the perceptron training algorithm.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="96" ssid = "7">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus  we randomly chosen 2  000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84  294 sentences)  then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'68'", "'33'", "'134'", "'96'"]
'101'
'68'
'33'
'134'
'96'
['101', '68', '33', '134', '96']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="89" ssid = "14">Function D derives the candidate result from the word-POS pair p and the candidate q at prior position of p.</S><S sid ="9" ssid = "5">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid ="79" ssid = "4">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid ="104" ssid = "15">According to the usual practice in syntactic analysis  we choose chapters 1  260 (18074 sentences) as training set  chapter 271  300 (348 sentences) as test set and chapter 301  325 (350 sentences) as development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'89'", "'9'", "'79'", "'104'"]
'68'
'89'
'9'
'79'
'104'
['68', '89', '9', '79', '104']
parsed_discourse_facet ['implication_citation']
<S sid ="125" ssid = "36">However unlike the three features  the word LM brings very tiny improvement.</S><S sid ="40" ssid = "12">Templates in the column below are expanded from the upper ones.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'40'", "'94'", "'2'", "'58'"]
'125'
'40'
'94'
'2'
'58'
['125', '40', '94', '2', '58']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="56" ssid = "7">To alleviate the drawbacks  we propose a cascaded linear model.</S><S sid ="61" ssid = "12">In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.</S><S sid ="93" ssid = "4">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid ="112" ssid = "23">Here the core perceptron was just the POS+ model in experiments above.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'56'", "'61'", "'93'", "'112'"]
'136'
'56'
'61'
'93'
'112'
['136', '56', '61', '93', '112']
parsed_discourse_facet ['method_citation']
<S sid ="99" ssid = "10">Then we trained LEX on each of the four corpora for 7 iterations.</S><S sid ="120" ssid = "31">Without the perceptron  the cascaded model (if we can still call it cascaded) performs poorly on both segmentation and Joint S&T.</S><S sid ="121" ssid = "32">Among other features  the 4-gram POS LM plays the most important role  removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&T.</S><S sid ="42" ssid = "14">As predications generated from such templates depend on the current character  we name these templates lexical-target.</S><S sid ="117" ssid = "28">Table 4 shows experiments results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'99'", "'120'", "'121'", "'42'", "'117'"]
'99'
'120'
'121'
'42'
'117'
['99', '120', '121', '42', '117']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="64" ssid = "15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S><S sid ="53" ssid = "4">Figure 2 shows the growing tendency of feature space with the introduction of these features as well as the character-based ones.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'134'", "'64'", "'53'", "'7'"]
'68'
'134'
'64'
'53'
'7'
['68', '134', '64', '53', '7']
parsed_discourse_facet ['results_citation']
<S sid ="3" ssid = "3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="84" ssid = "9">Algorithm 2 shows the decoding algorithm.</S><S sid ="18" ssid = "14">In this architecture  knowledge sources that are intractable to incorporate into the perceptron  can be easily incorporated into the outside linear model.</S><S sid ="92" ssid = "3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'136'", "'84'", "'18'", "'92'"]
'3'
'136'
'84'
'18'
'92'
['3', '136', '84', '18', '92']
parsed_discourse_facet ['implication_citation']
<S sid ="51" ssid = "2">Additional features most widely used are related to word or POS ngrams.</S><S sid ="13" ssid = "9">However  as such features are generated dynamically during the decoding procedure  two limitation arise: on the one hand  the amount of parameters increases rapidly  which is apt to overfit on training corpus; on the other hand  exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called averaged parameters (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="36" ssid = "8">All feature templates and their instances are shown in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'13'", "'7'", "'49'", "'36'"]
'51'
'13'
'7'
'49'
'36'
['51', '13', '7', '49', '36']
parsed_discourse_facet ['method_citation']
Length 0 input/ref/Task1/P08-1102_sweta.csv
0.0 0.0 0.0





input/ref/Task1/A97-1014_vardha.csv
input/res/Task1/A97-1014.csv
parsing: input/ref/Task1/A97-1014_vardha.csv
<S sid="72" ssid="17">In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
 <S sid="144" ssid="25">We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="4">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
  <S sid="36" ssid="26">As for free word order languages, the following features may cause problems: sition between the two poles.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
 <S sid="24" ssid="14">The typical treebank architecture is as follows: Structures: A context-free backbone is augmented with trace-filler representations of non-local dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
 <S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'"]
'151'
['151']
parsed_discourse_facet ['method_citation']
 <S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
 <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["'167'"]
'167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'"]
'39'
['39']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'"]
'151'
['151']
parsed_discourse_facet ['method_citation']
<S sid="71" ssid="16">However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'71'"]
'71'
['71']
parsed_discourse_facet ['method_citation']
 <S sid="144" ssid="25">We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A97-1014.csv
<S sid ="73" ssid = "18">While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.</S><S sid ="115" ssid = "28">3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here  where C stands for coordination  and VP for the actual category.</S><S sid ="135" ssid = "16">Figure 1 shows a screen dump of the tool.</S><S sid ="39" ssid = "29">Consider the German sentence (1) daran wird ihn Anna erkennen  &di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies  marked by co-references between traces and the corresponding nodes.</S><S sid ="84" ssid = "29">Consider (qui verbs where the subject of the infinitival VP is not realised syntactically  but co-referent with the subject or object. of the matrix equi verb: (3) er bat mich zu kommen he asked me to come (mich is the understood subject. of kommt n).</S>
original cit marker offset is 0
new cit marker offset is 0



["'73'", "'115'", "'135'", "'39'", "'84'"]
'73'
'115'
'135'
'39'
'84'
['73', '115', '135', '39', '84']
parsed_discourse_facet ['implication_citation']
<S sid ="110" ssid = "23">A similar convention ha.s been adopted for constructions in which scope ambiguities have syntactic effects but a one-to-one correspondence between scope and attachment. does not seem reasonable  cf. focus particles such as only or also.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="30" ssid = "20">Thus the context-free constituent backbone plays a pivotal role in the annotation scheme.</S><S sid ="113" ssid = "26">Since a precise structural description of non-constituent coordination would require a. rich inventory of incomplete phrase types  we have agreed on a sort of unde.rspe.cified representations: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'", "'80'", "'30'", "'113'", "'56'"]
'110'
'80'
'30'
'113'
'56'
['110', '80', '30', '113', '56']
parsed_discourse_facet ['implication_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S><S sid ="78" ssid = "23">Morphological information: Another set of labels represents morphological information.</S><S sid ="8" ssid = "5">A formalism complying with these requirements is described in section 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'56'", "'60'", "'78'", "'8'"]
'80'
'56'
'60'
'78'
'8'
['80', '56', '60', '78', '8']
parsed_discourse_facet ['results_citation']
<S sid ="0" ssid = "0">An Annotation Scheme for Free Word Order Languages</S><S sid ="18" ssid = "8">(Marcus et al.  1994).</S><S sid ="151" ssid = "32">For evaluation  the already annotated sentences were divided into two disjoint sets  one for training (90% of the corpus)  the other one for testing (10%).</S><S sid ="49" ssid = "39">Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.</S><S sid ="114" ssid = "27">Fig.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'18'", "'151'", "'49'", "'114'"]
'0'
'18'
'151'
'49'
'114'
['0', '18', '151', '49', '114']
parsed_discourse_facet ['method_citation']
<S sid ="65" ssid = "10">The tree resembles traditional constituent structures.</S><S sid ="62" ssid = "7">2.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="77" ssid = "22">Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'62'", "'80'", "'77'", "'5'"]
'65'
'62'
'80'
'77'
'5'
['65', '62', '80', '77', '5']
parsed_discourse_facet ['method_citation']
<S sid ="49" ssid = "39">Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="118" ssid = "31">An explicit coordinating conjunction need not be present.</S><S sid ="28" ssid = "18">(Bies et al.  1995)  (Sampson  1995)).</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'56'", "'118'", "'28'", "'80'"]
'49'
'56'
'118'
'28'
'80'
['49', '56', '118', '28', '80']
parsed_discourse_facet ['method_citation']
<S sid ="65" ssid = "10">The tree resembles traditional constituent structures.</S><S sid ="108" ssid = "21">However  full or partial disambiguation takes place in context  and the annotators do not consider unrealistic readings.</S><S sid ="96" ssid = "9">In (5)  either a lexical nominalisation rule for the adjective Gliickliche is stipulated  or the existence of an empty nominal head.</S><S sid ="86" ssid = "31">We call such additional edges secondary links and represent them as dotted lines  see fig.</S><S sid ="135" ssid = "16">Figure 1 shows a screen dump of the tool.</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'108'", "'96'", "'86'", "'135'"]
'65'
'108'
'96'
'86'
'135'
['65', '108', '96', '86', '135']
parsed_discourse_facet ['method_citation']
<S sid ="115" ssid = "28">3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here  where C stands for coordination  and VP for the actual category.</S><S sid ="77" ssid = "22">Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.</S><S sid ="109" ssid = "22">In addition  we have adopted a simple convention for those cases in which context information is insufficient  for total disambiguation: the highest possible attachment  site is chosen.</S><S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'77'", "'109'", "'60'", "'21'"]
'115'
'77'
'109'
'60'
'21'
['115', '77', '109', '60', '21']
parsed_discourse_facet ['method_citation']
<S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="73" ssid = "18">While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.</S><S sid ="86" ssid = "31">We call such additional edges secondary links and represent them as dotted lines  see fig.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S><S sid ="77" ssid = "22">Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'73'", "'86'", "'21'", "'77'"]
'56'
'73'
'86'
'21'
'77'
['56', '73', '86', '21', '77']
parsed_discourse_facet ['method_citation']
<S sid ="138" ssid = "19">This allows easy modification if needed.</S><S sid ="125" ssid = "6">The tool should also permit a convenient handling of node and edge labels.</S><S sid ="89" ssid = "2">However  some other standard analysts turn out to be problematic  mainly due to the partial  idealised character of competence grammars  which often marginalise or ignore such important. phenomena. as 'deficient' (e.g. headless) constructions  appositions  temporal expressions  etc.</S><S sid ="49" ssid = "39">Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.</S><S sid ="110" ssid = "23">A similar convention ha.s been adopted for constructions in which scope ambiguities have syntactic effects but a one-to-one correspondence between scope and attachment. does not seem reasonable  cf. focus particles such as only or also.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'", "'125'", "'89'", "'49'", "'110'"]
'138'
'125'
'89'
'49'
'110'
['138', '125', '89', '49', '110']
parsed_discourse_facet ['implication_citation']
<S sid ="147" ssid = "28">(Cutting et al.  1992) and (Feldweg  1995)).</S><S sid ="7" ssid = "4">On the basis of these considerations  we formulate several additional requirements.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="144" ssid = "25">We distinguish five degrees of automation: So far  about 1100 sentences of our corpus have been annotated.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'7'", "'80'", "'144'", "'21'"]
'147'
'7'
'80'
'144'
'21'
['147', '7', '80', '144', '21']
parsed_discourse_facet ['method_citation']
<S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S><S sid ="73" ssid = "18">While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.</S><S sid ="96" ssid = "9">In (5)  either a lexical nominalisation rule for the adjective Gliickliche is stipulated  or the existence of an empty nominal head.</S><S sid ="46" ssid = "36">These approaches provide an adequate explanation for several issues problematic for phrase-structure grammars (clause union  extraposition  diverse second-position phenomena).</S><S sid ="33" ssid = "23">(Lehmann et al.  1996)  (Marcus et al.  1994)  (Sampson  1995)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'73'", "'96'", "'46'", "'33'"]
'60'
'73'
'96'
'46'
'33'
['60', '73', '96', '46', '33']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "13">As the annotation scheme does not distinguish different bar levels or any similar intermediate categories  only a small set of node labels is needed (currently 16 tags  S  NP  AP ...).</S><S sid ="51" ssid = "41">This requirement speaks against the traditional sort of dependency trees  in which heads a re represented as non-terminal nodes  cf.</S><S sid ="109" ssid = "22">In addition  we have adopted a simple convention for those cases in which context information is insufficient  for total disambiguation: the highest possible attachment  site is chosen.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'51'", "'109'", "'11'", "'21'"]
'68'
'51'
'109'
'11'
'21'
['68', '51', '109', '11', '21']
parsed_discourse_facet ['method_citation']
<S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="138" ssid = "19">This allows easy modification if needed.</S><S sid ="115" ssid = "28">3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here  where C stands for coordination  and VP for the actual category.</S><S sid ="112" ssid = "25">A problem for the rudimentary argument. structure representations is the use of incomplete structures in natural language  i.e. phenomena such as coordination and ellipsis.</S><S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'138'", "'115'", "'112'", "'60'"]
'56'
'138'
'115'
'112'
'60'
['56', '138', '115', '112', '60']
parsed_discourse_facet ['results_citation']
['These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.']
['Separable verb prefixes are labeled SVP.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", '(Bies et al.  1995)  (Sampson  1995)).', 'An explicit coordinating conjunction need not be present.', 'Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:21', 'F:0']
['For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).']
['However  full or partial disambiguation takes place in context  and the annotators do not consider unrealistic readings.', 'We call such additional edges secondary links and represent them as dotted lines  see fig.', 'The tree resembles traditional constituent structures.', 'Figure 1 shows a screen dump of the tool.', 'In (5)  either a lexical nominalisation rule for the adjective Gliickliche is stipulated  or the existence of an empty nominal head.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:55', 'F:0']
['These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.']
['Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.', 'We call such additional edges secondary links and represent them as dotted lines  see fig.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.', 'Disambiguation is based on human processing skills (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:21', 'F:0']
['For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).']
['Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.', 'These approaches provide an adequate explanation for several issues problematic for phrase-structure grammars (clause union  extraposition  diverse second-position phenomena).', 'While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.', '(Lehmann et al.  1996)  (Marcus et al.  1994)  (Sampson  1995)).', 'In (5)  either a lexical nominalisation rule for the adjective Gliickliche is stipulated  or the existence of an empty nominal head.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:55', 'F:0']
['However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.']
['As the annotation scheme does not distinguish different bar levels or any similar intermediate categories  only a small set of node labels is needed (currently 16 tags  S  NP  AP ...).', 'Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.', 'In addition  we have adopted a simple convention for those cases in which context information is insufficient  for total disambiguation: the highest possible attachment  site is chosen.', 'This requirement speaks against the traditional sort of dependency trees  in which heads a re represented as non-terminal nodes  cf.', 'Disambiguation is based on human processing skills (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:28', 'F:0']
['These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.']
['This allows easy modification if needed.', "3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here  where C stands for coordination  and VP for the actual category.", "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.', 'A problem for the rudimentary argument. structure representations is the use of incomplete structures in natural language  i.e. phenomena such as coordination and ellipsis.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:21', 'F:0']
['Corpora annotated with syntactic structures are commonly referred to as trctbank.5.']
['Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.', 'A formalism complying with these requirements is described in section 3.', 'Separable verb prefixes are labeled SVP.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'Morphological information: Another set of labels represents morphological information.']
['system', 'ROUGE-S*', 'Average_R:', '0.00093', '(95%-conf.int.', '0.00093', '-', '0.00093)']
['system', 'ROUGE-S*', 'Average_P:', '0.03571', '(95%-conf.int.', '0.03571', '-', '0.03571)']
['system', 'ROUGE-S*', 'Average_F:', '0.00180', '(95%-conf.int.', '0.00180', '-', '0.00180)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:28', 'F:1']
["Consider the German sentence (1) daran wird ihn Anna erkennen, &di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes."]
['(Cutting et al.  1992) and (Feldweg  1995)).', 'Separable verb prefixes are labeled SVP.', 'We distinguish five degrees of automation: So far  about 1100 sentences of our corpus have been annotated.', 'On the basis of these considerations  we formulate several additional requirements.', 'Disambiguation is based on human processing skills (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:406', 'F:0']
['In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.']
['A similar convention ha.s been adopted for constructions in which scope ambiguities have syntactic effects but a one-to-one correspondence between scope and attachment. does not seem reasonable  cf. focus particles such as only or also.', 'This allows easy modification if needed.', "However  some other standard analysts turn out to be problematic  mainly due to the partial  idealised character of competence grammars  which often marginalise or ignore such important. phenomena. as 'deficient' (e.g. headless) constructions  appositions  temporal expressions  etc.", 'The tool should also permit a convenient handling of node and edge labels.', 'Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:91', 'F:0']
['We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.']
["We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'Separable verb prefixes are labeled SVP.', 'Thus the context-free constituent backbone plays a pivotal role in the annotation scheme.', 'A similar convention ha.s been adopted for constructions in which scope ambiguities have syntactic effects but a one-to-one correspondence between scope and attachment. does not seem reasonable  cf. focus particles such as only or also.', 'Since a precise structural description of non-constituent coordination would require a. rich inventory of incomplete phrase types  we have agreed on a sort of unde.rspe.cified representations: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links.']
['system', 'ROUGE-S*', 'Average_R:', '0.00036', '(95%-conf.int.', '0.00036', '-', '0.00036)']
['system', 'ROUGE-S*', 'Average_P:', '0.04762', '(95%-conf.int.', '0.04762', '-', '0.04762)']
['system', 'ROUGE-S*', 'Average_F:', '0.00072', '(95%-conf.int.', '0.00072', '-', '0.00072)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:21', 'F:1']
['Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.']
['For evaluation  the already annotated sentences were divided into two disjoint sets  one for training (90% of the corpus)  the other one for testing (10%).', 'An Annotation Scheme for Free Word Order Languages', 'Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.', 'Fig.', '(Marcus et al.  1994).']
['system', 'ROUGE-S*', 'Average_R:', '0.00476', '(95%-conf.int.', '0.00476', '-', '0.00476)']
['system', 'ROUGE-S*', 'Average_P:', '0.02857', '(95%-conf.int.', '0.02857', '-', '0.02857)']
['system', 'ROUGE-S*', 'Average_F:', '0.00816', '(95%-conf.int.', '0.00816', '-', '0.00816)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:105', 'F:3']
0.0101727271802 0.000549999995 0.000970909082083





input/ref/Task1/P08-1102_aakansha.csv
input/res/Task1/P08-1102.csv
parsing: input/ref/Task1/P08-1102_aakansha.csv
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="14">As predications generated from such templates depend on the current character, we name these templates lexical-target.</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="121" ssid="32">Among other features, the 4-gram POS LM plays the most important role, removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'121'"]
'121'
['121']
parsed_discourse_facet ['method_citation']
<S sid="37" ssid="9">C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'"]
'37'
['37']
parsed_discourse_facet ['method_citation']
<S sid="73" ssid="24">For instance, if the word w appears N times in training corpus and is labelled as POS t for n times, the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'73'"]
'73'
['73']
parsed_discourse_facet ['method_citation']
<S sid="46" ssid="18">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) &#8712; X &#215; Y to a feature vector 4)(x, y) &#8712; Rd, and a parameter vector &#945;&#65533; &#8712; Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'"]
'46'
['46']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S sid ="120" ssid = "31">Without the perceptron  the cascaded model (if we can still call it cascaded) performs poorly on both segmentation and Joint S&T.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'", "'58'", "'136'", "'31'", "'134'"]
'120'
'58'
'136'
'31'
'134'
['120', '58', '136', '31', '134']
parsed_discourse_facet ['implication_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="117" ssid = "28">Table 4 shows experiments results.</S><S sid ="93" ssid = "4">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid ="18" ssid = "14">In this architecture  knowledge sources that are intractable to incorporate into the perceptron  can be easily incorporated into the outside linear model.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'117'", "'93'", "'18'", "'136'"]
'68'
'117'
'93'
'18'
'136'
['68', '117', '93', '18', '136']
parsed_discourse_facet ['implication_citation']
<S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x  X to outputs y  Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="78" ssid = "3">Given a Chinese character sequence C1:n  the decoding procedure can proceed in a left-right fashion with a dynamic programming approach.</S><S sid ="79" ssid = "4">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="46" ssid = "18">Following Collins  we use a function GEN(x) generating all candidate results of an input x   a representation 4) mapping each training example (x  y)  X  Y to a feature vector 4)(x  y)  Rd  and a parameter vector   Rd corresponding to the feature vector. d means the dimension of the vector space  it equals to the amount of features in the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'", "'78'", "'79'", "'7'", "'46'"]
'45'
'78'
'79'
'7'
'46'
['45', '78', '79', '7', '46']
parsed_discourse_facet ['results_citation']
<S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="65" ssid = "16">As shown in Figure 1  the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.</S><S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="95" ssid = "6">For convenience of comparing with others  we focus only on the close test  which means that any extra resource is forbidden except the designated training corpus.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'65'", "'101'", "'95'", "'58'"]
'133'
'65'
'101'
'95'
'58'
['133', '65', '101', '95', '58']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="95" ssid = "6">For convenience of comparing with others  we focus only on the close test  which means that any extra resource is forbidden except the designated training corpus.</S><S sid ="61" ssid = "12">In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'47'", "'95'", "'61'", "'136'"]
'101'
'47'
'95'
'61'
'136'
['101', '47', '95', '61', '136']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called averaged parameters (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="71" ssid = "22">Using W = w1:m to denote the word sequence  T = t1:m to denote the corresponding POS sequence  P (T |W) to denote the probability that W is labelled as T  and P(W|T) to denote the probability that T generates W  we can define the cooccurrence model as follows: wt and tw denote the corresponding weights of the two components.</S><S sid ="92" ssid = "3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.</S><S sid ="82" ssid = "7">In addition  we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'49'", "'71'", "'92'", "'82'"]
'68'
'49'
'71'
'92'
'82'
['68', '49', '71', '92', '82']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "10">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid ="40" ssid = "12">Templates in the column below are expanded from the upper ones.</S><S sid ="43" ssid = "15">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S><S sid ="25" ssid = "21">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'58'", "'40'", "'43'", "'25'"]
'38'
'58'
'40'
'43'
'25'
['38', '58', '40', '43', '25']
parsed_discourse_facet ['method_citation']
<S sid ="93" ssid = "4">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid ="57" ssid = "8">It has a two-layer architecture  with a perceptron as the core and another linear model as the outside-layer.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="40" ssid = "12">Templates in the column below are expanded from the upper ones.</S><S sid ="17" ssid = "13">We will describe it in detail in Section 4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'93'", "'57'", "'136'", "'40'", "'17'"]
'93'
'57'
'136'
'40'
'17'
['93', '57', '136', '40', '17']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="33" ssid = "5">In following subsections  we describe the feature templates and the perceptron training algorithm.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="96" ssid = "7">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus  we randomly chosen 2  000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84  294 sentences)  then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'68'", "'33'", "'134'", "'96'"]
'101'
'68'
'33'
'134'
'96'
['101', '68', '33', '134', '96']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="89" ssid = "14">Function D derives the candidate result from the word-POS pair p and the candidate q at prior position of p.</S><S sid ="9" ssid = "5">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid ="79" ssid = "4">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid ="104" ssid = "15">According to the usual practice in syntactic analysis  we choose chapters 1  260 (18074 sentences) as training set  chapter 271  300 (348 sentences) as test set and chapter 301  325 (350 sentences) as development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'89'", "'9'", "'79'", "'104'"]
'68'
'89'
'9'
'79'
'104'
['68', '89', '9', '79', '104']
parsed_discourse_facet ['implication_citation']
<S sid ="125" ssid = "36">However unlike the three features  the word LM brings very tiny improvement.</S><S sid ="40" ssid = "12">Templates in the column below are expanded from the upper ones.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'40'", "'94'", "'2'", "'58'"]
'125'
'40'
'94'
'2'
'58'
['125', '40', '94', '2', '58']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="56" ssid = "7">To alleviate the drawbacks  we propose a cascaded linear model.</S><S sid ="61" ssid = "12">In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.</S><S sid ="93" ssid = "4">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid ="112" ssid = "23">Here the core perceptron was just the POS+ model in experiments above.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'56'", "'61'", "'93'", "'112'"]
'136'
'56'
'61'
'93'
'112'
['136', '56', '61', '93', '112']
parsed_discourse_facet ['method_citation']
<S sid ="99" ssid = "10">Then we trained LEX on each of the four corpora for 7 iterations.</S><S sid ="120" ssid = "31">Without the perceptron  the cascaded model (if we can still call it cascaded) performs poorly on both segmentation and Joint S&T.</S><S sid ="121" ssid = "32">Among other features  the 4-gram POS LM plays the most important role  removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&T.</S><S sid ="42" ssid = "14">As predications generated from such templates depend on the current character  we name these templates lexical-target.</S><S sid ="117" ssid = "28">Table 4 shows experiments results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'99'", "'120'", "'121'", "'42'", "'117'"]
'99'
'120'
'121'
'42'
'117'
['99', '120', '121', '42', '117']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="64" ssid = "15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S><S sid ="53" ssid = "4">Figure 2 shows the growing tendency of feature space with the introduction of these features as well as the character-based ones.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'134'", "'64'", "'53'", "'7'"]
'68'
'134'
'64'
'53'
'7'
['68', '134', '64', '53', '7']
parsed_discourse_facet ['results_citation']
<S sid ="3" ssid = "3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="84" ssid = "9">Algorithm 2 shows the decoding algorithm.</S><S sid ="18" ssid = "14">In this architecture  knowledge sources that are intractable to incorporate into the perceptron  can be easily incorporated into the outside linear model.</S><S sid ="92" ssid = "3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'136'", "'84'", "'18'", "'92'"]
'3'
'136'
'84'
'18'
'92'
['3', '136', '84', '18', '92']
parsed_discourse_facet ['implication_citation']
<S sid ="51" ssid = "2">Additional features most widely used are related to word or POS ngrams.</S><S sid ="13" ssid = "9">However  as such features are generated dynamically during the decoding procedure  two limitation arise: on the one hand  the amount of parameters increases rapidly  which is apt to overfit on training corpus; on the other hand  exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called averaged parameters (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="36" ssid = "8">All feature templates and their instances are shown in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'13'", "'7'", "'49'", "'36'"]
'51'
'13'
'7'
'49'
'36'
['51', '13', '7', '49', '36']
parsed_discourse_facet ['method_citation']
Length 0 input/ref/Task1/P08-1102_aakansha.csv
0.0 0.0 0.0





input/ref/Task1/A97-1014_sweta.csv
input/res/Task1/A97-1014.csv
parsing: input/ref/Task1/A97-1014_sweta.csv
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["168'"]
168'
['168']
parsed_discourse_facet ['method_citation']
<S sid="165" ssid="7">In addition the approach provides empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linea.rizations, and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.</S>
original cit marker offset is 0
new cit marker offset is 0



["165'"]
165'
['165']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="26">This amount of data suffices as training material to reliably assign the grammatical functions if the user determines the elements of a phrase and its type (step 1 of the list above).</S>
original cit marker offset is 0
new cit marker offset is 0



["145'"]
145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.<
original cit marker offset is 0
new cit marker offset is 0



["15'"]
15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="31">Apart from this rather technical problem, two further arguments speak against phrase structure as the structural pivot of the annotation scheme: Finally, the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies, which has proved tedious.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
<S sid="47" ssid="37">Argument structure can be represented in terms of unordered trees (with crossing branches).</S>
original cit marker offset is 0
new cit marker offset is 0



["47'"]
47'
['47']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
 <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["160'"]
160'
['160']
parsed_discourse_facet ['method_citation']
<S sid="166" ssid="8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
<S sid="166" ssid="8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="24">Sentences annotated in previous steps are used as training material for further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
  <S sid="71" ssid="16">However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["71'"]
71'
['71']
parsed_discourse_facet ['method_citation']
  <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A97-1014.csv
<S sid ="73" ssid = "18">While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.</S><S sid ="115" ssid = "28">3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here  where C stands for coordination  and VP for the actual category.</S><S sid ="135" ssid = "16">Figure 1 shows a screen dump of the tool.</S><S sid ="39" ssid = "29">Consider the German sentence (1) daran wird ihn Anna erkennen  &di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies  marked by co-references between traces and the corresponding nodes.</S><S sid ="84" ssid = "29">Consider (qui verbs where the subject of the infinitival VP is not realised syntactically  but co-referent with the subject or object. of the matrix equi verb: (3) er bat mich zu kommen he asked me to come (mich is the understood subject. of kommt n).</S>
original cit marker offset is 0
new cit marker offset is 0



["'73'", "'115'", "'135'", "'39'", "'84'"]
'73'
'115'
'135'
'39'
'84'
['73', '115', '135', '39', '84']
parsed_discourse_facet ['implication_citation']
<S sid ="110" ssid = "23">A similar convention ha.s been adopted for constructions in which scope ambiguities have syntactic effects but a one-to-one correspondence between scope and attachment. does not seem reasonable  cf. focus particles such as only or also.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="30" ssid = "20">Thus the context-free constituent backbone plays a pivotal role in the annotation scheme.</S><S sid ="113" ssid = "26">Since a precise structural description of non-constituent coordination would require a. rich inventory of incomplete phrase types  we have agreed on a sort of unde.rspe.cified representations: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'", "'80'", "'30'", "'113'", "'56'"]
'110'
'80'
'30'
'113'
'56'
['110', '80', '30', '113', '56']
parsed_discourse_facet ['implication_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S><S sid ="78" ssid = "23">Morphological information: Another set of labels represents morphological information.</S><S sid ="8" ssid = "5">A formalism complying with these requirements is described in section 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'56'", "'60'", "'78'", "'8'"]
'80'
'56'
'60'
'78'
'8'
['80', '56', '60', '78', '8']
parsed_discourse_facet ['results_citation']
<S sid ="0" ssid = "0">An Annotation Scheme for Free Word Order Languages</S><S sid ="18" ssid = "8">(Marcus et al.  1994).</S><S sid ="151" ssid = "32">For evaluation  the already annotated sentences were divided into two disjoint sets  one for training (90% of the corpus)  the other one for testing (10%).</S><S sid ="49" ssid = "39">Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.</S><S sid ="114" ssid = "27">Fig.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'18'", "'151'", "'49'", "'114'"]
'0'
'18'
'151'
'49'
'114'
['0', '18', '151', '49', '114']
parsed_discourse_facet ['method_citation']
<S sid ="65" ssid = "10">The tree resembles traditional constituent structures.</S><S sid ="62" ssid = "7">2.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="77" ssid = "22">Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'62'", "'80'", "'77'", "'5'"]
'65'
'62'
'80'
'77'
'5'
['65', '62', '80', '77', '5']
parsed_discourse_facet ['method_citation']
<S sid ="49" ssid = "39">Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="118" ssid = "31">An explicit coordinating conjunction need not be present.</S><S sid ="28" ssid = "18">(Bies et al.  1995)  (Sampson  1995)).</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'56'", "'118'", "'28'", "'80'"]
'49'
'56'
'118'
'28'
'80'
['49', '56', '118', '28', '80']
parsed_discourse_facet ['method_citation']
<S sid ="65" ssid = "10">The tree resembles traditional constituent structures.</S><S sid ="108" ssid = "21">However  full or partial disambiguation takes place in context  and the annotators do not consider unrealistic readings.</S><S sid ="96" ssid = "9">In (5)  either a lexical nominalisation rule for the adjective Gliickliche is stipulated  or the existence of an empty nominal head.</S><S sid ="86" ssid = "31">We call such additional edges secondary links and represent them as dotted lines  see fig.</S><S sid ="135" ssid = "16">Figure 1 shows a screen dump of the tool.</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'108'", "'96'", "'86'", "'135'"]
'65'
'108'
'96'
'86'
'135'
['65', '108', '96', '86', '135']
parsed_discourse_facet ['method_citation']
<S sid ="115" ssid = "28">3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here  where C stands for coordination  and VP for the actual category.</S><S sid ="77" ssid = "22">Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.</S><S sid ="109" ssid = "22">In addition  we have adopted a simple convention for those cases in which context information is insufficient  for total disambiguation: the highest possible attachment  site is chosen.</S><S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'77'", "'109'", "'60'", "'21'"]
'115'
'77'
'109'
'60'
'21'
['115', '77', '109', '60', '21']
parsed_discourse_facet ['method_citation']
<S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="73" ssid = "18">While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.</S><S sid ="86" ssid = "31">We call such additional edges secondary links and represent them as dotted lines  see fig.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S><S sid ="77" ssid = "22">Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'73'", "'86'", "'21'", "'77'"]
'56'
'73'
'86'
'21'
'77'
['56', '73', '86', '21', '77']
parsed_discourse_facet ['method_citation']
<S sid ="138" ssid = "19">This allows easy modification if needed.</S><S sid ="125" ssid = "6">The tool should also permit a convenient handling of node and edge labels.</S><S sid ="89" ssid = "2">However  some other standard analysts turn out to be problematic  mainly due to the partial  idealised character of competence grammars  which often marginalise or ignore such important. phenomena. as 'deficient' (e.g. headless) constructions  appositions  temporal expressions  etc.</S><S sid ="49" ssid = "39">Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.</S><S sid ="110" ssid = "23">A similar convention ha.s been adopted for constructions in which scope ambiguities have syntactic effects but a one-to-one correspondence between scope and attachment. does not seem reasonable  cf. focus particles such as only or also.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'", "'125'", "'89'", "'49'", "'110'"]
'138'
'125'
'89'
'49'
'110'
['138', '125', '89', '49', '110']
parsed_discourse_facet ['implication_citation']
<S sid ="147" ssid = "28">(Cutting et al.  1992) and (Feldweg  1995)).</S><S sid ="7" ssid = "4">On the basis of these considerations  we formulate several additional requirements.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="144" ssid = "25">We distinguish five degrees of automation: So far  about 1100 sentences of our corpus have been annotated.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'7'", "'80'", "'144'", "'21'"]
'147'
'7'
'80'
'144'
'21'
['147', '7', '80', '144', '21']
parsed_discourse_facet ['method_citation']
<S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S><S sid ="73" ssid = "18">While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.</S><S sid ="96" ssid = "9">In (5)  either a lexical nominalisation rule for the adjective Gliickliche is stipulated  or the existence of an empty nominal head.</S><S sid ="46" ssid = "36">These approaches provide an adequate explanation for several issues problematic for phrase-structure grammars (clause union  extraposition  diverse second-position phenomena).</S><S sid ="33" ssid = "23">(Lehmann et al.  1996)  (Marcus et al.  1994)  (Sampson  1995)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'73'", "'96'", "'46'", "'33'"]
'60'
'73'
'96'
'46'
'33'
['60', '73', '96', '46', '33']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "13">As the annotation scheme does not distinguish different bar levels or any similar intermediate categories  only a small set of node labels is needed (currently 16 tags  S  NP  AP ...).</S><S sid ="51" ssid = "41">This requirement speaks against the traditional sort of dependency trees  in which heads a re represented as non-terminal nodes  cf.</S><S sid ="109" ssid = "22">In addition  we have adopted a simple convention for those cases in which context information is insufficient  for total disambiguation: the highest possible attachment  site is chosen.</S><S sid ="11" ssid = "1">Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.</S><S sid ="21" ssid = "11">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'51'", "'109'", "'11'", "'21'"]
'68'
'51'
'109'
'11'
'21'
['68', '51', '109', '11', '21']
parsed_discourse_facet ['method_citation']
<S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="138" ssid = "19">This allows easy modification if needed.</S><S sid ="115" ssid = "28">3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here  where C stands for coordination  and VP for the actual category.</S><S sid ="112" ssid = "25">A problem for the rudimentary argument. structure representations is the use of incomplete structures in natural language  i.e. phenomena such as coordination and ellipsis.</S><S sid ="60" ssid = "5">Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'138'", "'115'", "'112'", "'60'"]
'56'
'138'
'115'
'112'
'60'
['56', '138', '115', '112', '60']
parsed_discourse_facet ['results_citation']
['In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.']
['Separable verb prefixes are labeled SVP.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", '(Bies et al.  1995)  (Sampson  1995)).', 'An explicit coordinating conjunction need not be present.', 'Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:91', 'F:0']
['In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.']
['However  full or partial disambiguation takes place in context  and the annotators do not consider unrealistic readings.', 'We call such additional edges secondary links and represent them as dotted lines  see fig.', 'The tree resembles traditional constituent structures.', 'Figure 1 shows a screen dump of the tool.', 'In (5)  either a lexical nominalisation rule for the adjective Gliickliche is stipulated  or the existence of an empty nominal head.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:91', 'F:0']
['These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.']
['Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.', 'We call such additional edges secondary links and represent them as dotted lines  see fig.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.', 'Disambiguation is based on human processing skills (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:21', 'F:0']
['Sentences annotated in previous steps are used as training material for further processing.']
['Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.', 'These approaches provide an adequate explanation for several issues problematic for phrase-structure grammars (clause union  extraposition  diverse second-position phenomena).', 'While in the first phase each annotator has to annotate structures as well as categories and functions  the refinement call be done separately for each representation level.', '(Lehmann et al.  1996)  (Marcus et al.  1994)  (Sampson  1995)).', 'In (5)  either a lexical nominalisation rule for the adjective Gliickliche is stipulated  or the existence of an empty nominal head.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:21', 'F:0']
['However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.']
['As the annotation scheme does not distinguish different bar levels or any similar intermediate categories  only a small set of node labels is needed (currently 16 tags  S  NP  AP ...).', 'Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.', 'In addition  we have adopted a simple convention for those cases in which context information is insufficient  for total disambiguation: the highest possible attachment  site is chosen.', 'This requirement speaks against the traditional sort of dependency trees  in which heads a re represented as non-terminal nodes  cf.', 'Disambiguation is based on human processing skills (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:28', 'F:0']
['For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).']
['This allows easy modification if needed.', "3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here  where C stands for coordination  and VP for the actual category.", "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.', 'A problem for the rudimentary argument. structure representations is the use of incomplete structures in natural language  i.e. phenomena such as coordination and ellipsis.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:55', 'F:0']
['This amount of data suffices as training material to reliably assign the grammatical functions if the user determines the elements of a phrase and its type (step 1 of the list above).']
['Syntactic categories  expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.', 'A formalism complying with these requirements is described in section 3.', 'Separable verb prefixes are labeled SVP.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'Morphological information: Another set of labels represents morphological information.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:136', 'F:0']
['Syntactically annotated corpora of German have been missing until now.']
['(Cutting et al.  1992) and (Feldweg  1995)).', 'Separable verb prefixes are labeled SVP.', 'We distinguish five degrees of automation: So far  about 1100 sentences of our corpus have been annotated.', 'On the basis of these considerations  we formulate several additional requirements.', 'Disambiguation is based on human processing skills (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:10', 'F:0']
['Syntactically annotated corpora of German have been missing until now.']
['A similar convention ha.s been adopted for constructions in which scope ambiguities have syntactic effects but a one-to-one correspondence between scope and attachment. does not seem reasonable  cf. focus particles such as only or also.', 'This allows easy modification if needed.', "However  some other standard analysts turn out to be problematic  mainly due to the partial  idealised character of competence grammars  which often marginalise or ignore such important. phenomena. as 'deficient' (e.g. headless) constructions  appositions  temporal expressions  etc.", 'The tool should also permit a convenient handling of node and edge labels.', 'Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:10', 'F:0']
['In addition the approach provides empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linea.rizations, and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.']
["We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'Separable verb prefixes are labeled SVP.', 'Thus the context-free constituent backbone plays a pivotal role in the annotation scheme.', 'A similar convention ha.s been adopted for constructions in which scope ambiguities have syntactic effects but a one-to-one correspondence between scope and attachment. does not seem reasonable  cf. focus particles such as only or also.', 'Since a precise structural description of non-constituent coordination would require a. rich inventory of incomplete phrase types  we have agreed on a sort of unde.rspe.cified representations: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links.']
['system', 'ROUGE-S*', 'Average_R:', '0.00072', '(95%-conf.int.', '0.00072', '-', '0.00072)']
['system', 'ROUGE-S*', 'Average_P:', '0.00615', '(95%-conf.int.', '0.00615', '-', '0.00615)']
['system', 'ROUGE-S*', 'Average_F:', '0.00129', '(95%-conf.int.', '0.00129', '-', '0.00129)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:325', 'F:2']
['Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.']
['For evaluation  the already annotated sentences were divided into two disjoint sets  one for training (90% of the corpus)  the other one for testing (10%).', 'An Annotation Scheme for Free Word Order Languages', 'Furthermore  the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions  e.g. every syntactic structure has a unique head.', 'Fig.', '(Marcus et al.  1994).']
['system', 'ROUGE-S*', 'Average_R:', '0.00476', '(95%-conf.int.', '0.00476', '-', '0.00476)']
['system', 'ROUGE-S*', 'Average_P:', '0.02857', '(95%-conf.int.', '0.02857', '-', '0.02857)']
['system', 'ROUGE-S*', 'Average_F:', '0.00816', '(95%-conf.int.', '0.00816', '-', '0.00816)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:105', 'F:3']
0.00315636360767 0.000498181813653 0.000859090901281





input/ref/Task1/A00-2030_vardha.csv
input/res/Task1/A00-2030.csv
parsing: input/ref/Task1/A00-2030_vardha.csv
 <S sid="11" ssid="1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="10">Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'"]
'50'
['50']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
 <S sid="106" ssid="3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
 <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
 <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="2">The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
 <S sid="3" ssid="1">Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'"]
'3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="15">Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'"]
'32'
['32']
parsed_discourse_facet ['method_citation']
 <S sid="10" ssid="8">Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="13">Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
    <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
 <S sid="26" ssid="9">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'"]
'26'
['26']
parsed_discourse_facet ['method_citation']
    <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="2">Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="102" ssid = "7">The results are summarized in Table 2.</S><S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'79'", "'82'", "'102'", "'56'"]
'68'
'79'
'82'
'102'
'56'
['68', '79', '82', '102', '56']
parsed_discourse_facet ['implication_citation']
<S sid ="65" ssid = "6">We illustrate the generation process by walking through a few of the steps of the parse shown in Figure 3.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="9" ssid = "7">Manually creating sourcespecific training data for syntax was not required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'96'", "'23'", "'88'", "'9'"]
'65'
'96'
'23'
'88'
'9'
['65', '96', '23', '88', '9']
parsed_discourse_facet ['implication_citation']
<S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'68'", "'36'", "'6'", "'96'"]
'88'
'68'
'36'
'6'
'96'
['88', '68', '36', '6', '96']
parsed_discourse_facet ['results_citation']
<S sid ="60" ssid = "1">In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'62'", "'13'", "'36'", "'105'"]
'60'
'62'
'13'
'36'
'105'
['60', '62', '13', '36', '105']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="47" ssid = "7">It soon became painfully obvious that this task could not be performed in the available time.</S><S sid ="73" ssid = "14">We now briefly summarize the probability structure of the model.</S><S sid ="97" ssid = "2">The evaluation results are summarized in Table 1.</S><S sid ="80" ssid = "2">However  because these estimates are too sparse to be relied upon  we use interpolated estimates consisting of mixtures of successively lowerorder estimates (as in Placeway et al. 1993).</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'47'", "'73'", "'97'", "'80'"]
'38'
'47'
'73'
'97'
'80'
['38', '47', '73', '97', '80']
parsed_discourse_facet ['method_citation']
<S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'", "'15'", "'45'", "'88'", "'56'"]
'79'
'15'
'45'
'88'
'56'
['79', '15', '45', '88', '56']
parsed_discourse_facet ['method_citation']
<S sid ="49" ssid = "9">By necessity  we adopted the strategy of hand marking only the semantics.</S><S sid ="70" ssid = "11">Post-modifier constituents for the PER/NP.</S><S sid ="69" ssid = "10">8.</S><S sid ="95" ssid = "14">The semantics  that is  the entities and relations  can then be directly extracted from these sentential trees.</S><S sid ="111" ssid = "3">The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies  either expressed or implied  of the Defense Advanced Research Projects Agency or the United States Government.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'70'", "'69'", "'95'", "'111'"]
'49'
'70'
'69'
'95'
'111'
['49', '70', '69', '95', '111']
parsed_discourse_facet ['method_citation']
<S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="4" ssid = "2">Yet  relatively few have embedded one of these algorithms in a task.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="67" ssid = "8">We pick up the derivation just after the topmost S and its head word  said  have been produced.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'79'", "'4'", "'82'", "'67'"]
'23'
'79'
'4'
'82'
'67'
['23', '79', '4', '82', '67']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "3">However  pipelined architectures suffer from a serious disadvantage: errors accumulate as they propagate through the pipeline.</S><S sid ="31" ssid = "14">If the single generalized model could then be extended to semantic analysis  all necessary sentence level processing would be contained in that model.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="84" ssid = "3">Although mathematically the model predicts tree elements in a top-down fashion  we search the space bottom-up using a chartbased search.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'31'", "'0'", "'38'", "'84'"]
'20'
'31'
'0'
'38'
'84'
['20', '31', '0', '38', '84']
parsed_discourse_facet ['method_citation']
<S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'38'", "'107'", "'104'", "'23'"]
'88'
'38'
'107'
'104'
'23'
['88', '38', '107', '104', '23']
parsed_discourse_facet ['implication_citation']
<S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="74" ssid = "15">The categories for head constituents  cl are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c _1  and the head word of their parent  wp.</S><S sid ="69" ssid = "10">8.</S><S sid ="111" ssid = "3">The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies  either expressed or implied  of the Defense Advanced Research Projects Agency or the United States Government.</S>
original cit marker offset is 0
new cit marker offset is 0



["'82'", "'45'", "'74'", "'69'", "'111'"]
'82'
'45'
'74'
'69'
'111'
['82', '45', '74', '69', '111']
parsed_discourse_facet ['method_citation']
<S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="102" ssid = "7">The results are summarized in Table 2.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'", "'102'", "'82'", "'62'", "'68'"]
'45'
'102'
'82'
'62'
'68'
['45', '102', '82', '62', '68']
parsed_discourse_facet ['method_citation']
<S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="97" ssid = "2">The evaluation results are summarized in Table 1.</S><S sid ="75" ssid = "16">Separate probabilities are maintained for left (pre) and right (post) modifiers: Part-of-speech tags  t    for modifiers are predicted based on the modifier  cm  the partof-speech tag of the head word  th  and the head word itself  wh: Head words  w for modifiers are predicted based on the modifier  cm  the part-of-speech tag of the modifier word   t the part-ofspeech tag of the head word   th  and the head word itself  wh: lAwmicm tm th wh)  e.g.</S><S sid ="95" ssid = "14">The semantics  that is  the entities and relations  can then be directly extracted from these sentential trees.</S><S sid ="59" ssid = "5">These labels serve to form a continuous chain between the relation and its argument.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'", "'97'", "'75'", "'95'", "'59'"]
'45'
'97'
'75'
'95'
'59'
['45', '97', '75', '95', '59']
parsed_discourse_facet ['method_citation']
<S sid ="76" ssid = "17">Finally  word features  fm  for modifiers are predicted based on the modifier  cm  the partof-speech tag of the modifier word   t the part-of-speech tag of the head word th  the head word itself  wh  and whether or not the modifier head word  w is known or unknown.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="112" ssid = "4">We thank Michael Collins of the University of Pennsylvania for his valuable suggestions.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'", "'36'", "'112'", "'101'", "'5'"]
'76'
'36'
'112'
'101'
'5'
['76', '36', '112', '101', '5']
parsed_discourse_facet ['results_citation']
<S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'68'", "'13'", "'38'", "'23'"]
'56'
'68'
'13'
'38'
'23'
['56', '68', '13', '38', '23']
parsed_discourse_facet ['implication_citation']
<S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'62'", "'36'", "'79'", "'107'"]
'51'
'62'
'36'
'79'
'107'
['51', '62', '36', '79', '107']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="49" ssid = "9">By necessity  we adopted the strategy of hand marking only the semantics.</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="39" ssid = "7">For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'49'", "'38'", "'5'", "'39'"]
'6'
'49'
'38'
'5'
'39'
['6', '49', '38', '5', '39']
parsed_discourse_facet ['results_citation']
<S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="31" ssid = "14">If the single generalized model could then be extended to semantic analysis  all necessary sentence level processing would be contained in that model.</S><S sid ="83" ssid = "2">More precisely  it must find the most likely augmented parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'57'", "'88'", "'31'", "'83'"]
'23'
'57'
'88'
'31'
'83'
['23', '57', '88', '31', '83']
parsed_discourse_facet ['aim_citation']
<S sid ="95" ssid = "14">The semantics  that is  the entities and relations  can then be directly extracted from these sentential trees.</S><S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="34" ssid = "2">In these trees  the standard TREEBANK structures are augmented to convey semantic information  that is  entities and relations.</S><S sid ="4" ssid = "2">Yet  relatively few have embedded one of these algorithms in a task.</S><S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'45'", "'34'", "'4'", "'6'"]
'95'
'45'
'34'
'4'
'6'
['95', '45', '34', '4', '6']
parsed_discourse_facet ['method_citation']
['In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['The next steps are to generate in order: In this case  there are none.', 'An integrated model can limit the propagation of errors by making all decisions jointly.', 'For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.', 'For example  in the phrase &quot;Lt. Cmdr.', 'Other labels indicate relations among entities.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:406', 'P:91', 'F:0']
['We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al.']
['Other labels indicate relations among entities.', 'By necessity  we adopted the strategy of hand marking only the semantics.', 'Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.', 'For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.', 'In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:153', 'F:0']
['In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['More precisely  it must find the most likely augmented parse tree.', 'If the single generalized model could then be extended to semantic analysis  all necessary sentence level processing would be contained in that model.', 'David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.', 'For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).', 'An integrated model can limit the propagation of errors by making all decisions jointly.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:91', 'F:0']
['We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.']
['It soon became painfully obvious that this task could not be performed in the available time.', 'However  because these estimates are too sparse to be relied upon  we use interpolated estimates consisting of mixtures of successively lowerorder estimates (as in Placeway et al. 1993).', 'Other labels indicate relations among entities.', 'We now briefly summarize the probability structure of the model.', 'The evaluation results are summarized in Table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:36', 'F:0']
['In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['For example  in the phrase &quot;Lt. Cmdr.', 'For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).', 'The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.', 'For each location  one must also give its type (city  province  county  body of water  etc.).', 'Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:91', 'F:0']
['In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['8.', 'The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies  either expressed or implied  of the Defense Advanced Research Projects Agency or the United States Government.', 'Post-modifier constituents for the PER/NP.', u'The semantics \u2014 that is  the entities and relations \u2014 can then be directly extracted from these sentential trees.', 'By necessity  we adopted the strategy of hand marking only the semantics.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:91', 'F:0']
['The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).']
['A Novel Use of Statistical Parsing to Extract Information from Text', 'If the single generalized model could then be extended to semantic analysis  all necessary sentence level processing would be contained in that model.', 'Other labels indicate relations among entities.', 'However  pipelined architectures suffer from a serious disadvantage: errors accumulate as they propagate through the pipeline.', 'Although mathematically the model predicts tree elements in a top-down fashion  we search the space bottom-up using a chartbased search.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:78', 'F:0']
['Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.']
['For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).', 'Other labels indicate relations among entities.', 'The semantic training corpus was produced by students according to a simple set of guidelines.', 'We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.', 'An integrated model can limit the propagation of errors by making all decisions jointly.']
['system', 'ROUGE-S*', 'Average_R:', '0.00097', '(95%-conf.int.', '0.00097', '-', '0.00097)']
['system', 'ROUGE-S*', 'Average_P:', '0.00395', '(95%-conf.int.', '0.00395', '-', '0.00395)']
['system', 'ROUGE-S*', 'Average_F:', '0.00155', '(95%-conf.int.', '0.00155', '-', '0.00155)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:253', 'F:1']
['Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.']
['In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.', 'The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.', u'The semantics \u2014 that is  the entities and relations \u2014 can then be directly extracted from these sentential trees.', 'Yet  relatively few have embedded one of these algorithms in a task.', 'In these trees  the standard TREEBANK structures are augmented to convey semantic information  that is  entities and relations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00111', '(95%-conf.int.', '0.00111', '-', '0.00111)']
['system', 'ROUGE-S*', 'Average_P:', '0.00362', '(95%-conf.int.', '0.00362', '-', '0.00362)']
['system', 'ROUGE-S*', 'Average_F:', '0.00170', '(95%-conf.int.', '0.00170', '-', '0.00170)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:276', 'F:1']
['Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.']
['The results are summarized in Table 2.', 'Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.', 'For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.', 'The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.', 'The next steps are to generate in order: In this case  there are none.']
['system', 'ROUGE-S*', 'Average_R:', '0.00189', '(95%-conf.int.', '0.00189', '-', '0.00189)']
['system', 'ROUGE-S*', 'Average_P:', '0.00395', '(95%-conf.int.', '0.00395', '-', '0.00395)']
['system', 'ROUGE-S*', 'Average_F:', '0.00256', '(95%-conf.int.', '0.00256', '-', '0.00256)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:528', 'P:253', 'F:1']
['We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.']
['For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.', 'To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.', 'The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.', 'Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.', 'The semantic training corpus was produced by students according to a simple set of guidelines.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:78', 'F:0']
['Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.']
[u'Finally  word features  fm  for modifiers are predicted based on the modifier  cm  the partof-speech tag of the modifier word   t\u201e\u201e the part-of-speech tag of the head word th  the head word itself  wh  and whether or not the modifier head word  w\u201e\u201e is known or unknown.', 'We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.', 'Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.', 'The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.', 'We thank Michael Collins of the University of Pennsylvania for his valuable suggestions.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:55', 'F:0']
['Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.']
['These labels serve to form a continuous chain between the relation and its argument.', u'Separate probabilities are maintained for left (pre) and right (post) modifiers: Part-of-speech tags  t    for modifiers are predicted based on the modifier  cm  the partof-speech tag of the head word  th  and the head word itself  wh: Head words  w\u201e\u201e for modifiers are predicted based on the modifier  cm  the part-of-speech tag of the modifier word   t\u201e\u201e the part-ofspeech tag of the head word   th  and the head word itself  wh: lAwmicm tm th wh)  e.g.', 'The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.', u'The semantics \u2014 that is  the entities and relations \u2014 can then be directly extracted from these sentential trees.', 'The evaluation results are summarized in Table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00037', '(95%-conf.int.', '0.00037', '-', '0.00037)']
['system', 'ROUGE-S*', 'Average_P:', '0.00735', '(95%-conf.int.', '0.00735', '-', '0.00735)']
['system', 'ROUGE-S*', 'Average_F:', '0.00070', '(95%-conf.int.', '0.00070', '-', '0.00070)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2701', 'P:136', 'F:1']
0.00145153845037 0.000333846151278 0.000500769226917





input/ref/Task1/P11-1061_swastika.csv
input/res/Task1/P11-1061.csv
parsing: input/ref/Task1/P11-1061_swastika.csv
    <S sid="144" ssid="7">For comparison, the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%, and goes up to 88.7% with a treebank dictionary.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
    <S sid="44" ssid="10">Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.</S
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
parsed_discourse_facet ['method_citation']
    <S sid="16" ssid="12">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (&#167;3), and then use graph label propagation to project syntactic information from English to the foreign language (&#167;4).</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="44" ssid="10">Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.</S
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
parsed_discourse_facet ['method_citation']
<S sid="110" ssid="10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S>
original cit marker offset is 0
new cit marker offset is 0



['110']
110
['110']
parsed_discourse_facet ['aim_citation']
    <S sid="115" ssid="15">The taggers were trained on datasets labeled with the universal tags.</S>
original cit marker offset is 0
new cit marker offset is 0



['115']
115
['115']
parsed_discourse_facet ['method_citation']
    <S sid="115" ssid="15">The taggers were trained on datasets labeled with the universal tags.</S>
original cit marker offset is 0
new cit marker offset is 0



['115']
115
['115']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['158']
158
['158']
parsed_discourse_facet ['result_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



['23']
23
['23']
parsed_discourse_facet ['result_citation']
<S sid="24" ssid="1">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['24']
24
['24']
parsed_discourse_facet ['aim_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['???']
???
['???']
parsed_discourse_facet ['method_citation']
    <S sid="16" ssid="12">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (&#167;3), and then use graph label propagation to project syntactic information from English to the foreign language (&#167;4).</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



['23']
23
['23']
parsed_discourse_facet ['result_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['158']
158
['158']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
<S sid="56" ssid="22">To define a similarity function between the English and the foreign vertices, we rely on high-confidence word alignments.</S>
original cit marker offset is 0
new cit marker offset is 0



['56']
56
['56']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



['161']
161
['161']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P11-1061.csv
<S sid ="14" ssid = "10">Our work is closest to that of Yarowsky and Ngai (2001)  but differs in two important ways.</S><S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S><S sid ="103" ssid = "3">The availability of these resources guided our selection of foreign languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'7'", "'56'", "'135'", "'103'"]
'14'
'7'
'56'
'135'
'103'
['14', '7', '56', '135', '103']
parsed_discourse_facet ['implication_citation']
<S sid ="75" ssid = "6">We use a squared loss to penalize neighboring vertices that have different label distributions: kqi  qjk2 = Ey(qi(y)  qj(y))2  and additionally regularize the label distributions towards the uniform distribution U over all possible labels Y.</S><S sid ="53" ssid = "19">Finally  note that while most feature concepts are lexicalized  others  such as the suffix concept  are not.</S><S sid ="34" ssid = "11">The following three sections elaborate these different stages is more detail.</S><S sid ="129" ssid = "29">Fortunately  performance was stable across various values  and we were able to use the same hyperparameters for all languages.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'53'", "'34'", "'129'", "'26'"]
'75'
'53'
'34'
'129'
'26'
['75', '53', '34', '129', '26']
parsed_discourse_facet ['implication_citation']
<S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="59" ssid = "25">So far the graph has been completely unlabeled.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'107'", "'59'", "'56'", "'39'"]
'1'
'107'
'59'
'56'
'39'
['1', '107', '59', '56', '39']
parsed_discourse_facet ['results_citation']
<S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : We describe how we choose  in 6.4.</S><S sid ="66" ssid = "32">In general  the neighborhoods can be more diverse and we allow a soft label distribution over the vertices.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'66'", "'107'", "'117'", "'135'"]
'83'
'66'
'107'
'117'
'135'
['83', '66', '107', '117', '135']
parsed_discourse_facet ['method_citation']
<S sid ="104" ssid = "4">For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi  2006; Nivre et al.  2007).</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="155" ssid = "18">Examining the word fidanzato for the No LP and With LP models is particularly instructive.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="98" ssid = "29">7).</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'", "'1'", "'155'", "'39'", "'98'"]
'104'
'1'
'155'
'39'
'98'
['104', '1', '155', '39', '98']
parsed_discourse_facet ['method_citation']
<S sid ="143" ssid = "6">Overall  it gives improvements ranging from 1.1% for German to 14.7% for Italian  for an average improvement of 8.3% over the unsupervised feature-HMM model.</S><S sid ="96" ssid = "27">The function A : F * C maps from the language specific fine-grained tagset F to the coarser universal tagset C and is described in detail in 6.2: Note that when tx(y) = 1 the feature value is 0 and has no effect on the model  while its value is oc when tx(y) = 0 and constrains the HMMs state space.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="68" ssid = "34">In the label propagation stage  we propagate the automatic English tags to the aligned Italian trigram types  followed by further propagation solely among the Italian vertices. the Italian vertices are connected to an automatically labeled English vertex.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'96'", "'97'", "'68'", "'26'"]
'143'
'96'
'97'
'68'
'26'
['143', '96', '97', '68', '26']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="129" ssid = "29">Fortunately  performance was stable across various values  and we were able to use the same hyperparameters for all languages.</S><S sid ="108" ssid = "8">However  we needed to restrict ourselves to these languages in order to be able to evaluate the performance of our approach.</S><S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'26'", "'129'", "'108'", "'37'"]
'7'
'26'
'129'
'108'
'37'
['7', '26', '129', '108', '37']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S><S sid ="122" ssid = "22">For each language  we took the same number of sentences from the bitext as there are in its treebank  and trained a supervised feature-HMM.</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="31" ssid = "8">By aggregating the POS labels of the English tokens to types  we can generate label distributions for the English vertices.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'110'", "'122'", "'19'", "'31'"]
'39'
'110'
'122'
'19'
'31'
['39', '110', '122', '19', '31']
parsed_discourse_facet ['method_citation']
<S sid ="73" ssid = "4">Note that because we extracted only high-confidence alignments  many foreign vertices will not be connected to any English vertices.</S><S sid ="24" ssid = "1">The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.</S><S sid ="150" ssid = "13">For all languages  the vocabulary sizes increase by several thousand words.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="139" ssid = "2">As expected  the vanilla HMM trained with EM performs the worst.</S>
original cit marker offset is 0
new cit marker offset is 0



["'73'", "'24'", "'150'", "'39'", "'139'"]
'73'
'24'
'150'
'39'
'139'
['73', '24', '150', '39', '139']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="45" ssid = "11">Furthermore  we do not connect the English vertices to each other  but only to foreign language vertices.4 The graph vertices are extracted from the different sides of a parallel corpus (De  Df) and an additional unlabeled monolingual foreign corpus Ff  which will be used later for training.</S><S sid ="69" ssid = "35">Label propagation is used to propagate these tags inwards and results in tag distributions for the middle word of each Italian trigram.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'45'", "'69'", "'135'", "'110'"]
'107'
'45'
'69'
'135'
'110'
['107', '45', '69', '135', '110']
parsed_discourse_facet ['implication_citation']
<S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : We describe how we choose  in 6.4.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'83'", "'25'", "'4'"]
'37'
'1'
'83'
'25'
'4'
['37', '1', '83', '25', '4']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="120" ssid = "20">We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="44" ssid = "10">Because all English vertices are going to be labeled  we do not need to disambiguate them by embedding them in trigrams.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'120'", "'117'", "'44'", "'26'"]
'54'
'120'
'117'
'44'
'26'
['54', '120', '117', '44', '26']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="47" ssid = "13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S><S sid ="161" ssid = "4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections  and bridge the gap between purely supervised and unsupervised POS tagging models.</S><S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'117'", "'47'", "'161'", "'7'"]
'39'
'117'
'47'
'161'
'7'
['39', '117', '47', '161', '7']
parsed_discourse_facet ['method_citation']
<S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="32" ssid = "9">Label propagation can then be used to transfer the labels to the peripheral foreign vertices (i.e. the ones adjacent to the English vertices) first  and then among all of the foreign vertices (4).</S><S sid ="86" ssid = "17">For a sentence x and a state sequence z  a first order Markov model defines a distribution: (9) where Val(X) corresponds to the entire vocabulary.</S><S sid ="132" ssid = "32">When extracting the vector t  used to compute the constraint feature from the graph  we tried three threshold values for r (see Eq.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'117'", "'32'", "'86'", "'132'"]
'37'
'117'
'32'
'86'
'132'
['37', '117', '32', '86', '132']
parsed_discourse_facet ['results_citation']
<S sid ="163" ssid = "2">We would also like to thank Amarnag Subramanya for helping us with the implementation of label propagation and Shankar Kumar for access to the parallel data.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S><S sid ="139" ssid = "2">As expected  the vanilla HMM trained with EM performs the worst.</S><S sid ="58" ssid = "24">Based on these high-confidence alignments we can extract tuples of the form [u H v]  where u is a foreign trigram type  whose middle word aligns to an English word type v. Our bilingual similarity function then sets the edge weights in proportion to these tuple counts.</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : We describe how we choose  in 6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'110'", "'139'", "'58'", "'83'"]
'163'
'110'
'139'
'58'
'83'
['163', '110', '139', '58', '83']
parsed_discourse_facet ['implication_citation']
<S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="134" ssid = "34">Because we dont have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S><S sid ="72" ssid = "3">In the first stage  we run a single step of label propagation  which transfers the label distributions from the English vertices to the connected foreign language vertices (say  Vf) at the periphery of the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'117'", "'134'", "'110'", "'72'"]
'37'
'117'
'134'
'110'
'72'
['37', '117', '134', '110', '72']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="104" ssid = "4">For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi  2006; Nivre et al.  2007).</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'39'", "'26'", "'104'", "'135'"]
'54'
'39'
'26'
'104'
'135'
['54', '39', '26', '104', '135']
parsed_discourse_facet ['results_citation']
<S sid ="33" ssid = "10">The POS distributions over the foreign trigram types are used as features to learn a better unsupervised POS tagger (5).</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S><S sid ="90" ssid = "21">All features were conjoined with the state z.</S><S sid ="96" ssid = "27">The function A : F * C maps from the language specific fine-grained tagset F to the coarser universal tagset C and is described in detail in 6.2: Note that when tx(y) = 1 the feature value is 0 and has no effect on the model  while its value is oc when tx(y) = 0 and constrains the HMMs state space.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'117'", "'4'", "'90'", "'96'"]
'33'
'117'
'4'
'90'
'96'
['33', '117', '4', '90', '96']
parsed_discourse_facet ['aim_citation']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.']
['Based on these high-confidence alignments we can extract tuples of the form [u H v]  where u is a foreign trigram type  whose middle word aligns to an English word type v. Our bilingual similarity function then sets the edge weights in proportion to these tuple counts.', u'We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value \u03c4: We describe how we choose \u03c4 in \xa76.4.', 'As expected  the vanilla HMM trained with EM performs the worst.', 'We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.', 'We would also like to thank Amarnag Subramanya for helping us with the implementation of label propagation and Shankar Kumar for access to the parallel data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00264', '(95%-conf.int.', '0.00264', '-', '0.00264)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.00514', '(95%-conf.int.', '0.00514', '-', '0.00514)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:55', 'F:5']
['For comparison, the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%, and goes up to 88.7% with a treebank dictionary.']
['The availability of these resources guided our selection of foreign languages.', 'For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.', 'To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.', 'Our work is closest to that of Yarowsky and Ngai (2001)  but differs in two important ways.', 'However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:136', 'F:0']
['To this end, we construct a bilingual graph over word types to establish a connection between the two languages (&#167;3), and then use graph label propagation to project syntactic information from English to the foreign language (&#167;4).']
['Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).', 'They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.', 'In other words  the set of hidden states F was chosen to be the fine set of treebank tags.', 'However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.', 'Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections  and bridge the gap between purely supervised and unsupervised POS tagging models.']
['system', 'ROUGE-S*', 'Average_R:', '0.00615', '(95%-conf.int.', '0.00615', '-', '0.00615)']
['system', 'ROUGE-S*', 'Average_P:', '0.06061', '(95%-conf.int.', '0.06061', '-', '0.06061)']
['system', 'ROUGE-S*', 'Average_F:', '0.01116', '(95%-conf.int.', '0.01116', '-', '0.01116)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:231', 'F:14']
['Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).']
['The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.', 'For all languages  the vocabulary sizes increase by several thousand words.', 'They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.', 'Note that because we extracted only high-confidence alignments  many foreign vertices will not be connected to any English vertices.', 'As expected  the vanilla HMM trained with EM performs the worst.']
['system', 'ROUGE-S*', 'Average_R:', '0.00726', '(95%-conf.int.', '0.00726', '-', '0.00726)']
['system', 'ROUGE-S*', 'Average_P:', '0.02419', '(95%-conf.int.', '0.02419', '-', '0.02419)']
['system', 'ROUGE-S*', 'Average_F:', '0.01117', '(95%-conf.int.', '0.01117', '-', '0.01117)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:496', 'F:12']
['To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.']
['We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.', 'Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.', 'In other words  the set of hidden states F was chosen to be the fine set of treebank tags.', u'As discussed in more detail in \u0e22\u0e073  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', 'Because all English vertices are going to be labeled  we do not need to disambiguate them by embedding them in trigrams.']
['system', 'ROUGE-S*', 'Average_R:', '0.00492', '(95%-conf.int.', '0.00492', '-', '0.00492)']
['system', 'ROUGE-S*', 'Average_P:', '0.01705', '(95%-conf.int.', '0.01705', '-', '0.01705)']
['system', 'ROUGE-S*', 'Average_F:', '0.00763', '(95%-conf.int.', '0.00763', '-', '0.00763)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:528', 'F:9']
['The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.']
['Label propagation is used to propagate these tags inwards and results in tag distributions for the middle word of each Italian trigram.', 'Furthermore  we do not connect the English vertices to each other  but only to foreign language vertices.4 The graph vertices are extracted from the different sides of a parallel corpus (De  Df) and an additional unlabeled monolingual foreign corpus Ff  which will be used later for training.', 'Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.', 'For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.', 'We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.']
['system', 'ROUGE-S*', 'Average_R:', '0.00437', '(95%-conf.int.', '0.00437', '-', '0.00437)']
['system', 'ROUGE-S*', 'Average_P:', '0.08791', '(95%-conf.int.', '0.08791', '-', '0.08791)']
['system', 'ROUGE-S*', 'Average_F:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:91', 'F:8']
['We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.']
['We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.', '7).', 'They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.', u'Examining the word fidanzato for the \u201cNo LP\u201d and \u201cWith LP\u201d models is particularly instructive.', 'For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi  2006; Nivre et al.  2007).']
['system', 'ROUGE-S*', 'Average_R:', '0.00159', '(95%-conf.int.', '0.00159', '-', '0.00159)']
['system', 'ROUGE-S*', 'Average_P:', '0.14286', '(95%-conf.int.', '0.14286', '-', '0.14286)']
['system', 'ROUGE-S*', 'Average_F:', '0.00314', '(95%-conf.int.', '0.00314', '-', '0.00314)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:21', 'F:3']
['Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.']
['Fortunately  performance was stable across various values  and we were able to use the same hyperparameters for all languages.', u'We use a squared loss to penalize neighboring vertices that have different label distributions: kqi \u2212 qjk2 = Ey(qi(y) \u2212 qj(y))2  and additionally regularize the label distributions towards the uniform distribution U over all possible labels Y.', u'As discussed in more detail in \xa73  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', 'Finally  note that while most feature concepts are lexicalized  others  such as the suffix concept  are not.', 'The following three sections elaborate these different stages is more detail.']
['system', 'ROUGE-S*', 'Average_R:', '0.00195', '(95%-conf.int.', '0.00195', '-', '0.00195)']
['system', 'ROUGE-S*', 'Average_P:', '0.20000', '(95%-conf.int.', '0.20000', '-', '0.20000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00386', '(95%-conf.int.', '0.00386', '-', '0.00386)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:15', 'F:3']
['Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.']
[u'The POS distributions over the foreign trigram types are used as features to learn a better unsupervised POS tagger (\xa75).', 'All features were conjoined with the state z.', 'In other words  the set of hidden states F was chosen to be the fine set of treebank tags.', 'Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.', u'The function A : F \u2014* C maps from the language specific fine-grained tagset F to the coarser universal tagset C and is described in detail in \xa76.2: Note that when tx(y) = 1 the feature value is 0 and has no effect on the model  while its value is \u2212oc when tx(y) = 0 and constrains the HMM\u2019s state space.']
['system', 'ROUGE-S*', 'Average_R:', '0.00509', '(95%-conf.int.', '0.00509', '-', '0.00509)']
['system', 'ROUGE-S*', 'Average_P:', '0.09559', '(95%-conf.int.', '0.09559', '-', '0.09559)']
['system', 'ROUGE-S*', 'Average_F:', '0.00966', '(95%-conf.int.', '0.00966', '-', '0.00966)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:136', 'F:13']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.']
['Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.', 'By aggregating the POS labels of the English tokens to types  we can generate label distributions for the English vertices.', 'They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.', 'For each language  we took the same number of sentences from the bitext as there are in its treebank  and trained a supervised feature-HMM.', 'We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.']
['system', 'ROUGE-S*', 'Average_R:', '0.00186', '(95%-conf.int.', '0.00186', '-', '0.00186)']
['system', 'ROUGE-S*', 'Average_P:', '0.07273', '(95%-conf.int.', '0.07273', '-', '0.07273)']
['system', 'ROUGE-S*', 'Average_F:', '0.00364', '(95%-conf.int.', '0.00364', '-', '0.00364)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:55', 'F:4']
['The taggers were trained on datasets labeled with the universal tags.']
['Fortunately  performance was stable across various values  and we were able to use the same hyperparameters for all languages.', u'As discussed in more detail in \u0e22\u0e073  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', 'Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.', 'However  we needed to restrict ourselves to these languages in order to be able to evaluate the performance of our approach.', 'However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:15', 'F:0']
0.0719863629819 0.00325727269766 0.00579363631097





input/ref/Task1/W99-0623_vardha.csv
input/res/Task1/W99-0623.csv
parsing: input/ref/Task1/W99-0623_vardha.csv
    <S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="117" ssid="46">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'117'"]
'117'
['117']
parsed_discourse_facet ['method_citation']
<S sid="72" ssid="1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
    <S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'"]
'120'
['120']
parsed_discourse_facet ['method_citation']
  <S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'139'"]
'139'
['139']
parsed_discourse_facet ['method_citation']
 <S sid="84" ssid="13">The first shows how constituent features and context do not help in deciding which parser to trust.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'"]
'84'
['84']
parsed_discourse_facet ['method_citation']
    <S sid="76" ssid="5">The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'"]
'76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'"]
'38'
['38']
parsed_discourse_facet ['method_citation']
  <S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
 <S sid="72" ssid="1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
    <S sid="87" ssid="16">It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'"]
'87'
['87']
parsed_discourse_facet ['method_citation']
    <S sid="51" ssid="37">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
    <S sid="79" ssid="8">Precision is the portion of hypothesized constituents that are correct and recall is the portion of the Treebank constituents that are hypothesized.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
  <S sid="27" ssid="13">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'"]
'27'
['27']
parsed_discourse_facet ['method_citation']
 <S sid="77" ssid="6">Each parse is converted into a set of constituents represented as a tuples: (label, start, end).</S>
original cit marker offset is 0
new cit marker offset is 0



["'77'"]
'77'
['77']
parsed_discourse_facet ['method_citation']
  <S sid="11" ssid="7">Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
  <S sid="116" ssid="45">The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'116'"]
'116'
['116']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0623.csv
<S sid ="34" ssid = "20">Mi(c) is a binary function returning t when parser i (from among the k parsers) suggests constituent c should be in the parse.</S><S sid ="106" ssid = "35">In each figure the upper graph shows the isolated constituent precision and the bottom graph shows the corresponding number of hypothesized constituents.</S><S sid ="66" ssid = "52">Each decision determines the inclusion or exclusion of a candidate constituent.</S><S sid ="65" ssid = "51">We model each parse as the decisions made to create it  and model those decisions as independent events.</S><S sid ="2" ssid = "2">Two general approaches are presented and two combination techniques are described for each approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'106'", "'66'", "'65'", "'2'"]
'34'
'106'
'66'
'65'
'2'
['34', '106', '66', '65', '2']
Error in Discourse Facet
<S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'15'", "'130'", "'57'", "'50'"]
'18'
'15'
'130'
'57'
'50'
['18', '15', '130', '57', '50']
Error in Discourse Facet
<S sid ="122" ssid = "51">All of these systems were run on data that was not seen during their development.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="24" ssid = "10">We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="107" ssid = "36">Again we notice that the isolated constituent precision is larger than 0.5 only in those partitions that contain very few samples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'122'", "'11'", "'24'", "'61'", "'107'"]
'122'
'11'
'24'
'61'
'107'
['122', '11', '24', '61', '107']
Error in Discourse Facet
<S sid ="92" ssid = "21">While we cannot prove there are no such useful features on which one should condition trust  we can give some insight into why the features we explored offered no gain.</S><S sid ="111" ssid = "40">The first row represents the average accuracy of the three parsers we combine.</S><S sid ="104" ssid = "33">In the cases where isolated constituent precision is larger than 0.5 the affected portion of the hypotheses is negligible.</S><S sid ="42" ssid = "28">Call the crossing constituents A and B.</S><S sid ="117" ssid = "46">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'111'", "'104'", "'42'", "'117'"]
'92'
'111'
'104'
'42'
'117'
['92', '111', '104', '42', '117']
Error in Discourse Facet
<S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="39" ssid = "25">There are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.</S><S sid ="63" ssid = "49">The probabilistic version of this procedure is straightforward: We once again assume independence among our various member parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'139'", "'11'", "'39'", "'63'"]
'15'
'139'
'11'
'39'
'63'
['15', '139', '11', '39', '63']
Error in Discourse Facet
<S sid ="39" ssid = "25">There are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.</S><S sid ="86" ssid = "15">Finally we show the combining techniques degrade very little when a poor parser is added to the set.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a nave Bayes classifier to determine which constituents to include in the parse.</S><S sid ="17" ssid = "3">The substructures that are unanimously hypothesized by the parsers should be preserved after combination  and the combination technique should not foolishly create substructures for which there is no supporting evidence.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'86'", "'27'", "'17'", "'61'"]
'39'
'86'
'27'
'17'
'61'
['39', '86', '27', '17', '61']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="24" ssid = "10">We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'72'", "'21'", "'24'", "'139'"]
'112'
'72'
'21'
'24'
'139'
['112', '72', '21', '24', '139']
Error in Discourse Facet
<S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S><S sid ="96" ssid = "25">We call such a constituent an isolated constituent.</S><S sid ="132" ssid = "61">The results of this experiment can be seen in Table 5.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and nave Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="125" ssid = "54">The constituent voting and nave Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'139'", "'96'", "'132'", "'38'", "'125'"]
'139'
'96'
'132'
'38'
'125'
['139', '96', '132', '38', '125']
Error in Discourse Facet
<S sid ="43" ssid = "29">A receives a votes  and B receives b votes.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="84" ssid = "13">The first shows how constituent features and context do not help in deciding which parser to trust.</S><S sid ="93" ssid = "22">Because we are working with only three parsers  the only situation in which context will help us is when it can indicate we should choose to believe a single parser that disagrees with the majority hypothesis instead of the majority hypothesis itself.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'15'", "'52'", "'84'", "'93'"]
'43'
'15'
'52'
'84'
'93'
['43', '15', '52', '84', '93']
Error in Discourse Facet
<S sid ="101" ssid = "30">We show the results of three of the experiments we conducted to measure isolated constituent precision under various partitioning schemes.</S><S sid ="126" ssid = "55">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a nave Bayes classifier to determine which constituents to include in the parse.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="43" ssid = "29">A receives a votes  and B receives b votes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'126'", "'27'", "'52'", "'43'"]
'101'
'126'
'27'
'52'
'43'
['101', '126', '27', '52', '43']
Error in Discourse Facet
<S sid ="126" ssid = "55">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S><S sid ="142" ssid = "4">Both of the switching techniques  as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.</S><S sid ="111" ssid = "40">The first row represents the average accuracy of the three parsers we combine.</S><S sid ="43" ssid = "29">A receives a votes  and B receives b votes.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'142'", "'111'", "'43'", "'52'"]
'126'
'142'
'111'
'43'
'52'
['126', '142', '111', '43', '52']
Error in Discourse Facet
<S sid ="101" ssid = "30">We show the results of three of the experiments we conducted to measure isolated constituent precision under various partitioning schemes.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="106" ssid = "35">In each figure the upper graph shows the isolated constituent precision and the bottom graph shows the corresponding number of hypothesized constituents.</S><S sid ="22" ssid = "8">If enough parsers suggest that a particular constituent belongs in the parse  we include it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'50'", "'15'", "'106'", "'22'"]
'101'
'50'
'15'
'106'
'22'
['101', '50', '15', '106', '22']
Error in Discourse Facet
<S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S><S sid ="111" ssid = "40">The first row represents the average accuracy of the three parsers we combine.</S><S sid ="30" ssid = "16">This is equivalent to the assumption used in probability estimation for nave Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.</S><S sid ="28" ssid = "14">The development of a nave Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a nave Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'111'", "'30'", "'28'", "'27'"]
'50'
'111'
'30'
'28'
'27'
['50', '111', '30', '28', '27']
Error in Discourse Facet
<S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="140" ssid = "2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S><S sid ="131" ssid = "60">It was then tested on section 22 of the Treebank in conjunction with the other parsers.</S><S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="34" ssid = "20">Mi(c) is a binary function returning t when parser i (from among the k parsers) suggests constituent c should be in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'140'", "'131'", "'112'", "'34'"]
'61'
'140'
'131'
'112'
'34'
['61', '140', '131', '112', '34']
Error in Discourse Facet
<S sid ="87" ssid = "16">It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S><S sid ="37" ssid = "23">Here NO counts the number of hypothesized constituents in the development set that match the binary predicate specified as an argument.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="100" ssid = "29">When this metric is less than 0.5  we expect to incur more errors' than we will remove by adding those constituents to the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'", "'57'", "'37'", "'11'", "'100'"]
'87'
'57'
'37'
'11'
'100'
['87', '57', '37', '11', '100']
Error in Discourse Facet
<S sid ="83" ssid = "12">We performed three experiments to evaluate our techniques.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="108" ssid = "37">From this we see that a finer-grained model for parser combination  at least for the features we have examined  will not give us any additional power.</S><S sid ="107" ssid = "36">Again we notice that the isolated constituent precision is larger than 0.5 only in those partitions that contain very few samples.</S><S sid ="118" ssid = "47">The maximum precision oracle is an upper bound on the possible gain we can achieve by parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'11'", "'108'", "'107'", "'118'"]
'83'
'11'
'108'
'107'
'118'
['83', '11', '108', '107', '118']
Error in Discourse Facet
<S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="30" ssid = "16">This is equivalent to the assumption used in probability estimation for nave Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="113" ssid = "42">The next two rows are results of oracle experiments.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'30'", "'52'", "'113'", "'50'"]
'18'
'30'
'52'
'113'
'50'
['18', '30', '52', '113', '50']
Error in Discourse Facet
['The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.']
['These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.', u'This is equivalent to the assumption used in probability estimation for na\xc3\xafve Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.', 'There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.', 'The next two rows are results of oracle experiments.', 'This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:66', 'F:0']
['Each parse is converted into a set of constituents represented as a tuples: (label, start, end).']
['Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).', "When this metric is less than 0.5  we expect to incur more errors' than we will remove by adding those constituents to the parse.", 'The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.', 'Here NO counts the number of hypothesized constituents in the development set that match the binary predicate specified as an argument.', 'It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00181', '(95%-conf.int.', '0.00181', '-', '0.00181)']
['system', 'ROUGE-S*', 'Average_P:', '0.08333', '(95%-conf.int.', '0.08333', '-', '0.08333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00355', '(95%-conf.int.', '0.00355', '-', '0.00355)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:36', 'F:3']
['One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure.']
['In each figure the upper graph shows the isolated constituent precision and the bottom graph shows the corresponding number of hypothesized constituents.', 'We show the results of three of the experiments we conducted to measure isolated constituent precision under various partitioning schemes.', 'We are interested in combining the substructures of the input parses to produce a better parse.', 'There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.', 'If enough parsers suggest that a particular constituent belongs in the parse  we include it.']
['system', 'ROUGE-S*', 'Average_R:', '0.00101', '(95%-conf.int.', '0.00101', '-', '0.00101)']
['system', 'ROUGE-S*', 'Average_P:', '0.00735', '(95%-conf.int.', '0.00735', '-', '0.00735)']
['system', 'ROUGE-S*', 'Average_F:', '0.00178', '(95%-conf.int.', '0.00178', '-', '0.00178)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:136', 'F:1']
['Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.']
['These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.', 'The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.', 'The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.', 'We are interested in combining the substructures of the input parses to produce a better parse.', 'There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:36', 'F:0']
['The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.']
['We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.', 'We have presented two general approaches to studying parser combination: parser switching and parse hybridization.', 'The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.', "One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.", "The second row is the accuracy of the best of the three parsers.'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00386', '(95%-conf.int.', '0.00386', '-', '0.00386)']
['system', 'ROUGE-S*', 'Average_P:', '0.07273', '(95%-conf.int.', '0.07273', '-', '0.07273)']
['system', 'ROUGE-S*', 'Average_F:', '0.00734', '(95%-conf.int.', '0.00734', '-', '0.00734)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:55', 'F:4']
['Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.']
['The results of this experiment can be seen in Table 5.', 'We have presented two general approaches to studying parser combination: parser switching and parse hybridization.', u'Under certain conditions the constituent voting and na\xefve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.', u'The constituent voting and na\xefve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.', 'We call such a constituent an isolated constituent.']
['system', 'ROUGE-S*', 'Average_R:', '0.09713', '(95%-conf.int.', '0.09713', '-', '0.09713)']
['system', 'ROUGE-S*', 'Average_P:', '0.87500', '(95%-conf.int.', '0.87500', '-', '0.87500)']
['system', 'ROUGE-S*', 'Average_F:', '0.17485', '(95%-conf.int.', '0.17485', '-', '0.17485)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:120', 'F:105']
['The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.']
['Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).', 'We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.', 'All of these systems were run on data that was not seen during their development.', 'Again we notice that the isolated constituent precision is larger than 0.5 only in those partitions that contain very few samples.', 'We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.']
['system', 'ROUGE-S*', 'Average_R:', '0.00089', '(95%-conf.int.', '0.00089', '-', '0.00089)']
['system', 'ROUGE-S*', 'Average_P:', '0.00735', '(95%-conf.int.', '0.00735', '-', '0.00735)']
['system', 'ROUGE-S*', 'Average_F:', '0.00158', '(95%-conf.int.', '0.00158', '-', '0.00158)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:136', 'F:1']
['It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.']
['The first row represents the average accuracy of the three parsers we combine.', 'Both of the switching techniques  as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.', 'A receives a votes  and B receives b votes.', 'Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.', 'This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.']
['system', 'ROUGE-S*', 'Average_R:', '0.00427', '(95%-conf.int.', '0.00427', '-', '0.00427)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00802', '(95%-conf.int.', '0.00802', '-', '0.00802)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:45', 'F:3']
0.139053748262 0.0136212498297 0.024639999692





input/ref/Task1/D09-1092_swastika.csv
input/res/Task1/D09-1092.csv
parsing: input/ref/Task1/D09-1092_swastika.csv
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="114" ssid="63">Ideally, the &#8220;glue&#8221; documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.</S>
original cit marker offset is 0
new cit marker offset is 0



['114']
114
['114']
parsed_discourse_facet ['aim_citation']
<S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



['138']
138
['138']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



['55']
55
['55']
parsed_discourse_facet ['method_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['192']
192
['192']
parsed_discourse_facet ['result_citation']
<S sid="119" ssid="68">The lower the divergence, the more similar the distributions are to each other.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['result_citation']
<S sid="148" ssid="97">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="163" ssid="112">Performance continues to improve with longer documents, most likely due to better topic inference.</S>
original cit marker offset is 0
new cit marker offset is 0



['163']
163
['163']
parsed_discourse_facet ['result_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="148" ssid="97">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['192']
192
['192']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['method_citation']
    <S sid="10" ssid="6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="176" ssid = "10">We dropped all articles in non-English languages that did not link to an English article.</S><S sid ="145" ssid = "94">For T = 800  the top English and Spanish words in 448 topics were exact translations of one another.</S><S sid ="155" ssid = "104">Finally  for each pair of languages (query and target) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'18'", "'176'", "'145'", "'155'"]
'147'
'18'
'176'
'145'
'155'
['147', '18', '176', '145', '155']
parsed_discourse_facet ['implication_citation']
<S sid ="63" ssid = "12">Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.</S><S sid ="156" ssid = "105">We use both Jensen-Shannon divergence and cosine distance.</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="64" ssid = "13">The first topic contains words relating to the European Central Bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'156'", "'32'", "'23'", "'64'"]
'63'
'156'
'32'
'23'
'64'
['63', '156', '32', '23', '64']
parsed_discourse_facet ['implication_citation']
<S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S><S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="182" ssid = "16">As with EuroParl  we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.</S><S sid ="190" ssid = "24">Meanwhile  interest in actors and actresses (center) is consistent across all languages.</S><S sid ="104" ssid = "53">This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'30'", "'182'", "'190'", "'104'"]
'38'
'30'
'182'
'190'
'104'
['38', '30', '182', '190', '104']
parsed_discourse_facet ['results_citation']
<S sid ="164" ssid = "113">Results vary by language.</S><S sid ="160" ssid = "109">It is important to note that the length of documents matters.</S><S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of objective. The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S><S sid ="104" ssid = "53">This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.</S><S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of 1  .</S>
original cit marker offset is 0
new cit marker offset is 0



["'164'", "'160'", "'69'", "'104'", "'43'"]
'164'
'160'
'69'
'104'
'43'
['164', '160', '69', '104', '43']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S><S sid ="172" ssid = "6">Second  because comparable texts may not use exactly the same topics  it becomes crucially important to be able to characterize differences in topic prevalence at the document level (do different languages have different perspectives on the same article?) and at the language-wide level (which topics do particular languages focus on?).</S><S sid ="63" ssid = "12">Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'23'", "'38'", "'172'", "'63'"]
'32'
'23'
'38'
'172'
'63'
['32', '23', '38', '172', '63']
parsed_discourse_facet ['method_citation']
<S sid ="135" ssid = "84">We do not consider multi-word terms.</S><S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of objective. The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S><S sid ="175" ssid = "9">We preprocessed the data by removing tables  references  images and info-boxes.</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="176" ssid = "10">We dropped all articles in non-English languages that did not link to an English article.</S>
original cit marker offset is 0
new cit marker offset is 0



["'135'", "'69'", "'175'", "'32'", "'176'"]
'135'
'69'
'175'
'32'
'176'
['135', '69', '175', '32', '176']
parsed_discourse_facet ['method_citation']
<S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="97" ssid = "46">Rather  these results are intended as a quantitative analysis of the difference between the two models.</S><S sid ="40" ssid = "6">Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter  and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl  P(wl  |zl l) = 11n lwl |zl .</S><S sid ="21" ssid = "17">The second corpus  Wikipedia articles in twelve languages  contains sets of documents that are not translations of one another  but are very likely to be about similar concepts.</S><S sid ="75" ssid = "24">We compute histograms of these maximum topic probabilities for T  {50 100  200  400  800}.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'97'", "'40'", "'21'", "'75'"]
'147'
'97'
'40'
'21'
'75'
['147', '97', '40', '21', '75']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="184" ssid = "18">Interestingly  we find that almost all languages in our corpus  including several pairs that have historically been in conflict  show average JS divergences of between approximately 0.08 and 0.12 for T = 400  consistent with our findings for EuroParl translations.</S><S sid ="87" ssid = "36">The probability of previously unseen held-out document tuples given these estimates can then be computed.</S><S sid ="145" ssid = "94">For T = 800  the top English and Spanish words in 448 topics were exact translations of one another.</S><S sid ="178" ssid = "12">For efficiency  we truncated each article to the nearest word after 1000 characters and dropped the 50 most common word types in each language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'184'", "'87'", "'145'", "'178'"]
'32'
'184'
'87'
'145'
'178'
['32', '184', '87', '145', '178']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="48" ssid = "14">.</S><S sid ="124" ssid = "73">At p = 0.01 (1% glue documents)  German and French both include words relating to Russia  while the English and Italian word distributions appear locally consistent but unrelated to Russia.</S><S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'48'", "'124'", "'1'", "'38'"]
'30'
'48'
'124'
'1'
'38'
['30', '48', '124', '1', '38']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of objective. The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="9" ssid = "5">In this paper  we present the polylingual topic model (PLTM).</S><S sid ="40" ssid = "6">Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter  and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl  P(wl  |zl l) = 11n lwl |zl .</S><S sid ="186" ssid = "20">Although we find that if Wikipedia contains an article on a particular subject in some language  the article will tend to be topically similar to the articles about that subject in other languages  we also find that across the whole collection different languages emphasize topics to different extents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'23'", "'9'", "'40'", "'186'"]
'69'
'23'
'9'
'40'
'186'
['69', '23', '9', '40', '186']
parsed_discourse_facet ['implication_citation']
<S sid ="46" ssid = "12">  L and m from P(1  ...   L  m  |W'  ) or by evaluating a point estimate.</S><S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="72" ssid = "21">To answer this question  we compute the posterior probability of each topic in each tuple under the trained model.</S><S sid ="28" ssid = "4">We take a simpler approach that is more suitable for topically similar document tuples (where documents are not direct translations of one another) in more than two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'30'", "'32'", "'72'", "'28'"]
'46'
'30'
'32'
'72'
'28'
['46', '30', '32', '72', '28']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual synsets by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="45" ssid = "11">.</S><S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="67" ssid = "16">(Interestingly  all languages except Greek and Finnish use closely related words for youth or young in a separate topic.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'131'", "'45'", "'59'", "'67'"]
'30'
'131'
'45'
'59'
'67'
['30', '131', '45', '59', '67']
parsed_discourse_facet ['method_citation']
<S sid ="155" ssid = "104">Finally  for each pair of languages (query and target) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S><S sid ="29" ssid = "5">A recent extended abstract  developed concurrently by Ni et al. (Ni et al.  2009)  discusses a multilingual topic model similar to the one presented here.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="173" ssid = "7">We downloaded XML copies of all Wikipedia articles in twelve different languages: Welsh  German  Greek  English  Farsi  Finnish  French  Hebrew  Italian  Polish  Russian and Turkish.</S><S sid ="103" ssid = "52">PLTM topics therefore have a higher granularity  i.e.  they are more specific.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'", "'29'", "'19'", "'173'", "'103'"]
'155'
'29'
'19'
'173'
'103'
['155', '29', '19', '173', '103']
parsed_discourse_facet ['method_citation']
<S sid ="135" ssid = "84">We do not consider multi-word terms.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual synsets by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="89" ssid = "38">Analytically calculating the probability of a set of held-out document tuples given 1  ...   L and m is intractable  due to the summation over an exponential number of topic assignments for these held-out documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'135'", "'131'", "'147'", "'1'", "'89'"]
'135'
'131'
'147'
'1'
'89'
['135', '131', '147', '1', '89']
parsed_discourse_facet ['results_citation']
<S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of 1  .</S><S sid ="136" ssid = "85">We expect that simple analysis of topic assignments for sequential words would yield such collocations  but we leave this for future work.</S><S sid ="156" ssid = "105">We use both Jensen-Shannon divergence and cosine distance.</S><S sid ="45" ssid = "11">.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'136'", "'156'", "'45'", "'23'"]
'43'
'136'
'156'
'45'
'23'
['43', '136', '156', '45', '23']
parsed_discourse_facet ['implication_citation']
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual synsets by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="84" ssid = "33">As the number of topics increases  greater variability in topic distributions causes divergence to increase.</S><S sid ="63" ssid = "12">Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.</S><S sid ="175" ssid = "9">We preprocessed the data by removing tables  references  images and info-boxes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'147'", "'84'", "'63'", "'175'"]
'131'
'147'
'84'
'63'
'175'
['131', '147', '84', '63', '175']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of 1  .</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="175" ssid = "9">We preprocessed the data by removing tables  references  images and info-boxes.</S><S sid ="53" ssid = "2">In this case  we can be confident that the topic distribution is genuinely shared across all languages.</S><S sid ="193" ssid = "2">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'32'", "'175'", "'53'", "'193'"]
'43'
'32'
'175'
'53'
'193'
['43', '32', '175', '53', '193']
parsed_discourse_facet ['results_citation']
<S sid ="97" ssid = "46">Rather  these results are intended as a quantitative analysis of the difference between the two models.</S><S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="135" ssid = "84">We do not consider multi-word terms.</S><S sid ="103" ssid = "52">PLTM topics therefore have a higher granularity  i.e.  they are more specific.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'52'", "'30'", "'135'", "'103'"]
'97'
'52'
'30'
'135'
'103'
['97', '52', '30', '135', '103']
parsed_discourse_facet ['aim_citation']
<S sid ="14" ssid = "10">Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.</S><S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="190" ssid = "24">Meanwhile  interest in actors and actresses (center) is consistent across all languages.</S><S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'59'", "'190'", "'1'", "'31'"]
'14'
'59'
'190'
'1'
'31'
['14', '59', '190', '1', '31']
parsed_discourse_facet ['method_citation']
['In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models.']
['Results vary by language.', u'English and the Romance languages use only singular and plural versions of \u201cobjective.\u201d The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.', 'It is important to note that the length of documents matters.', u'These tasks can either be accomplished by averaging over samples of \u03a61  .', 'This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00097', '(95%-conf.int.', '0.00097', '-', '0.00097)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00185', '(95%-conf.int.', '0.00185', '-', '0.00185)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:45', 'F:1']
['We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.']
['As with EuroParl  we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.', 'This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.', 'Meanwhile  interest in actors and actresses (center) is consistent across all languages.', 'This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.', 'However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:78', 'F:0']
['We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).']
['Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.', 'They also provide little analysis of the differences between polylingual and single-language topic models.', 'The remaining collection consists of over 121 million words.', 'Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.', 'Meanwhile  interest in actors and actresses (center) is consistent across all languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00664', '(95%-conf.int.', '0.00664', '-', '0.00664)']
['system', 'ROUGE-S*', 'Average_P:', '0.05000', '(95%-conf.int.', '0.05000', '-', '0.05000)']
['system', 'ROUGE-S*', 'Average_F:', '0.01173', '(95%-conf.int.', '0.01173', '-', '0.01173)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:120', 'F:6']
['Ideally, the &#8220;glue&#8221; documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.']
['Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.', 'The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.', 'Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.', 'The first topic contains words relating to the European Central Bank.', 'We use both Jensen-Shannon divergence and cosine distance.']
['system', 'ROUGE-S*', 'Average_R:', '0.00816', '(95%-conf.int.', '0.00816', '-', '0.00816)']
['system', 'ROUGE-S*', 'Average_P:', '0.08333', '(95%-conf.int.', '0.08333', '-', '0.08333)']
['system', 'ROUGE-S*', 'Average_F:', '0.01487', '(95%-conf.int.', '0.01487', '-', '0.01487)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:120', 'F:10']
['The lower the divergence, the more similar the distributions are to each other.']
['These aligned document pairs could then be fed into standard machine translation systems as training data.', u'We compute histograms of these maximum topic probabilities for T \u2208 {50 100  200  400  800}.', u'Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter \u03b1 and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl \u223c P(wl  |zl \u03a6l) = 11n \u03c6lwl |zl .', 'Rather  these results are intended as a quantitative analysis of the difference between the two models.', 'The second corpus  Wikipedia articles in twelve languages  contains sets of documents that are not translations of one another  but are very likely to be about similar concepts.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:6', 'F:0']
['The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.']
['Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.', 'Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.', 'This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.', 'Second  because comparable texts may not use exactly the same topics  it becomes crucially important to be able to characterize differences in topic prevalence at the document level (do different languages have different perspectives on the same article?) and at the language-wide level (which topics do particular languages focus on?).', 'The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00047', '(95%-conf.int.', '0.00047', '-', '0.00047)']
['system', 'ROUGE-S*', 'Average_P:', '0.00526', '(95%-conf.int.', '0.00526', '-', '0.00526)']
['system', 'ROUGE-S*', 'Average_F:', '0.00086', '(95%-conf.int.', '0.00086', '-', '0.00086)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:190', 'F:1']
['To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.']
['Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.', 'These aligned document pairs could then be fed into standard machine translation systems as training data.', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', u'Analytically calculating the probability of a set of held-out document tuples given \u03a61  ...   \u03a6L and \u03b1m is intractable  due to the summation over an exponential number of topic assignments for these held-out documents.', 'We do not consider multi-word terms.']
['system', 'ROUGE-S*', 'Average_R:', '0.02572', '(95%-conf.int.', '0.02572', '-', '0.02572)']
['system', 'ROUGE-S*', 'Average_P:', '0.20952', '(95%-conf.int.', '0.20952', '-', '0.20952)']
['system', 'ROUGE-S*', 'Average_F:', '0.04581', '(95%-conf.int.', '0.04581', '-', '0.04581)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:210', 'F:44']
['Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.']
['.', 'The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.', 'We use both Jensen-Shannon divergence and cosine distance.', u'These tasks can either be accomplished by averaging over samples of \xce\xa61  .', 'We expect that simple analysis of topic assignments for sequential words would yield such collocations  but we leave this for future work.']
['system', 'ROUGE-S*', 'Average_R:', '0.00357', '(95%-conf.int.', '0.00357', '-', '0.00357)']
['system', 'ROUGE-S*', 'Average_P:', '0.00866', '(95%-conf.int.', '0.00866', '-', '0.00866)']
['system', 'ROUGE-S*', 'Average_F:', '0.00505', '(95%-conf.int.', '0.00505', '-', '0.00505)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:231', 'F:2']
['We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.']
['These aligned document pairs could then be fed into standard machine translation systems as training data.', u'Finally  for each pair of languages (\u201cquery\u201d and \u201ctarget\u201d) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.', 'For T = 800  the top English and Spanish words in 448 topics were exact translations of one another.', 'We dropped all articles in non-English languages that did not link to an English article.', 'In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.']
['system', 'ROUGE-S*', 'Average_R:', '0.00067', '(95%-conf.int.', '0.00067', '-', '0.00067)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00131', '(95%-conf.int.', '0.00131', '-', '0.00131)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:45', 'F:1']
['We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.', 'We preprocessed the data by removing tables  references  images and info-boxes.', 'We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.', u'These tasks can either be accomplished by averaging over samples of \xce\xa61  .', 'In this case  we can be confident that the topic distribution is genuinely shared across all languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.01932', '(95%-conf.int.', '0.01932', '-', '0.01932)']
['system', 'ROUGE-S*', 'Average_P:', '0.44444', '(95%-conf.int.', '0.44444', '-', '0.44444)']
['system', 'ROUGE-S*', 'Average_F:', '0.03704', '(95%-conf.int.', '0.03704', '-', '0.03704)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:45', 'F:20']
0.0845649991544 0.00655199993448 0.0118519998815





input/ref/Task1/D09-1092_sweta.csv
input/res/Task1/D09-1092.csv
parsing: input/ref/Task1/D09-1092_sweta.csv
 <S sid="32" ssid="8">Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="5">Additionally, PLTM assumes that each &#8220;topic&#8221; consists of a set of discrete distributions over words&#8212;one for each language l = 1, ... , L. In other words, rather than using a single set of topics &#934; = {&#966;1, ... , &#966;T}, as in LDA, there are L sets of language-specific topics, &#934;1, ... , &#934;L, each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter &#946;l.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



["138'"]
138'
['138']
parsed_discourse_facet ['method_citation']
 <S sid="196" ssid="5">When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>
original cit marker offset is 0
new cit marker offset is 0



["196'"]
196'
['196']
parsed_discourse_facet ['method_citation']
<S sid="111" ssid="60">In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts &#8211; i.e., we put each of these documents in a single-document tuple.</S>
original cit marker offset is 0
new cit marker offset is 0



["111'"]
111'
['111']
parsed_discourse_facet ['method_citation']
<S sid="128" ssid="77">Although the PLTM is clearly not a substitute for a machine translation system&#8212;it has no way to represent syntax or even multi-word phrases&#8212;it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.</S>
original cit marker offset is 0
new cit marker offset is 0



["128'"]
128'
['128']
parsed_discourse_facet ['method_citation']
<S sid="122" ssid="71">Divergence drops significantly when the proportion of &#8220;glue&#8221; tuples increases from 0.01 to 0.25.</S>
original cit marker offset is 0
new cit marker offset is 0



["122'"]
122'
['122']
parsed_discourse_facet ['method_citation']
 <S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
 <S sid="110" ssid="59">Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S>
original cit marker offset is 0
new cit marker offset is 0



["110'"]
110'
['110']
parsed_discourse_facet ['method_citation']
<S sid="30" ssid="6">However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["30'"]
30'
['30']
parsed_discourse_facet ['method_citation']
    <S sid="146" ssid="95">In addition to enhancing lexicons by aligning topic-specific vocabulary, PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["146'"]
146'
['146']
parsed_discourse_facet ['method_citation']
<S sid="77" ssid="26">Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.</
original cit marker offset is 0
new cit marker offset is 0



["77'"]
77'
['77']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="105">We use both Jensen-Shannon divergence and cosine distance.</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="7">They also provide little analysis of the differences between polylingual and single-language topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>
original cit marker offset is 0
new cit marker offset is 0



["19'"]
19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="131" ssid="80">We evaluate sets of high-probability words in each topic and multilingual &#8220;synsets&#8221; by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["131'"]
131'
['131']
parsed_discourse_facet ['method_citation']
<S sid="196" ssid="5">When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>
original cit marker offset is 0
new cit marker offset is 0



["196'"]
196'
['196']
parsed_discourse_facet ['method_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["192'"]
192'
['192']
parsed_discourse_facet ['method_citation']
<S sid="195" ssid="4">Additionally, PLTM can support the creation of bilingual lexica for low resource language pairs, providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["195'"]
195'
['195']
parsed_discourse_facet ['method_citation']
<S sid="6" ssid="2">Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["6'"]
6'
['6']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="18" ssid = "14">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid ="176" ssid = "10">We dropped all articles in non-English languages that did not link to an English article.</S><S sid ="145" ssid = "94">For T = 800  the top English and Spanish words in 448 topics were exact translations of one another.</S><S sid ="155" ssid = "104">Finally  for each pair of languages (query and target) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'18'", "'176'", "'145'", "'155'"]
'147'
'18'
'176'
'145'
'155'
['147', '18', '176', '145', '155']
parsed_discourse_facet ['implication_citation']
<S sid ="63" ssid = "12">Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.</S><S sid ="156" ssid = "105">We use both Jensen-Shannon divergence and cosine distance.</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="64" ssid = "13">The first topic contains words relating to the European Central Bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'156'", "'32'", "'23'", "'64'"]
'63'
'156'
'32'
'23'
'64'
['63', '156', '32', '23', '64']
parsed_discourse_facet ['implication_citation']
<S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S><S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="182" ssid = "16">As with EuroParl  we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.</S><S sid ="190" ssid = "24">Meanwhile  interest in actors and actresses (center) is consistent across all languages.</S><S sid ="104" ssid = "53">This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'30'", "'182'", "'190'", "'104'"]
'38'
'30'
'182'
'190'
'104'
['38', '30', '182', '190', '104']
parsed_discourse_facet ['results_citation']
<S sid ="164" ssid = "113">Results vary by language.</S><S sid ="160" ssid = "109">It is important to note that the length of documents matters.</S><S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of objective. The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S><S sid ="104" ssid = "53">This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.</S><S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of 1  .</S>
original cit marker offset is 0
new cit marker offset is 0



["'164'", "'160'", "'69'", "'104'", "'43'"]
'164'
'160'
'69'
'104'
'43'
['164', '160', '69', '104', '43']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S><S sid ="172" ssid = "6">Second  because comparable texts may not use exactly the same topics  it becomes crucially important to be able to characterize differences in topic prevalence at the document level (do different languages have different perspectives on the same article?) and at the language-wide level (which topics do particular languages focus on?).</S><S sid ="63" ssid = "12">Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'23'", "'38'", "'172'", "'63'"]
'32'
'23'
'38'
'172'
'63'
['32', '23', '38', '172', '63']
parsed_discourse_facet ['method_citation']
<S sid ="135" ssid = "84">We do not consider multi-word terms.</S><S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of objective. The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S><S sid ="175" ssid = "9">We preprocessed the data by removing tables  references  images and info-boxes.</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="176" ssid = "10">We dropped all articles in non-English languages that did not link to an English article.</S>
original cit marker offset is 0
new cit marker offset is 0



["'135'", "'69'", "'175'", "'32'", "'176'"]
'135'
'69'
'175'
'32'
'176'
['135', '69', '175', '32', '176']
parsed_discourse_facet ['method_citation']
<S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="97" ssid = "46">Rather  these results are intended as a quantitative analysis of the difference between the two models.</S><S sid ="40" ssid = "6">Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter  and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl  P(wl  |zl l) = 11n lwl |zl .</S><S sid ="21" ssid = "17">The second corpus  Wikipedia articles in twelve languages  contains sets of documents that are not translations of one another  but are very likely to be about similar concepts.</S><S sid ="75" ssid = "24">We compute histograms of these maximum topic probabilities for T  {50 100  200  400  800}.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'97'", "'40'", "'21'", "'75'"]
'147'
'97'
'40'
'21'
'75'
['147', '97', '40', '21', '75']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="184" ssid = "18">Interestingly  we find that almost all languages in our corpus  including several pairs that have historically been in conflict  show average JS divergences of between approximately 0.08 and 0.12 for T = 400  consistent with our findings for EuroParl translations.</S><S sid ="87" ssid = "36">The probability of previously unseen held-out document tuples given these estimates can then be computed.</S><S sid ="145" ssid = "94">For T = 800  the top English and Spanish words in 448 topics were exact translations of one another.</S><S sid ="178" ssid = "12">For efficiency  we truncated each article to the nearest word after 1000 characters and dropped the 50 most common word types in each language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'184'", "'87'", "'145'", "'178'"]
'32'
'184'
'87'
'145'
'178'
['32', '184', '87', '145', '178']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="48" ssid = "14">.</S><S sid ="124" ssid = "73">At p = 0.01 (1% glue documents)  German and French both include words relating to Russia  while the English and Italian word distributions appear locally consistent but unrelated to Russia.</S><S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'48'", "'124'", "'1'", "'38'"]
'30'
'48'
'124'
'1'
'38'
['30', '48', '124', '1', '38']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of objective. The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid ="9" ssid = "5">In this paper  we present the polylingual topic model (PLTM).</S><S sid ="40" ssid = "6">Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter  and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl  P(wl  |zl l) = 11n lwl |zl .</S><S sid ="186" ssid = "20">Although we find that if Wikipedia contains an article on a particular subject in some language  the article will tend to be topically similar to the articles about that subject in other languages  we also find that across the whole collection different languages emphasize topics to different extents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'23'", "'9'", "'40'", "'186'"]
'69'
'23'
'9'
'40'
'186'
['69', '23', '9', '40', '186']
parsed_discourse_facet ['implication_citation']
<S sid ="46" ssid = "12">  L and m from P(1  ...   L  m  |W'  ) or by evaluating a point estimate.</S><S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="72" ssid = "21">To answer this question  we compute the posterior probability of each topic in each tuple under the trained model.</S><S sid ="28" ssid = "4">We take a simpler approach that is more suitable for topically similar document tuples (where documents are not direct translations of one another) in more than two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'30'", "'32'", "'72'", "'28'"]
'46'
'30'
'32'
'72'
'28'
['46', '30', '32', '72', '28']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual synsets by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="45" ssid = "11">.</S><S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="67" ssid = "16">(Interestingly  all languages except Greek and Finnish use closely related words for youth or young in a separate topic.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'131'", "'45'", "'59'", "'67'"]
'30'
'131'
'45'
'59'
'67'
['30', '131', '45', '59', '67']
parsed_discourse_facet ['method_citation']
<S sid ="155" ssid = "104">Finally  for each pair of languages (query and target) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S><S sid ="29" ssid = "5">A recent extended abstract  developed concurrently by Ni et al. (Ni et al.  2009)  discusses a multilingual topic model similar to the one presented here.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid ="173" ssid = "7">We downloaded XML copies of all Wikipedia articles in twelve different languages: Welsh  German  Greek  English  Farsi  Finnish  French  Hebrew  Italian  Polish  Russian and Turkish.</S><S sid ="103" ssid = "52">PLTM topics therefore have a higher granularity  i.e.  they are more specific.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'", "'29'", "'19'", "'173'", "'103'"]
'155'
'29'
'19'
'173'
'103'
['155', '29', '19', '173', '103']
parsed_discourse_facet ['method_citation']
<S sid ="135" ssid = "84">We do not consider multi-word terms.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual synsets by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="89" ssid = "38">Analytically calculating the probability of a set of held-out document tuples given 1  ...   L and m is intractable  due to the summation over an exponential number of topic assignments for these held-out documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'135'", "'131'", "'147'", "'1'", "'89'"]
'135'
'131'
'147'
'1'
'89'
['135', '131', '147', '1', '89']
parsed_discourse_facet ['results_citation']
<S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of 1  .</S><S sid ="136" ssid = "85">We expect that simple analysis of topic assignments for sequential words would yield such collocations  but we leave this for future work.</S><S sid ="156" ssid = "105">We use both Jensen-Shannon divergence and cosine distance.</S><S sid ="45" ssid = "11">.</S><S sid ="23" ssid = "19">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'136'", "'156'", "'45'", "'23'"]
'43'
'136'
'156'
'45'
'23'
['43', '136', '156', '45', '23']
parsed_discourse_facet ['implication_citation']
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual synsets by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="84" ssid = "33">As the number of topics increases  greater variability in topic distributions causes divergence to increase.</S><S sid ="63" ssid = "12">Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.</S><S sid ="175" ssid = "9">We preprocessed the data by removing tables  references  images and info-boxes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'147'", "'84'", "'63'", "'175'"]
'131'
'147'
'84'
'63'
'175'
['131', '147', '84', '63', '175']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of 1  .</S><S sid ="32" ssid = "8">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid ="175" ssid = "9">We preprocessed the data by removing tables  references  images and info-boxes.</S><S sid ="53" ssid = "2">In this case  we can be confident that the topic distribution is genuinely shared across all languages.</S><S sid ="193" ssid = "2">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'32'", "'175'", "'53'", "'193'"]
'43'
'32'
'175'
'53'
'193'
['43', '32', '175', '53', '193']
parsed_discourse_facet ['results_citation']
<S sid ="97" ssid = "46">Rather  these results are intended as a quantitative analysis of the difference between the two models.</S><S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="30" ssid = "6">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid ="135" ssid = "84">We do not consider multi-word terms.</S><S sid ="103" ssid = "52">PLTM topics therefore have a higher granularity  i.e.  they are more specific.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'52'", "'30'", "'135'", "'103'"]
'97'
'52'
'30'
'135'
'103'
['97', '52', '30', '135', '103']
parsed_discourse_facet ['aim_citation']
<S sid ="14" ssid = "10">Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.</S><S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="190" ssid = "24">Meanwhile  interest in actors and actresses (center) is consistent across all languages.</S><S sid ="1" ssid = "1">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid ="31" ssid = "7">They also provide little analysis of the differences between polylingual and single-language topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'59'", "'190'", "'1'", "'31'"]
'14'
'59'
'190'
'1'
'31'
['14', '59', '190', '1', '31']
parsed_discourse_facet ['method_citation']
['When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.']
['Results vary by language.', u'English and the Romance languages use only singular and plural versions of \u201cobjective.\u201d The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.', 'It is important to note that the length of documents matters.', u'These tasks can either be accomplished by averaging over samples of \u03a61  .', 'This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00097', '(95%-conf.int.', '0.00097', '-', '0.00097)']
['system', 'ROUGE-S*', 'Average_P:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00173', '(95%-conf.int.', '0.00173', '-', '0.00173)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:120', 'F:1']
['We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.']
['As with EuroParl  we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.', 'This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.', 'Meanwhile  interest in actors and actresses (center) is consistent across all languages.', 'This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.', 'However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:78', 'F:0']
['Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).']
['Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.', 'They also provide little analysis of the differences between polylingual and single-language topic models.', 'The remaining collection consists of over 121 million words.', 'Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.', 'Meanwhile  interest in actors and actresses (center) is consistent across all languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.02658', '(95%-conf.int.', '0.02658', '-', '0.02658)']
['system', 'ROUGE-S*', 'Average_P:', '0.03810', '(95%-conf.int.', '0.03810', '-', '0.03810)']
['system', 'ROUGE-S*', 'Average_F:', '0.03131', '(95%-conf.int.', '0.03131', '-', '0.03131)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:630', 'F:24']
['Additionally, PLTM assumes that each &#8220;topic&#8221; consists of a set of discrete distributions over words&#8212;one for each language l = 1, ... , L. In other words, rather than using a single set of topics &#934; = {&#966;1, ... , &#966;T}, as in LDA, there are L sets of language-specific topics, &#934;1, ... , &#934;L, each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter &#946;l.']
['Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.', 'The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.', 'Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.', 'The first topic contains words relating to the European Central Bank.', 'We use both Jensen-Shannon divergence and cosine distance.']
['system', 'ROUGE-S*', 'Average_R:', '0.01633', '(95%-conf.int.', '0.01633', '-', '0.01633)']
['system', 'ROUGE-S*', 'Average_P:', '0.02845', '(95%-conf.int.', '0.02845', '-', '0.02845)']
['system', 'ROUGE-S*', 'Average_F:', '0.02075', '(95%-conf.int.', '0.02075', '-', '0.02075)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:703', 'F:20']
['Divergence drops significantly when the proportion of &#8220;glue&#8221; tuples increases from 0.01 to 0.25.']
['These aligned document pairs could then be fed into standard machine translation systems as training data.', u'We compute histograms of these maximum topic probabilities for T \u2208 {50 100  200  400  800}.', u'Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter \u03b1 and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl \u223c P(wl  |zl \u03a6l) = 11n \u03c6lwl |zl .', 'Rather  these results are intended as a quantitative analysis of the difference between the two models.', 'The second corpus  Wikipedia articles in twelve languages  contains sets of documents that are not translations of one another  but are very likely to be about similar concepts.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:78', 'F:0']
['In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts &#8211; i.e., we put each of these documents in a single-document tuple.']
['Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.', 'Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.', 'This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.', 'Second  because comparable texts may not use exactly the same topics  it becomes crucially important to be able to characterize differences in topic prevalence at the document level (do different languages have different perspectives on the same article?) and at the language-wide level (which topics do particular languages focus on?).', 'The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00466', '(95%-conf.int.', '0.00466', '-', '0.00466)']
['system', 'ROUGE-S*', 'Average_P:', '0.05848', '(95%-conf.int.', '0.05848', '-', '0.05848)']
['system', 'ROUGE-S*', 'Average_F:', '0.00864', '(95%-conf.int.', '0.00864', '-', '0.00864)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:171', 'F:10']
['We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.']
['Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.', 'These aligned document pairs could then be fed into standard machine translation systems as training data.', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', u'Analytically calculating the probability of a set of held-out document tuples given \u03a61  ...   \u03a6L and \u03b1m is intractable  due to the summation over an exponential number of topic assignments for these held-out documents.', 'We do not consider multi-word terms.']
['system', 'ROUGE-S*', 'Average_R:', '0.00468', '(95%-conf.int.', '0.00468', '-', '0.00468)']
['system', 'ROUGE-S*', 'Average_P:', '0.08791', '(95%-conf.int.', '0.08791', '-', '0.08791)']
['system', 'ROUGE-S*', 'Average_F:', '0.00888', '(95%-conf.int.', '0.00888', '-', '0.00888)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:91', 'F:8']
['We evaluate sets of high-probability words in each topic and multilingual &#8220;synsets&#8221; by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).']
['.', 'The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.', 'We use both Jensen-Shannon divergence and cosine distance.', u'These tasks can either be accomplished by averaging over samples of \xce\xa61  .', 'We expect that simple analysis of topic assignments for sequential words would yield such collocations  but we leave this for future work.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:171', 'F:0']
['Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.']
['These aligned document pairs could then be fed into standard machine translation systems as training data.', u'Finally  for each pair of languages (\u201cquery\u201d and \u201ctarget\u201d) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.', 'For T = 800  the top English and Spanish words in 448 topics were exact translations of one another.', 'We dropped all articles in non-English languages that did not link to an English article.', 'In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.']
['system', 'ROUGE-S*', 'Average_R:', '0.00135', '(95%-conf.int.', '0.00135', '-', '0.00135)']
['system', 'ROUGE-S*', 'Average_P:', '0.01667', '(95%-conf.int.', '0.01667', '-', '0.01667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00249', '(95%-conf.int.', '0.00249', '-', '0.00249)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:120', 'F:2']
['We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.', 'We preprocessed the data by removing tables  references  images and info-boxes.', 'We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.', u'These tasks can either be accomplished by averaging over samples of \xce\xa61  .', 'In this case  we can be confident that the topic distribution is genuinely shared across all languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.01932', '(95%-conf.int.', '0.01932', '-', '0.01932)']
['system', 'ROUGE-S*', 'Average_P:', '0.44444', '(95%-conf.int.', '0.44444', '-', '0.44444)']
['system', 'ROUGE-S*', 'Average_F:', '0.03704', '(95%-conf.int.', '0.03704', '-', '0.03704)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:45', 'F:20']
0.0682379993176 0.00738899992611 0.0110839998892





input/ref/Task1/W99-0613_vardha.csv
input/res/Task1/W99-0613.csv
parsing: input/ref/Task1/W99-0613_vardha.csv
    <S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="35" ssid="29">AdaBoost finds a weighted combination of simple (weak) classifiers, where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
    <S sid="134" ssid="1">This section describes an algorithm based on boosting algorithms, which were previously developed for supervised machine learning problems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'"]
'134'
['134']
parsed_discourse_facet ['method_citation']
    <S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'"]
'236'
['236']
parsed_discourse_facet ['method_citation']
    <S sid="8" ssid="2">Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
    <S sid="42" ssid="36">(Riloff and Shepherd 97) describe a bootstrapping approach for acquiring nouns in particular categories (such as &amp;quot;vehicle&amp;quot; or &amp;quot;weapon&amp;quot; categories).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
    <S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'"]
'236'
['236']
parsed_discourse_facet ['method_citation']
    <S sid="222" ssid="1">The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'222'"]
'222'
['222']
parsed_discourse_facet ['method_citation']
    <S sid="30" ssid="24">(Blum and Mitchell 98) offer a promising formulation of redundancy, also prove some results about how the use of can help classification, and suggest an objective function when training with unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'"]
'30'
['30']
parsed_discourse_facet ['method_citation']
 <S sid="26" ssid="20">We present two algorithms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'"]
'26'
['26']
parsed_discourse_facet ['method_citation']
    <S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="26">The algorithm can be viewed as heuristically optimizing an objective function suggested by (Blum and Mitchell 98); empirically it is shown to be quite successful in optimizing this criterion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'"]
'32'
['32']
parsed_discourse_facet ['method_citation']
    <S sid="47" ssid="1">971,746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'"]
'47'
['47']
parsed_discourse_facet ['method_citation']
    <S sid="127" ssid="60">The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'"]
'127'
['127']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="125" ssid = "58">It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'178'", "'125'", "'41'"]
'256'
'198'
'178'
'125'
'41'
['256', '198', '178', '125', '41']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'97'", "'70'", "'198'"]
'256'
'178'
'97'
'70'
'198'
['256', '178', '97', '70', '198']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="128" ssid = "61">At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'128'", "'198'", "'178'", "'70'"]
'256'
'128'
'198'
'178'
'70'
['256', '128', '198', '178', '70']
parsed_discourse_facet ['results_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'97'", "'70'", "'198'", "'178'"]
'256'
'97'
'70'
'198'
'178'
['256', '97', '70', '198', '178']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="70" ssid = "3">.</S><S sid ="220" ssid = "87">(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'198'", "'70'", "'220'"]
'256'
'178'
'198'
'70'
'220'
['256', '178', '198', '70', '220']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'178'", "'70'", "'236'"]
'256'
'198'
'178'
'70'
'236'
['256', '198', '178', '70', '236']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="70" ssid = "3">.</S><S sid ="128" ssid = "61">At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'70'", "'128'", "'97'"]
'256'
'198'
'70'
'128'
'97'
['256', '198', '70', '128', '97']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S><S sid ="220" ssid = "87">(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'70'", "'220'", "'198'"]
'256'
'178'
'70'
'220'
'198'
['256', '178', '70', '220', '198']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'236'", "'41'", "'97'"]
'256'
'198'
'236'
'41'
'97'
['256', '198', '236', '41', '97']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="70" ssid = "3">.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'97'", "'236'", "'70'"]
'256'
'178'
'97'
'236'
'70'
['256', '178', '97', '236', '70']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="125" ssid = "58">It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'198'", "'125'", "'41'"]
'256'
'178'
'198'
'125'
'41'
['256', '178', '198', '125', '41']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'97'", "'178'", "'198'", "'236'"]
'256'
'97'
'178'
'198'
'236'
['256', '97', '178', '198', '236']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'70'", "'198'", "'178'", "'236'"]
'256'
'70'
'198'
'178'
'236'
['256', '70', '198', '178', '236']
parsed_discourse_facet ['method_citation']
['This section describes an algorithm based on boosting algorithms, which were previously developed for supervised machine learning problems.']
['At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.', '.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00379', '(95%-conf.int.', '0.00379', '-', '0.00379)']
['system', 'ROUGE-S*', 'Average_P:', '0.03030', '(95%-conf.int.', '0.03030', '-', '0.03030)']
['system', 'ROUGE-S*', 'Average_F:', '0.00673', '(95%-conf.int.', '0.00673', '-', '0.00673)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:528', 'P:66', 'F:2']
['We present two algorithms.']
['We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', '.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:1', 'F:0']
['We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.']
['(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).', '.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00645', '(95%-conf.int.', '0.00645', '-', '0.00645)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01176', '(95%-conf.int.', '0.01176', '-', '0.01176)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:45', 'F:3']
['The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.']
['We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', '.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:36', 'F:0']
['We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.']
['At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', '.']
['system', 'ROUGE-S*', 'Average_R:', '0.00450', '(95%-conf.int.', '0.00450', '-', '0.00450)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00844', '(95%-conf.int.', '0.00844', '-', '0.00844)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:45', 'F:3']
['The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.']
['(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).', '.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:153', 'F:0']
['AdaBoost finds a weighted combination of simple (weak) classifiers, where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.']
['.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:136', 'F:0']
['The algorithm can be viewed as heuristically optimizing an objective function suggested by (Blum and Mitchell 98); empirically it is shown to be quite successful in optimizing this criterion.']
['We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:105', 'F:0']
['Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.']
['We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', '.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:105', 'F:0']
0.0181822220202 0.00163777775958 0.00299222218898





input/ref/Task1/W99-0613_sweta.csv
input/res/Task1/W99-0613.csv
parsing: input/ref/Task1/W99-0613_sweta.csv
<S sid="121" ssid="54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page, and other pages pointing to the page).</S>
original cit marker offset is 0
new cit marker offset is 0



["121'"]
121'
['121']
parsed_discourse_facet ['method_citation']
<S sid="252" ssid="3">The method uses a &amp;quot;soft&amp;quot; measure of the agreement between two classifiers as an objective function; we described an algorithm which directly optimizes this function.</S>
original cit marker offset is 0
new cit marker offset is 0



["252'"]
252'
['252']
parsed_discourse_facet ['method_citation']
 <S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



["213'"]
213'
['213']
parsed_discourse_facet ['method_citation']
 <S sid="250" ssid="1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S>
original cit marker offset is 0
new cit marker offset is 0



["250'"]
250'
['250']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="33">(Brin 98) ,describes a system for extracting (author, book-title) pairs from the World Wide Web using an approach that bootstraps from an initial seed set of examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="202" ssid="69">The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels, and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.</S>
original cit marker offset is 0
new cit marker offset is 0



["202'"]
202'
['202']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="15">The following features were used: full-string=x The full string (e.g., for Maury Cooper, full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word, this feature applies for any words that the string contains (e.g., Maury Cooper contributes two such features, contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g., IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods, and contains at least one period.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="176" ssid="43">(7) is at 0 when: 1) Vi : sign(gi (xi)) = sign(g2 (xi)); 2) Ig3(xi)l oo; and 3) sign(gi (xi)) = yi for i = 1, , m. In fact, Zco provides a bound on the sum of the classification error of the labeled examples and the number of disagreements between the two classifiers on the unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["176'"]
176'
['176']
parsed_discourse_facet ['method_citation']
 <S sid="108" ssid="41">In the cotraining case, (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples, and (2) must agree with each other on the unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S>
    <S sid="28" ssid="22">(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'", "'28'"]
27'
'28'
['27', '28']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["7'"]
7'
['7']
parsed_discourse_facet ['method_citation']
 <S sid="172" ssid="39">To see this, note thai the first two terms in the above equation correspond to the function that AdaBoost attempts to minimize in the standard supervised setting (Equ.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["85'"]
85'
['85']
parsed_discourse_facet ['method_citation']
 <S sid="214" ssid="81">This modification brings the method closer to the DL-CoTrain algorithm described earlier, and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples, preventing one label from dominating &#8212; this deserves more theoretical investigation.</S>
original cit marker offset is 0
new cit marker offset is 0



["214'"]
214'
['214']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="125" ssid = "58">It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'178'", "'125'", "'41'"]
'256'
'198'
'178'
'125'
'41'
['256', '198', '178', '125', '41']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'97'", "'70'", "'198'"]
'256'
'178'
'97'
'70'
'198'
['256', '178', '97', '70', '198']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="128" ssid = "61">At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'128'", "'198'", "'178'", "'70'"]
'256'
'128'
'198'
'178'
'70'
['256', '128', '198', '178', '70']
parsed_discourse_facet ['results_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'97'", "'70'", "'198'", "'178'"]
'256'
'97'
'70'
'198'
'178'
['256', '97', '70', '198', '178']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="70" ssid = "3">.</S><S sid ="220" ssid = "87">(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'198'", "'70'", "'220'"]
'256'
'178'
'198'
'70'
'220'
['256', '178', '198', '70', '220']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'178'", "'70'", "'236'"]
'256'
'198'
'178'
'70'
'236'
['256', '198', '178', '70', '236']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="70" ssid = "3">.</S><S sid ="128" ssid = "61">At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'70'", "'128'", "'97'"]
'256'
'198'
'70'
'128'
'97'
['256', '198', '70', '128', '97']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S><S sid ="220" ssid = "87">(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'70'", "'220'", "'198'"]
'256'
'178'
'70'
'220'
'198'
['256', '178', '70', '220', '198']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'236'", "'41'", "'97'"]
'256'
'198'
'236'
'41'
'97'
['256', '198', '236', '41', '97']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="70" ssid = "3">.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'97'", "'236'", "'70'"]
'256'
'178'
'97'
'236'
'70'
['256', '178', '97', '236', '70']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="125" ssid = "58">It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'198'", "'125'", "'41'"]
'256'
'178'
'198'
'125'
'41'
['256', '178', '198', '125', '41']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'97'", "'178'", "'198'", "'236'"]
'256'
'97'
'178'
'198'
'236'
['256', '97', '178', '198', '236']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'70'", "'198'", "'178'", "'236'"]
'256'
'70'
'198'
'178'
'236'
['256', '70', '198', '178', '236']
parsed_discourse_facet ['method_citation']
['There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.']
['At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.', '.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00568', '(95%-conf.int.', '0.00568', '-', '0.00568)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00990', '(95%-conf.int.', '0.00990', '-', '0.00990)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:528', 'P:78', 'F:3']
['The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).', '(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance.']
['We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', '.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_P:', '0.02597', '(95%-conf.int.', '0.02597', '-', '0.02597)']
['system', 'ROUGE-S*', 'Average_F:', '0.01394', '(95%-conf.int.', '0.01394', '-', '0.01394)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:231', 'F:6']
['In the cotraining case, (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples, and (2) must agree with each other on the unlabeled examples.']
['We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', '(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:231', 'F:0']
['Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.']
['(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).', '.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:55', 'F:0']
['This modification brings the method closer to the DL-CoTrain algorithm described earlier, and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples, preventing one label from dominating &#8212; this deserves more theoretical investigation.']
['We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', '.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:231', 'F:0']
['The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels, and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.']
['At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', '.']
['system', 'ROUGE-S*', 'Average_R:', '0.00150', '(95%-conf.int.', '0.00150', '-', '0.00150)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00264', '(95%-conf.int.', '0.00264', '-', '0.00264)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:91', 'F:1']
['The following features were used: full-string=x The full string (e.g., for Maury Cooper, full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word, this feature applies for any words that the string contains (e.g., Maury Cooper contributes two such features, contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g., IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods, and contains at least one period.']
['(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).', '.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:903', 'F:0']
['The method uses a &quot;soft&quot; measure of the agreement between two classifiers as an objective function; we described an algorithm which directly optimizes this function.']
['.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00645', '(95%-conf.int.', '0.00645', '-', '0.00645)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.01105', '(95%-conf.int.', '0.01105', '-', '0.01105)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:78', 'F:3']
['(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.']
['We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00122', '(95%-conf.int.', '0.00122', '-', '0.00122)']
['system', 'ROUGE-S*', 'Average_P:', '0.00159', '(95%-conf.int.', '0.00159', '-', '0.00159)']
['system', 'ROUGE-S*', 'Average_F:', '0.00138', '(95%-conf.int.', '0.00138', '-', '0.00138)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:630', 'F:1']
['Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.']
['We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', '.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:55', 'F:0']
0.0115469998845 0.00243699997563 0.00389099996109





input/ref/Task1/D10-1044_aakansha.csv
input/res/Task1/D10-1044.csv
parsing: input/ref/Task1/D10-1044_aakansha.csv
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S>
    <S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'96'"]
'95'
'96'
['95', '96']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'"]
'75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="28">For comparison to information-retrieval inspired baselines, eg (L&#168;u et al., 2007), we select sentences from OUT using language model perplexities from IN.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'"]
'22'
['22']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="23">The 2nd block contains the IR system, which was tuned by selecting text in multiples of the size of the EMEA training corpus, according to dev set performance.</S>
    <S sid="120" ssid="24">This significantly underperforms log-linear combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'", "'120'"]
'119'
'120'
['119', '120']
parsed_discourse_facet ['result_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
    <S sid="24" ssid="21">Sentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="4">We focus here instead on adapting the two most important features: the language model (LM), which estimates the probability p(wIh) of a target word w following an ngram h; and the translation models (TM) p(slt) and p(t1s), which give the probability of source phrase s translating to target phrase t, and vice versa.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D10-1044.csv
<S sid ="32" ssid = "29">This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.</S><S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="101" ssid = "5">The second setting uses the news-related subcorpora for the NIST09 MT Chinese to English evaluation8 as IN  and the remaining NIST parallel Chinese/English corpora (UN  Hong Kong Laws  and Hong Kong Hansard) as OUT.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'2'", "'114'", "'101'", "'104'"]
'32'
'2'
'114'
'101'
'104'
['32', '2', '114', '101', '104']
parsed_discourse_facet ['implication_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="138" ssid = "7">However  for multinomial models like our LMs and TMs  there is a one to one correspondence between instances and features  eg the correspondence between a phrase pair (s  t) and its conditional multinomial probability p(s1t).</S><S sid ="144" ssid = "1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S><S sid ="133" ssid = "2">It is difficult to directly compare the Matsoukas et al results with ours  since our out-of-domain corpus is homogeneous; given heterogeneous training data  however  it would be trivial to include Matsoukas-style identity features in our instance-weighting model.</S><S sid ="99" ssid = "3">The dev and test sets were randomly chosen from the EMEA corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'138'", "'144'", "'133'", "'99'"]
'67'
'138'
'144'
'133'
'99'
['67', '138', '144', '133', '99']
parsed_discourse_facet ['implication_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="142" ssid = "11">There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).</S><S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'142'", "'8'", "'2'", "'38'"]
'67'
'142'
'8'
'2'
'38'
['67', '142', '8', '2', '38']
parsed_discourse_facet ['results_citation']
<S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="32" ssid = "29">This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.</S><S sid ="83" ssid = "20">We have not yet tried this.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daumes (2007) work that splits each feature into domain-specific and general copies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'32'", "'83'", "'78'", "'136'"]
'8'
'32'
'83'
'78'
'136'
['8', '32', '83', '78', '136']
parsed_discourse_facet ['method_citation']
<S sid ="133" ssid = "2">It is difficult to directly compare the Matsoukas et al results with ours  since our out-of-domain corpus is homogeneous; given heterogeneous training data  however  it would be trivial to include Matsoukas-style identity features in our instance-weighting model.</S><S sid ="118" ssid = "22">Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.</S><S sid ="5" ssid = "2">Even when there is training data available in the domain of interest  there is often additional data from other domains that could in principle be used to improve performance.</S><S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'118'", "'5'", "'1'", "'10'"]
'133'
'118'
'5'
'1'
'10'
['133', '118', '5', '1', '10']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="5" ssid = "2">Even when there is training data available in the domain of interest  there is often additional data from other domains that could in principle be used to improve performance.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daumes (2007) work that splits each feature into domain-specific and general copies.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S><S sid ="14" ssid = "11">There is a fairly large body of work on SMT adaptation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'5'", "'136'", "'104'", "'14'"]
'10'
'5'
'136'
'104'
'14'
['10', '5', '136', '104', '14']
parsed_discourse_facet ['method_citation']
<S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="95" ssid = "32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="125" ssid = "29">Somewhat surprisingly  there do not appear to be large systematic differences between linear and MAP combinations.</S><S sid ="20" ssid = "17">Daume (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'", "'95'", "'78'", "'125'", "'20'"]
'64'
'95'
'78'
'125'
'20'
['64', '95', '78', '125', '20']
parsed_discourse_facet ['method_citation']
<S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="36" ssid = "33">Section 5 covers relevant previous work on SMT adaptation  and section 6 concludes.</S><S sid ="19" ssid = "16">The idea of distinguishing between general and domain-specific examples is due to Daume and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.</S><S sid ="79" ssid = "16">This combination generalizes (2) and (3): we use either at = a to obtain a fixed-weight linear combination  or at = cI(t)/(cI(t) + 0) to obtain a MAP combination.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'36'", "'19'", "'79'", "'114'"]
'151'
'36'
'19'
'79'
'114'
['151', '36', '19', '79', '114']
parsed_discourse_facet ['method_citation']
<S sid ="118" ssid = "22">Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.</S><S sid ="50" ssid = "14">Linear weights are difficult to incorporate into the standard MERT procedure because they are hidden within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.</S><S sid ="88" ssid = "25">We have not explored this strategy.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S><S sid ="19" ssid = "16">The idea of distinguishing between general and domain-specific examples is due to Daume and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'", "'50'", "'88'", "'104'", "'19'"]
'118'
'50'
'88'
'104'
'19'
['118', '50', '88', '104', '19']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "15">For the LM  adaptive weights are set as follows: where  is a weight vector containing an element i for each domain (just IN and OUT in our case)  pi are the corresponding domain-specific models  and p(w  h) is an empirical distribution from a targetlanguage training corpuswe used the IN dev set for this.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the flattened variant described in section 3.2.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="118" ssid = "22">Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'114'", "'127'", "'67'", "'118'"]
'51'
'114'
'127'
'67'
'118'
['51', '114', '127', '67', '118']
parsed_discourse_facet ['implication_citation']
<S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="96" ssid = "33">We used it to score all phrase pairs in the OUT table  in order to provide a feature for the instance-weighting model.</S><S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="106" ssid = "10">The corpora for both settings are summarized in table 1.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'96'", "'128'", "'106'", "'111'"]
'1'
'96'
'128'
'106'
'111'
['1', '96', '128', '106', '111']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "7">Its success depends on the two domains being relatively close  and on the OUT corpus not being so large as to overwhelm the contribution of IN.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="41" ssid = "5">We do not adapt the alignment procedure for generating the phrase table from which the TM distributions are derived.</S><S sid ="60" ssid = "24">The matching sentence pairs are then added to the IN corpus  and the system is re-trained.</S><S sid ="0" ssid = "0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'64'", "'41'", "'60'", "'0'"]
'43'
'64'
'41'
'60'
'0'
['43', '64', '41', '60', '0']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="15" ssid = "12">We introduce several new ideas.</S><S sid ="71" ssid = "8">Finally  we incorporate the instance-weighting model into a general linear combination  and learn weights and mixing parameters simultaneously. where c(s  t) is a modified count for pair (s  t) in OUT  u(s|t) is a prior distribution  and y is a prior weight.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'67'", "'15'", "'71'", "'104'"]
'8'
'67'
'15'
'71'
'104'
['8', '67', '15', '71', '104']
parsed_discourse_facet ['method_citation']
<S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S><S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="43" ssid = "7">Its success depends on the two domains being relatively close  and on the OUT corpus not being so large as to overwhelm the contribution of IN.</S><S sid ="135" ssid = "4">Finally  we note that Jiangs instance-weighting framework is broader than we have presented above  encompassing among other possibilities the use of unlabelled IN data  which is applicable to SMT settings where source-only IN corpora are available.</S><S sid ="146" ssid = "3">The features are weighted within a logistic model to give an overall weight that is applied to the phrase pairs frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'", "'151'", "'43'", "'135'", "'146'"]
'104'
'151'
'43'
'135'
'146'
['104', '151', '43', '135', '146']
parsed_discourse_facet ['results_citation']
<S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="83" ssid = "20">We have not yet tried this.</S><S sid ="28" ssid = "25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'114'", "'83'", "'28'", "'17'"]
'16'
'114'
'83'
'28'
'17'
['16', '114', '83', '28', '17']
parsed_discourse_facet ['implication_citation']
<S sid ="89" ssid = "26">We used 22 features for the logistic weighting model  divided into two groups: one intended to reflect the degree to which a phrase pair belongs to general language  and one intended to capture similarity to the IN domain.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="110" ssid = "14">Je voudrais preciser  a` ladresse du commissaire Liikanen  quil nest pas aise de recourir aux tribunaux nationaux.</S><S sid ="109" ssid = "13"> I would also like to point out to commissioner Liikanen that it is not easy to take a matter to a national court.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'78'", "'110'", "'109'", "'67'"]
'89'
'78'
'110'
'109'
'67'
['89', '78', '110', '109', '67']
parsed_discourse_facet ['method_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="36" ssid = "33">Section 5 covers relevant previous work on SMT adaptation  and section 6 concludes.</S><S sid ="131" ssid = "35">The general-language features have a slight advantage over the similarity features  and both are better than the SVM feature.</S><S sid ="47" ssid = "11">Apart from MERT difficulties  a conceptual problem with log-linear combination is that it multiplies feature probabilities  essentially forcing different features to agree on high-scoring candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'64'", "'36'", "'131'", "'47'"]
'67'
'64'
'36'
'131'
'47'
['67', '64', '36', '131', '47']
parsed_discourse_facet ['results_citation']
<S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daumes (2007) work that splits each feature into domain-specific and general copies.</S><S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'136'", "'8'", "'11'", "'111'"]
'114'
'136'
'8'
'11'
'111'
['114', '136', '8', '11', '111']
parsed_discourse_facet ['aim_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S><S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="92" ssid = "29">6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.</S><S sid ="95" ssid = "32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'111'", "'128'", "'92'", "'95'"]
'67'
'111'
'128'
'92'
'95'
['67', '111', '128', '92', '95']
parsed_discourse_facet ['method_citation']
['Our second contribution is to apply instance weighting at the level of phrase pairs.']
['We have not yet tried this.', 'It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.', 'We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.', 'Previous approaches have tried to find examples that are similar to the target domain.', 'First  we aim to explicitly characterize examples from OUT as belonging to general language or not.']
['system', 'ROUGE-S*', 'Average_R:', '0.00097', '(95%-conf.int.', '0.00097', '-', '0.00097)']
['system', 'ROUGE-S*', 'Average_P:', '0.04762', '(95%-conf.int.', '0.04762', '-', '0.04762)']
['system', 'ROUGE-S*', 'Average_F:', '0.00189', '(95%-conf.int.', '0.00189', '-', '0.00189)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:21', 'F:1']
['To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.']
['We have not yet tried this.', ')  which precludes a single universal approach to adaptation.', 'This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.', u'It is also worth pointing out a connection with Daum\xb4e\u2019s (2007) work that splits each feature into domain-specific and general copies.', 'This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:120', 'F:0']
['Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.', 'We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.']
['It is difficult to directly compare the Matsoukas et al results with ours  since our out-of-domain corpus is homogeneous; given heterogeneous training data  however  it would be trivial to include Matsoukas-style identity features in our instance-weighting model.', 'The dev and test sets were randomly chosen from the EMEA corpus.', 'In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.', 'However  for multinomial models like our LMs and TMs  there is a one to one correspondence between instances and features  eg the correspondence between a phrase pair (s  t) and its conditional multinomial probability p(s1t).', 'We extend the Matsoukas et al approach in several ways.']
['system', 'ROUGE-S*', 'Average_R:', '0.05254', '(95%-conf.int.', '0.05254', '-', '0.05254)']
['system', 'ROUGE-S*', 'Average_P:', '0.13229', '(95%-conf.int.', '0.13229', '-', '0.13229)']
['system', 'ROUGE-S*', 'Average_F:', '0.07521', '(95%-conf.int.', '0.07521', '-', '0.07521)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:703', 'F:93']
['In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.']
['This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.', 'This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.', '(Thus the domain of the dev and test corpora matches IN.)', 'The second setting uses the news-related subcorpora for the NIST09 MT Chinese to English evaluation8 as IN  and the remaining NIST parallel Chinese/English corpora (UN  Hong Kong Laws  and Hong Kong Hansard) as OUT.', 'It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00307', '(95%-conf.int.', '0.00307', '-', '0.00307)']
['system', 'ROUGE-S*', 'Average_P:', '0.08974', '(95%-conf.int.', '0.08974', '-', '0.08974)']
['system', 'ROUGE-S*', 'Average_F:', '0.00594', '(95%-conf.int.', '0.00594', '-', '0.00594)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:78', 'F:7']
['We focus here instead on adapting the two most important features: the language model (LM), which estimates the probability p(wIh) of a target word w following an ngram h; and the translation models (TM) p(slt) and p(t1s), which give the probability of source phrase s translating to target phrase t, and vice versa.']
['Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.', 'Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.', '6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.', 'We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.', 'We extend the Matsoukas et al approach in several ways.']
['system', 'ROUGE-S*', 'Average_R:', '0.00918', '(95%-conf.int.', '0.00918', '-', '0.00918)']
['system', 'ROUGE-S*', 'Average_P:', '0.08262', '(95%-conf.int.', '0.08262', '-', '0.08262)']
['system', 'ROUGE-S*', 'Average_F:', '0.01652', '(95%-conf.int.', '0.01652', '-', '0.01652)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:351', 'F:29']
['In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.']
[u'The idea of distinguishing between general and domain-specific examples is due to Daum\xb4e and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.', 'We have not explored this strategy.', 'Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.', '(Thus the domain of the dev and test corpora matches IN.)', u'Linear weights are difficult to incorporate into the standard MERT procedure because they are \u201chidden\u201d within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.']
['system', 'ROUGE-S*', 'Average_R:', '0.00407', '(95%-conf.int.', '0.00407', '-', '0.00407)']
['system', 'ROUGE-S*', 'Average_P:', '0.04286', '(95%-conf.int.', '0.04286', '-', '0.04286)']
['system', 'ROUGE-S*', 'Average_F:', '0.00743', '(95%-conf.int.', '0.00743', '-', '0.00743)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:210', 'F:9']
['Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.']
['In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.', u'The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair\u2019s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).', u'Finally  we note that Jiang\u2019s instance-weighting framework is broader than we have presented above  encompassing among other possibilities the use of unlabelled IN data  which is applicable to SMT settings where source-only IN corpora are available.', 'Its success depends on the two domains being relatively close  and on the OUT corpus not being so large as to overwhelm the contribution of IN.', '(Thus the domain of the dev and test corpora matches IN.)']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:78', 'F:0']
['Sentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language.', 'Our second contribution is to apply instance weighting at the level of phrase pairs.']
['It is difficult when IN and OUT are dissimilar  as they are in the cases we study.', 'It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.', 'We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.', u'It is also worth pointing out a connection with Daum\xb4e\u2019s (2007) work that splits each feature into domain-specific and general copies.', ')  which precludes a single universal approach to adaptation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00169', '(95%-conf.int.', '0.00169', '-', '0.00169)']
['system', 'ROUGE-S*', 'Average_P:', '0.01961', '(95%-conf.int.', '0.01961', '-', '0.01961)']
['system', 'ROUGE-S*', 'Average_F:', '0.00312', '(95%-conf.int.', '0.00312', '-', '0.00312)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:153', 'F:3']
['In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.']
[')  which precludes a single universal approach to adaptation.', 'The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.', 'This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.', 'There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).', 'We extend the Matsoukas et al approach in several ways.']
['system', 'ROUGE-S*', 'Average_R:', '0.00058', '(95%-conf.int.', '0.00058', '-', '0.00058)']
['system', 'ROUGE-S*', 'Average_P:', '0.00476', '(95%-conf.int.', '0.00476', '-', '0.00476)']
['system', 'ROUGE-S*', 'Average_F:', '0.00104', '(95%-conf.int.', '0.00104', '-', '0.00104)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:210', 'F:1']
0.0466111105932 0.0080111110221 0.0123499998628





input/ref/Task1/P11-1061_aakansha.csv
input/res/Task1/P11-1061.csv
parsing: input/ref/Task1/P11-1061_aakansha.csv
<S sid="40" ssid="6">We extend Subramanya et al.&#8217;s intuitions to our bilingual setup.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="11">First, we use a novel graph-based framework for projecting syntactic information across language boundaries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="29" ssid="6">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'"]
'158'
['158']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'"]
'158'
['158']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="1">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="17" ssid="13">Second, we treat the projected labels as features in an unsupervised model (&#167;5), rather than using them directly for supervised training.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="111" ssid="11">We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).</S>
original cit marker offset is 0
new cit marker offset is 0



["'111'"]
'111'
['111']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="29" ssid="6">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'161'"]
'161'
['161']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1061.csv
<S sid ="14" ssid = "10">Our work is closest to that of Yarowsky and Ngai (2001)  but differs in two important ways.</S><S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S><S sid ="103" ssid = "3">The availability of these resources guided our selection of foreign languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'7'", "'56'", "'135'", "'103'"]
'14'
'7'
'56'
'135'
'103'
['14', '7', '56', '135', '103']
parsed_discourse_facet ['implication_citation']
<S sid ="75" ssid = "6">We use a squared loss to penalize neighboring vertices that have different label distributions: kqi  qjk2 = Ey(qi(y)  qj(y))2  and additionally regularize the label distributions towards the uniform distribution U over all possible labels Y.</S><S sid ="53" ssid = "19">Finally  note that while most feature concepts are lexicalized  others  such as the suffix concept  are not.</S><S sid ="34" ssid = "11">The following three sections elaborate these different stages is more detail.</S><S sid ="129" ssid = "29">Fortunately  performance was stable across various values  and we were able to use the same hyperparameters for all languages.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'53'", "'34'", "'129'", "'26'"]
'75'
'53'
'34'
'129'
'26'
['75', '53', '34', '129', '26']
parsed_discourse_facet ['implication_citation']
<S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="59" ssid = "25">So far the graph has been completely unlabeled.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'107'", "'59'", "'56'", "'39'"]
'1'
'107'
'59'
'56'
'39'
['1', '107', '59', '56', '39']
parsed_discourse_facet ['results_citation']
<S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : We describe how we choose  in 6.4.</S><S sid ="66" ssid = "32">In general  the neighborhoods can be more diverse and we allow a soft label distribution over the vertices.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'66'", "'107'", "'117'", "'135'"]
'83'
'66'
'107'
'117'
'135'
['83', '66', '107', '117', '135']
parsed_discourse_facet ['method_citation']
<S sid ="104" ssid = "4">For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi  2006; Nivre et al.  2007).</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="155" ssid = "18">Examining the word fidanzato for the No LP and With LP models is particularly instructive.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="98" ssid = "29">7).</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'", "'1'", "'155'", "'39'", "'98'"]
'104'
'1'
'155'
'39'
'98'
['104', '1', '155', '39', '98']
parsed_discourse_facet ['method_citation']
<S sid ="143" ssid = "6">Overall  it gives improvements ranging from 1.1% for German to 14.7% for Italian  for an average improvement of 8.3% over the unsupervised feature-HMM model.</S><S sid ="96" ssid = "27">The function A : F * C maps from the language specific fine-grained tagset F to the coarser universal tagset C and is described in detail in 6.2: Note that when tx(y) = 1 the feature value is 0 and has no effect on the model  while its value is oc when tx(y) = 0 and constrains the HMMs state space.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="68" ssid = "34">In the label propagation stage  we propagate the automatic English tags to the aligned Italian trigram types  followed by further propagation solely among the Italian vertices. the Italian vertices are connected to an automatically labeled English vertex.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'96'", "'97'", "'68'", "'26'"]
'143'
'96'
'97'
'68'
'26'
['143', '96', '97', '68', '26']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="129" ssid = "29">Fortunately  performance was stable across various values  and we were able to use the same hyperparameters for all languages.</S><S sid ="108" ssid = "8">However  we needed to restrict ourselves to these languages in order to be able to evaluate the performance of our approach.</S><S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'26'", "'129'", "'108'", "'37'"]
'7'
'26'
'129'
'108'
'37'
['7', '26', '129', '108', '37']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S><S sid ="122" ssid = "22">For each language  we took the same number of sentences from the bitext as there are in its treebank  and trained a supervised feature-HMM.</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="31" ssid = "8">By aggregating the POS labels of the English tokens to types  we can generate label distributions for the English vertices.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'110'", "'122'", "'19'", "'31'"]
'39'
'110'
'122'
'19'
'31'
['39', '110', '122', '19', '31']
parsed_discourse_facet ['method_citation']
<S sid ="73" ssid = "4">Note that because we extracted only high-confidence alignments  many foreign vertices will not be connected to any English vertices.</S><S sid ="24" ssid = "1">The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.</S><S sid ="150" ssid = "13">For all languages  the vocabulary sizes increase by several thousand words.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="139" ssid = "2">As expected  the vanilla HMM trained with EM performs the worst.</S>
original cit marker offset is 0
new cit marker offset is 0



["'73'", "'24'", "'150'", "'39'", "'139'"]
'73'
'24'
'150'
'39'
'139'
['73', '24', '150', '39', '139']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="45" ssid = "11">Furthermore  we do not connect the English vertices to each other  but only to foreign language vertices.4 The graph vertices are extracted from the different sides of a parallel corpus (De  Df) and an additional unlabeled monolingual foreign corpus Ff  which will be used later for training.</S><S sid ="69" ssid = "35">Label propagation is used to propagate these tags inwards and results in tag distributions for the middle word of each Italian trigram.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'45'", "'69'", "'135'", "'110'"]
'107'
'45'
'69'
'135'
'110'
['107', '45', '69', '135', '110']
parsed_discourse_facet ['implication_citation']
<S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : We describe how we choose  in 6.4.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'83'", "'25'", "'4'"]
'37'
'1'
'83'
'25'
'4'
['37', '1', '83', '25', '4']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="120" ssid = "20">We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="44" ssid = "10">Because all English vertices are going to be labeled  we do not need to disambiguate them by embedding them in trigrams.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'120'", "'117'", "'44'", "'26'"]
'54'
'120'
'117'
'44'
'26'
['54', '120', '117', '44', '26']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="47" ssid = "13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S><S sid ="161" ssid = "4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections  and bridge the gap between purely supervised and unsupervised POS tagging models.</S><S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'117'", "'47'", "'161'", "'7'"]
'39'
'117'
'47'
'161'
'7'
['39', '117', '47', '161', '7']
parsed_discourse_facet ['method_citation']
<S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="32" ssid = "9">Label propagation can then be used to transfer the labels to the peripheral foreign vertices (i.e. the ones adjacent to the English vertices) first  and then among all of the foreign vertices (4).</S><S sid ="86" ssid = "17">For a sentence x and a state sequence z  a first order Markov model defines a distribution: (9) where Val(X) corresponds to the entire vocabulary.</S><S sid ="132" ssid = "32">When extracting the vector t  used to compute the constraint feature from the graph  we tried three threshold values for r (see Eq.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'117'", "'32'", "'86'", "'132'"]
'37'
'117'
'32'
'86'
'132'
['37', '117', '32', '86', '132']
parsed_discourse_facet ['results_citation']
<S sid ="163" ssid = "2">We would also like to thank Amarnag Subramanya for helping us with the implementation of label propagation and Shankar Kumar for access to the parallel data.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S><S sid ="139" ssid = "2">As expected  the vanilla HMM trained with EM performs the worst.</S><S sid ="58" ssid = "24">Based on these high-confidence alignments we can extract tuples of the form [u H v]  where u is a foreign trigram type  whose middle word aligns to an English word type v. Our bilingual similarity function then sets the edge weights in proportion to these tuple counts.</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : We describe how we choose  in 6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'110'", "'139'", "'58'", "'83'"]
'163'
'110'
'139'
'58'
'83'
['163', '110', '139', '58', '83']
parsed_discourse_facet ['implication_citation']
<S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="134" ssid = "34">Because we dont have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S><S sid ="72" ssid = "3">In the first stage  we run a single step of label propagation  which transfers the label distributions from the English vertices to the connected foreign language vertices (say  Vf) at the periphery of the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'117'", "'134'", "'110'", "'72'"]
'37'
'117'
'134'
'110'
'72'
['37', '117', '134', '110', '72']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="104" ssid = "4">For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi  2006; Nivre et al.  2007).</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'39'", "'26'", "'104'", "'135'"]
'54'
'39'
'26'
'104'
'135'
['54', '39', '26', '104', '135']
parsed_discourse_facet ['results_citation']
<S sid ="33" ssid = "10">The POS distributions over the foreign trigram types are used as features to learn a better unsupervised POS tagger (5).</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S><S sid ="90" ssid = "21">All features were conjoined with the state z.</S><S sid ="96" ssid = "27">The function A : F * C maps from the language specific fine-grained tagset F to the coarser universal tagset C and is described in detail in 6.2: Note that when tx(y) = 1 the feature value is 0 and has no effect on the model  while its value is oc when tx(y) = 0 and constrains the HMMs state space.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'117'", "'4'", "'90'", "'96'"]
'33'
'117'
'4'
'90'
'96'
['33', '117', '4', '90', '96']
parsed_discourse_facet ['aim_citation']
['We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).']
['Based on these high-confidence alignments we can extract tuples of the form [u H v]  where u is a foreign trigram type  whose middle word aligns to an English word type v. Our bilingual similarity function then sets the edge weights in proportion to these tuple counts.', u'We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value \u03c4: We describe how we choose \u03c4 in \xa76.4.', 'As expected  the vanilla HMM trained with EM performs the worst.', 'We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.', 'We would also like to thank Amarnag Subramanya for helping us with the implementation of label propagation and Shankar Kumar for access to the parallel data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00106', '(95%-conf.int.', '0.00106', '-', '0.00106)']
['system', 'ROUGE-S*', 'Average_P:', '0.00221', '(95%-conf.int.', '0.00221', '-', '0.00221)']
['system', 'ROUGE-S*', 'Average_F:', '0.00143', '(95%-conf.int.', '0.00143', '-', '0.00143)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:903', 'F:2']
['We extend Subramanya et al.&#8217;s intuitions to our bilingual setup.']
['The availability of these resources guided our selection of foreign languages.', 'For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.', 'To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.', 'Our work is closest to that of Yarowsky and Ngai (2001)  but differs in two important ways.', 'However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:21', 'F:0']
['The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.']
['Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).', 'They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.', 'In other words  the set of hidden states F was chosen to be the fine set of treebank tags.', 'However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.', 'Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections  and bridge the gap between purely supervised and unsupervised POS tagging models.']
['system', 'ROUGE-S*', 'Average_R:', '0.00219', '(95%-conf.int.', '0.00219', '-', '0.00219)']
['system', 'ROUGE-S*', 'Average_P:', '0.05495', '(95%-conf.int.', '0.05495', '-', '0.05495)']
['system', 'ROUGE-S*', 'Average_F:', '0.00422', '(95%-conf.int.', '0.00422', '-', '0.00422)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:91', 'F:5']
['These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.']
['The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.', 'For all languages  the vocabulary sizes increase by several thousand words.', 'They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.', 'Note that because we extracted only high-confidence alignments  many foreign vertices will not be connected to any English vertices.', 'As expected  the vanilla HMM trained with EM performs the worst.']
['system', 'ROUGE-S*', 'Average_R:', '0.00907', '(95%-conf.int.', '0.00907', '-', '0.00907)']
['system', 'ROUGE-S*', 'Average_P:', '0.05435', '(95%-conf.int.', '0.05435', '-', '0.05435)']
['system', 'ROUGE-S*', 'Average_F:', '0.01555', '(95%-conf.int.', '0.01555', '-', '0.01555)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:276', 'F:15']
['To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.']
['We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.', 'Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.', 'In other words  the set of hidden states F was chosen to be the fine set of treebank tags.', u'As discussed in more detail in \u0e22\u0e073  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', 'Because all English vertices are going to be labeled  we do not need to disambiguate them by embedding them in trigrams.']
['system', 'ROUGE-S*', 'Average_R:', '0.00492', '(95%-conf.int.', '0.00492', '-', '0.00492)']
['system', 'ROUGE-S*', 'Average_P:', '0.01705', '(95%-conf.int.', '0.01705', '-', '0.01705)']
['system', 'ROUGE-S*', 'Average_F:', '0.00763', '(95%-conf.int.', '0.00763', '-', '0.00763)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:528', 'F:9']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.']
['Label propagation is used to propagate these tags inwards and results in tag distributions for the middle word of each Italian trigram.', 'Furthermore  we do not connect the English vertices to each other  but only to foreign language vertices.4 The graph vertices are extracted from the different sides of a parallel corpus (De  Df) and an additional unlabeled monolingual foreign corpus Ff  which will be used later for training.', 'Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.', 'For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.', 'We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.']
['system', 'ROUGE-S*', 'Average_R:', '0.00273', '(95%-conf.int.', '0.00273', '-', '0.00273)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.00531', '(95%-conf.int.', '0.00531', '-', '0.00531)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:55', 'F:5']
['To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.']
['We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.', '7).', 'They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.', u'Examining the word fidanzato for the \u201cNo LP\u201d and \u201cWith LP\u201d models is particularly instructive.', 'For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi  2006; Nivre et al.  2007).']
['system', 'ROUGE-S*', 'Average_R:', '0.00582', '(95%-conf.int.', '0.00582', '-', '0.00582)']
['system', 'ROUGE-S*', 'Average_P:', '0.03134', '(95%-conf.int.', '0.03134', '-', '0.03134)']
['system', 'ROUGE-S*', 'Average_F:', '0.00981', '(95%-conf.int.', '0.00981', '-', '0.00981)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:351', 'F:11']
['To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.']
['Fortunately  performance was stable across various values  and we were able to use the same hyperparameters for all languages.', u'We use a squared loss to penalize neighboring vertices that have different label distributions: kqi \u2212 qjk2 = Ey(qi(y) \u2212 qj(y))2  and additionally regularize the label distributions towards the uniform distribution U over all possible labels Y.', u'As discussed in more detail in \xa73  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', 'Finally  note that while most feature concepts are lexicalized  others  such as the suffix concept  are not.', 'The following three sections elaborate these different stages is more detail.']
['system', 'ROUGE-S*', 'Average_R:', '0.00584', '(95%-conf.int.', '0.00584', '-', '0.00584)']
['system', 'ROUGE-S*', 'Average_P:', '0.01705', '(95%-conf.int.', '0.01705', '-', '0.01705)']
['system', 'ROUGE-S*', 'Average_F:', '0.00870', '(95%-conf.int.', '0.00870', '-', '0.00870)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:528', 'F:9']
['Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.']
[u'The POS distributions over the foreign trigram types are used as features to learn a better unsupervised POS tagger (\xa75).', 'All features were conjoined with the state z.', 'In other words  the set of hidden states F was chosen to be the fine set of treebank tags.', 'Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.', u'The function A : F \u2014* C maps from the language specific fine-grained tagset F to the coarser universal tagset C and is described in detail in \xa76.2: Note that when tx(y) = 1 the feature value is 0 and has no effect on the model  while its value is \u2212oc when tx(y) = 0 and constrains the HMM\u2019s state space.']
['system', 'ROUGE-S*', 'Average_R:', '0.00509', '(95%-conf.int.', '0.00509', '-', '0.00509)']
['system', 'ROUGE-S*', 'Average_P:', '0.09559', '(95%-conf.int.', '0.09559', '-', '0.09559)']
['system', 'ROUGE-S*', 'Average_F:', '0.00966', '(95%-conf.int.', '0.00966', '-', '0.00966)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:136', 'F:13']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.']
['Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.', 'By aggregating the POS labels of the English tokens to types  we can generate label distributions for the English vertices.', 'They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.', 'For each language  we took the same number of sentences from the bitext as there are in its treebank  and trained a supervised feature-HMM.', 'We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.']
['system', 'ROUGE-S*', 'Average_R:', '0.00186', '(95%-conf.int.', '0.00186', '-', '0.00186)']
['system', 'ROUGE-S*', 'Average_P:', '0.07273', '(95%-conf.int.', '0.07273', '-', '0.07273)']
['system', 'ROUGE-S*', 'Average_F:', '0.00364', '(95%-conf.int.', '0.00364', '-', '0.00364)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:55', 'F:4']
['To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).']
['Fortunately  performance was stable across various values  and we were able to use the same hyperparameters for all languages.', u'As discussed in more detail in \u0e22\u0e073  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', 'Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.', 'However  we needed to restrict ourselves to these languages in order to be able to evaluate the performance of our approach.', 'However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:66', 'F:0']
0.0396527269122 0.00350727269539 0.00599545449095





input/ref/Task1/P05-1013_aakansha.csv
input/res/Task1/P05-1013.csv
parsing: input/ref/Task1/P05-1013_aakansha.csv
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="17">However, the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech, with a best performance of 73% UAS (Holan, 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="86" ssid="13">As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'"]
'86'
['86']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="7">As shown in Table 3, the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.</S>
    <S sid="81"  ssid="8">However, the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'81'"]
'80'
'81'
['80', '81']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'"]
'49'
['49']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="10">While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P05-1013.csv
<S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S><S sid ="53" ssid = "24">In the third and final scheme  denoted Path  we keep the extra infor2Note that this is a baseline for the parsing experiment only (Experiment 2).</S><S sid ="37" ssid = "8">Here we use a slightly different notion of lift  applying to individual arcs and moving their head upwards one step at a time: Intuitively  lifting an arc makes the word wk dependent on the head wi of its original head wj (which is unique in a well-formed dependency graph)  unless wj is a root in which case the operation is undefined (but then wj * wk is necessarily projective if the dependency graph is well-formed).</S><S sid ="83" ssid = "10">It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'53'", "'37'", "'83'", "'75'"]
'51'
'53'
'37'
'83'
'75'
['51', '53', '37', '83', '75']
parsed_discourse_facet ['implication_citation']
<S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S><S sid ="38" ssid = "9">Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'38'", "'78'", "'34'", "'51'"]
'75'
'38'
'78'
'34'
'51'
['75', '38', '78', '34', '51']
parsed_discourse_facet ['implication_citation']
<S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S><S sid ="23" ssid = "19">By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S><S sid ="110" ssid = "2">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="25" ssid = "21">The rest of the paper is structured as follows.</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'70'", "'23'", "'110'", "'25'", "'11'"]
'70'
'23'
'110'
'25'
'11'
['70', '23', '110', '25', '11']
parsed_discourse_facet ['results_citation']
<S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="77" ssid = "4">The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'24'", "'95'", "'77'", "'11'"]
'34'
'24'
'95'
'77'
'11'
['34', '24', '95', '77', '11']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "14">Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.</S><S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="18" ssid = "14">In addition  there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington  1990; Kahane et al.  1998; Duchier and Debusmann  2001; Holan et al.  2001; Hellwig  2003).</S><S sid ="103" ssid = "14">On the other hand  given that all schemes have similar parsing accuracy overall  this means that the Path scheme is the least likely to introduce errors on projective arcs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'65'", "'14'", "'18'", "'103'"]
'43'
'65'
'14'
'18'
'103'
['43', '65', '14', '18', '103']
parsed_discourse_facet ['method_citation']
<S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S><S sid ="42" ssid = "13">Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="55" ssid = "26">As can be seen from the last column in Table 1  both Head and Head+Path may theoretically lead to a quadratic increase in the number of distinct arc labels (Head+Path being worse than Head only by a constant factor)  while the increase is only linear in the case of Path.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'42'", "'14'", "'55'", "'104'"]
'75'
'42'
'14'
'55'
'104'
['75', '42', '14', '55', '104']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="107" ssid = "18">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="66" ssid = "5">The choice between different actions is in general nondeterministic  and the parser relies on a memorybased classifier  trained on treebank data  to predict the next action based on features of the current parser configuration.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'107'", "'14'", "'95'", "'66'"]
'11'
'107'
'14'
'95'
'66'
['11', '107', '14', '95', '66']
parsed_discourse_facet ['method_citation']
<S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S><S sid ="12" ssid = "8">Prague Dependency Treebank (Hajic et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'70'", "'81'", "'12'", "'78'", "'89'"]
'70'
'81'
'12'
'78'
'89'
['70', '81', '12', '78', '89']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Prague Dependency Treebank (Hajic et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S><S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="5" ssid = "1">It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order  where discontinuous syntactic constructions are more common than in languages like English (Melcuk  1988; Covington  1990).</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S><S sid ="66" ssid = "5">The choice between different actions is in general nondeterministic  and the parser relies on a memorybased classifier  trained on treebank data  to predict the next action based on features of the current parser configuration.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'65'", "'5'", "'71'", "'66'"]
'12'
'65'
'5'
'71'
'66'
['12', '65', '5', '71', '66']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S><S sid ="82" ssid = "9">The last four columns in Table 3 show the distribution of nonprojective arcs with respect to the number of lifts required.</S><S sid ="86" ssid = "13">As expected  the most informative encoding  Head+Path  gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S><S sid ="84" ssid = "11">In the second part of the experiment  we applied the inverse transformation based on breadth-first search under the three different encoding schemes.</S><S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'82'", "'86'", "'84'", "'70'"]
'51'
'82'
'86'
'84'
'70'
['51', '82', '86', '84', '70']
parsed_discourse_facet ['implication_citation']
<S sid ="85" ssid = "12">The results are given in Table 4.</S><S sid ="33" ssid = "4">If (wi  r  wj) E A  we say that wi is the head of wj and wj a dependent of wi.</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="43" ssid = "14">Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.</S><S sid ="44" ssid = "15">Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'", "'33'", "'60'", "'43'", "'44'"]
'85'
'33'
'60'
'43'
'44'
['85', '33', '60', '43', '44']
parsed_discourse_facet ['method_citation']
<S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="25" ssid = "21">The rest of the paper is structured as follows.</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'14'", "'25'", "'60'", "'75'"]
'65'
'14'
'25'
'60'
'75'
['65', '14', '25', '60', '75']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="26" ssid = "22">In section 2 we introduce the graph transformation techniques used to projectivize and deprojectivize dependency graphs  and in section 3 we describe the data-driven dependency parser that is the core of our system.</S><S sid ="61" ssid = "32">Before we turn to the evaluation  however  we need to introduce the data-driven dependency parser used in the latter experiments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'104'", "'21'", "'26'", "'61'"]
'11'
'104'
'21'
'26'
'61'
['11', '104', '21', '26', '61']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="43" ssid = "14">Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S><S sid ="63" ssid = "2">The parser builds dependency graphs by traversing the input from left to right  using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'11'", "'43'", "'81'", "'63'"]
'34'
'11'
'43'
'81'
'63'
['34', '11', '43', '81', '63']
parsed_discourse_facet ['results_citation']
<S sid ="86" ssid = "13">As expected  the most informative encoding  Head+Path  gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S><S sid ="57" ssid = "28">In approaching this problem  a variety of different methods are conceivable  including a more or less sophisticated use of machine learning.</S><S sid ="73" ssid = "12">More details on the memory-based prediction can be found in Nivre et al. (2004) and Nivre and Scholz (2004).</S><S sid ="61" ssid = "32">Before we turn to the evaluation  however  we need to introduce the data-driven dependency parser used in the latter experiments.</S><S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'57'", "'73'", "'61'", "'34'"]
'86'
'57'
'73'
'61'
'34'
['86', '57', '73', '61', '34']
parsed_discourse_facet ['implication_citation']
<S sid ="83" ssid = "10">It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="101" ssid = "12">However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.</S><S sid ="25" ssid = "21">The rest of the paper is structured as follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'106'", "'78'", "'101'", "'25'"]
'83'
'106'
'78'
'101'
'25'
['83', '106', '78', '101', '25']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="77" ssid = "4">The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'24'", "'95'", "'77'", "'11'"]
'34'
'24'
'95'
'77'
'11'
['34', '24', '95', '77', '11']
parsed_discourse_facet ['results_citation']
<S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="92" ssid = "3">Evaluation metrics used are Attachment Score (AS)  i.e. the proportion of tokens that are attached to the correct head  and Exact Match (EM)  i.e. the proportion of sentences for which the dependency graph exactly matches the gold standard.</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'65'", "'92'", "'60'", "'71'"]
'95'
'65'
'92'
'60'
'71'
['95', '65', '92', '60', '71']
parsed_discourse_facet ['aim_citation']
<S sid ="67" ssid = "6">Table 2 shows the features used in the current version of the parser.</S><S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S><S sid ="82" ssid = "9">The last four columns in Table 3 show the distribution of nonprojective arcs with respect to the number of lifts required.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'70'", "'82'", "'14'", "'81'"]
'67'
'70'
'82'
'14'
'81'
['67', '70', '82', '14', '81']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S><S sid ="63" ssid = "2">The parser builds dependency graphs by traversing the input from left to right  using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'11'", "'34'", "'75'", "'63'"]
'51'
'11'
'34'
'75'
'63'
['51', '11', '34', '75', '63']
parsed_discourse_facet ['aim_citation']
['We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).']
['In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.', 'The rest of the paper is structured as follows.', u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'More details on the parsing algorithm can be found in Nivre (2003).', u'While the proportion of sentences containing non-projective dependencies is often 15\u201325%  the total proportion of non-projective arcs is normally only 1\u20132%.']
['system', 'ROUGE-S*', 'Average_R:', '0.00314', '(95%-conf.int.', '0.00314', '-', '0.00314)']
['system', 'ROUGE-S*', 'Average_P:', '0.07273', '(95%-conf.int.', '0.07273', '-', '0.07273)']
['system', 'ROUGE-S*', 'Average_F:', '0.00602', '(95%-conf.int.', '0.00602', '-', '0.00602)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:55', 'F:4']
['By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.']
['The choice between different actions is in general nondeterministic  and the parser relies on a memorybased classifier  trained on treebank data  to predict the next action based on features of the current parser configuration.', u'Prague Dependency Treebank (Haji\u02c7c et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.', u'It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order  where discontinuous syntactic constructions are more common than in languages like English (Mel\u2019\u02c7cuk  1988; Covington  1990).', 'More details on the parsing algorithm can be found in Nivre (2003).', 'For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.']
['system', 'ROUGE-S*', 'Average_R:', '0.00444', '(95%-conf.int.', '0.00444', '-', '0.00444)']
['system', 'ROUGE-S*', 'Average_P:', '0.03427', '(95%-conf.int.', '0.03427', '-', '0.03427)']
['system', 'ROUGE-S*', 'Average_F:', '0.00786', '(95%-conf.int.', '0.00786', '-', '0.00786)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3828', 'P:496', 'F:17']
['As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.']
['We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).', 'The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).', 'This is in contrast to dependency treebanks  e.g.', 'The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.', u'In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi \u2014*\u2217 wj for the reflexive and transitive closure of the (unlabeled) arc relation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00044', '(95%-conf.int.', '0.00044', '-', '0.00044)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00084', '(95%-conf.int.', '0.00084', '-', '0.00084)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:91', 'F:1']
['As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.']
[u'Prague Dependency Treebank (Haji\xcb\u2021c et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.', 'However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.', 'The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.', 'The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.', 'Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.']
['system', 'ROUGE-S*', 'Average_R:', '0.01428', '(95%-conf.int.', '0.01428', '-', '0.01428)']
['system', 'ROUGE-S*', 'Average_P:', '0.07143', '(95%-conf.int.', '0.07143', '-', '0.07143)']
['system', 'ROUGE-S*', 'Average_F:', '0.02380', '(95%-conf.int.', '0.02380', '-', '0.02380)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:378', 'F:27']
['While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.']
['In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.', 'Evaluation metrics used are Attachment Score (AS)  i.e. the proportion of tokens that are attached to the correct head  and Exact Match (EM)  i.e. the proportion of sentences for which the dependency graph exactly matches the gold standard.', 'The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.', 'More details on the parsing algorithm can be found in Nivre (2003).', 'For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.']
['system', 'ROUGE-S*', 'Average_R:', '0.00571', '(95%-conf.int.', '0.00571', '-', '0.00571)']
['system', 'ROUGE-S*', 'Average_P:', '0.16484', '(95%-conf.int.', '0.16484', '-', '0.16484)']
['system', 'ROUGE-S*', 'Average_F:', '0.01103', '(95%-conf.int.', '0.01103', '-', '0.01103)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:91', 'F:15']
['We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).']
['The last four columns in Table 3 show the distribution of nonprojective arcs with respect to the number of lifts required.', 'As expected  the most informative encoding  Head+Path  gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.', u'In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p\xe2\u2020\u201c.', 'In the second part of the experiment  we applied the inverse transformation based on breadth-first search under the three different encoding schemes.', 'Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:55', 'F:0']
['In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
['This is in contrast to dependency treebanks  e.g.', u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'The parser builds dependency graphs by traversing the input from left to right  using a stack to store tokens that are not yet complete with respect to their dependents.', u'In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p\u2193.', u'In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi \u2014*\u2217 wj for the reflexive and transitive closure of the (unlabeled) arc relation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00136', '(95%-conf.int.', '0.00136', '-', '0.00136)']
['system', 'ROUGE-S*', 'Average_P:', '0.03297', '(95%-conf.int.', '0.03297', '-', '0.03297)']
['system', 'ROUGE-S*', 'Average_F:', '0.00261', '(95%-conf.int.', '0.00261', '-', '0.00261)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:91', 'F:3']
['In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
['In addition  there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington  1990; Kahane et al.  1998; Duchier and Debusmann  2001; Holan et al.  2001; Hellwig  2003).', 'On the other hand  given that all schemes have similar parsing accuracy overall  this means that the Path scheme is the least likely to introduce errors on projective arcs.', 'Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.', u'While the proportion of sentences containing non-projective dependencies is often 15\u201325%  the total proportion of non-projective arcs is normally only 1\u20132%.', 'More details on the parsing algorithm can be found in Nivre (2003).']
['system', 'ROUGE-S*', 'Average_R:', '0.00328', '(95%-conf.int.', '0.00328', '-', '0.00328)']
['system', 'ROUGE-S*', 'Average_P:', '0.06593', '(95%-conf.int.', '0.06593', '-', '0.06593)']
['system', 'ROUGE-S*', 'Average_F:', '0.00625', '(95%-conf.int.', '0.00625', '-', '0.00625)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:91', 'F:6']
['However, the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech, with a best performance of 73% UAS (Holan, 2004).']
['This is in contrast to dependency treebanks  e.g.', 'The rest of the paper is structured as follows.', 'By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.', 'The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.', 'Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.']
['system', 'ROUGE-S*', 'Average_R:', '0.00939', '(95%-conf.int.', '0.00939', '-', '0.00939)']
['system', 'ROUGE-S*', 'Average_P:', '0.22857', '(95%-conf.int.', '0.22857', '-', '0.22857)']
['system', 'ROUGE-S*', 'Average_F:', '0.01804', '(95%-conf.int.', '0.01804', '-', '0.01804)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:105', 'F:24']
['We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).']
[u'In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi \u2014*\u2217 wj for the reflexive and transitive closure of the (unlabeled) arc relation.', u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.', u'In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p\u2193.', 'The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.']
['system', 'ROUGE-S*', 'Average_R:', '0.00035', '(95%-conf.int.', '0.00035', '-', '0.00035)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00069', '(95%-conf.int.', '0.00069', '-', '0.00069)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:55', 'F:1']
['The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.']
['It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.', 'However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.', 'However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).', 'The rest of the paper is structured as follows.', 'The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.']
['system', 'ROUGE-S*', 'Average_R:', '0.01042', '(95%-conf.int.', '0.01042', '-', '0.01042)']
['system', 'ROUGE-S*', 'Average_P:', '0.02834', '(95%-conf.int.', '0.02834', '-', '0.02834)']
['system', 'ROUGE-S*', 'Average_F:', '0.01523', '(95%-conf.int.', '0.01523', '-', '0.01523)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:741', 'F:21']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['This is in contrast to dependency treebanks  e.g.', 'The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.', 'Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.', 'The choice between different actions is in general nondeterministic  and the parser relies on a memorybased classifier  trained on treebank data  to predict the next action based on features of the current parser configuration.', u'While the proportion of sentences containing non-projective dependencies is often 15\u201325%  the total proportion of non-projective arcs is normally only 1\u20132%.']
['system', 'ROUGE-S*', 'Average_R:', '0.00974', '(95%-conf.int.', '0.00974', '-', '0.00974)']
['system', 'ROUGE-S*', 'Average_P:', '0.28571', '(95%-conf.int.', '0.28571', '-', '0.28571)']
['system', 'ROUGE-S*', 'Average_F:', '0.01883', '(95%-conf.int.', '0.01883', '-', '0.01883)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:105', 'F:30']
['In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
[u'Here we use a slightly different notion of lift  applying to individual arcs and moving their head upwards one step at a time: Intuitively  lifting an arc makes the word wk dependent on the head wi of its original head wj (which is unique in a well-formed dependency graph)  unless wj is a root in which case the operation is undefined (but then wj \u2014* wk is necessarily projective if the dependency graph is well-formed).', u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', u'In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p\u2193.', 'In the third and final scheme  denoted Path  we keep the extra infor2Note that this is a baseline for the parsing experiment only (Experiment 2).', 'It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.']
['system', 'ROUGE-S*', 'Average_R:', '0.00155', '(95%-conf.int.', '0.00155', '-', '0.00155)']
['system', 'ROUGE-S*', 'Average_P:', '0.08791', '(95%-conf.int.', '0.08791', '-', '0.08791)']
['system', 'ROUGE-S*', 'Average_F:', '0.00305', '(95%-conf.int.', '0.00305', '-', '0.00305)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5151', 'P:91', 'F:8']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).', 'The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).', 'This is in contrast to dependency treebanks  e.g.', 'The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.', u'In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi \u2014*\u2217 wj for the reflexive and transitive closure of the (unlabeled) arc relation.']
['system', 'ROUGE-S*', 'Average_R:', '0.01141', '(95%-conf.int.', '0.01141', '-', '0.01141)']
['system', 'ROUGE-S*', 'Average_P:', '0.24762', '(95%-conf.int.', '0.24762', '-', '0.24762)']
['system', 'ROUGE-S*', 'Average_F:', '0.02182', '(95%-conf.int.', '0.02182', '-', '0.02182)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:105', 'F:26']
['As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.']
['As can be seen from the last column in Table 1  both Head and Head+Path may theoretically lead to a quadratic increase in the number of distinct arc labels (Head+Path being worse than Head only by a constant factor)  while the increase is only linear in the case of Path.', u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.', u'While the proportion of sentences containing non-projective dependencies is often 15\u201325%  the total proportion of non-projective arcs is normally only 1\u20132%.', 'The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.01117', '(95%-conf.int.', '0.01117', '-', '0.01117)']
['system', 'ROUGE-S*', 'Average_P:', '0.08201', '(95%-conf.int.', '0.08201', '-', '0.08201)']
['system', 'ROUGE-S*', 'Average_F:', '0.01966', '(95%-conf.int.', '0.01966', '-', '0.01966)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:378', 'F:31']
['As shown in Table 3, the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.', 'However, the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.']
['If (wi  r  wj) E A  we say that wi is the head of wj and wj a dependent of wi.', 'In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.', 'Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.', 'Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.', 'The results are given in Table 4.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:171', 'F:0']
0.0894687494408 0.00541749996614 0.00973312493917





input/ref/Task1/P08-1043_aakansha.csv
input/res/Task1/P08-1043.csv
parsing: input/ref/Task1/P08-1043_aakansha.csv
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="69" ssid="1">We represent all morphological analyses of a given utterance using a lattice structure.</S>
    <S sid="70" ssid="2">Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'70'"]
'69'
'70'
['69', '70']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="17">The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty, 2006) and (Cohen and Smith, 2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="11">Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
    <S sid="134" ssid="12">Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training 	similar to (Cohen and Smith, 2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'134'"]
'133'
'134'
['133', '134']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="17">The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="155" ssid="33">Evaluation We use 8 different measures to evaluate the performance of our system on the joint disambiguation task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S sid ="54" ssid = "1">A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.</S><S sid ="89" ssid = "21">Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree  s.t.</S><S sid ="179" ssid = "17">On the surface  our model may seem as a special case of Cohen and Smith in which  = 0.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the  hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="148" ssid = "26">Removing the leaves from the resulting tree yields a parse for L under G  with the desired probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'89'", "'179'", "'183'", "'148'"]
'54'
'89'
'179'
'183'
'148'
['54', '89', '179', '183', '148']
parsed_discourse_facet ['implication_citation']
<S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="33" ssid = "12">The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="169" ssid = "7">Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.</S><S sid ="188" ssid = "2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'33'", "'156'", "'169'", "'188'"]
'80'
'33'
'156'
'169'
'188'
['80', '33', '156', '169', '188']
parsed_discourse_facet ['implication_citation']
<S sid ="130" ssid = "8">To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and malformed7 were removed.</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="48" ssid = "6">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S><S sid ="176" ssid = "14">Furthermore  the combination of pruning and vertical markovization of the grammar outperforms the Oracle results reported by Cohen and Smith.</S><S sid ="97" ssid = "29">Thus our proposed model is a proper model assigning probability mass to all (7r  L) pairs  where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'80'", "'48'", "'176'", "'97'"]
'130'
'80'
'48'
'176'
'97'
['130', '80', '48', '176', '97']
parsed_discourse_facet ['results_citation']
<S sid ="49" ssid = "7">Tsarfaty and Simaan (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S><S sid ="163" ssid = "1">The accuracy results for segmentation  tagging and parsing using our different models and our standard data split are summarized in Table 1.</S><S sid ="71" ssid = "3">This is by now a fairly standard representation for multiple morphological segmentation of Hebrew utterances (Adler  2001; Bar-Haim et al.  2005; Smith et al.  2005; Cohen and Smith  2007; Adler  2007).</S><S sid ="33" ssid = "12">The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).</S><S sid ="148" ssid = "26">Removing the leaves from the resulting tree yields a parse for L under G  with the desired probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'163'", "'71'", "'33'", "'148'"]
'49'
'163'
'71'
'33'
'148'
['49', '163', '71', '33', '148']
parsed_discourse_facet ['method_citation']
<S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="45" ssid = "3">Morphological disambiguators that consider a token in context (an utterance) and propose the most likely morphological analysis of an utterance (including segmentation) were presented by Bar-Haim et al. (2005)  Adler and Elhadad (2006)  Shacham and Wintner (2007)  and achieved good results (the best segmentation result so far is around 98%).</S><S sid ="157" ssid = "35">SEGTok(noH) is the segmentation accuracy ignoring mistakes involving the implicit definite article h.11 To evaluate our performance on the tagging task we report CPOS and FPOS corresponding to coarse- and fine-grained PoS tagging results (F1) measure.</S><S sid ="169" ssid = "7">Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'156'", "'45'", "'157'", "'169'"]
'180'
'156'
'45'
'157'
'169'
['180', '156', '45', '157', '169']
parsed_discourse_facet ['method_citation']
<S sid ="169" ssid = "7">Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="161" ssid = "39">We report the F1 value of both measures.</S><S sid ="67" ssid = "14">Hence  we take the probability of the event fmnh analyzed as REL VB to be This means that we generate f and mnh independently depending on their corresponding PoS tags  and the context (as well as the syntactic relation between the two) is modeled via the derivation resulting in a sequence REL VB spanning the form fmnh. based on linear context.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'", "'156'", "'80'", "'161'", "'67'"]
'169'
'156'
'80'
'161'
'67'
['169', '156', '80', '161', '67']
parsed_discourse_facet ['method_citation']
<S sid ="88" ssid = "20">M(wi) = Li).</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="36" ssid = "15">Furthermore  the systematic way in which particles are prefixed to one another and onto an open-class category gives rise to a distinct sort of morphological ambiguity: space-delimited tokens may be ambiguous between several different segmentation possibilities.</S><S sid ="26" ssid = "5">The relativizer f(that) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.</S><S sid ="73" ssid = "5">We use double-circles to indicate the space-delimited token boundaries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'156'", "'36'", "'26'", "'73'"]
'88'
'156'
'36'
'26'
'73'
['88', '156', '36', '26', '73']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="38" ssid = "17">The form mnh itself can be read as at least three different verbs (counted  appointed  was appointed)  a noun (a portion)  and a possessed noun (her kind).</S><S sid ="108" ssid = "40">Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p * (s  p)) > 0  while other segments have never been observed as a lexical event before.</S><S sid ="49" ssid = "7">Tsarfaty and Simaan (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S><S sid ="158" ssid = "36">Evaluating parsing results in our joint framework  as argued by Tsarfaty (2006)  is not trivial under the joint disambiguation task  as the hypothesized yield need not coincide with the correct one.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'38'", "'108'", "'49'", "'158'"]
'80'
'38'
'108'
'49'
'158'
['80', '38', '108', '49', '158']
parsed_discourse_facet ['method_citation']
<S sid ="182" ssid = "20">In addition  as the CRF and PCFG look at similar sorts of information from within two inherently different models  they are far from independent and optimizing their product is meaningless.</S><S sid ="101" ssid = "33">The possible analyses of a surface token pose constraints on the analyses of specific segments.</S><S sid ="163" ssid = "1">The accuracy results for segmentation  tagging and parsing using our different models and our standard data split are summarized in Table 1.</S><S sid ="58" ssid = "5">Such tag sequences are often treated as complex tags (e.g.</S><S sid ="44" ssid = "2">Such analyzers propose multiple segmentation possibilities and their corresponding analyses for a token in isolation but have no means to determine the most likely ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["'182'", "'101'", "'163'", "'58'", "'44'"]
'182'
'101'
'163'
'58'
'44'
['182', '101', '163', '58', '44']
parsed_discourse_facet ['method_citation']
<S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="195" ssid = "9">We further thank Khalil Simaan (ILLCUvA) for his careful advise concerning the formal details of the proposal.</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="98" ssid = "30">The Grammar Our parser looks for the most likely tree spanning a single path through the lattice of which the yield is a sequence of lexemes.</S><S sid ="191" ssid = "5">In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'195'", "'80'", "'98'", "'191'"]
'180'
'195'
'80'
'98'
'191'
['180', '195', '80', '98', '191']
parsed_discourse_facet ['implication_citation']
<S sid ="120" ssid = "52">From now on all lattice arcs are tagged segments and the assignment of probability P(p * (s  p)) to lattice arcs proceeds as usual.4 A rather pathological case is when our lexical heuristics prune away all segmentation possibilities and we remain with an empty lattice.</S><S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="158" ssid = "36">Evaluating parsing results in our joint framework  as argued by Tsarfaty (2006)  is not trivial under the joint disambiguation task  as the hypothesized yield need not coincide with the correct one.</S><S sid ="157" ssid = "35">SEGTok(noH) is the segmentation accuracy ignoring mistakes involving the implicit definite article h.11 To evaluate our performance on the tagging task we report CPOS and FPOS corresponding to coarse- and fine-grained PoS tagging results (F1) measure.</S><S sid ="49" ssid = "7">Tsarfaty and Simaan (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'", "'180'", "'158'", "'157'", "'49'"]
'120'
'180'
'158'
'157'
'49'
['120', '180', '158', '157', '49']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="26" ssid = "5">The relativizer f(that) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.</S><S sid ="54" ssid = "1">A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.</S><S sid ="130" ssid = "8">To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and malformed7 were removed.</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'26'", "'54'", "'130'", "'80'"]
'14'
'26'
'54'
'130'
'80'
['14', '26', '54', '130', '80']
parsed_discourse_facet ['method_citation']
<S sid ="154" ssid = "32">For all grammars  we use fine-grained PoS tags indicating various morphological features annotated therein.</S><S sid ="194" ssid = "8">Acknowledgments We thank Meni Adler and Michael Elhadad (BGU) for helpful comments and discussion.</S><S sid ="97" ssid = "29">Thus our proposed model is a proper model assigning probability mass to all (7r  L) pairs  where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S><S sid ="130" ssid = "8">To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and malformed7 were removed.</S><S sid ="149" ssid = "27">We use a patched version of BitPar allowing for direct input of probabilities instead of counts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'154'", "'194'", "'97'", "'130'", "'149'"]
'154'
'194'
'97'
'130'
'149'
['154', '194', '97', '130', '149']
parsed_discourse_facet ['method_citation']
<S sid ="5" ssid = "1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  tokens) that constitute the unanalyzed surface forms (utterances).</S><S sid ="76" ssid = "8">Furthermore  some of the arcs represent lexemes not present in the input tokens (e.g. h/DT  fl/POS)  however these are parts of valid analyses of the token (cf. super-segmental morphology section 2).</S><S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="108" ssid = "40">Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p * (s  p)) > 0  while other segments have never been observed as a lexical event before.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'76'", "'180'", "'108'", "'133'"]
'5'
'76'
'180'
'108'
'133'
['5', '76', '180', '108', '133']
parsed_discourse_facet ['results_citation']
<S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="48" ssid = "6">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S><S sid ="181" ssid = "19">Many morphological decisions are based on long distance dependencies  and when the global syntactic evidence disagrees with evidence based on local linear context  the two models compete with one another  despite the fact that the PCFG takes also local context into account.</S><S sid ="95" ssid = "27">A compatible view is presented by Charniak et al. (1996) who consider the kind of probabilities a generative parser should get from a PoS tagger  and concludes that these should be P(w|t) and nothing fancier.3 In our setting  therefore  the Lattice is not used to induce a probability distribution on a linear context  but rather  it is used as a common-denominator of state-indexation of all segmentations possibilities of a surface form.</S><S sid ="54" ssid = "1">A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'48'", "'181'", "'95'", "'54'"]
'80'
'48'
'181'
'95'
'54'
['80', '48', '181', '95', '54']
parsed_discourse_facet ['implication_citation']
<S sid ="164" ssid = "2">In addition we report for each model its performance on goldsegmented input (GS) to indicate the upper bound 11Overt definiteness errors may be seen as a wrong feature rather than as wrong constituent and it is by now an accepted standard to report accuracy with and without such errors. for the grammars performance on the parsing task.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S><S sid ="22" ssid = "1">Segmental morphology Hebrew consists of seven particles m(from) f(when/who/that) h(the) w(and) k(like) l(to) and b(in). which may never appear in isolation and must always attach as prefixes to the following open-class category item we refer to as stem.</S><S sid ="126" ssid = "4">When a comparison against previous results requires additional pre-processing  we state it explicitly to allow for the reader to replicate the reported results.</S><S sid ="173" ssid = "11">The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter () which is tuned separately for each of the tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'164'", "'133'", "'22'", "'126'", "'173'"]
'164'
'133'
'22'
'126'
'173'
['164', '133', '22', '126', '173']
parsed_discourse_facet ['method_citation']
['The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.']
['Evaluating parsing results in our joint framework  as argued by Tsarfaty (2006)  is not trivial under the joint disambiguation task  as the hypothesized yield need not coincide with the correct one.', 'In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.', u'The form mnh itself can be read as at least three different verbs (\u201ccounted\u201d  \u201cappointed\u201d  \u201cwas appointed\u201d)  a noun (\u201ca portion\u201d)  and a possessed noun (\u201cher kind\u201d).', u'Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p \u2014* (s  p)) > 0  while other segments have never been observed as a lexical event before.', u'Tsarfaty and Sima\u2019an (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00085', '(95%-conf.int.', '0.00085', '-', '0.00085)']
['system', 'ROUGE-S*', 'Average_P:', '0.02564', '(95%-conf.int.', '0.02564', '-', '0.02564)']
['system', 'ROUGE-S*', 'Average_F:', '0.00165', '(95%-conf.int.', '0.00165', '-', '0.00165)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:78', 'F:2']
['Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.']
['A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.', u'Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree \u03c0 s.t.', 'Removing the leaves from the resulting tree yields a parse for L under G  with the desired probabilities.', u'On the surface  our model may seem as a special case of Cohen and Smith in which \u03b1 = 0.', u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00528', '(95%-conf.int.', '0.00528', '-', '0.00528)']
['system', 'ROUGE-S*', 'Average_P:', '0.04575', '(95%-conf.int.', '0.04575', '-', '0.04575)']
['system', 'ROUGE-S*', 'Average_F:', '0.00947', '(95%-conf.int.', '0.00947', '-', '0.00947)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:153', 'F:7']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['We further thank Khalil Simaan (ILLCUvA) for his careful advise concerning the formal details of the proposal.', 'The Grammar Our parser looks for the most likely tree spanning a single path through the lattice of which the yield is a sequence of lexemes.', 'However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.', 'In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.', 'In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.']
['system', 'ROUGE-S*', 'Average_R:', '0.00443', '(95%-conf.int.', '0.00443', '-', '0.00443)']
['system', 'ROUGE-S*', 'Average_P:', '0.04167', '(95%-conf.int.', '0.04167', '-', '0.04167)']
['system', 'ROUGE-S*', 'Average_F:', '0.00801', '(95%-conf.int.', '0.00801', '-', '0.00801)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:120', 'F:5']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).', 'The accuracy results for segmentation  tagging and parsing using our different models and our standard data split are summarized in Table 1.', 'Removing the leaves from the resulting tree yields a parse for L under G  with the desired probabilities.', 'This is by now a fairly standard representation for multiple morphological segmentation of Hebrew utterances (Adler  2001; Bar-Haim et al.  2005; Smith et al.  2005; Cohen and Smith  2007; Adler  2007).', u'Tsarfaty and Sima\xe2\u20ac\u2122an (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00095', '(95%-conf.int.', '0.00095', '-', '0.00095)']
['system', 'ROUGE-S*', 'Average_P:', '0.02500', '(95%-conf.int.', '0.02500', '-', '0.02500)']
['system', 'ROUGE-S*', 'Average_F:', '0.00183', '(95%-conf.int.', '0.00183', '-', '0.00183)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:120', 'F:3']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['Furthermore  the combination of pruning and vertical markovization of the grammar outperforms the Oracle results reported by Cohen and Smith.', 'In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.', u'To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and \u201cmalformed\u201d7 were removed.', 'Thus our proposed model is a proper model assigning probability mass to all (7r  L) pairs  where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.', 'Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.']
['system', 'ROUGE-S*', 'Average_R:', '0.00045', '(95%-conf.int.', '0.00045', '-', '0.00045)']
['system', 'ROUGE-S*', 'Average_P:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00086', '(95%-conf.int.', '0.00086', '-', '0.00086)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:120', 'F:1']
['Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).', 'In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.', 'The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.', 'Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.', 'To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).']
['system', 'ROUGE-S*', 'Average_R:', '0.00708', '(95%-conf.int.', '0.00708', '-', '0.00708)']
['system', 'ROUGE-S*', 'Average_P:', '0.11640', '(95%-conf.int.', '0.11640', '-', '0.11640)']
['system', 'ROUGE-S*', 'Average_F:', '0.01335', '(95%-conf.int.', '0.01335', '-', '0.01335)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6216', 'P:378', 'F:44']
['We represent all morphological analyses of a given utterance using a lattice structure.', 'Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.']
['M(wi) = Li).', u'The relativizer f(\u201cthat\u201d) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.', 'We use double-circles to indicate the space-delimited token boundaries.', 'Furthermore  the systematic way in which particles are prefixed to one another and onto an open-class category gives rise to a distinct sort of morphological ambiguity: space-delimited tokens may be ambiguous between several different segmentation possibilities.', 'To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).']
['system', 'ROUGE-S*', 'Average_R:', '0.00171', '(95%-conf.int.', '0.00171', '-', '0.00171)']
['system', 'ROUGE-S*', 'Average_P:', '0.02339', '(95%-conf.int.', '0.02339', '-', '0.02339)']
['system', 'ROUGE-S*', 'Average_F:', '0.00318', '(95%-conf.int.', '0.00318', '-', '0.00318)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:171', 'F:4']
['Evaluation We use 8 different measures to evaluate the performance of our system on the joint disambiguation task.']
[u'In addition we report for each model its performance on goldsegmented input (GS) to indicate the upper bound 11Overt definiteness errors may be seen as a wrong feature rather than as wrong constituent and it is by now an accepted standard to report accuracy with and without such errors. for the grammars\u2019 performance on the parsing task.', 'Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.', u'The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter (\u03b1) which is tuned separately for each of the tasks.', 'When a comparison against previous results requires additional pre-processing  we state it explicitly to allow for the reader to replicate the reported results.', u'Segmental morphology Hebrew consists of seven particles m(\u201cfrom\u201d) f(\u201cwhen\u201d/\u201cwho\u201d/\u201cthat\u201d) h(\u201cthe\u201d) w(\u201cand\u201d) k(\u201clike\u201d) l(\u201cto\u201d) and b(\u201cin\u201d). which may never appear in isolation and must always attach as prefixes to the following open-class category item we refer to as stem.']
['system', 'ROUGE-S*', 'Average_R:', '0.00032', '(95%-conf.int.', '0.00032', '-', '0.00032)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00064', '(95%-conf.int.', '0.00064', '-', '0.00064)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:36', 'F:1']
['Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.', 'Morphological disambiguators that consider a token in context (an utterance) and propose the most likely morphological analysis of an utterance (including segmentation) were presented by Bar-Haim et al. (2005)  Adler and Elhadad (2006)  Shacham and Wintner (2007)  and achieved good results (the best segmentation result so far is around 98%).', 'Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.', 'SEGTok(noH) is the segmentation accuracy ignoring mistakes involving the implicit definite article h.11 To evaluate our performance on the tagging task we report CPOS and FPOS corresponding to coarse- and fine-grained PoS tagging results (F1) measure.', 'To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).']
['system', 'ROUGE-S*', 'Average_R:', '0.00129', '(95%-conf.int.', '0.00129', '-', '0.00129)']
['system', 'ROUGE-S*', 'Average_P:', '0.02116', '(95%-conf.int.', '0.02116', '-', '0.02116)']
['system', 'ROUGE-S*', 'Average_F:', '0.00243', '(95%-conf.int.', '0.00243', '-', '0.00243)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6216', 'P:378', 'F:8']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['In addition  as the CRF and PCFG look at similar sorts of information from within two inherently different models  they are far from independent and optimizing their product is meaningless.', 'Such analyzers propose multiple segmentation possibilities and their corresponding analyses for a token in isolation but have no means to determine the most likely ones.', 'The possible analyses of a surface token pose constraints on the analyses of specific segments.', u'Such tag sequences are often treated as \u201ccomplex tags\u201d (e.g.', 'The accuracy results for segmentation  tagging and parsing using our different models and our standard data split are summarized in Table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00093', '(95%-conf.int.', '0.00093', '-', '0.00093)']
['system', 'ROUGE-S*', 'Average_P:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00167', '(95%-conf.int.', '0.00167', '-', '0.00167)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:120', 'F:1']
0.0343449996566 0.00232899997671 0.00430899995691





input/ref/Task1/J01-2004_aakansha.csv
input/res/Task1/J01-2004.csv
parsing: input/ref/Task1/J01-2004_aakansha.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'"]
'372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="17" ssid="5">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



["'302'"]
'302'
['302']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="37">This underspecification of the nonterminal predictions (e.g., VP-VBD in the example in Figure 2, as opposed to NP), allows lexical items to become part of the left context, and so be used to condition production probabilities, even the production probabilities of constituents that dominate them in the unfactored tree.</S>
    <S sid="80" ssid="38">It also brings words further downstream into the look-ahead at the point of specification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'", "'80'"]
'79'
'80'
['79', '80']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="42">Our approach is found to yield very accurate parses efficiently, and, in addition, to lend itself straightforwardly to estimating word probabilities on-line, that is, in a single pass from left to right.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="291" ssid="47">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'291'"]
'291'
['291']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['method_citation']
<S sid="209" ssid="113">This parser is essentially a stochastic version of the top-down parser described in Aho, Sethi, and Ullman (1986).</S>
    <S sid="210" ssid="114">It uses a PCFG with a conditional probability model of the sort defined in the previous section.</S>
original cit marker offset is 0
new cit marker offset is 0



["'209'", "'210'"]
'209'
'210'
['209', '210']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'"]
'372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="8">Two features of our top-down parsing approach will emerge as key to its success.</S>
    <S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S><S sid="32" ssid="20">A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'21'", "'32'"]
'20'
'21'
'32'
['20', '21', '32']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="21">Because the rooted partial derivation is fully connected, all of the conditioning information that might be extracted from the top-down left context has already been specified, and a conditional probability model built on this information will not impose any additional burden on the search.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'"]
'33'
['33']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S sid ="321" ssid = "77">In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.</S><S sid ="387" ssid = "143">Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.</S><S sid ="315" ssid = "71">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;the sum of the probabilities over the vocabulary is less than one.</S><S sid ="344" ssid = "100">However  when we interpolate with the trigram  we see that the additional improvement is greater than the one they experienced.</S><S sid ="403" ssid = "16">Perhaps some compromise between the fully connected structures and extreme underspecification will yield an efficiency improvement.</S>
original cit marker offset is 0
new cit marker offset is 0



["'321'", "'387'", "'315'", "'344'", "'403'"]
'321'
'387'
'315'
'344'
'403'
['321', '387', '315', '344', '403']
parsed_discourse_facet ['implication_citation']
<S sid ="380" ssid = "136">Future work will include more substantial word recognition experiments.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S><S sid ="344" ssid = "100">However  when we interpolate with the trigram  we see that the additional improvement is greater than the one they experienced.</S><S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S>
original cit marker offset is 0
new cit marker offset is 0



["'380'", "'280'", "'391'", "'344'", "'336'"]
'380'
'280'
'391'
'344'
'336'
['380', '280', '391', '344', '336']
parsed_discourse_facet ['implication_citation']
<S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="338" ssid = "94">Table 4 compares the perplexity of our model with Chelba and Jelinek (1998a  1998b) on the same training and testing corpora.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="326" ssid = "82">We obtained the training and testing corpora from them (which we will denote C&J corpus)  and also created intermediate corpora  upon which only the first two modifications were carried out (which we will denote no punct).</S><S sid ="343" ssid = "99">Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'</S>
original cit marker offset is 0
new cit marker offset is 0



["'349'", "'338'", "'280'", "'326'", "'343'"]
'349'
'338'
'280'
'326'
'343'
['349', '338', '280', '326', '343']
parsed_discourse_facet ['results_citation']
<S sid ="372" ssid = "128">The small size of our training data  as well as the fact that we are rescoring n-best lists  rather than working directly on lattices  makes comparison with the other models not particularly informative.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="309" ssid = "65">Let Ht be the priority queue H  before any processing has begun with word w  in the look-ahead.</S><S sid ="266" ssid = "22">From this set of measures  we will also include the crossing bracket scores: average crossing brackets (CB)  percentage of sentences with no crossing brackets (0 CB)  and the percentage of sentences with two crossing brackets or fewer (< 2 CB).</S><S sid ="268" ssid = "24">This is an incremental parser with a pruning strategy and no backtracking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'", "'301'", "'309'", "'266'", "'268'"]
'372'
'301'
'309'
'266'
'268'
['372', '301', '309', '266', '268']
parsed_discourse_facet ['method_citation']
<S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="355" ssid = "111">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task  we performed a very small preliminary experiment on n-best lists.</S><S sid ="340" ssid = "96">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid ="354" ssid = "110">Interestingly  at the word where the failure occurred  the sum of the probabilities was 0.9301.</S>
original cit marker offset is 0
new cit marker offset is 0



["'336'", "'301'", "'355'", "'340'", "'354'"]
'336'
'301'
'355'
'340'
'354'
['336', '301', '355', '340', '354']
parsed_discourse_facet ['method_citation']
<S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="361" ssid = "117">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S><S sid ="319" ssid = "75">The hope  however  is that the improved parsing model provided by our conditional probability model will cause the distribution over structures to be more peaked  thus enabling us to capture more of the total probability mass  and making this a fairly snug upper bound on the perplexity.</S><S sid ="355" ssid = "111">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task  we performed a very small preliminary experiment on n-best lists.</S>
original cit marker offset is 0
new cit marker offset is 0



["'301'", "'349'", "'361'", "'319'", "'355'"]
'301'
'349'
'361'
'319'
'355'
['301', '349', '361', '319', '355']
parsed_discourse_facet ['method_citation']
<S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="324" ssid = "80">Their modifications included: (i) removing orthographic cues to structure (e.g.  punctuation); (ii) replacing all numbers with the single token N; and (iii) closing the vocabulary at 10 000  replacing all other words with the UNK token.</S><S sid ="343" ssid = "99">Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'</S><S sid ="387" ssid = "143">Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.</S><S sid ="346" ssid = "102">These results are particularly remarkable  given that we did not build our model as a language model per se  but rather as a parsing model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'349'", "'324'", "'343'", "'387'", "'346'"]
'349'
'324'
'343'
'387'
'346'
['349', '324', '343', '387', '346']
parsed_discourse_facet ['method_citation']
<S sid ="315" ssid = "71">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;the sum of the probabilities over the vocabulary is less than one.</S><S sid ="377" ssid = "133">The point of this small experiment was to see if our parsing model could provide useful information even in the case that recognition errors occur  as opposed to the (generally) fully grammatical strings upon which the perplexity results were obtained.</S><S sid ="288" ssid = "44">Interestingly  conditioning all POS expansions on two c-commanding heads made no difference in accuracy compared to conditioning only leftmost POS expansions on a single c-commanding head; but it did improve the efficiency.</S><S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="382" ssid = "138">The base beam factor that we have used to this point is 10'  which is quite wide.</S>
original cit marker offset is 0
new cit marker offset is 0



["'315'", "'377'", "'288'", "'358'", "'382'"]
'315'
'377'
'288'
'358'
'382'
['315', '377', '288', '358', '382']
parsed_discourse_facet ['method_citation']
<S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="382" ssid = "138">The base beam factor that we have used to this point is 10'  which is quite wide.</S><S sid ="283" ssid = "39">Unlike the Roark and Johnson parser  however  our coverage did not substantially drop as the amount of conditioning information increased  and in some cases  coverage improved slightly.</S><S sid ="380" ssid = "136">Future work will include more substantial word recognition experiments.</S><S sid ="268" ssid = "24">This is an incremental parser with a pruning strategy and no backtracking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'301'", "'382'", "'283'", "'380'", "'268'"]
'301'
'382'
'283'
'380'
'268'
['301', '382', '283', '380', '268']
parsed_discourse_facet ['method_citation']
<S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S><S sid ="321" ssid = "77">In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.</S><S sid ="262" ssid = "18">Recall is the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD.</S>
original cit marker offset is 0
new cit marker offset is 0



["'336'", "'280'", "'391'", "'321'", "'262'"]
'336'
'280'
'391'
'321'
'262'
['336', '280', '391', '321', '262']
parsed_discourse_facet ['implication_citation']
<S sid ="315" ssid = "71">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;the sum of the probabilities over the vocabulary is less than one.</S><S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S><S sid ="295" ssid = "51">Our observed times look polynomial  which is to be expected given our pruning strategy: the denser the competitors within a narrow probability range of the best analysis  the more time will be spent working on these competitors; and the farther along in the sentence  the more chance for ambiguities that can lead to such a situation.</S><S sid ="382" ssid = "138">The base beam factor that we have used to this point is 10'  which is quite wide.</S><S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'315'", "'336'", "'295'", "'382'", "'391'"]
'315'
'336'
'295'
'382'
'391'
['315', '336', '295', '382', '391']
parsed_discourse_facet ['method_citation']
<S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="258" ssid = "14">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S><S sid ="372" ssid = "128">The small size of our training data  as well as the fact that we are rescoring n-best lists  rather than working directly on lattices  makes comparison with the other models not particularly informative.</S><S sid ="298" ssid = "54">What is perhaps surprising is that the difference is not greater.</S><S sid ="270" ssid = "26">In such a case  the parser fails to return a complete parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'301'", "'258'", "'372'", "'298'", "'270'"]
'301'
'258'
'372'
'298'
'270'
['301', '258', '372', '298', '270']
parsed_discourse_facet ['method_citation']
<S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S><S sid ="398" ssid = "11">By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="354" ssid = "110">Interestingly  at the word where the failure occurred  the sum of the probabilities was 0.9301.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'391'", "'398'", "'301'", "'354'", "'404'"]
'391'
'398'
'301'
'354'
'404'
['391', '398', '301', '354', '404']
parsed_discourse_facet ['method_citation']
<S sid ="402" ssid = "15">In fact  left-corner parsing can be simulated by a top-down parser by transforming the grammar  as was done in Roark and Johnson (1999)  and so an approach very similar to the one outlined here could be used in that case.</S><S sid ="398" ssid = "11">By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="371" ssid = "127">For our model and the Treebank trigram model  the LM weight that resulted in the lowest error rates is given.</S><S sid ="343" ssid = "99">Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'</S>
original cit marker offset is 0
new cit marker offset is 0



["'402'", "'398'", "'280'", "'371'", "'343'"]
'402'
'398'
'280'
'371'
'343'
['402', '398', '280', '371', '343']
parsed_discourse_facet ['results_citation']
<S sid ="398" ssid = "11">By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="361" ssid = "117">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S><S sid ="258" ssid = "14">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S>
original cit marker offset is 0
new cit marker offset is 0



["'398'", "'301'", "'390'", "'361'", "'258'"]
'398'
'301'
'390'
'361'
'258'
['398', '301', '390', '361', '258']
parsed_discourse_facet ['implication_citation']
<S sid ="258" ssid = "14">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S><S sid ="262" ssid = "18">Recall is the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD.</S><S sid ="361" ssid = "117">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S><S sid ="339" ssid = "95">We built an interpolated trigram model to serve as a baseline (as they did)  and also interpolated our model's perplexity with the trigram  using the same mixing coefficient as they did in their trials (taking 36 percent of the estimate from the trigram).'</S><S sid ="324" ssid = "80">Their modifications included: (i) removing orthographic cues to structure (e.g.  punctuation); (ii) replacing all numbers with the single token N; and (iii) closing the vocabulary at 10 000  replacing all other words with the UNK token.</S>
original cit marker offset is 0
new cit marker offset is 0



["'258'", "'262'", "'361'", "'339'", "'324'"]
'258'
'262'
'361'
'339'
'324'
['258', '262', '361', '339', '324']
parsed_discourse_facet ['method_citation']
<S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="257" ssid = "13">A constituent for evaluation purposes consists of a label (e.g.  NP) and a span (beginning and ending word positions).</S><S sid ="340" ssid = "96">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="387" ssid = "143">Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.</S>
original cit marker offset is 0
new cit marker offset is 0



["'349'", "'257'", "'340'", "'301'", "'387'"]
'349'
'257'
'340'
'301'
'387'
['349', '257', '340', '301', '387']
parsed_discourse_facet ['results_citation']
['Because the rooted partial derivation is fully connected, all of the conditioning information that might be extracted from the top-down left context has already been specified, and a conditional probability model built on this information will not impose any additional burden on the search.']
["By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).", 'Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.', 'The trigram model was also trained on Sections 00-20 of the C&J corpus.', "One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'", 'A constituent for evaluation purposes consists of a label (e.g.  NP) and a span (beginning and ending word positions).']
['system', 'ROUGE-S*', 'Average_R:', '0.00121', '(95%-conf.int.', '0.00121', '-', '0.00121)']
['system', 'ROUGE-S*', 'Average_P:', '0.01053', '(95%-conf.int.', '0.01053', '-', '0.01053)']
['system', 'ROUGE-S*', 'Average_F:', '0.00217', '(95%-conf.int.', '0.00217', '-', '0.00217)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:190', 'F:2']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
["By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).", 'These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.', 'By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.', 'Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.', 'Interestingly  at the word where the failure occurred  the sum of the probabilities was 0.9301.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:91', 'F:0']
['It also brings words further downstream into the look-ahead at the point of specification.', 'This underspecification of the nonterminal predictions (e.g., VP-VBD in the example in Figure 2, as opposed to NP), allows lexical items to become part of the left context, and so be used to condition production probabilities, even the production probabilities of constituents that dominate them in the unfactored tree.']
['These results are particularly remarkable  given that we did not build our model as a language model per se  but rather as a parsing model.', 'Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.', "One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'", 'Their modifications included: (i) removing orthographic cues to structure (e.g.  punctuation); (ii) replacing all numbers with the single token N; and (iii) closing the vocabulary at 10 000  replacing all other words with the UNK token.', "Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00140', '(95%-conf.int.', '0.00140', '-', '0.00140)']
['system', 'ROUGE-S*', 'Average_P:', '0.00739', '(95%-conf.int.', '0.00739', '-', '0.00739)']
['system', 'ROUGE-S*', 'Average_F:', '0.00235', '(95%-conf.int.', '0.00235', '-', '0.00235)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:406', 'F:3']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['However  when we interpolate with the trigram  we see that the additional improvement is greater than the one they experienced.', 'In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.', 'Perhaps some compromise between the fully connected structures and extreme underspecification will yield an efficiency improvement.', u'Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;\xe2\u20ac\u201dthe sum of the probabilities over the vocabulary is less than one.', 'Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.']
['system', 'ROUGE-S*', 'Average_R:', '0.00078', '(95%-conf.int.', '0.00078', '-', '0.00078)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00146', '(95%-conf.int.', '0.00146', '-', '0.00146)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:91', 'F:1']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['Our observed times look polynomial  which is to be expected given our pruning strategy: the denser the competitors within a narrow probability range of the best analysis  the more time will be spent working on these competitors; and the farther along in the sentence  the more chance for ambiguities that can lead to such a situation.', "The base beam factor that we have used to this point is 10'  which is quite wide.", 'The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.', 'These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.', u'Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;\xe2\u20ac\u201dthe sum of the probabilities over the vocabulary is less than one.']
['system', 'ROUGE-S*', 'Average_R:', '0.00273', '(95%-conf.int.', '0.00273', '-', '0.00273)']
['system', 'ROUGE-S*', 'Average_P:', '0.08791', '(95%-conf.int.', '0.08791', '-', '0.08791)']
['system', 'ROUGE-S*', 'Average_F:', '0.00530', '(95%-conf.int.', '0.00530', '-', '0.00530)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:91', 'F:8']
['Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.']
['There are a couple of things to notice from these results.', 'These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.', 'The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.', 'In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.', 'Recall is the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD.']
['system', 'ROUGE-S*', 'Average_R:', '0.00325', '(95%-conf.int.', '0.00325', '-', '0.00325)']
['system', 'ROUGE-S*', 'Average_P:', '0.01149', '(95%-conf.int.', '0.01149', '-', '0.01149)']
['system', 'ROUGE-S*', 'Average_F:', '0.00506', '(95%-conf.int.', '0.00506', '-', '0.00506)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:435', 'F:5']
['This parser is essentially a stochastic version of the top-down parser described in Aho, Sethi, and Ullman (1986).', 'It uses a PCFG with a conditional probability model of the sort defined in the previous section.']
['In such a case  the parser fails to return a complete parse.', "By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).", 'For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.', 'The small size of our training data  as well as the fact that we are rescoring n-best lists  rather than working directly on lattices  makes comparison with the other models not particularly informative.', 'What is perhaps surprising is that the difference is not greater.']
['system', 'ROUGE-S*', 'Average_R:', '0.00676', '(95%-conf.int.', '0.00676', '-', '0.00676)']
['system', 'ROUGE-S*', 'Average_P:', '0.04575', '(95%-conf.int.', '0.04575', '-', '0.04575)']
['system', 'ROUGE-S*', 'Average_F:', '0.01178', '(95%-conf.int.', '0.01178', '-', '0.01178)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:153', 'F:7']
['This paper will examine language modeling for speech recognition from a natural language processing point of view.']
['Future work will include more substantial word recognition experiments.', 'There are a couple of things to notice from these results.', 'These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.', 'The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.', 'However  when we interpolate with the trigram  we see that the additional improvement is greater than the one they experienced.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:55', 'F:0']
0.021757499728 0.0020162499748 0.00351499995606





input/ref/Task1/P08-1102_swastika.csv
input/res/Task1/P08-1102.csv
parsing: input/ref/Task1/P08-1102_swastika.csv
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="10">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="8">All feature templates and their instances are shown in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



['36']
36
['36']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
<S sid="97" ssid="8">Figure 3 shows their learning curves depicting the F-measure on the development set after 1 to 10 training iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



['97']
97
['97']
parsed_discourse_facet ['result_citation']
<S sid="91" ssid="2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['aim_citation']
<S sid="16" ssid="12">Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



['34']
34
['34']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S sid ="120" ssid = "31">Without the perceptron  the cascaded model (if we can still call it cascaded) performs poorly on both segmentation and Joint S&T.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'", "'58'", "'136'", "'31'", "'134'"]
'120'
'58'
'136'
'31'
'134'
['120', '58', '136', '31', '134']
parsed_discourse_facet ['implication_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="117" ssid = "28">Table 4 shows experiments results.</S><S sid ="93" ssid = "4">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid ="18" ssid = "14">In this architecture  knowledge sources that are intractable to incorporate into the perceptron  can be easily incorporated into the outside linear model.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'117'", "'93'", "'18'", "'136'"]
'68'
'117'
'93'
'18'
'136'
['68', '117', '93', '18', '136']
parsed_discourse_facet ['implication_citation']
<S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x  X to outputs y  Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="78" ssid = "3">Given a Chinese character sequence C1:n  the decoding procedure can proceed in a left-right fashion with a dynamic programming approach.</S><S sid ="79" ssid = "4">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="46" ssid = "18">Following Collins  we use a function GEN(x) generating all candidate results of an input x   a representation 4) mapping each training example (x  y)  X  Y to a feature vector 4)(x  y)  Rd  and a parameter vector   Rd corresponding to the feature vector. d means the dimension of the vector space  it equals to the amount of features in the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'", "'78'", "'79'", "'7'", "'46'"]
'45'
'78'
'79'
'7'
'46'
['45', '78', '79', '7', '46']
parsed_discourse_facet ['results_citation']
<S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="65" ssid = "16">As shown in Figure 1  the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.</S><S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="95" ssid = "6">For convenience of comparing with others  we focus only on the close test  which means that any extra resource is forbidden except the designated training corpus.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'65'", "'101'", "'95'", "'58'"]
'133'
'65'
'101'
'95'
'58'
['133', '65', '101', '95', '58']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="95" ssid = "6">For convenience of comparing with others  we focus only on the close test  which means that any extra resource is forbidden except the designated training corpus.</S><S sid ="61" ssid = "12">In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'47'", "'95'", "'61'", "'136'"]
'101'
'47'
'95'
'61'
'136'
['101', '47', '95', '61', '136']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called averaged parameters (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="71" ssid = "22">Using W = w1:m to denote the word sequence  T = t1:m to denote the corresponding POS sequence  P (T |W) to denote the probability that W is labelled as T  and P(W|T) to denote the probability that T generates W  we can define the cooccurrence model as follows: wt and tw denote the corresponding weights of the two components.</S><S sid ="92" ssid = "3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.</S><S sid ="82" ssid = "7">In addition  we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'49'", "'71'", "'92'", "'82'"]
'68'
'49'
'71'
'92'
'82'
['68', '49', '71', '92', '82']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "10">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid ="40" ssid = "12">Templates in the column below are expanded from the upper ones.</S><S sid ="43" ssid = "15">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S><S sid ="25" ssid = "21">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'58'", "'40'", "'43'", "'25'"]
'38'
'58'
'40'
'43'
'25'
['38', '58', '40', '43', '25']
parsed_discourse_facet ['method_citation']
<S sid ="93" ssid = "4">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid ="57" ssid = "8">It has a two-layer architecture  with a perceptron as the core and another linear model as the outside-layer.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="40" ssid = "12">Templates in the column below are expanded from the upper ones.</S><S sid ="17" ssid = "13">We will describe it in detail in Section 4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'93'", "'57'", "'136'", "'40'", "'17'"]
'93'
'57'
'136'
'40'
'17'
['93', '57', '136', '40', '17']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="33" ssid = "5">In following subsections  we describe the feature templates and the perceptron training algorithm.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="96" ssid = "7">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus  we randomly chosen 2  000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84  294 sentences)  then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'68'", "'33'", "'134'", "'96'"]
'101'
'68'
'33'
'134'
'96'
['101', '68', '33', '134', '96']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="89" ssid = "14">Function D derives the candidate result from the word-POS pair p and the candidate q at prior position of p.</S><S sid ="9" ssid = "5">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid ="79" ssid = "4">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid ="104" ssid = "15">According to the usual practice in syntactic analysis  we choose chapters 1  260 (18074 sentences) as training set  chapter 271  300 (348 sentences) as test set and chapter 301  325 (350 sentences) as development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'89'", "'9'", "'79'", "'104'"]
'68'
'89'
'9'
'79'
'104'
['68', '89', '9', '79', '104']
parsed_discourse_facet ['implication_citation']
<S sid ="125" ssid = "36">However unlike the three features  the word LM brings very tiny improvement.</S><S sid ="40" ssid = "12">Templates in the column below are expanded from the upper ones.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'40'", "'94'", "'2'", "'58'"]
'125'
'40'
'94'
'2'
'58'
['125', '40', '94', '2', '58']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="56" ssid = "7">To alleviate the drawbacks  we propose a cascaded linear model.</S><S sid ="61" ssid = "12">In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.</S><S sid ="93" ssid = "4">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid ="112" ssid = "23">Here the core perceptron was just the POS+ model in experiments above.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'56'", "'61'", "'93'", "'112'"]
'136'
'56'
'61'
'93'
'112'
['136', '56', '61', '93', '112']
parsed_discourse_facet ['method_citation']
<S sid ="99" ssid = "10">Then we trained LEX on each of the four corpora for 7 iterations.</S><S sid ="120" ssid = "31">Without the perceptron  the cascaded model (if we can still call it cascaded) performs poorly on both segmentation and Joint S&T.</S><S sid ="121" ssid = "32">Among other features  the 4-gram POS LM plays the most important role  removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&T.</S><S sid ="42" ssid = "14">As predications generated from such templates depend on the current character  we name these templates lexical-target.</S><S sid ="117" ssid = "28">Table 4 shows experiments results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'99'", "'120'", "'121'", "'42'", "'117'"]
'99'
'120'
'121'
'42'
'117'
['99', '120', '121', '42', '117']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="64" ssid = "15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S><S sid ="53" ssid = "4">Figure 2 shows the growing tendency of feature space with the introduction of these features as well as the character-based ones.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'134'", "'64'", "'53'", "'7'"]
'68'
'134'
'64'
'53'
'7'
['68', '134', '64', '53', '7']
parsed_discourse_facet ['results_citation']
<S sid ="3" ssid = "3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid ="136" ssid = "7">How can we utilize these knowledge sources effectively?</S><S sid ="84" ssid = "9">Algorithm 2 shows the decoding algorithm.</S><S sid ="18" ssid = "14">In this architecture  knowledge sources that are intractable to incorporate into the perceptron  can be easily incorporated into the outside linear model.</S><S sid ="92" ssid = "3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'136'", "'84'", "'18'", "'92'"]
'3'
'136'
'84'
'18'
'92'
['3', '136', '84', '18', '92']
parsed_discourse_facet ['implication_citation']
<S sid ="51" ssid = "2">Additional features most widely used are related to word or POS ngrams.</S><S sid ="13" ssid = "9">However  as such features are generated dynamically during the decoding procedure  two limitation arise: on the one hand  the amount of parameters increases rapidly  which is apt to overfit on training corpus; on the other hand  exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called averaged parameters (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="36" ssid = "8">All feature templates and their instances are shown in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'13'", "'7'", "'49'", "'36'"]
'51'
'13'
'7'
'49'
'36'
['51', '13', '7', '49', '36']
parsed_discourse_facet ['method_citation']
['Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.']
[u'Following Collins  we use a function GEN(x) generating all candidate results of an input x   a representation 4) mapping each training example (x  y) \u2208 X \xd7 Y to a feature vector 4)(x  y) \u2208 Rd  and a parameter vector \u03b1\ufffd \u2208 Rd corresponding to the feature vector. d means the dimension of the vector space  it equals to the amount of features in the model.', u'We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x \u2208 X to outputs y \u2208 Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.', 'CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.', 'By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.', 'Given a Chinese character sequence C1:n  the decoding procedure can proceed in a left-right fashion with a dynamic programming approach.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:66', 'F:0']
['All feature templates and their instances are shown in Table 1.']
['It is an important measure of fluency of the translation in SMT.', 'In addition  we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates.', 'The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.', u'Using W = w1:m to denote the word sequence  T = t1:m to denote the corresponding POS sequence  P (T |W) to denote the probability that W is labelled as T  and P(W|T) to denote the probability that T generates W  we can define the cooccurrence model as follows: \u03bbwt and \u03bbtw denote the corresponding weights of the two components.', u'To alleviate overfitting on the training examples  we use the refinement strategy called \u201caveraged parameters\u201d (Collins  2002) to the algorithm in Algorithm 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00044', '(95%-conf.int.', '0.00044', '-', '0.00044)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00087', '(95%-conf.int.', '0.00087', '-', '0.00087)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:15', 'F:1']
['The feature templates we adopted are selected from those of Ng and Low (2004).']
['In this architecture  knowledge sources that are intractable to incorporate into the perceptron  can be easily incorporated into the outside linear model.', 'Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.', 'The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.', 'How can we utilize these knowledge sources effectively?', 'Algorithm 2 shows the decoding algorithm.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:21', 'F:0']
['The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.']
['For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.', 'On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).', 'For convenience of comparing with others  we focus only on the close test  which means that any extra resource is forbidden except the designated training corpus.', 'How can we utilize these knowledge sources effectively?', 'In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:91', 'F:0']
['The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.']
['It is an important measure of fluency of the translation in SMT.', 'To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).', 'Function D derives the candidate result from the word-POS pair p and the candidate q at prior position of p.', u'According to the usual practice in syntactic analysis  we choose chapters 1 \u2212 260 (18074 sentences) as training set  chapter 271 \u2212 300 (348 sentences) as test set and chapter 301 \u2212 325 (350 sentences) as development set.', 'By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.']
['system', 'ROUGE-S*', 'Average_R:', '0.00128', '(95%-conf.int.', '0.00128', '-', '0.00128)']
['system', 'ROUGE-S*', 'Average_P:', '0.03297', '(95%-conf.int.', '0.03297', '-', '0.03297)']
['system', 'ROUGE-S*', 'Average_F:', '0.00246', '(95%-conf.int.', '0.00246', '-', '0.00246)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:91', 'F:3']
['We trained a character-based perceptron for Chinese Joint S&T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.']
['According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.', 'Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.', 'Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.', 'Templates in the column below are expanded from the upper ones.', 'Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.']
['system', 'ROUGE-S*', 'Average_R:', '0.00360', '(95%-conf.int.', '0.00360', '-', '0.00360)']
['system', 'ROUGE-S*', 'Average_P:', '0.10989', '(95%-conf.int.', '0.10989', '-', '0.10989)']
['system', 'ROUGE-S*', 'Average_F:', '0.00698', '(95%-conf.int.', '0.00698', '-', '0.00698)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:91', 'F:10']
['The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.']
['However  as such features are generated dynamically during the decoding procedure  two limitation arise: on the one hand  the amount of parameters increases rapidly  which is apt to overfit on training corpus; on the other hand  exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.', 'Additional features most widely used are related to word or POS ngrams.', 'All feature templates and their instances are shown in Table 1.', 'CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.', u'To alleviate overfitting on the training examples  we use the refinement strategy called \u201caveraged parameters\u201d (Collins  2002) to the algorithm in Algorithm 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:91', 'F:0']
0.0299328567152 0.000759999989143 0.00147285712182





input/ref/Task1/W11-2123_aakansha.csv
input/res/Task1/W11-2123.csv
parsing: input/ref/Task1/W11-2123_aakansha.csv
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="136" ssid="8">We offer a state function s(wn1) = wn&#65533; where substring wn&#65533; is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'"]
'136'
['136']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="3">Queries take the form p(wn|wn&#8722;1 1 ) where wn1 is an n-gram.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="205" ssid="24">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al., 2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'205'"]
'205'
['205']
parsed_discourse_facet ['method_citation']
<S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
<S sid="204" ssid="23">For RandLM, we used the settings in the documentation: 8 bits per value and false positive probability 1 256.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'"]
'204'
['204']
parsed_discourse_facet ['method_citation']
<S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="229" ssid="48">Then we ran binary search to determine the least amount of memory with which it would run.</S>
original cit marker offset is 0
new cit marker offset is 0



["'229'"]
'229'
['229']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="93" ssid="71">The cost of storing these averages, in bits, is Because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.</S>
original cit marker offset is 0
new cit marker offset is 0



["'93'"]
'93'
['93']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W11-2123.csv
<S sid ="270" ssid = "12">This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S>
original cit marker offset is 0
new cit marker offset is 0



["'270'", "'276'", "'265'", "'286'", "'284'"]
'270'
'276'
'265'
'286'
'284'
['270', '276', '265', '286', '284']
parsed_discourse_facet ['implication_citation']
<S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S>
original cit marker offset is 0
new cit marker offset is 0



["'262'", "'280'", "'265'", "'277'", "'284'"]
'262'
'280'
'265'
'277'
'284'
['262', '280', '265', '277', '284']
parsed_discourse_facet ['implication_citation']
<S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="285" ssid = "6">Juri Ganitkevitch answered questions about Joshua.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'265'", "'284'", "'278'", "'285'", "'276'"]
'265'
'284'
'278'
'285'
'276'
['265', '284', '278', '285', '276']
parsed_discourse_facet ['results_citation']
<S sid ="260" ssid = "2">For speed  we plan to implement the direct-mapped cache from BerkeleyLM.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>
original cit marker offset is 0
new cit marker offset is 0



["'260'", "'262'", "'280'", "'283'", "'266'"]
'260'
'262'
'280'
'283'
'266'
['260', '262', '280', '283', '266']
parsed_discourse_facet ['method_citation']
<S sid ="272" ssid = "14">Generalizing state minimization  the model could also provide explicit bounds on probability for both backward and forward extension.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="268" ssid = "10">For example  syntactic decoders (Koehn et al.  2007; Dyer et al.  2010; Li et al.  2009) perform dynamic programming parametrized by both backward- and forward-looking state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'272'", "'265'", "'283'", "'286'", "'268'"]
'272'
'265'
'283'
'286'
'268'
['272', '265', '283', '286', '268']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="264" ssid = "6">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'284'", "'265'", "'264'", "'267'"]
'256'
'284'
'265'
'264'
'267'
['256', '284', '265', '264', '267']
parsed_discourse_facet ['method_citation']
<S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S>
original cit marker offset is 0
new cit marker offset is 0



["'283'", "'284'", "'280'", "'265'", "'275'"]
'283'
'284'
'280'
'265'
'275'
['283', '284', '280', '265', '275']
parsed_discourse_facet ['method_citation']
<S sid ="261" ssid = "3">Much could be done to further reduce memory consumption.</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'261'", "'277'", "'286'", "'284'", "'283'"]
'261'
'277'
'286'
'284'
'283'
['261', '277', '286', '284', '283']
parsed_discourse_facet ['method_citation']
<S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S><S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'286'", "'280'", "'267'", "'279'", "'283'"]
'286'
'280'
'267'
'279'
'283'
['286', '280', '267', '279', '283']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'287'", "'266'", "'262'", "'265'"]
'256'
'287'
'266'
'262'
'265'
['256', '287', '266', '262', '265']
parsed_discourse_facet ['implication_citation']
<S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="264" ssid = "6">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'262'", "'278'", "'277'", "'265'", "'264'"]
'262'
'278'
'277'
'265'
'264'
['262', '278', '277', '265', '264']
parsed_discourse_facet ['method_citation']
<S sid ="261" ssid = "3">Much could be done to further reduce memory consumption.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S><S sid ="269" ssid = "11">If they knew that the first four words in a hypergraph node would never extend to the left and form a 5-gram  then three or even fewer words could be kept in the backward state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'261'", "'265'", "'266'", "'267'", "'269'"]
'261'
'265'
'266'
'267'
'269'
['261', '265', '266', '267', '269']
parsed_discourse_facet ['method_citation']
<S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'287'", "'266'", "'286'", "'262'", "'274'"]
'287'
'266'
'286'
'262'
'274'
['287', '266', '286', '262', '274']
parsed_discourse_facet ['method_citation']
<S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'286'", "'256'", "'265'", "'284'", "'276'"]
'286'
'256'
'265'
'284'
'276'
['286', '256', '265', '284', '276']
parsed_discourse_facet ['results_citation']
<S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S><S sid ="270" ssid = "12">This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.</S>
original cit marker offset is 0
new cit marker offset is 0



["'278'", "'262'", "'265'", "'275'", "'270'"]
'278'
'262'
'265'
'275'
'270'
['278', '262', '265', '275', '270']
parsed_discourse_facet ['implication_citation']
<S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'280'", "'262'", "'265'", "'279'", "'256'"]
'280'
'262'
'265'
'279'
'256'
['280', '262', '265', '279', '256']
parsed_discourse_facet ['method_citation']
<S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'279'", "'256'", "'262'", "'265'", "'276'"]
'279'
'256'
'262'
'265'
'276'
['279', '256', '262', '265', '276']
parsed_discourse_facet ['results_citation']
<S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="264" ssid = "6">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'277'", "'286'", "'287'", "'264'"]
'256'
'277'
'286'
'287'
'264'
['256', '277', '286', '287', '264']
parsed_discourse_facet ['aim_citation']
<S sid ="263" ssid = "5">Quantization can be improved by jointly encoding probability and backoff.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="258" ssid = "77">However  the point of RandLM is to scale to even larger data  compensating for this loss in quality.</S>
original cit marker offset is 0
new cit marker offset is 0



["'263'", "'265'", "'287'", "'283'", "'258'"]
'263'
'265'
'287'
'283'
'258'
['263', '265', '287', '283', '258']
parsed_discourse_facet ['method_citation']
<S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'265'", "'280'", "'256'", "'278'", "'267'"]
'265'
'280'
'256'
'278'
'267'
['265', '280', '256', '278', '267']
parsed_discourse_facet ['aim_citation']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', 'We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.', 'Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.', '0750271 and by the DARPA GALE program.', 'This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.']
['system', 'ROUGE-S*', 'Average_R:', '0.02220', '(95%-conf.int.', '0.02220', '-', '0.02220)']
['system', 'ROUGE-S*', 'Average_P:', '0.26923', '(95%-conf.int.', '0.26923', '-', '0.26923)']
['system', 'ROUGE-S*', 'Average_F:', '0.04102', '(95%-conf.int.', '0.04102', '-', '0.04102)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:78', 'F:21']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['If they knew that the first four words in a hypergraph node would never extend to the left and form a 5-gram  then three or even fewer words could be kept in the backward state.', 'Much could be done to further reduce memory consumption.', 'While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.', 'Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).']
['system', 'ROUGE-S*', 'Average_R:', '0.00490', '(95%-conf.int.', '0.00490', '-', '0.00490)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.00921', '(95%-conf.int.', '0.00921', '-', '0.00921)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:78', 'F:6']
['The cost of storing these averages, in bits, is Because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.']
['For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.', 'This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.', u'We elected run Moses single-threaded to minimize the impact of RandLM\xe2\u20ac\u2122s cache on memory use.', '0750271 and by the DARPA GALE program.', 'These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.']
['system', 'ROUGE-S*', 'Average_R:', '0.00073', '(95%-conf.int.', '0.00073', '-', '0.00073)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00136', '(95%-conf.int.', '0.00136', '-', '0.00136)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:91', 'F:1']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['Generalizing state minimization  the model could also provide explicit bounds on probability for both backward and forward extension.', 'For example  syntactic decoders (Koehn et al.  2007; Dyer et al.  2010; Li et al.  2009) perform dynamic programming parametrized by both backward- and forward-looking state.', 'Nicola Bertoldi and Marcello Federico assisted with IRSTLM.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:55', 'F:0']
['We offer a state function s(wn1) = wn&#65533; where substring wn&#65533; is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.']
['We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.', 'Juri Ganitkevitch answered questions about Joshua.', 'Chris Dyer integrated the code into cdec.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.']
['system', 'ROUGE-S*', 'Average_R:', '0.00116', '(95%-conf.int.', '0.00116', '-', '0.00116)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00207', '(95%-conf.int.', '0.00207', '-', '0.00207)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:105', 'F:1']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', 'Nicola Bertoldi and Marcello Federico assisted with IRSTLM.', 'For speed  we plan to implement the direct-mapped cache from BerkeleyLM.', 'Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.', 'Alon Lavie advised on this work.']
['system', 'ROUGE-S*', 'Average_R:', '0.00476', '(95%-conf.int.', '0.00476', '-', '0.00476)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00847', '(95%-conf.int.', '0.00847', '-', '0.00847)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:78', 'F:3']
['We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.']
['We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', 'For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.']
['system', 'ROUGE-S*', 'Average_R:', '0.00164', '(95%-conf.int.', '0.00164', '-', '0.00164)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00320', '(95%-conf.int.', '0.00320', '-', '0.00320)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:45', 'F:3']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.', u'We elected run Moses single-threaded to minimize the impact of RandLM\xe2\u20ac\u2122s cache on memory use.', 'The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.']
['system', 'ROUGE-S*', 'Average_R:', '0.00101', '(95%-conf.int.', '0.00101', '-', '0.00101)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00187', '(95%-conf.int.', '0.00187', '-', '0.00187)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:78', 'F:1']
['The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.']
['Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', 'These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.', 'Chris Dyer integrated the code into cdec.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'Alon Lavie advised on this work.']
['system', 'ROUGE-S*', 'Average_R:', '0.00116', '(95%-conf.int.', '0.00116', '-', '0.00116)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00218', '(95%-conf.int.', '0.00218', '-', '0.00218)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:55', 'F:1']
['Then we ran binary search to determine the least amount of memory with which it would run.']
['The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.', 'Alon Lavie advised on this work.', u'We elected run Moses single-threaded to minimize the impact of RandLM\xe2\u20ac\u2122s cache on memory use.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:21', 'F:0']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.', 'Chris Dyer integrated the code into cdec.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.', 'This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:55', 'F:0']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['Alon Lavie advised on this work.', 'Nicola Bertoldi and Marcello Federico assisted with IRSTLM.', 'Chris Dyer integrated the code into cdec.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.']
['system', 'ROUGE-S*', 'Average_R:', '0.01008', '(95%-conf.int.', '0.01008', '-', '0.01008)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.01783', '(95%-conf.int.', '0.01783', '-', '0.01783)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:78', 'F:6']
['Queries take the form p(wn|wn&#8722;1 1 ) where wn1 is an n-gram.']
['While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.', u'We elected run Moses single-threaded to minimize the impact of RandLM\xe2\u20ac\u2122s cache on memory use.', 'Chris Dyer integrated the code into cdec.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:36', 'F:0']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', 'This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.', 'The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).']
['system', 'ROUGE-S*', 'Average_R:', '0.00528', '(95%-conf.int.', '0.00528', '-', '0.00528)']
['system', 'ROUGE-S*', 'Average_P:', '0.08974', '(95%-conf.int.', '0.08974', '-', '0.08974)']
['system', 'ROUGE-S*', 'Average_F:', '0.00997', '(95%-conf.int.', '0.00997', '-', '0.00997)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:78', 'F:7']
['For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.']
['We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.', 'Alon Lavie advised on this work.', 'While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.', u'We elected run Moses single-threaded to minimize the impact of RandLM\xe2\u20ac\u2122s cache on memory use.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).']
['system', 'ROUGE-S*', 'Average_R:', '0.00078', '(95%-conf.int.', '0.00078', '-', '0.00078)']
['system', 'ROUGE-S*', 'Average_P:', '0.00308', '(95%-conf.int.', '0.00308', '-', '0.00308)']
['system', 'ROUGE-S*', 'Average_F:', '0.00125', '(95%-conf.int.', '0.00125', '-', '0.00125)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:325', 'F:1']
['We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al., 2009).']
['Nicola Bertoldi and Marcello Federico assisted with IRSTLM.', 'Much could be done to further reduce memory consumption.', 'This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.', 'Chris Dyer integrated the code into cdec.', 'These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.']
['system', 'ROUGE-S*', 'Average_R:', '0.00135', '(95%-conf.int.', '0.00135', '-', '0.00135)']
['system', 'ROUGE-S*', 'Average_P:', '0.00735', '(95%-conf.int.', '0.00735', '-', '0.00735)']
['system', 'ROUGE-S*', 'Average_F:', '0.00228', '(95%-conf.int.', '0.00228', '-', '0.00228)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:136', 'F:1']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.', u'We elected run Moses single-threaded to minimize the impact of RandLM\xe2\u20ac\u2122s cache on memory use.', 'Chris Dyer integrated the code into cdec.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.']
['system', 'ROUGE-S*', 'Average_R:', '0.00106', '(95%-conf.int.', '0.00106', '-', '0.00106)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00195', '(95%-conf.int.', '0.00195', '-', '0.00195)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:78', 'F:1']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['Quantization can be improved by jointly encoding probability and backoff.', 'Nicola Bertoldi and Marcello Federico assisted with IRSTLM.', 'However  the point of RandLM is to scale to even larger data  compensating for this loss in quality.', '0750271 and by the DARPA GALE program.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).']
['system', 'ROUGE-S*', 'Average_R:', '0.00178', '(95%-conf.int.', '0.00178', '-', '0.00178)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00313', '(95%-conf.int.', '0.00313', '-', '0.00313)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:78', 'F:1']
0.0391955553378 0.00321611109324 0.00587722218957





input/ref/Task1/P87-1015_vardha.csv
input/res/Task1/P87-1015.csv
parsing: input/ref/Task1/P87-1015_vardha.csv
<S sid="165" ssid="50">This class of formalisms have the properties that their derivation trees are local sets, and manipulate objects, using a finite number of composition operations that use a finite number of symbols.</S>
original cit marker offset is 0
new cit marker offset is 0



["'165'"]
'165'
['165']
parsed_discourse_facet ['method_citation']
 <S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'"]
'119'
['119']
parsed_discourse_facet ['method_citation']
    <S sid="3" ssid="1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'"]
'3'
['3']
parsed_discourse_facet ['method_citation']
    <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
    <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
    <S sid="133" ssid="18">To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'"]
'133'
['133']
parsed_discourse_facet ['method_citation']
 <S sid="138" ssid="23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
    <S sid="156" ssid="41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'"]
'156'
['156']
parsed_discourse_facet ['method_citation']
 <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
 <S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'"]
'119'
['119']
parsed_discourse_facet ['method_citation']
  <S sid="133" ssid="18">To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'"]
'133'
['133']
parsed_discourse_facet ['method_citation']
 <S sid="164" ssid="49">Although embedding this version of LCFRS's in the framework of ILFP developed by Rounds (1985) is straightforward, our motivation was to capture properties shared by a family of grammatical systems and generalize them defining a class of related formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'164'"]
'164'
['164']
parsed_discourse_facet ['method_citation']
 <S sid="204" ssid="10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's, HG's, TAG's, and MCTAG's are all local sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'"]
'204'
['204']
parsed_discourse_facet ['method_citation']
    <S sid="9" ssid="7">We examine both the complexity of the paths of trees in the tree sets, and the kinds of dependencies that the formalisms can impose between paths.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="28" ssid="13">A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
 <S sid="138" ssid="23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P87-1015.csv
<S sid ="191" ssid = "76">In addition to the tapes required to store the indices  M requires one work tape for splitting the substrings.</S><S sid ="206" ssid = "12">As suggested in Section 4.3.2  a derivation with independent paths can be divided into subcomputations with limited sharing of information.</S><S sid ="186" ssid = "71">To do this  the x's and y's are stored in the next 2ni + 2n2 tapes  and M goes to a universal state.</S><S sid ="104" ssid = "10">Pumping t2 will change only one branch and leave the other branch unaffected.</S><S sid ="192" ssid = "77">Thus  the ATM has no more than 6km&quot; + 1 work tapes  where km&quot; is the maximum number of substrings spanned by a derived structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'191'", "'206'", "'186'", "'104'", "'192'"]
'191'
'206'
'186'
'104'
'192'
['191', '206', '186', '104', '192']
parsed_discourse_facet ['implication_citation']
<S sid ="84" ssid = "69">((fii  Q2  Pa)    (01  i32  03)   e) (02 e) Oa  en) The path complexity of the tee set generated by a MCTAG is not necessarily context-free.</S><S sid ="54" ssid = "39">An IG can be viewed as a CFG in which each nonterminal is associated with a stack.</S><S sid ="101" ssid = "7">The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently.</S><S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="71" ssid = "56">0n0'i'i0'2&quot;bin242bn I n = 711 + n2 } On the other hand  no linguistic use is made of this general form of composition and Steedman (personal communication) and Steedman (1986) argues that a more limited definition of composition is more natural.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'54'", "'101'", "'59'", "'71'"]
'84'
'54'
'101'
'59'
'71'
['84', '54', '101', '59', '71']
parsed_discourse_facet ['implication_citation']
<S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="44" ssid = "29">The edge from the root to the subtree for the derivation of 7i is labeled by the address ni.</S><S sid ="136" ssid = "21">These two restrictions impose the constraint that the result of composing any two structures should be a structure whose &quot;size&quot; is the sum of its constituents plus some constant For example  the operation 4  discussed in the case of CFG's (in Section 4.1) adds the constant equal to the sum of the length of the strings VI  un+r Since we are considering formalisms with arbitrary structures it is difficult to precisely specify all of the restrictions on the composition operations that we believe would appropriately generalize the concatenation operation for the particular structures used by the formalism.</S><S sid ="28" ssid = "13">A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'44'", "'136'", "'28'", "'153'"]
'151'
'44'
'136'
'28'
'153'
['151', '44', '136', '28', '153']
parsed_discourse_facet ['results_citation']
<S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="134" ssid = "19">These systems are similar to those described by Pollard (1984) as Generalized Context-Free Grammars (GCFG's).</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="112" ssid = "18">.t The path set of tree sets at level k +1 have the complexity of the string language of level k. The independence of paths in a tree set appears to be an important property.</S><S sid ="20" ssid = "5">As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'134'", "'153'", "'112'", "'20'"]
'59'
'134'
'153'
'112'
'20'
['59', '134', '153', '112', '20']
parsed_discourse_facet ['method_citation']
<S sid ="106" ssid = "12">We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths.</S><S sid ="54" ssid = "39">An IG can be viewed as a CFG in which each nonterminal is associated with a stack.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="101" ssid = "7">The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently.</S><S sid ="182" ssid = "67">Since each zi is a contiguous substring of the input (say ai )  and no two substrings overlap  we can represent zi by the pair of integers (i2  i2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'54'", "'143'", "'101'", "'182'"]
'106'
'54'
'143'
'101'
'182'
['106', '54', '143', '101', '182']
parsed_discourse_facet ['method_citation']
<S sid ="55" ssid = "40">Each production can push or pop symbols on the stack as can be seen in the following productions that generate tree of the form shown in Figure 4b.</S><S sid ="117" ssid = "2">Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.</S><S sid ="200" ssid = "6">The importance of this property becomes clear in contrasting theories underlying GPSG (Gazdar  Klein  Pulluna  and Sag  1985)  and GB (as described by Berwick  1984) with those underlying LFG and FUG.</S><S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="106" ssid = "12">We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'117'", "'200'", "'59'", "'106'"]
'55'
'117'
'200'
'59'
'106'
['55', '117', '200', '59', '106']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "17">When 7' is adjoined at ?I in the tree 7 we obtain a tree v&quot;.</S><S sid ="121" ssid = "6">First  any grammar must involve a finite number of elementary structures  composed using a finite number of composition operations.</S><S sid ="19" ssid = "4">It can be easily shown from Thatcher's result that the path set of every local set is a regular set.</S><S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="138" ssid = "23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'121'", "'19'", "'151'", "'138'"]
'32'
'121'
'19'
'151'
'138'
['32', '121', '19', '151', '138']
parsed_discourse_facet ['method_citation']
<S sid ="188" ssid = "73">Thus  for example  one successor process will be have M to be in the existential state qa with the indices encoding xi     xn  in the first 2n i tapes.</S><S sid ="118" ssid = "3">In the remainder of the paper  we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S><S sid ="89" ssid = "74">Hence  trees shown in Figure 8 can not be generated by any MCTAG (but can be generated by an IG) because the number of pairs of dependent paths grows with n. Since the derivation tees of TAG's  MCTAG's  and HG's are local sets  the choice of the structure used at each point in a derivation in these systems does not depend on the context at that point within the derivation.</S><S sid ="96" ssid = "2">A tree set may be said to have dependencies between paths if some &quot;appropriate&quot; subset can be shown to have dependent paths as defined above.</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'188'", "'118'", "'89'", "'96'", "'227'"]
'188'
'118'
'89'
'96'
'227'
['188', '118', '89', '96', '227']
parsed_discourse_facet ['method_citation']
<S sid ="84" ssid = "69">((fii  Q2  Pa)    (01  i32  03)   e) (02 e) Oa  en) The path complexity of the tee set generated by a MCTAG is not necessarily context-free.</S><S sid ="98" ssid = "4">Thatcher (1973) describes a tee pumping lemma for recognizable sets related to the string pumping lemma for regular sets.</S><S sid ="12" ssid = "10">We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.</S><S sid ="179" ssid = "64">It can be seen that M performs a top-down recognition of the input al ... nin logspace.</S><S sid ="185" ssid = "70">Each spawned process must check if xi     xn  and   yn  can be derived from B and C  respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'98'", "'12'", "'179'", "'185'"]
'84'
'98'
'12'
'179'
'185'
['84', '98', '12', '179', '185']
parsed_discourse_facet ['method_citation']
<S sid ="207" ssid = "13">We outlined the definition of a family of constrained grammatical formalisms  called Linear Context-Free Rewriting Systems.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="26" ssid = "11">The productions of HG's are very similar to those of CFG's except that the operation used must be made explicit.</S><S sid ="178" ssid = "63">We define an ATM  M  recognizing a language generated by a grammar  G  having the properties discussed in Section 43.</S><S sid ="155" ssid = "40">CFG's  TAG's  MCTAG's and HG's are all members of this class since they satisfy these restrictions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'153'", "'26'", "'178'", "'155'"]
'207'
'153'
'26'
'178'
'155'
['207', '153', '26', '178', '155']
parsed_discourse_facet ['implication_citation']
<S sid ="38" ssid = "23">Thus  the derivation trees for TAG's have the same structure as local sets.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="165" ssid = "50">This class of formalisms have the properties that their derivation trees are local sets  and manipulate objects  using a finite number of composition operations that use a finite number of symbols.</S><S sid ="62" ssid = "47">TAG's can be shown to be equivalent to this restricted system.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'153'", "'165'", "'62'", "'143'"]
'38'
'153'
'165'
'62'
'143'
['38', '153', '165', '62', '143']
parsed_discourse_facet ['method_citation']
<S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="132" ssid = "17">In TAG's the elementary tree and addresses where adjunction takes place are used to instantiate the operation.</S><S sid ="101" ssid = "7">The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently.</S><S sid ="96" ssid = "2">A tree set may be said to have dependencies between paths if some &quot;appropriate&quot; subset can be shown to have dependent paths as defined above.</S><S sid ="94" ssid = "79">The semilinearity of Tree Adjoining Languages (TAL's)  MCTAL's  and Head Languages (HL's) can be proved using this property  with suitable restrictions on the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'132'", "'101'", "'96'", "'94'"]
'143'
'132'
'101'
'96'
'94'
['143', '132', '101', '96', '94']
parsed_discourse_facet ['method_citation']
<S sid ="89" ssid = "74">Hence  trees shown in Figure 8 can not be generated by any MCTAG (but can be generated by an IG) because the number of pairs of dependent paths grows with n. Since the derivation tees of TAG's  MCTAG's  and HG's are local sets  the choice of the structure used at each point in a derivation in these systems does not depend on the context at that point within the derivation.</S><S sid ="164" ssid = "49">Although embedding this version of LCFRS's in the framework of ILFP developed by Rounds (1985) is straightforward  our motivation was to capture properties shared by a family of grammatical systems and generalize them defining a class of related formalisms.</S><S sid ="17" ssid = "2">We define the path set of a tree 1 as the set of strings that label a path from the root to frontier of 7.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="26" ssid = "11">The productions of HG's are very similar to those of CFG's except that the operation used must be made explicit.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'164'", "'17'", "'143'", "'26'"]
'89'
'164'
'17'
'143'
'26'
['89', '164', '17', '143', '26']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">For example  Gazdar (1985) discusses the applicability of Indexed Grammars (IG's) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar (LFG) and Government and Bindings grammars (GB).</S><S sid ="91" ssid = "76">We characterize a class of formalisms that have this property in Section 4.</S><S sid ="68" ssid = "53">This kind of dependency arises from the use of the composition operation to compose two arbitrarily large categories.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="189" ssid = "74">For rules p : A fpo such that fp is constant function  giving an elementary structure  fp is defined such that fp() = (Si ... xi() where each z is a constant string.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'91'", "'68'", "'143'", "'189'"]
'6'
'91'
'68'
'143'
'189'
['6', '91', '68', '143', '189']
parsed_discourse_facet ['results_citation']
<S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="90" ssid = "75">Thus  as in CFG's  at any point in the derivation  the set of structures that can be applied is determined only by a finite set of rules encapsulated by the grammar.</S><S sid ="185" ssid = "70">Each spawned process must check if xi     xn  and   yn  can be derived from B and C  respectively.</S><S sid ="141" ssid = "26">Thus  the length of any string in L is a linear combination of the length of strings in some fixed finite subset of L  and thus L is said to have the constant growth property.</S><S sid ="158" ssid = "43">We can represent any derived tree of a TAG by the two substrings that appear in its frontier  and then define how the adjunction operation concatenates the substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'90'", "'185'", "'141'", "'158'"]
'151'
'90'
'185'
'141'
'158'
['151', '90', '185', '141', '158']
parsed_discourse_facet ['implication_citation']
<S sid ="156" ssid = "41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S><S sid ="106" ssid = "12">We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths.</S><S sid ="32" ssid = "17">When 7' is adjoined at ?I in the tree 7 we obtain a tree v&quot;.</S><S sid ="12" ssid = "10">We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.</S><S sid ="55" ssid = "40">Each production can push or pop symbols on the stack as can be seen in the following productions that generate tree of the form shown in Figure 4b.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'", "'106'", "'32'", "'12'", "'55'"]
'156'
'106'
'32'
'12'
'55'
['156', '106', '32', '12', '55']
parsed_discourse_facet ['method_citation']
["We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments."]
["It can be easily shown from Thatcher's result that the path set of every local set is a regular set.", "When 7' is adjoined at ?I in the tree 7 we obtain a tree v&quot;.", "We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).", "We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.", 'First  any grammar must involve a finite number of elementary structures  composed using a finite number of composition operations.']
['system', 'ROUGE-S*', 'Average_R:', '0.05612', '(95%-conf.int.', '0.05612', '-', '0.05612)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.10628', '(95%-conf.int.', '0.10628', '-', '0.10628)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:66', 'F:66']
['This class of formalisms have the properties that their derivation trees are local sets, and manipulate objects, using a finite number of composition operations that use a finite number of symbols.']
["To do this  the x's and y's are stored in the next 2ni + 2n2 tapes  and M goes to a universal state.", 'Thus  the ATM has no more than 6km&quot; + 1 work tapes  where km&quot; is the maximum number of substrings spanned by a derived structure.', 'In addition to the tapes required to store the indices  M requires one work tape for splitting the substrings.', 'Pumping t2 will change only one branch and leave the other branch unaffected.', 'As suggested in Section 4.3.2  a derivation with independent paths can be divided into subcomputations with limited sharing of information.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:120', 'F:0']
["In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs."]
['An IG can be viewed as a CFG in which each nonterminal is associated with a stack.', "The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently.", 'Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.', "0n0'i'i0'2&quot;bin242bn I n = 711 + n2 } On the other hand  no linguistic use is made of this general form of composition and Steedman (personal communication) and Steedman (1986) argues that a more limited definition of composition is more natural.", u'((fii  Q2  Pa)   \u2014\u25a0 (01  i32  03)   e) (02 e) Oa  en) The path complexity of the tee set generated by a MCTAG is not necessarily context-free.']
['system', 'ROUGE-S*', 'Average_R:', '0.00108', '(95%-conf.int.', '0.00108', '-', '0.00108)']
['system', 'ROUGE-S*', 'Average_P:', '0.04545', '(95%-conf.int.', '0.04545', '-', '0.04545)']
['system', 'ROUGE-S*', 'Average_F:', '0.00211', '(95%-conf.int.', '0.00211', '-', '0.00211)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:66', 'F:3']
["We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments."]
['Each production can push or pop symbols on the stack as can be seen in the following productions that generate tree of the form shown in Figure 4b.', "When 7' is adjoined at ?I in the tree 7 we obtain a tree v&quot;.", 'We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.', "We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths.", "Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings."]
['system', 'ROUGE-S*', 'Average_R:', '0.00307', '(95%-conf.int.', '0.00307', '-', '0.00307)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.00594', '(95%-conf.int.', '0.00594', '-', '0.00594)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:66', 'F:6']
['A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.']
["We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).", "Thus  as in CFG's  at any point in the derivation  the set of structures that can be applied is determined only by a finite set of rules encapsulated by the grammar.", 'We can represent any derived tree of a TAG by the two substrings that appear in its frontier  and then define how the adjunction operation concatenates the substrings.', 'Thus  the length of any string in L is a linear combination of the length of strings in some fixed finite subset of L  and thus L is said to have the constant growth property.', 'Each spawned process must check if xi     xn  and   yn  can be derived from B and C  respectively.']
['system', 'ROUGE-S*', 'Average_R:', '0.00408', '(95%-conf.int.', '0.00408', '-', '0.00408)']
['system', 'ROUGE-S*', 'Average_P:', '0.11111', '(95%-conf.int.', '0.11111', '-', '0.11111)']
['system', 'ROUGE-S*', 'Average_F:', '0.00787', '(95%-conf.int.', '0.00787', '-', '0.00787)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:45', 'F:5']
['We examine both the complexity of the paths of trees in the tree sets, and the kinds of dependencies that the formalisms can impose between paths.']
['We characterize a class of formalisms that have this property in Section 4.', 'This kind of dependency arises from the use of the composition operation to compose two arbitrarily large categories.', 'For rules p : A fpo such that fp is constant function  giving an elementary structure  fp is defined such that fp() = (Si ... xi() where each z is a constant string.', 'The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.', "For example  Gazdar (1985) discusses the applicability of Indexed Grammars (IG's) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar (LFG) and Government and Bindings grammars (GB)."]
['system', 'ROUGE-S*', 'Average_R:', '0.00050', '(95%-conf.int.', '0.00050', '-', '0.00050)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00097', '(95%-conf.int.', '0.00097', '-', '0.00097)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:55', 'F:1']
['To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.']
['Each production can push or pop symbols on the stack as can be seen in the following productions that generate tree of the form shown in Figure 4b.', 'The importance of this property becomes clear in contrasting theories underlying GPSG (Gazdar  Klein  Pulluna  and Sag  1985)  and GB (as described by Berwick  1984) with those underlying LFG and FUG.', 'Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.', 'Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.', "We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths."]
['system', 'ROUGE-S*', 'Average_R:', '0.00196', '(95%-conf.int.', '0.00196', '-', '0.00196)']
['system', 'ROUGE-S*', 'Average_P:', '0.03676', '(95%-conf.int.', '0.03676', '-', '0.03676)']
['system', 'ROUGE-S*', 'Average_F:', '0.00371', '(95%-conf.int.', '0.00371', '-', '0.00371)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:136', 'F:5']
['To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.']
['This class of formalisms have the properties that their derivation trees are local sets  and manipulate objects  using a finite number of composition operations that use a finite number of symbols.', 'In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.', "Thus  the derivation trees for TAG's have the same structure as local sets.", "TAG's can be shown to be equivalent to this restricted system.", 'The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.']
['system', 'ROUGE-S*', 'Average_R:', '0.02187', '(95%-conf.int.', '0.02187', '-', '0.02187)']
['system', 'ROUGE-S*', 'Average_P:', '0.21324', '(95%-conf.int.', '0.21324', '-', '0.21324)']
['system', 'ROUGE-S*', 'Average_F:', '0.03967', '(95%-conf.int.', '0.03967', '-', '0.03967)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:136', 'F:29']
['Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.']
["We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).", 'The edge from the root to the subtree for the derivation of 7i is labeled by the address ni.', 'A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.', u"These two restrictions impose the constraint that the result of composing any two structures should be a structure whose &quot;size&quot; is the sum of its constituents plus some constant For example  the operation 4  discussed in the case of CFG's (in Section 4.1) adds the constant equal to the sum of the length of the strings VI  un+r\xe2\u20ac\xa2 Since we are considering formalisms with arbitrary structures it is difficult to precisely specify all of the restrictions on the composition operations that we believe would appropriately generalize the concatenation operation for the particular structures used by the formalism.", 'In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.']
['system', 'ROUGE-S*', 'Average_R:', '0.00027', '(95%-conf.int.', '0.00027', '-', '0.00027)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00054', '(95%-conf.int.', '0.00054', '-', '0.00054)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3655', 'P:55', 'F:1']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
["As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.", 'In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.', '.t The path set of tree sets at level k +1 have the complexity of the string language of level k. The independence of paths in a tree set appears to be an important property.', 'Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.', "These systems are similar to those described by Pollard (1984) as Generalized Context-Free Grammars (GCFG's)."]
['system', 'ROUGE-S*', 'Average_R:', '0.00171', '(95%-conf.int.', '0.00171', '-', '0.00171)']
['system', 'ROUGE-S*', 'Average_P:', '0.03333', '(95%-conf.int.', '0.03333', '-', '0.03333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00324', '(95%-conf.int.', '0.00324', '-', '0.00324)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:120', 'F:4']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
['Thatcher (1973) describes a tee pumping lemma for recognizable sets related to the string pumping lemma for regular sets.', 'We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.', 'It can be seen that M performs a top-down recognition of the input al ... nin logspace.', u'((fii  Q2  Pa)   \u2014\u25a0 (01  i32  03)   e) (02 e) Oa  en) The path complexity of the tee set generated by a MCTAG is not necessarily context-free.', 'Each spawned process must check if xi     xn  and   yn  can be derived from B and C  respectively.']
['system', 'ROUGE-S*', 'Average_R:', '0.00060', '(95%-conf.int.', '0.00060', '-', '0.00060)']
['system', 'ROUGE-S*', 'Average_P:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00113', '(95%-conf.int.', '0.00113', '-', '0.00113)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:120', 'F:1']
0.143226362334 0.00829636356094 0.0155872725856





input/ref/Task1/A00-2018_akanksha.csv
input/res/Task1/A00-2018.csv
parsing: input/ref/Task1/A00-2018_akanksha.csv
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'"]
'90'
'91'
['90', '91']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="17">Maximum-entropy models have two benefits for a parser builder.</S>
    <S sid="49" ssid="18">First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &amp;quot;features&amp;quot; suggests that the probability model should be easily changeable &#8212; just change the set of features used.</S>
    <S sid="51" ssid="20">Second, and this is a point we have not yet mentioned, the features used in these models need have no particular independence of one another.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'49'", "'51'"]
'48'
'49'
'51'
['48', '49', '51']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
    <S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
    <S sid="93" ssid="4">This allows the second pass to see expansions not present in the training corpus.</S>
    <S sid="94" ssid="5">We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'", "'92'", "'93'", "'94'"]
'90'
'91'
'92'
'93'
'94'
['90', '91', '92', '93', '94']
parsed_discourse_facet ['method_citation']
NA
original cit marker offset is 0
new cit marker offset is 0



['0']
0
['0']
Error in Reference Offset
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
    <S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
    <S sid="93" ssid="4">This allows the second pass to see expansions not present in the training corpus.</S>
    <S sid="94" ssid="5">We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'", "'92'", "'93'", "'94'"]
'90'
'91'
'92'
'93'
'94'
['90', '91', '92', '93', '94']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="7">To compute a probability in a log-linear model one first defines a set of &amp;quot;features&amp;quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.</S>
    <S sid="39" ssid="8">In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.</S>
    <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'39'", "'40'"]
'38'
'39'
'40'
['38', '39', '40']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['result_citation']
<S sid="85" ssid="54">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models, this simplifies the model significantly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="32">As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &amp;quot;before&amp;quot;), and the label of the grandparent of c (la).</S><S sid="143" ssid="34">The first is simply that if we first guess the pre-terminal, when we go to guess the head the first thing we can condition upon is the pre-terminal, i.e., we compute p(h I t).</S><S sid="146" ssid="37">The second major reason why first guessing the pre-terminal makes so much difference is that it can be used when backing off the lexical head in computing the probability of the rule expansion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'143'", "'146'"]
'63'
'143'
'146'
['63', '143', '146']
parsed_discourse_facet ['method_citation']
???<S sid="78" ssid="47">With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.</S>                                        <S sid="79" ssid="48">We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.</S>
original cit marker offset is 0
new cit marker offset is 0



["'78'", "'79'"]
'78'
'79'
['78', '79']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/A00-2018.csv
<S sid ="114" ssid = "5">That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.</S><S sid ="159" ssid = "50">Something very much like this is done in [15].</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="165" ssid = "56">Note that we also tried including this information using a standard deleted-interpolation model.</S><S sid ="163" ssid = "54">Next we add the less obvious conditioning events noted in our previous discussion of the final model  grandparent label lg and left sibling label /b.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'159'", "'2'", "'165'", "'163'"]
'114'
'159'
'2'
'165'
'163'
['114', '159', '2', '165', '163']
parsed_discourse_facet ['implication_citation']
<S sid ="114" ssid = "5">That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.</S><S sid ="3" ssid = "3">The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.</S><S sid ="99" ssid = "10">Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S><S sid ="129" ssid = "20">It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments)  it does not guess the pre-terminal before guessing the lexical head  and it uses a tree-bank grammar rather than a Markov grammar.</S><S sid ="92" ssid = "3">For runs with the generative model based upon Markov grammar statistics  the first pass uses the same statistics  but conditioned only on standard PCFG information.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'3'", "'99'", "'129'", "'92'"]
'114'
'3'
'99'
'129'
'92'
['114', '3', '99', '129', '92']
parsed_discourse_facet ['implication_citation']
<S sid ="132" ssid = "23">Between the Old model and the Best model  Figure 2 gives precision/recall measurements for several different versions of our parser.</S><S sid ="69" ssid = "38">For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.</S><S sid ="184" ssid = "11">The first is the slight  but important  improvement achieved by using this model over conventional deleted interpolation  as indicated in Figure 2.</S><S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="85" ssid = "54">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models  this simplifies the model significantly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'132'", "'69'", "'184'", "'157'", "'85'"]
'132'
'69'
'184'
'157'
'85'
['132', '69', '184', '157', '85']
parsed_discourse_facet ['results_citation']
<S sid ="137" ssid = "28">However  Collins in [10] does not stress the decision to guess the head's pre-terminal first  and it might be lost on the casual reader.</S><S sid ="51" ssid = "20">Second  and this is a point we have not yet mentioned  the features used in these models need have no particular independence of one another.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="58" ssid = "27">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid ="150" ssid = "41">The second modification is the explicit marking of noun and verb-phrase coordination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'51'", "'2'", "'58'", "'150'"]
'137'
'51'
'2'
'58'
'150'
['137', '51', '2', '58', '150']
parsed_discourse_facet ['method_citation']
<S sid ="49" ssid = "18">First  as already implicit in our discussion  factoring the probability computation into a sequence of values corresponding to various &quot;features&quot; suggests that the probability model should be easily changeable  just change the set of features used.</S><S sid ="4" ssid = "4">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c  information outside c that our probability model deems important in determining the probability in question.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="165" ssid = "56">Note that we also tried including this information using a standard deleted-interpolation model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'4'", "'13'", "'2'", "'165'"]
'49'
'4'
'13'
'2'
'165'
['49', '4', '13', '2', '165']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "3">Also  remember that H is a pla ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.</S><S sid ="11" ssid = "7">What fundamentally distinguishes probabilistic generative parsers is how they compute p(r)  and it is to that topic we turn next.</S><S sid ="96" ssid = "7">As noted above  the probability model uses five smoothed probability distributions  one each for Li  M Ri t  and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7.</S><S sid ="99" ssid = "10">Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S><S sid ="19" ssid = "8">The method we use follows that of [10].</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'11'", "'96'", "'99'", "'19'"]
'34'
'11'
'96'
'99'
'19'
['34', '11', '96', '99', '19']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.</S><S sid ="185" ssid = "12">We expect that as we experiment with other  more semantic conditioning information  the importance of this aspect of the model will increase.</S><S sid ="80" ssid = "49">(Our experience is that rather than requiring 50 or so iterations  three suffice.)</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="144" ssid = "35">This quantity is a relatively intuitive one (as  for example  it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it  in effect  as the unsmoothed probability upon which all smoothing of p(h) is based.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'185'", "'80'", "'54'", "'144'"]
'3'
'185'
'80'
'54'
'144'
['3', '185', '80', '54', '144']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "16">The intuitive idea is that each factor gi is larger than one if the feature in question makes the probability more likely  one if the feature has no effect  and smaller than one if it makes the probability less likely.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S><S sid ="85" ssid = "54">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models  this simplifies the model significantly.</S><S sid ="22" ssid = "11">For us the non-terminal symbols are those of the tree-bank  augmented by the symbols aux and auxg  which have been assigned deterministically to certain auxiliary verbs such as &quot;have&quot; or &quot;having&quot;.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'2'", "'41'", "'85'", "'22'"]
'47'
'2'
'41'
'85'
'22'
['47', '2', '41', '85', '22']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="55" ssid = "24">This method is known as &quot;deleted interpolation&quot; smoothing.</S><S sid ="66" ssid = "35">In many cases this is clearly warranted.</S><S sid ="4" ssid = "4">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid ="121" ssid = "12">Without these enhancements Char97 performs at the 86.6% level for sentences of length < 40.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'55'", "'66'", "'4'", "'121'"]
'157'
'55'
'66'
'4'
'121'
['157', '55', '66', '4', '121']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="0" ssid = "0">A Maximum-Entropy-Inspired Parser *</S><S sid ="37" ssid = "6">Rather  we concentrate on the aspects of these models that most directly influenced the model presented here.</S><S sid ="21" ssid = "10">(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'157'", "'0'", "'37'", "'21'"]
'2'
'157'
'0'
'37'
'21'
['2', '157', '0', '37', '21']
parsed_discourse_facet ['implication_citation']
<S sid ="48" ssid = "17">Maximum-entropy models have two benefits for a parser builder.</S><S sid ="21" ssid = "10">(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="177" ssid = "4">The results reported here disprove this conjecture.</S><S sid ="159" ssid = "50">Something very much like this is done in [15].</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'21'", "'54'", "'177'", "'159'"]
'48'
'21'
'54'
'177'
'159'
['48', '21', '54', '177', '159']
parsed_discourse_facet ['method_citation']
<S sid ="35" ssid = "4">In the past few years the maximum entropy  or log-linear  approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing [1 17].</S><S sid ="121" ssid = "12">Without these enhancements Char97 performs at the 86.6% level for sentences of length < 40.</S><S sid ="69" ssid = "38">For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.</S><S sid ="138" ssid = "29">Indeed  it was lost on the present author until he went back after the fact and found it there.</S><S sid ="114" ssid = "5">That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'", "'121'", "'69'", "'138'", "'114'"]
'35'
'121'
'69'
'138'
'114'
['35', '121', '69', '138', '114']
parsed_discourse_facet ['method_citation']
<S sid ="169" ssid = "60">Up to this point all the models considered in this section are tree-bank grammar models.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="134" ssid = "25">As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.</S><S sid ="99" ssid = "10">Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S><S sid ="155" ssid = "46">For example  in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'", "'125'", "'134'", "'99'", "'155'"]
'169'
'125'
'134'
'99'
'155'
['169', '125', '134', '99', '155']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid ="34" ssid = "3">Also  remember that H is a pla ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.</S><S sid ="165" ssid = "56">Note that we also tried including this information using a standard deleted-interpolation model.</S><S sid ="69" ssid = "38">For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.</S><S sid ="0" ssid = "0">A Maximum-Entropy-Inspired Parser *</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'34'", "'165'", "'69'", "'0'"]
'7'
'34'
'165'
'69'
'0'
['7', '34', '165', '69', '0']
parsed_discourse_facet ['results_citation']
<S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S><S sid ="37" ssid = "6">Rather  we concentrate on the aspects of these models that most directly influenced the model presented here.</S><S sid ="169" ssid = "60">Up to this point all the models considered in this section are tree-bank grammar models.</S><S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'", "'37'", "'169'", "'7'", "'125'"]
'41'
'37'
'169'
'7'
'125'
['41', '37', '169', '7', '125']
parsed_discourse_facet ['implication_citation']
<S sid ="88" ssid = "57">While we could have smoothed in the same fashion  we choose instead to use standard deleted interpolation.</S><S sid ="124" ssid = "15">We note here that this corpus is somewhat more difficult than the &quot;official&quot; test corpus.</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="21" ssid = "10">(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)</S><S sid ="63" ssid = "32">As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'124'", "'54'", "'21'", "'63'"]
'88'
'124'
'54'
'21'
'63'
['88', '124', '54', '21', '63']
parsed_discourse_facet ['method_citation']
['As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.', 'We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.']
['Something very much like this is done in [15].', 'That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.', u'Next we add the less obvious conditioning events noted in our previous discussion of the final model \xe2\u20ac\u201d grandparent label lg and left sibling label /b.', 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].', 'Note that we also tried including this information using a standard deleted-interpolation model.']
['system', 'ROUGE-S*', 'Average_R:', '0.02104', '(95%-conf.int.', '0.02104', '-', '0.02104)']
['system', 'ROUGE-S*', 'Average_P:', '0.03387', '(95%-conf.int.', '0.03387', '-', '0.03387)']
['system', 'ROUGE-S*', 'Average_F:', '0.02596', '(95%-conf.int.', '0.02596', '-', '0.02596)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:561', 'F:19']
['In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.', 'To compute a probability in a log-linear model one first defines a set of &quot;features&quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.', 'In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.']
['This method is known as &quot;deleted interpolation&quot; smoothing.', 'Without these enhancements Char97 performs at the 86.6% level for sentences of length ', 'In many cases this is clearly warranted.', 'Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.', "We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head."]
['system', 'ROUGE-S*', 'Average_R:', '0.00106', '(95%-conf.int.', '0.00106', '-', '0.00106)']
['system', 'ROUGE-S*', 'Average_P:', '0.00106', '(95%-conf.int.', '0.00106', '-', '0.00106)']
['system', 'ROUGE-S*', 'Average_F:', '0.00106', '(95%-conf.int.', '0.00106', '-', '0.00106)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:946', 'F:1']
['Second, and this is a point we have not yet mentioned, the features used in these models need have no particular independence of one another.', 'Maximum-entropy models have two benefits for a parser builder.', 'First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &quot;features&quot; suggests that the probability model should be easily changeable &#8212; just change the set of features used.']
['Second  and this is a point we have not yet mentioned  the features used in these models need have no particular independence of one another.', 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].', "However  Collins in [10] does not stress the decision to guess the head's pre-terminal first  and it might be lost on the casual reader.", 'Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.', 'The second modification is the explicit marking of noun and verb-phrase coordination.']
['system', 'ROUGE-S*', 'Average_R:', '0.01895', '(95%-conf.int.', '0.01895', '-', '0.01895)']
['system', 'ROUGE-S*', 'Average_P:', '0.07957', '(95%-conf.int.', '0.07957', '-', '0.07957)']
['system', 'ROUGE-S*', 'Average_F:', '0.03060', '(95%-conf.int.', '0.03060', '-', '0.03060)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:465', 'F:37']
['We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.']
['This quantity is a relatively intuitive one (as  for example  it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it  in effect  as the unsmoothed probability upon which all smoothing of p(h) is based.', "The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).", 'The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.', '(Our experience is that rather than requiring 50 or so iterations  three suffice.)', 'We expect that as we experiment with other  more semantic conditioning information  the importance of this aspect of the model will increase.']
['system', 'ROUGE-S*', 'Average_R:', '0.00471', '(95%-conf.int.', '0.00471', '-', '0.00471)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.00895', '(95%-conf.int.', '0.00895', '-', '0.00895)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:66', 'F:6']
['We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.']
['For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.', 'Rather  we concentrate on the aspects of these models that most directly influenced the model presented here.', 'Up to this point all the models considered in this section are tree-bank grammar models.', 'Following [5 10]  our parser is based upon a probabilistic generative model.', "For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise."]
['system', 'ROUGE-S*', 'Average_R:', '0.00269', '(95%-conf.int.', '0.00269', '-', '0.00269)']
['system', 'ROUGE-S*', 'Average_P:', '0.06061', '(95%-conf.int.', '0.06061', '-', '0.06061)']
['system', 'ROUGE-S*', 'Average_F:', '0.00516', '(95%-conf.int.', '0.00516', '-', '0.00516)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:66', 'F:4']
no Reference Text in gold A00-2018
['We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.']
['Between the Old model and the Best model  Figure 2 gives precision/recall measurements for several different versions of our parser.', 'As partition-function calculation is typically the major on-line computational problem for maximum-entropy models  this simplifies the model significantly.', 'Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.', 'The first is the slight  but important  improvement achieved by using this model over conventional deleted interpolation  as indicated in Figure 2.', "For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label."]
['system', 'ROUGE-S*', 'Average_R:', '0.00452', '(95%-conf.int.', '0.00452', '-', '0.00452)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.00862', '(95%-conf.int.', '0.00862', '-', '0.00862)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:66', 'F:6']
['As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.', 'We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.', 'This allows the second pass to see expansions not present in the training corpus.', 'For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.', 'We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.']
['Note that we also tried including this information using a standard deleted-interpolation model.', u'Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c \u2014 information outside c that our probability model deems important in determining the probability in question.', 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].', "We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.", u'First  as already implicit in our discussion  factoring the probability computation into a sequence of values corresponding to various &quot;features&quot; suggests that the probability model should be easily changeable \u2014 just change the set of features used.']
['system', 'ROUGE-S*', 'Average_R:', '0.03333', '(95%-conf.int.', '0.03333', '-', '0.03333)']
['system', 'ROUGE-S*', 'Average_P:', '0.04297', '(95%-conf.int.', '0.04297', '-', '0.04297)']
['system', 'ROUGE-S*', 'Average_F:', '0.03754', '(95%-conf.int.', '0.03754', '-', '0.03754)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:2211', 'F:95']
['We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.']
['Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.', 'That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.', 'The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.', 'For runs with the generative model based upon Markov grammar statistics  the first pass uses the same statistics  but conditioned only on standard PCFG information.', 'It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments)  it does not guess the pre-terminal before guessing the lexical head  and it uses a tree-bank grammar rather than a Markov grammar.']
['system', 'ROUGE-S*', 'Average_R:', '0.03154', '(95%-conf.int.', '0.03154', '-', '0.03154)']
['system', 'ROUGE-S*', 'Average_P:', '0.07822', '(95%-conf.int.', '0.07822', '-', '0.07822)']
['system', 'ROUGE-S*', 'Average_F:', '0.04496', '(95%-conf.int.', '0.04496', '-', '0.04496)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:946', 'F:74']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.']
["The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).", 'While we could have smoothed in the same fashion  we choose instead to use standard deleted interpolation.', "As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).", 'We note here that this corpus is somewhat more difficult than the &quot;official&quot; test corpus.', '(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)']
['system', 'ROUGE-S*', 'Average_R:', '0.00226', '(95%-conf.int.', '0.00226', '-', '0.00226)']
['system', 'ROUGE-S*', 'Average_P:', '0.00985', '(95%-conf.int.', '0.00985', '-', '0.00985)']
['system', 'ROUGE-S*', 'Average_F:', '0.00368', '(95%-conf.int.', '0.00368', '-', '0.00368)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:406', 'F:4']
['As partition-function calculation is typically the major on-line computational problem for maximum-entropy models, this simplifies the model significantly.']
['Without these enhancements Char97 performs at the 86.6% level for sentences of length ', 'In the past few years the maximum entropy  or log-linear  approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing [1 17].']
['system', 'ROUGE-S*', 'Average_R:', '0.01087', '(95%-conf.int.', '0.01087', '-', '0.01087)']
['system', 'ROUGE-S*', 'Average_P:', '0.03297', '(95%-conf.int.', '0.03297', '-', '0.03297)']
['system', 'ROUGE-S*', 'Average_F:', '0.01635', '(95%-conf.int.', '0.01635', '-', '0.01635)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:276', 'P:91', 'F:3']
['We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.', 'With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.']
['Note that we also tried including this information using a standard deleted-interpolation model.', "For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.", 'A Maximum-Entropy-Inspired Parser *', 'Following [5 10]  our parser is based upon a probabilistic generative model.', 'Also  remember that H is a pla ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.']
['system', 'ROUGE-S*', 'Average_R:', '0.00168', '(95%-conf.int.', '0.00168', '-', '0.00168)']
['system', 'ROUGE-S*', 'Average_P:', '0.00215', '(95%-conf.int.', '0.00215', '-', '0.00215)']
['system', 'ROUGE-S*', 'Average_F:', '0.00189', '(95%-conf.int.', '0.00189', '-', '0.00189)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:465', 'F:1']
0.0475536359313 0.0120590907995 0.0167972725746





input/ref/Task1/A00-2030_aakansha.csv
input/res/Task1/A00-2030.csv
parsing: input/ref/Task1/A00-2030_aakansha.csv
<S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="9">By necessity, we adopted the strategy of hand marking only the semantics.</S>
    <S sid="50" ssid="10">Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'50'"]
'49'
'50'
['49', '50']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="8">Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'"]
'16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S><S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'", "'33'", "'34'"]
'23'
'24'
'33'
'34'
['23', '24', '33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="60" ssid="1">In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).</S>
    <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'61'"]
'60'
'61'
['60', '61']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'"]
'33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).</S>
    <S sid="12" ssid="2">The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).</S><S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'12'", "'16'"]
'11'
'12'
'16'
['11', '12', '16']
parsed_discourse_facet ['method_citation']
<S sid="105" ssid="2">A single model proved capable of performing all necessary sentential processing, both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'"]
'105'
['105']
parsed_discourse_facet ['result_citation']
<S sid="60" ssid="1">In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).</S>
    <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'61'"]
'60'
'61'
['60', '61']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'"]
'16'
['16']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="102" ssid = "7">The results are summarized in Table 2.</S><S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'79'", "'82'", "'102'", "'56'"]
'68'
'79'
'82'
'102'
'56'
['68', '79', '82', '102', '56']
parsed_discourse_facet ['implication_citation']
<S sid ="65" ssid = "6">We illustrate the generation process by walking through a few of the steps of the parse shown in Figure 3.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="9" ssid = "7">Manually creating sourcespecific training data for syntax was not required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'96'", "'23'", "'88'", "'9'"]
'65'
'96'
'23'
'88'
'9'
['65', '96', '23', '88', '9']
parsed_discourse_facet ['implication_citation']
<S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="96" ssid = "1">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'68'", "'36'", "'6'", "'96'"]
'88'
'68'
'36'
'6'
'96'
['88', '68', '36', '6', '96']
parsed_discourse_facet ['results_citation']
<S sid ="60" ssid = "1">In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'62'", "'13'", "'36'", "'105'"]
'60'
'62'
'13'
'36'
'105'
['60', '62', '13', '36', '105']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="47" ssid = "7">It soon became painfully obvious that this task could not be performed in the available time.</S><S sid ="73" ssid = "14">We now briefly summarize the probability structure of the model.</S><S sid ="97" ssid = "2">The evaluation results are summarized in Table 1.</S><S sid ="80" ssid = "2">However  because these estimates are too sparse to be relied upon  we use interpolated estimates consisting of mixtures of successively lowerorder estimates (as in Placeway et al. 1993).</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'47'", "'73'", "'97'", "'80'"]
'38'
'47'
'73'
'97'
'80'
['38', '47', '73', '97', '80']
parsed_discourse_facet ['method_citation']
<S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="15" ssid = "5">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'", "'15'", "'45'", "'88'", "'56'"]
'79'
'15'
'45'
'88'
'56'
['79', '15', '45', '88', '56']
parsed_discourse_facet ['method_citation']
<S sid ="49" ssid = "9">By necessity  we adopted the strategy of hand marking only the semantics.</S><S sid ="70" ssid = "11">Post-modifier constituents for the PER/NP.</S><S sid ="69" ssid = "10">8.</S><S sid ="95" ssid = "14">The semantics  that is  the entities and relations  can then be directly extracted from these sentential trees.</S><S sid ="111" ssid = "3">The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies  either expressed or implied  of the Defense Advanced Research Projects Agency or the United States Government.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'70'", "'69'", "'95'", "'111'"]
'49'
'70'
'69'
'95'
'111'
['49', '70', '69', '95', '111']
parsed_discourse_facet ['method_citation']
<S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="4" ssid = "2">Yet  relatively few have embedded one of these algorithms in a task.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="67" ssid = "8">We pick up the derivation just after the topmost S and its head word  said  have been produced.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'79'", "'4'", "'82'", "'67'"]
'23'
'79'
'4'
'82'
'67'
['23', '79', '4', '82', '67']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "3">However  pipelined architectures suffer from a serious disadvantage: errors accumulate as they propagate through the pipeline.</S><S sid ="31" ssid = "14">If the single generalized model could then be extended to semantic analysis  all necessary sentence level processing would be contained in that model.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="84" ssid = "3">Although mathematically the model predicts tree elements in a top-down fashion  we search the space bottom-up using a chartbased search.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'31'", "'0'", "'38'", "'84'"]
'20'
'31'
'0'
'38'
'84'
['20', '31', '0', '38', '84']
parsed_discourse_facet ['method_citation']
<S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S><S sid ="104" ssid = "1">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'38'", "'107'", "'104'", "'23'"]
'88'
'38'
'107'
'104'
'23'
['88', '38', '107', '104', '23']
parsed_discourse_facet ['implication_citation']
<S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="74" ssid = "15">The categories for head constituents  cl are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c _1  and the head word of their parent  wp.</S><S sid ="69" ssid = "10">8.</S><S sid ="111" ssid = "3">The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies  either expressed or implied  of the Defense Advanced Research Projects Agency or the United States Government.</S>
original cit marker offset is 0
new cit marker offset is 0



["'82'", "'45'", "'74'", "'69'", "'111'"]
'82'
'45'
'74'
'69'
'111'
['82', '45', '74', '69', '111']
parsed_discourse_facet ['method_citation']
<S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="102" ssid = "7">The results are summarized in Table 2.</S><S sid ="82" ssid = "1">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'", "'102'", "'82'", "'62'", "'68'"]
'45'
'102'
'82'
'62'
'68'
['45', '102', '82', '62', '68']
parsed_discourse_facet ['method_citation']
<S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="97" ssid = "2">The evaluation results are summarized in Table 1.</S><S sid ="75" ssid = "16">Separate probabilities are maintained for left (pre) and right (post) modifiers: Part-of-speech tags  t    for modifiers are predicted based on the modifier  cm  the partof-speech tag of the head word  th  and the head word itself  wh: Head words  w for modifiers are predicted based on the modifier  cm  the part-of-speech tag of the modifier word   t the part-ofspeech tag of the head word   th  and the head word itself  wh: lAwmicm tm th wh)  e.g.</S><S sid ="95" ssid = "14">The semantics  that is  the entities and relations  can then be directly extracted from these sentential trees.</S><S sid ="59" ssid = "5">These labels serve to form a continuous chain between the relation and its argument.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'", "'97'", "'75'", "'95'", "'59'"]
'45'
'97'
'75'
'95'
'59'
['45', '97', '75', '95', '59']
parsed_discourse_facet ['method_citation']
<S sid ="76" ssid = "17">Finally  word features  fm  for modifiers are predicted based on the modifier  cm  the partof-speech tag of the modifier word   t the part-of-speech tag of the head word th  the head word itself  wh  and whether or not the modifier head word  w is known or unknown.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="112" ssid = "4">We thank Michael Collins of the University of Pennsylvania for his valuable suggestions.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'", "'36'", "'112'", "'101'", "'5'"]
'76'
'36'
'112'
'101'
'5'
['76', '36', '112', '101', '5']
parsed_discourse_facet ['results_citation']
<S sid ="56" ssid = "2">For example  in the phrase &quot;Lt. Cmdr.</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="13" ssid = "3">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'68'", "'13'", "'38'", "'23'"]
'56'
'68'
'13'
'38'
'23'
['56', '68', '13', '38', '23']
parsed_discourse_facet ['implication_citation']
<S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="36" ssid = "4">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S><S sid ="79" ssid = "1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S><S sid ="107" ssid = "4">The semantic training corpus was produced by students according to a simple set of guidelines.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'62'", "'36'", "'79'", "'107'"]
'51'
'62'
'36'
'79'
'107'
['51', '62', '36', '79', '107']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid ="49" ssid = "9">By necessity  we adopted the strategy of hand marking only the semantics.</S><S sid ="38" ssid = "6">Other labels indicate relations among entities.</S><S sid ="5" ssid = "3">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid ="39" ssid = "7">For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'49'", "'38'", "'5'", "'39'"]
'6'
'49'
'38'
'5'
'39'
['6', '49', '38', '5', '39']
parsed_discourse_facet ['results_citation']
<S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="31" ssid = "14">If the single generalized model could then be extended to semantic analysis  all necessary sentence level processing would be contained in that model.</S><S sid ="83" ssid = "2">More precisely  it must find the most likely augmented parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'57'", "'88'", "'31'", "'83'"]
'23'
'57'
'88'
'31'
'83'
['23', '57', '88', '31', '83']
parsed_discourse_facet ['aim_citation']
<S sid ="95" ssid = "14">The semantics  that is  the entities and relations  can then be directly extracted from these sentential trees.</S><S sid ="45" ssid = "5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S><S sid ="34" ssid = "2">In these trees  the standard TREEBANK structures are augmented to convey semantic information  that is  entities and relations.</S><S sid ="4" ssid = "2">Yet  relatively few have embedded one of these algorithms in a task.</S><S sid ="6" ssid = "4">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'45'", "'34'", "'4'", "'6'"]
'95'
'45'
'34'
'4'
'6'
['95', '45', '34', '4', '6']
parsed_discourse_facet ['method_citation']
Length 0 input/ref/Task1/A00-2030_aakansha.csv
0.0 0.0 0.0





input/ref/Task1/W06-3114_sweta.csv
input/res/Task1/W06-3114.csv
parsing: input/ref/Task1/W06-3114_sweta.csv
 <S sid="108" ssid="1">The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="27">For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="44">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages, as demonstrated by the results for English-German and English-French.</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
 <S sid="172" ssid="3">Due to many similarly performing systems, we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
 <S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["36'"]
36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="103" ssid="19">Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["103'"]
103'
['103']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="60">One annotator suggested that this was the case for as much as 10% of our test sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["102'"]
102'
['102']
parsed_discourse_facet ['method_citation']
<S sid="123" ssid="16">For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.</S>
original cit marker offset is 0
new cit marker offset is 0



["123'"]
123'
['123']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="27">For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="1">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="19">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["126'"]
126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="173" ssid="4">The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["173'"]
173'
['173']
parsed_discourse_facet ['method_citation']
 <S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["170'"]
170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="84" ssid="23">The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["84'"]
84'
['84']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="22" ssid = "15">The text type are editorials instead of speech transcripts.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S><S sid ="11" ssid = "4">To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.</S><S sid ="167" ssid = "60">One annotator suggested that this was the case for as much as 10% of our test sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'22'", "'155'", "'11'", "'167'"]
'59'
'22'
'155'
'11'
'167'
['59', '22', '155', '11', '167']
parsed_discourse_facet ['implication_citation']
<S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="34" ssid = "27">For more on the participating systems  please refer to the respective system description in the proceedings of the workshop.</S><S sid ="89" ssid = "5">In words  the judgements are normalized  so that the average normalized judgement per judge is 3.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S><S sid ="160" ssid = "53">Annotators suggested that long sentences are almost impossible to judge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'34'", "'89'", "'155'", "'160'"]
'136'
'34'
'89'
'155'
'160'
['136', '34', '89', '155', '160']
parsed_discourse_facet ['implication_citation']
<S sid ="27" ssid = "20">Microsofts approach uses dependency trees  others use hierarchical phrase models.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="115" ssid = "8">Often  two systems can not be distinguished with a confidence of over 95%  so there are ranked the same.</S><S sid ="15" ssid = "8">Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.</S><S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'", "'1'", "'115'", "'15'", "'59'"]
'27'
'1'
'115'
'15'
'59'
['27', '1', '115', '15', '59']
parsed_discourse_facet ['results_citation']
<S sid ="140" ssid = "33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid ="22" ssid = "15">The text type are editorials instead of speech transcripts.</S><S sid ="26" ssid = "19">Most of these groups follow a phrase-based statistical approach to machine translation.</S><S sid ="82" ssid = "21">This decreases the statistical significance of our results compared to those studies.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'22'", "'26'", "'82'", "'136'"]
'140'
'22'
'26'
'82'
'136'
['140', '22', '26', '82', '136']
parsed_discourse_facet ['method_citation']
<S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="131" ssid = "24">The manual scores are averages over the raw unnormalized scores.</S><S sid ="119" ssid = "12">There may be occasionally a system clearly at the top or at the bottom  but most systems are so close that it is hard to distinguish them.</S><S sid ="44" ssid = "10">We computed BLEU scores for each submission with a single reference translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'84'", "'131'", "'119'", "'44'"]
'125'
'84'
'131'
'119'
'44'
['125', '84', '131', '119', '44']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "12">We aligned the texts at a sentence level across all four languages  resulting in 1064 sentence per language.</S><S sid ="63" ssid = "2">Many human evaluation metrics have been proposed.</S><S sid ="135" ssid = "28">The differences in difficulty are better reflected in the BLEU scores than in the raw un-normalized manual judgements.</S><S sid ="11" ssid = "4">To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'63'", "'135'", "'11'", "'1'"]
'19'
'63'
'135'
'11'
'1'
['19', '63', '135', '11', '1']
parsed_discourse_facet ['method_citation']
<S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="51" ssid = "17">When dropping the top and bottom 2.5% the remaining BLEU scores define the range of the confidence interval.</S><S sid ="146" ssid = "39">This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.</S><S sid ="139" ssid = "32">Given the closeness of most systems and the wide over-lapping confidence intervals it is hard to make strong statements about the correlation between human judgements and automatic scoring methods such as BLEU.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'51'", "'146'", "'139'", "'1'"]
'163'
'51'
'146'
'139'
'1'
['163', '51', '146', '139', '1']
parsed_discourse_facet ['method_citation']
<S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="67" ssid = "6">Participants and other volunteers contributed about 180 hours of labor in the manual evaluation.</S><S sid ="26" ssid = "19">Most of these groups follow a phrase-based statistical approach to machine translation.</S><S sid ="55" ssid = "21">If one system is better in 95% of the sample sets  we conclude that its higher BLEU score is statistically significantly better.</S><S sid ="89" ssid = "5">In words  the judgements are normalized  so that the average normalized judgement per judge is 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'67'", "'26'", "'55'", "'89'"]
'163'
'67'
'26'
'55'
'89'
['163', '67', '26', '55', '89']
parsed_discourse_facet ['method_citation']
<S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="121" ssid = "14">For the automatic scoring method BLEU  we can distinguish three quarters of the systems.</S><S sid ="161" ssid = "54">Since all long sentence translation are somewhat muddled  even a contrastive evaluation between systems was difficult.</S><S sid ="30" ssid = "23">The other half was replaced by other participants  so we ended up with roughly the same number.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'136'", "'121'", "'161'", "'30'"]
'84'
'136'
'121'
'161'
'30'
['84', '136', '121', '161', '30']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="50" ssid = "16">Following this method  we repeatedly  say  1000 times  sample sets of sentences from the output of each system  measure their BLEU score  and use these 1000 BLEU scores as basis for estimating a confidence interval.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="83" ssid = "22">The number of judgements is additionally fragmented by our breakup of sentences into in-domain and out-of-domain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'163'", "'50'", "'84'", "'83'"]
'136'
'163'
'50'
'84'
'83'
['136', '163', '50', '84', '83']
parsed_discourse_facet ['implication_citation']
<S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="119" ssid = "12">There may be occasionally a system clearly at the top or at the bottom  but most systems are so close that it is hard to distinguish them.</S><S sid ="82" ssid = "21">This decreases the statistical significance of our results compared to those studies.</S><S sid ="43" ssid = "9">At the very least  we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.</S><S sid ="10" ssid = "3">Figure 1 provides some statistics about this corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'119'", "'82'", "'43'", "'10'"]
'59'
'119'
'82'
'43'
'10'
['59', '119', '82', '43', '10']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "8">We settled on contrastive evaluations of 5 system outputs for a single test sentence.</S><S sid ="99" ssid = "15">Systems that generally do worse than others will receive a negative one.</S><S sid ="112" ssid = "5">The normalization on a per-judge basis gave very similar ranking  only slightly less consistent with the ranking from the pairwise comparisons.</S><S sid ="62" ssid = "1">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems  they are only a imperfect substitute for human assessment of translation quality  or as the acronym BLEU puts it  a bilingual evaluation understudy.</S><S sid ="98" ssid = "14">Systems that generally do better than others will receive a positive average normalizedjudgement per sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'99'", "'112'", "'62'", "'98'"]
'69'
'99'
'112'
'62'
'98'
['69', '99', '112', '62', '98']
parsed_discourse_facet ['method_citation']
<S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="110" ssid = "3">In the graphs  system scores are indicated by a point  the confidence intervals by shaded areas around the point.</S><S sid ="146" ssid = "39">This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.</S><S sid ="90" ssid = "6">Another way to view the judgements is that they are less quality judgements of machine translation systems per se  but rankings of machine translation systems.</S><S sid ="33" ssid = "26">While building a machine translation system is a serious undertaking  in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'110'", "'146'", "'90'", "'33'"]
'125'
'110'
'146'
'90'
'33'
['125', '110', '146', '90', '33']
parsed_discourse_facet ['method_citation']
<S sid ="98" ssid = "14">Systems that generally do better than others will receive a positive average normalizedjudgement per sentence.</S><S sid ="148" ssid = "41">The best answer to this is: many research labs have very competitive systems whose performance is hard to tell apart.</S><S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S><S sid ="37" ssid = "3">It rewards matches of n-gram sequences  but measures only at most indirectly overall grammatical coherence.</S><S sid ="102" ssid = "18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems  we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'98'", "'148'", "'6'", "'37'", "'102'"]
'98'
'148'
'6'
'37'
'102'
['98', '148', '6', '37', '102']
parsed_discourse_facet ['results_citation']
<S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="177" ssid = "1">This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency  Contract No.</S><S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="3" ssid = "1">was done by the participants.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'177'", "'59'", "'1'", "'3'"]
'125'
'177'
'59'
'1'
'3'
['125', '177', '59', '1', '3']
parsed_discourse_facet ['implication_citation']
<S sid ="90" ssid = "6">Another way to view the judgements is that they are less quality judgements of machine translation systems per se  but rankings of machine translation systems.</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="135" ssid = "28">The differences in difficulty are better reflected in the BLEU scores than in the raw un-normalized manual judgements.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'0'", "'125'", "'126'", "'135'"]
'90'
'0'
'125'
'126'
'135'
['90', '0', '125', '126', '135']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "17">When dropping the top and bottom 2.5% the remaining BLEU scores define the range of the confidence interval.</S><S sid ="39" ssid = "5">However  a recent study (Callison-Burch et al.  2006)  pointed out that this correlation may not always be strong.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S><S sid ="26" ssid = "19">Most of these groups follow a phrase-based statistical approach to machine translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'39'", "'1'", "'162'", "'26'"]
'51'
'39'
'1'
'162'
'26'
['51', '39', '1', '162', '26']
parsed_discourse_facet ['results_citation']
<S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="112" ssid = "5">The normalization on a per-judge basis gave very similar ranking  only slightly less consistent with the ranking from the pairwise comparisons.</S><S sid ="174" ssid = "5">The manual evaluation of scoring translation on a graded scale from 15 seems to be very hard to perform.</S><S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'112'", "'174'", "'59'", "'125'"]
'136'
'112'
'174'
'59'
'125'
['136', '112', '174', '59', '125']
parsed_discourse_facet ['aim_citation']
['The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.']
[u'The manual evaluation of scoring translation on a graded scale from 1\xe2\u20ac\u201c5 seems to be very hard to perform.', 'The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.', 'The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).', 'We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.', 'The normalization on a per-judge basis gave very similar ranking  only slightly less consistent with the ranking from the pairwise comparisons.']
['system', 'ROUGE-S*', 'Average_R:', '0.00053', '(95%-conf.int.', '0.00053', '-', '0.00053)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00103', '(95%-conf.int.', '0.00103', '-', '0.00103)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:45', 'F:1']
['The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.']
['To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.', 'For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.', 'One annotator suggested that this was the case for as much as 10% of our test sentences.', 'The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.', 'The text type are editorials instead of speech transcripts.']
['system', 'ROUGE-S*', 'Average_R:', '0.00097', '(95%-conf.int.', '0.00097', '-', '0.00097)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00183', '(95%-conf.int.', '0.00183', '-', '0.00183)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:55', 'F:1']
['One annotator suggested that this was the case for as much as 10% of our test sentences.']
['For the automatic scoring method BLEU  we can distinguish three quarters of the systems.', 'The other half was replaced by other participants  so we ended up with roughly the same number.', 'The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'Since all long sentence translation are somewhat muddled  even a contrastive evaluation between systems was difficult.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:15', 'F:0']
['In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'Often  two systems can not be distinguished with a confidence of over 95%  so there are ranked the same.', u'Microsoft\u2019s approach uses dependency trees  others use hierarchical phrase models.', 'Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.', 'The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.']
['system', 'ROUGE-S*', 'Average_R:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Average_P:', '0.06593', '(95%-conf.int.', '0.06593', '-', '0.06593)']
['system', 'ROUGE-S*', 'Average_F:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:961191', 'P:91', 'F:6']
['The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages, as demonstrated by the results for English-German and English-French.']
['This decreases the statistical significance of our results compared to those studies.', 'Most of these groups follow a phrase-based statistical approach to machine translation.', 'The text type are editorials instead of speech transcripts.', 'We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.', 'The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).']
['system', 'ROUGE-S*', 'Average_R:', '0.03656', '(95%-conf.int.', '0.03656', '-', '0.03656)']
['system', 'ROUGE-S*', 'Average_P:', '0.31618', '(95%-conf.int.', '0.31618', '-', '0.31618)']
['system', 'ROUGE-S*', 'Average_F:', '0.06555', '(95%-conf.int.', '0.06555', '-', '0.06555)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:136', 'F:43']
['While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.']
['This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.', 'Another way to view the judgements is that they are less quality judgements of machine translation systems per se  but rankings of machine translation systems.', 'While building a machine translation system is a serious undertaking  in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.', 'We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.', 'In the graphs  system scores are indicated by a point  the confidence intervals by shaded areas around the point.']
['system', 'ROUGE-S*', 'Average_R:', '0.00643', '(95%-conf.int.', '0.00643', '-', '0.00643)']
['system', 'ROUGE-S*', 'Average_P:', '0.04762', '(95%-conf.int.', '0.04762', '-', '0.04762)']
['system', 'ROUGE-S*', 'Average_F:', '0.01133', '(95%-conf.int.', '0.01133', '-', '0.01133)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:231', 'F:11']
['The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.']
['The manual scores are averages over the raw unnormalized scores.', 'There may be occasionally a system clearly at the top or at the bottom  but most systems are so close that it is hard to distinguish them.', 'We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'We computed BLEU scores for each submission with a single reference translation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:66', 'F:0']
['For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.']
['Systems that generally do worse than others will receive a negative one.', 'We settled on contrastive evaluations of 5 system outputs for a single test sentence.', 'While automatic measures are an invaluable tool for the day-to-day development of machine translation systems  they are only a imperfect substitute for human assessment of translation quality  or as the acronym BLEU puts it  a bilingual evaluation understudy.', 'Systems that generally do better than others will receive a positive average normalizedjudgement per sentence.', 'The normalization on a per-judge basis gave very similar ranking  only slightly less consistent with the ranking from the pairwise comparisons.']
['system', 'ROUGE-S*', 'Average_R:', '0.00070', '(95%-conf.int.', '0.00070', '-', '0.00070)']
['system', 'ROUGE-S*', 'Average_P:', '0.03571', '(95%-conf.int.', '0.03571', '-', '0.03571)']
['system', 'ROUGE-S*', 'Average_F:', '0.00137', '(95%-conf.int.', '0.00137', '-', '0.00137)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:28', 'F:1']
['The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data.']
['This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency  Contract No.', u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'was done by the participants.', 'The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.', 'We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.']
['system', 'ROUGE-S*', 'Average_R:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Average_P:', '0.13333', '(95%-conf.int.', '0.13333', '-', '0.13333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:965355', 'P:45', 'F:6']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['Another way to view the judgements is that they are less quality judgements of machine translation systems per se  but rankings of machine translation systems.', 'The differences in difficulty are better reflected in the BLEU scores than in the raw un-normalized manual judgements.', 'We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'Manual and Automatic Evaluation of Machine Translation between European Languages']
['system', 'ROUGE-S*', 'Average_R:', '0.01524', '(95%-conf.int.', '0.01524', '-', '0.01524)']
['system', 'ROUGE-S*', 'Average_P:', '0.38182', '(95%-conf.int.', '0.38182', '-', '0.38182)']
['system', 'ROUGE-S*', 'Average_F:', '0.02931', '(95%-conf.int.', '0.02931', '-', '0.02931)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:55', 'F:21']
['For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.']
['This decreases the statistical significance of our results compared to those studies.', 'There may be occasionally a system clearly at the top or at the bottom  but most systems are so close that it is hard to distinguish them.', 'The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.', 'At the very least  we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.', 'Figure 1 provides some statistics about this corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00270', '(95%-conf.int.', '0.00270', '-', '0.00270)']
['system', 'ROUGE-S*', 'Average_P:', '0.07143', '(95%-conf.int.', '0.07143', '-', '0.07143)']
['system', 'ROUGE-S*', 'Average_F:', '0.00520', '(95%-conf.int.', '0.00520', '-', '0.00520)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:28', 'F:2']
['For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.']
['Annotators suggested that long sentences are almost impossible to judge.', 'In words  the judgements are normalized  so that the average normalized judgement per judge is 3.', 'For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.', 'The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).', 'For more on the participating systems  please refer to the respective system description in the proceedings of the workshop.']
['system', 'ROUGE-S*', 'Average_R:', '0.02112', '(95%-conf.int.', '0.02112', '-', '0.02112)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.04136', '(95%-conf.int.', '0.04136', '-', '0.04136)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:28', 'F:28']
['The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'However  a recent study (Callison-Burch et al.  2006)  pointed out that this correlation may not always be strong.', 'When dropping the top and bottom 2.5% the remaining BLEU scores define the range of the confidence interval.', 'Most of these groups follow a phrase-based statistical approach to machine translation.', 'A few annotators suggested to break up long sentences into clauses and evaluate these separately.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.03571', '(95%-conf.int.', '0.03571', '-', '0.03571)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:966745', 'P:28', 'F:1']
0.163702306433 0.00648230764244 0.012076922984





input/ref/Task1/J01-2004_swastika.csv
input/res/Task1/J01-2004.csv
parsing: input/ref/Task1/J01-2004_swastika.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="3">In the past few years, however, some improvements have been made over these language models through the use of statistical methods of natural language processing, and the development of innovative, linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
<S sid="215" ssid="119">The first word in the string remaining to be parsed, w1, we will call the look-ahead word.</S>
original cit marker offset is 0
new cit marker offset is 0



['215']
215
['215']
parsed_discourse_facet ['method_citation']
    <S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



['302']
302
['302']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="100" ssid="4">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['method_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S sid ="321" ssid = "77">In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.</S><S sid ="387" ssid = "143">Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.</S><S sid ="315" ssid = "71">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;the sum of the probabilities over the vocabulary is less than one.</S><S sid ="344" ssid = "100">However  when we interpolate with the trigram  we see that the additional improvement is greater than the one they experienced.</S><S sid ="403" ssid = "16">Perhaps some compromise between the fully connected structures and extreme underspecification will yield an efficiency improvement.</S>
original cit marker offset is 0
new cit marker offset is 0



["'321'", "'387'", "'315'", "'344'", "'403'"]
'321'
'387'
'315'
'344'
'403'
['321', '387', '315', '344', '403']
parsed_discourse_facet ['implication_citation']
<S sid ="380" ssid = "136">Future work will include more substantial word recognition experiments.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S><S sid ="344" ssid = "100">However  when we interpolate with the trigram  we see that the additional improvement is greater than the one they experienced.</S><S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S>
original cit marker offset is 0
new cit marker offset is 0



["'380'", "'280'", "'391'", "'344'", "'336'"]
'380'
'280'
'391'
'344'
'336'
['380', '280', '391', '344', '336']
parsed_discourse_facet ['implication_citation']
<S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="338" ssid = "94">Table 4 compares the perplexity of our model with Chelba and Jelinek (1998a  1998b) on the same training and testing corpora.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="326" ssid = "82">We obtained the training and testing corpora from them (which we will denote C&J corpus)  and also created intermediate corpora  upon which only the first two modifications were carried out (which we will denote no punct).</S><S sid ="343" ssid = "99">Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'</S>
original cit marker offset is 0
new cit marker offset is 0



["'349'", "'338'", "'280'", "'326'", "'343'"]
'349'
'338'
'280'
'326'
'343'
['349', '338', '280', '326', '343']
parsed_discourse_facet ['results_citation']
<S sid ="372" ssid = "128">The small size of our training data  as well as the fact that we are rescoring n-best lists  rather than working directly on lattices  makes comparison with the other models not particularly informative.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="309" ssid = "65">Let Ht be the priority queue H  before any processing has begun with word w  in the look-ahead.</S><S sid ="266" ssid = "22">From this set of measures  we will also include the crossing bracket scores: average crossing brackets (CB)  percentage of sentences with no crossing brackets (0 CB)  and the percentage of sentences with two crossing brackets or fewer (< 2 CB).</S><S sid ="268" ssid = "24">This is an incremental parser with a pruning strategy and no backtracking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'", "'301'", "'309'", "'266'", "'268'"]
'372'
'301'
'309'
'266'
'268'
['372', '301', '309', '266', '268']
parsed_discourse_facet ['method_citation']
<S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="355" ssid = "111">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task  we performed a very small preliminary experiment on n-best lists.</S><S sid ="340" ssid = "96">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid ="354" ssid = "110">Interestingly  at the word where the failure occurred  the sum of the probabilities was 0.9301.</S>
original cit marker offset is 0
new cit marker offset is 0



["'336'", "'301'", "'355'", "'340'", "'354'"]
'336'
'301'
'355'
'340'
'354'
['336', '301', '355', '340', '354']
parsed_discourse_facet ['method_citation']
<S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="361" ssid = "117">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S><S sid ="319" ssid = "75">The hope  however  is that the improved parsing model provided by our conditional probability model will cause the distribution over structures to be more peaked  thus enabling us to capture more of the total probability mass  and making this a fairly snug upper bound on the perplexity.</S><S sid ="355" ssid = "111">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task  we performed a very small preliminary experiment on n-best lists.</S>
original cit marker offset is 0
new cit marker offset is 0



["'301'", "'349'", "'361'", "'319'", "'355'"]
'301'
'349'
'361'
'319'
'355'
['301', '349', '361', '319', '355']
parsed_discourse_facet ['method_citation']
<S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="324" ssid = "80">Their modifications included: (i) removing orthographic cues to structure (e.g.  punctuation); (ii) replacing all numbers with the single token N; and (iii) closing the vocabulary at 10 000  replacing all other words with the UNK token.</S><S sid ="343" ssid = "99">Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'</S><S sid ="387" ssid = "143">Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.</S><S sid ="346" ssid = "102">These results are particularly remarkable  given that we did not build our model as a language model per se  but rather as a parsing model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'349'", "'324'", "'343'", "'387'", "'346'"]
'349'
'324'
'343'
'387'
'346'
['349', '324', '343', '387', '346']
parsed_discourse_facet ['method_citation']
<S sid ="315" ssid = "71">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;the sum of the probabilities over the vocabulary is less than one.</S><S sid ="377" ssid = "133">The point of this small experiment was to see if our parsing model could provide useful information even in the case that recognition errors occur  as opposed to the (generally) fully grammatical strings upon which the perplexity results were obtained.</S><S sid ="288" ssid = "44">Interestingly  conditioning all POS expansions on two c-commanding heads made no difference in accuracy compared to conditioning only leftmost POS expansions on a single c-commanding head; but it did improve the efficiency.</S><S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="382" ssid = "138">The base beam factor that we have used to this point is 10'  which is quite wide.</S>
original cit marker offset is 0
new cit marker offset is 0



["'315'", "'377'", "'288'", "'358'", "'382'"]
'315'
'377'
'288'
'358'
'382'
['315', '377', '288', '358', '382']
parsed_discourse_facet ['method_citation']
<S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="382" ssid = "138">The base beam factor that we have used to this point is 10'  which is quite wide.</S><S sid ="283" ssid = "39">Unlike the Roark and Johnson parser  however  our coverage did not substantially drop as the amount of conditioning information increased  and in some cases  coverage improved slightly.</S><S sid ="380" ssid = "136">Future work will include more substantial word recognition experiments.</S><S sid ="268" ssid = "24">This is an incremental parser with a pruning strategy and no backtracking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'301'", "'382'", "'283'", "'380'", "'268'"]
'301'
'382'
'283'
'380'
'268'
['301', '382', '283', '380', '268']
parsed_discourse_facet ['method_citation']
<S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S><S sid ="321" ssid = "77">In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.</S><S sid ="262" ssid = "18">Recall is the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD.</S>
original cit marker offset is 0
new cit marker offset is 0



["'336'", "'280'", "'391'", "'321'", "'262'"]
'336'
'280'
'391'
'321'
'262'
['336', '280', '391', '321', '262']
parsed_discourse_facet ['implication_citation']
<S sid ="315" ssid = "71">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;the sum of the probabilities over the vocabulary is less than one.</S><S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S><S sid ="295" ssid = "51">Our observed times look polynomial  which is to be expected given our pruning strategy: the denser the competitors within a narrow probability range of the best analysis  the more time will be spent working on these competitors; and the farther along in the sentence  the more chance for ambiguities that can lead to such a situation.</S><S sid ="382" ssid = "138">The base beam factor that we have used to this point is 10'  which is quite wide.</S><S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'315'", "'336'", "'295'", "'382'", "'391'"]
'315'
'336'
'295'
'382'
'391'
['315', '336', '295', '382', '391']
parsed_discourse_facet ['method_citation']
<S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="258" ssid = "14">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S><S sid ="372" ssid = "128">The small size of our training data  as well as the fact that we are rescoring n-best lists  rather than working directly on lattices  makes comparison with the other models not particularly informative.</S><S sid ="298" ssid = "54">What is perhaps surprising is that the difference is not greater.</S><S sid ="270" ssid = "26">In such a case  the parser fails to return a complete parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'301'", "'258'", "'372'", "'298'", "'270'"]
'301'
'258'
'372'
'298'
'270'
['301', '258', '372', '298', '270']
parsed_discourse_facet ['method_citation']
<S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S><S sid ="398" ssid = "11">By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="354" ssid = "110">Interestingly  at the word where the failure occurred  the sum of the probabilities was 0.9301.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'391'", "'398'", "'301'", "'354'", "'404'"]
'391'
'398'
'301'
'354'
'404'
['391', '398', '301', '354', '404']
parsed_discourse_facet ['method_citation']
<S sid ="402" ssid = "15">In fact  left-corner parsing can be simulated by a top-down parser by transforming the grammar  as was done in Roark and Johnson (1999)  and so an approach very similar to the one outlined here could be used in that case.</S><S sid ="398" ssid = "11">By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="371" ssid = "127">For our model and the Treebank trigram model  the LM weight that resulted in the lowest error rates is given.</S><S sid ="343" ssid = "99">Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'</S>
original cit marker offset is 0
new cit marker offset is 0



["'402'", "'398'", "'280'", "'371'", "'343'"]
'402'
'398'
'280'
'371'
'343'
['402', '398', '280', '371', '343']
parsed_discourse_facet ['results_citation']
<S sid ="398" ssid = "11">By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="361" ssid = "117">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S><S sid ="258" ssid = "14">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S>
original cit marker offset is 0
new cit marker offset is 0



["'398'", "'301'", "'390'", "'361'", "'258'"]
'398'
'301'
'390'
'361'
'258'
['398', '301', '390', '361', '258']
parsed_discourse_facet ['implication_citation']
<S sid ="258" ssid = "14">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S><S sid ="262" ssid = "18">Recall is the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD.</S><S sid ="361" ssid = "117">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S><S sid ="339" ssid = "95">We built an interpolated trigram model to serve as a baseline (as they did)  and also interpolated our model's perplexity with the trigram  using the same mixing coefficient as they did in their trials (taking 36 percent of the estimate from the trigram).'</S><S sid ="324" ssid = "80">Their modifications included: (i) removing orthographic cues to structure (e.g.  punctuation); (ii) replacing all numbers with the single token N; and (iii) closing the vocabulary at 10 000  replacing all other words with the UNK token.</S>
original cit marker offset is 0
new cit marker offset is 0



["'258'", "'262'", "'361'", "'339'", "'324'"]
'258'
'262'
'361'
'339'
'324'
['258', '262', '361', '339', '324']
parsed_discourse_facet ['method_citation']
<S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="257" ssid = "13">A constituent for evaluation purposes consists of a label (e.g.  NP) and a span (beginning and ending word positions).</S><S sid ="340" ssid = "96">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="387" ssid = "143">Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.</S>
original cit marker offset is 0
new cit marker offset is 0



["'349'", "'257'", "'340'", "'301'", "'387'"]
'349'
'257'
'340'
'301'
'387'
['349', '257', '340', '301', '387']
parsed_discourse_facet ['results_citation']
['First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.']
["By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).", 'Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.', 'The trigram model was also trained on Sections 00-20 of the C&J corpus.', "One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'", 'A constituent for evaluation purposes consists of a label (e.g.  NP) and a span (beginning and ending word positions).']
['system', 'ROUGE-S*', 'Average_R:', '0.02480', '(95%-conf.int.', '0.02480', '-', '0.02480)']
['system', 'ROUGE-S*', 'Average_P:', '0.12615', '(95%-conf.int.', '0.12615', '-', '0.12615)']
['system', 'ROUGE-S*', 'Average_F:', '0.04146', '(95%-conf.int.', '0.04146', '-', '0.04146)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:325', 'F:41']
['The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.']
["By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).", 'These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.', 'By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.', 'Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.', 'Interestingly  at the word where the failure occurred  the sum of the probabilities was 0.9301.']
['system', 'ROUGE-S*', 'Average_R:', '0.01212', '(95%-conf.int.', '0.01212', '-', '0.01212)']
['system', 'ROUGE-S*', 'Average_P:', '0.07407', '(95%-conf.int.', '0.07407', '-', '0.07407)']
['system', 'ROUGE-S*', 'Average_F:', '0.02083', '(95%-conf.int.', '0.02083', '-', '0.02083)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:351', 'F:26']
['The first word in the string remaining to be parsed, w1, we will call the look-ahead word.']
['These results are particularly remarkable  given that we did not build our model as a language model per se  but rather as a parsing model.', 'Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.', "One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'", 'Their modifications included: (i) removing orthographic cues to structure (e.g.  punctuation); (ii) replacing all numbers with the single token N; and (iii) closing the vocabulary at 10 000  replacing all other words with the UNK token.', "Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00233', '(95%-conf.int.', '0.00233', '-', '0.00233)']
['system', 'ROUGE-S*', 'Average_P:', '0.13889', '(95%-conf.int.', '0.13889', '-', '0.13889)']
['system', 'ROUGE-S*', 'Average_F:', '0.00459', '(95%-conf.int.', '0.00459', '-', '0.00459)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:36', 'F:5']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['However  when we interpolate with the trigram  we see that the additional improvement is greater than the one they experienced.', 'In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.', 'Perhaps some compromise between the fully connected structures and extreme underspecification will yield an efficiency improvement.', u'Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;\xe2\u20ac\u201dthe sum of the probabilities over the vocabulary is less than one.', 'Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.']
['system', 'ROUGE-S*', 'Average_R:', '0.00078', '(95%-conf.int.', '0.00078', '-', '0.00078)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00146', '(95%-conf.int.', '0.00146', '-', '0.00146)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:91', 'F:1']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['Our observed times look polynomial  which is to be expected given our pruning strategy: the denser the competitors within a narrow probability range of the best analysis  the more time will be spent working on these competitors; and the farther along in the sentence  the more chance for ambiguities that can lead to such a situation.', "The base beam factor that we have used to this point is 10'  which is quite wide.", 'The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.', 'These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.', u'Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;\xe2\u20ac\u201dthe sum of the probabilities over the vocabulary is less than one.']
['system', 'ROUGE-S*', 'Average_R:', '0.00273', '(95%-conf.int.', '0.00273', '-', '0.00273)']
['system', 'ROUGE-S*', 'Average_P:', '0.08791', '(95%-conf.int.', '0.08791', '-', '0.08791)']
['system', 'ROUGE-S*', 'Average_F:', '0.00530', '(95%-conf.int.', '0.00530', '-', '0.00530)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:91', 'F:8']
["In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds."]
['There are a couple of things to notice from these results.', 'These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.', 'The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.', 'In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.', 'Recall is the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:78', 'F:0']
['Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).']
['In such a case  the parser fails to return a complete parse.', "By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).", 'For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.', 'The small size of our training data  as well as the fact that we are rescoring n-best lists  rather than working directly on lattices  makes comparison with the other models not particularly informative.', 'What is perhaps surprising is that the difference is not greater.']
['system', 'ROUGE-S*', 'Average_R:', '0.00193', '(95%-conf.int.', '0.00193', '-', '0.00193)']
['system', 'ROUGE-S*', 'Average_P:', '0.00667', '(95%-conf.int.', '0.00667', '-', '0.00667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00300', '(95%-conf.int.', '0.00300', '-', '0.00300)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:300', 'F:2']
['In the past few years, however, some improvements have been made over these language models through the use of statistical methods of natural language processing, and the development of innovative, linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.']
['Future work will include more substantial word recognition experiments.', 'There are a couple of things to notice from these results.', 'These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.', 'The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.', 'However  when we interpolate with the trigram  we see that the additional improvement is greater than the one they experienced.']
['system', 'ROUGE-S*', 'Average_R:', '0.00452', '(95%-conf.int.', '0.00452', '-', '0.00452)']
['system', 'ROUGE-S*', 'Average_P:', '0.02000', '(95%-conf.int.', '0.02000', '-', '0.02000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00738', '(95%-conf.int.', '0.00738', '-', '0.00738)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:300', 'F:6']
0.0580849992739 0.00615124992311 0.0105024998687





input/ref/Task1/D10-1044_swastika.csv
input/res/Task1/D10-1044.csv
parsing: input/ref/Task1/D10-1044_swastika.csv
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
<S sid="71" ssid="8">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c&#955;(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



['71']
71
['71']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
<S sid="71" ssid="8">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c&#955;(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



['71']
71
['71']
parsed_discourse_facet ['result_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="9">An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (Och, 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



['45']
45
['45']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



['75']
75
['75']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
  <S sid="42" ssid="6">The natural baseline approach is to concatenate data from IN and OUT.</S>
original cit marker offset is 0
new cit marker offset is 0



['42']
42
['42']
parsed_discourse_facet ['aim_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/D10-1044.csv
<S sid ="32" ssid = "29">This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.</S><S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="101" ssid = "5">The second setting uses the news-related subcorpora for the NIST09 MT Chinese to English evaluation8 as IN  and the remaining NIST parallel Chinese/English corpora (UN  Hong Kong Laws  and Hong Kong Hansard) as OUT.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'2'", "'114'", "'101'", "'104'"]
'32'
'2'
'114'
'101'
'104'
['32', '2', '114', '101', '104']
parsed_discourse_facet ['implication_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="138" ssid = "7">However  for multinomial models like our LMs and TMs  there is a one to one correspondence between instances and features  eg the correspondence between a phrase pair (s  t) and its conditional multinomial probability p(s1t).</S><S sid ="144" ssid = "1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S><S sid ="133" ssid = "2">It is difficult to directly compare the Matsoukas et al results with ours  since our out-of-domain corpus is homogeneous; given heterogeneous training data  however  it would be trivial to include Matsoukas-style identity features in our instance-weighting model.</S><S sid ="99" ssid = "3">The dev and test sets were randomly chosen from the EMEA corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'138'", "'144'", "'133'", "'99'"]
'67'
'138'
'144'
'133'
'99'
['67', '138', '144', '133', '99']
parsed_discourse_facet ['implication_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="142" ssid = "11">There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).</S><S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'142'", "'8'", "'2'", "'38'"]
'67'
'142'
'8'
'2'
'38'
['67', '142', '8', '2', '38']
parsed_discourse_facet ['results_citation']
<S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="32" ssid = "29">This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.</S><S sid ="83" ssid = "20">We have not yet tried this.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daumes (2007) work that splits each feature into domain-specific and general copies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'32'", "'83'", "'78'", "'136'"]
'8'
'32'
'83'
'78'
'136'
['8', '32', '83', '78', '136']
parsed_discourse_facet ['method_citation']
<S sid ="133" ssid = "2">It is difficult to directly compare the Matsoukas et al results with ours  since our out-of-domain corpus is homogeneous; given heterogeneous training data  however  it would be trivial to include Matsoukas-style identity features in our instance-weighting model.</S><S sid ="118" ssid = "22">Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.</S><S sid ="5" ssid = "2">Even when there is training data available in the domain of interest  there is often additional data from other domains that could in principle be used to improve performance.</S><S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'118'", "'5'", "'1'", "'10'"]
'133'
'118'
'5'
'1'
'10'
['133', '118', '5', '1', '10']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="5" ssid = "2">Even when there is training data available in the domain of interest  there is often additional data from other domains that could in principle be used to improve performance.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daumes (2007) work that splits each feature into domain-specific and general copies.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S><S sid ="14" ssid = "11">There is a fairly large body of work on SMT adaptation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'5'", "'136'", "'104'", "'14'"]
'10'
'5'
'136'
'104'
'14'
['10', '5', '136', '104', '14']
parsed_discourse_facet ['method_citation']
<S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="95" ssid = "32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="125" ssid = "29">Somewhat surprisingly  there do not appear to be large systematic differences between linear and MAP combinations.</S><S sid ="20" ssid = "17">Daume (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'", "'95'", "'78'", "'125'", "'20'"]
'64'
'95'
'78'
'125'
'20'
['64', '95', '78', '125', '20']
parsed_discourse_facet ['method_citation']
<S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="36" ssid = "33">Section 5 covers relevant previous work on SMT adaptation  and section 6 concludes.</S><S sid ="19" ssid = "16">The idea of distinguishing between general and domain-specific examples is due to Daume and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.</S><S sid ="79" ssid = "16">This combination generalizes (2) and (3): we use either at = a to obtain a fixed-weight linear combination  or at = cI(t)/(cI(t) + 0) to obtain a MAP combination.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'36'", "'19'", "'79'", "'114'"]
'151'
'36'
'19'
'79'
'114'
['151', '36', '19', '79', '114']
parsed_discourse_facet ['method_citation']
<S sid ="118" ssid = "22">Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.</S><S sid ="50" ssid = "14">Linear weights are difficult to incorporate into the standard MERT procedure because they are hidden within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.</S><S sid ="88" ssid = "25">We have not explored this strategy.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S><S sid ="19" ssid = "16">The idea of distinguishing between general and domain-specific examples is due to Daume and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'", "'50'", "'88'", "'104'", "'19'"]
'118'
'50'
'88'
'104'
'19'
['118', '50', '88', '104', '19']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "15">For the LM  adaptive weights are set as follows: where  is a weight vector containing an element i for each domain (just IN and OUT in our case)  pi are the corresponding domain-specific models  and p(w  h) is an empirical distribution from a targetlanguage training corpuswe used the IN dev set for this.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the flattened variant described in section 3.2.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="118" ssid = "22">Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'114'", "'127'", "'67'", "'118'"]
'51'
'114'
'127'
'67'
'118'
['51', '114', '127', '67', '118']
parsed_discourse_facet ['implication_citation']
<S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="96" ssid = "33">We used it to score all phrase pairs in the OUT table  in order to provide a feature for the instance-weighting model.</S><S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="106" ssid = "10">The corpora for both settings are summarized in table 1.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'96'", "'128'", "'106'", "'111'"]
'1'
'96'
'128'
'106'
'111'
['1', '96', '128', '106', '111']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "7">Its success depends on the two domains being relatively close  and on the OUT corpus not being so large as to overwhelm the contribution of IN.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="41" ssid = "5">We do not adapt the alignment procedure for generating the phrase table from which the TM distributions are derived.</S><S sid ="60" ssid = "24">The matching sentence pairs are then added to the IN corpus  and the system is re-trained.</S><S sid ="0" ssid = "0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'64'", "'41'", "'60'", "'0'"]
'43'
'64'
'41'
'60'
'0'
['43', '64', '41', '60', '0']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="15" ssid = "12">We introduce several new ideas.</S><S sid ="71" ssid = "8">Finally  we incorporate the instance-weighting model into a general linear combination  and learn weights and mixing parameters simultaneously. where c(s  t) is a modified count for pair (s  t) in OUT  u(s|t) is a prior distribution  and y is a prior weight.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'67'", "'15'", "'71'", "'104'"]
'8'
'67'
'15'
'71'
'104'
['8', '67', '15', '71', '104']
parsed_discourse_facet ['method_citation']
<S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S><S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="43" ssid = "7">Its success depends on the two domains being relatively close  and on the OUT corpus not being so large as to overwhelm the contribution of IN.</S><S sid ="135" ssid = "4">Finally  we note that Jiangs instance-weighting framework is broader than we have presented above  encompassing among other possibilities the use of unlabelled IN data  which is applicable to SMT settings where source-only IN corpora are available.</S><S sid ="146" ssid = "3">The features are weighted within a logistic model to give an overall weight that is applied to the phrase pairs frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'", "'151'", "'43'", "'135'", "'146'"]
'104'
'151'
'43'
'135'
'146'
['104', '151', '43', '135', '146']
parsed_discourse_facet ['results_citation']
<S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="83" ssid = "20">We have not yet tried this.</S><S sid ="28" ssid = "25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'114'", "'83'", "'28'", "'17'"]
'16'
'114'
'83'
'28'
'17'
['16', '114', '83', '28', '17']
parsed_discourse_facet ['implication_citation']
<S sid ="89" ssid = "26">We used 22 features for the logistic weighting model  divided into two groups: one intended to reflect the degree to which a phrase pair belongs to general language  and one intended to capture similarity to the IN domain.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="110" ssid = "14">Je voudrais preciser  a` ladresse du commissaire Liikanen  quil nest pas aise de recourir aux tribunaux nationaux.</S><S sid ="109" ssid = "13"> I would also like to point out to commissioner Liikanen that it is not easy to take a matter to a national court.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'78'", "'110'", "'109'", "'67'"]
'89'
'78'
'110'
'109'
'67'
['89', '78', '110', '109', '67']
parsed_discourse_facet ['method_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="36" ssid = "33">Section 5 covers relevant previous work on SMT adaptation  and section 6 concludes.</S><S sid ="131" ssid = "35">The general-language features have a slight advantage over the similarity features  and both are better than the SVM feature.</S><S sid ="47" ssid = "11">Apart from MERT difficulties  a conceptual problem with log-linear combination is that it multiplies feature probabilities  essentially forcing different features to agree on high-scoring candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'64'", "'36'", "'131'", "'47'"]
'67'
'64'
'36'
'131'
'47'
['67', '64', '36', '131', '47']
parsed_discourse_facet ['results_citation']
<S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daumes (2007) work that splits each feature into domain-specific and general copies.</S><S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'136'", "'8'", "'11'", "'111'"]
'114'
'136'
'8'
'11'
'111'
['114', '136', '8', '11', '111']
parsed_discourse_facet ['aim_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S><S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="92" ssid = "29">6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.</S><S sid ="95" ssid = "32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'111'", "'128'", "'92'", "'95'"]
'67'
'111'
'128'
'92'
'95'
['67', '111', '128', '92', '95']
parsed_discourse_facet ['method_citation']
['To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.']
['We have not yet tried this.', ')  which precludes a single universal approach to adaptation.', 'This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.', u'It is also worth pointing out a connection with Daum\xb4e\u2019s (2007) work that splits each feature into domain-specific and general copies.', 'This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:120', 'F:0']
['In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.']
['This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.', 'This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.', '(Thus the domain of the dev and test corpora matches IN.)', 'The second setting uses the news-related subcorpora for the NIST09 MT Chinese to English evaluation8 as IN  and the remaining NIST parallel Chinese/English corpora (UN  Hong Kong Laws  and Hong Kong Hansard) as OUT.', 'It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00176', '(95%-conf.int.', '0.00176', '-', '0.00176)']
['system', 'ROUGE-S*', 'Average_P:', '0.01905', '(95%-conf.int.', '0.01905', '-', '0.01905)']
['system', 'ROUGE-S*', 'Average_F:', '0.00322', '(95%-conf.int.', '0.00322', '-', '0.00322)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:210', 'F:4']
['Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.']
['In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.', u'The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair\u2019s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).', u'Finally  we note that Jiang\u2019s instance-weighting framework is broader than we have presented above  encompassing among other possibilities the use of unlabelled IN data  which is applicable to SMT settings where source-only IN corpora are available.', 'Its success depends on the two domains being relatively close  and on the OUT corpus not being so large as to overwhelm the contribution of IN.', '(Thus the domain of the dev and test corpora matches IN.)']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:78', 'F:0']
['To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.']
[u'The idea of distinguishing between general and domain-specific examples is due to Daum\xb4e and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.', 'We have not explored this strategy.', 'Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.', '(Thus the domain of the dev and test corpora matches IN.)', u'Linear weights are difficult to incorporate into the standard MERT procedure because they are \u201chidden\u201d within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:120', 'F:0']
['We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.']
['We have not yet tried this.', 'It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.', 'We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.', 'Previous approaches have tried to find examples that are similar to the target domain.', 'First  we aim to explicitly characterize examples from OUT as belonging to general language or not.']
['system', 'ROUGE-S*', 'Average_R:', '0.00193', '(95%-conf.int.', '0.00193', '-', '0.00193)']
['system', 'ROUGE-S*', 'Average_P:', '0.04444', '(95%-conf.int.', '0.04444', '-', '0.04444)']
['system', 'ROUGE-S*', 'Average_F:', '0.00370', '(95%-conf.int.', '0.00370', '-', '0.00370)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:45', 'F:2']
['We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.']
['It is difficult when IN and OUT are dissimilar  as they are in the cases we study.', 'It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.', 'We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.', u'It is also worth pointing out a connection with Daum\xb4e\u2019s (2007) work that splits each feature into domain-specific and general copies.', ')  which precludes a single universal approach to adaptation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00169', '(95%-conf.int.', '0.00169', '-', '0.00169)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00331', '(95%-conf.int.', '0.00331', '-', '0.00331)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:45', 'F:3']
['In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.']
[')  which precludes a single universal approach to adaptation.', 'The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.', 'This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.', 'There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).', 'We extend the Matsoukas et al approach in several ways.']
['system', 'ROUGE-S*', 'Average_R:', '0.00058', '(95%-conf.int.', '0.00058', '-', '0.00058)']
['system', 'ROUGE-S*', 'Average_P:', '0.00476', '(95%-conf.int.', '0.00476', '-', '0.00476)']
['system', 'ROUGE-S*', 'Average_F:', '0.00104', '(95%-conf.int.', '0.00104', '-', '0.00104)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:210', 'F:1']
0.0192742854389 0.000851428559265 0.001609999977





input/ref/Task1/P11-1060_sweta.csv
input/res/Task1/P11-1060.csv
parsing: input/ref/Task1/P11-1060_sweta.csv
<S sid="8" ssid="4">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
 <S sid="45" ssid="21">The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["45'"]
45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="27">It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent&#8217;s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
 <S sid="166" ssid="51">Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language&#8212;think grounded al., 2010). compositional semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
 <S sid="8" ssid="4">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
 <S sid="11" ssid="7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["11'"]
11'
['11']
parsed_discourse_facet ['method_citation']
  <S sid="115" ssid="91">After training, given a new utterance x, our system outputs the most likely y, summing out the latent logical form z: argmaxy p&#952;(T)(y  |x, z &#8712; &#732;ZL,&#952;(T)).</S>
original cit marker offset is 0
new cit marker offset is 0



["115'"]
115'
['115']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="70">We now turn to the task of mapping natural language For the example in Figure 4(b), the de- utterances to DCS trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["94'"]
94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="157" ssid="42">Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["157'"]
157'
['157']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["106'"]
106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="52">In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="23">Next, we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S>
original cit marker offset is 0
new cit marker offset is 0



["138'"]
138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="16">The CSP has two types of constraints: (i) x &#8712; w(p) for each node x labeled with predicate p &#8712; P; and (ii) xj = yj0 (the j-th component of x must equal the j'-th component of y) for each edge (x, y) labeled with j0j &#8712; R. A solution to the CSP is an assignment of nodes to values that satisfies all the constraints.</S>
original cit marker offset is 0
new cit marker offset is 0



["40'"]
40'
['40']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["171'"]
171'
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S sid ="57" ssid = "33">But consider Figure 4: (a) is headed by borders  but states needs to be extracted; in (b)  the quantifier no is syntactically dominated by the head verb borders but needs to take wider scope.</S><S sid ="39" ssid = "15">Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.</S><S sid ="60" ssid = "36">Then higher up in the tree  we invoke it with an execute relation Xi to create the desired semantic scope.2 This mark-execute construct acts non-locally  so to maintain compositionality  we must augment the denotation d = JzKw to include any information about the marked nodes in z that can be accessed by an execute relation later on.</S><S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="167" ssid = "52">In DCS  we start with lexical triggers  which are 6 Conclusion more basic than CCG lexical entries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'", "'39'", "'60'", "'22'", "'167'"]
'57'
'39'
'60'
'22'
'167'
['57', '39', '60', '22', '167']
parsed_discourse_facet ['implication_citation']
<S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="71" ssid = "47">Let z be a DCS tree.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="120" ssid = "5">GEO has 48 non-value predicates and JOBS has 26.</S><S sid ="85" ssid = "61">Extraction allows us to return the set of consistent values of a marked non-root node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'71'", "'141'", "'120'", "'85'"]
'15'
'71'
'141'
'120'
'85'
['15', '71', '141', '120', '85']
parsed_discourse_facet ['implication_citation']
<S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S><S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parents denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="39" ssid = "15">Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.</S><S sid ="147" ssid = "32">However  training on just these examples is enough to improve the parameters  and this 29% increases to 66% and then to 95% over the next few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'51'", "'3'", "'39'", "'147'"]
'46'
'51'
'3'
'39'
'147'
['46', '51', '3', '39', '147']
parsed_discourse_facet ['results_citation']
<S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parents denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid ="84" ssid = "60">There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.</S><S sid ="170" ssid = "55">Our features as soft preferences.</S><S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'84'", "'170'", "'46'", "'98'"]
'51'
'84'
'170'
'46'
'98'
['51', '84', '170', '46', '98']
parsed_discourse_facet ['method_citation']
<S sid ="122" ssid = "7">For JOBS  if we use the standard Jobs database  close to half the ys are empty  which makes it uninteresting.</S><S sid ="94" ssid = "70">We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.</S><S sid ="102" ssid = "78">As a running example  consider x = city that is in California and z = hcity; 11:hloc; 21:hCAiii  where city triggers city and California triggers CA.</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S><S sid ="95" ssid = "71">Our first question is: given notation of the DCS tree before execution is an utterance x  what trees z  Z are permissible?</S>
original cit marker offset is 0
new cit marker offset is 0



["'122'", "'94'", "'102'", "'98'", "'95'"]
'122'
'94'
'102'
'98'
'95'
['122', '94', '102', '98', '95']
parsed_discourse_facet ['method_citation']
<S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="84" ssid = "60">There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'", "'3'", "'6'", "'84'", "'162'"]
'22'
'3'
'6'
'84'
'162'
['22', '3', '6', '84', '162']
parsed_discourse_facet ['method_citation']
<S sid ="13" ssid = "9">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x  y)  which is much cheaper to obtain than (x  z) pairs.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="29" ssid = "5">Let P be a set of predicates (e.g.  state  count  P)  which are just symbols.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="172" ssid = "57">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms  we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus  and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'134'", "'29'", "'77'", "'172'"]
'13'
'134'
'29'
'77'
'172'
['13', '134', '29', '77', '172']
parsed_discourse_facet ['method_citation']
<S sid ="96" ssid = "72">To California cities)  and it also allows us to underspecify L. In particular  our L will not include verbs or prepositions; rather  we rely on the predicates corresponding to those words to be triggered by traces.</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S><S sid ="94" ssid = "70">We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.</S><S sid ="124" ssid = "9">For each data predicate p (e.g.  language)  we add each possible tuple (e.g.  (job37  Java)) to w(p) independently with probability 0.8.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S>
original cit marker offset is 0
new cit marker offset is 0



["'96'", "'88'", "'94'", "'124'", "'162'"]
'96'
'88'
'94'
'124'
'162'
['96', '88', '94', '124', '162']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "84">However  in order to learn  we need to sum over {z  ZL(x) : JzKw = y}  and unfortunately  the additional constraint JzKw = y does not factorize.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="84" ssid = "60">There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'3'", "'84'", "'134'", "'46'"]
'108'
'3'
'84'
'134'
'46'
['108', '3', '84', '134', '46']
parsed_discourse_facet ['method_citation']
<S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'", "'98'", "'77'", "'162'", "'88'"]
'22'
'98'
'77'
'162'
'88'
['22', '98', '77', '162', '88']
parsed_discourse_facet ['implication_citation']
<S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'134'", "'3'", "'141'", "'98'"]
'12'
'134'
'3'
'141'
'98'
['12', '134', '3', '141', '98']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="29" ssid = "5">Let P be a set of predicates (e.g.  state  count  P)  which are just symbols.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="129" ssid = "14">We also define an augmented lexicon L+ which includes a prototype word x for each predicate appearing in (iii) above (e.g.  (large  size))  which cancels the predicates triggered by xs POS tag.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'29'", "'77'", "'129'", "'8'"]
'3'
'29'
'77'
'129'
'8'
['3', '29', '77', '129', '8']
parsed_discourse_facet ['method_citation']
<S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S><S sid ="142" ssid = "27">This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.</S><S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="144" ssid = "29">Intuitions How is our system learning?</S><S sid ="112" ssid = "88">Our learning algorithm alternates between (i) using the current parameters  to generate the K-best set ZL (x) for each training example x  and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'142'", "'10'", "'144'", "'112'"]
'46'
'142'
'10'
'144'
'112'
['46', '142', '10', '144', '112']
parsed_discourse_facet ['method_citation']
<S sid ="133" ssid = "18">Table 2 shows that our system using lexical triggers L (henceforth  DCS) outperforms SEMRESP (78.9% over 73.2%).</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="163" ssid = "48">Feedback from the context; for example  the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x  y)  which means (Schuler  2003) and semantic parsing (Popescu et borders looks right for the first argument and left al.  2003; Clarke et al.  2010).</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'134'", "'163'", "'141'", "'10'"]
'133'
'134'
'163'
'141'
'10'
['133', '134', '163', '141', '10']
parsed_discourse_facet ['results_citation']
<S sid ="86" ssid = "62">Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  (i  )]{1 = }.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="113" ssid = "89">Formally  let O(  ') be the objective function O() with ZL(x) ZL I(x).</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'162'", "'77'", "'3'", "'113'"]
'86'
'162'
'77'
'3'
'113'
['86', '162', '77', '3', '113']
parsed_discourse_facet ['implication_citation']
<S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="55" ssid = "31">The tree structure still enables us to compute denotations efficiently based on (1) and (2).</S><S sid ="80" ssid = "56">The full definition of join is as follows: Aggregate The aggregate operation takes a denotation and forms a set out of the tuples in the first column for each setting of the rest of the columns: Now we turn to the mark (M) and execute (Xi) operations  which handles the divergence between syntactic and semantic scope.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'134'", "'3'", "'55'", "'80'"]
'10'
'134'
'3'
'55'
'80'
['10', '134', '3', '55', '80']
parsed_discourse_facet ['method_citation']
<S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="50" ssid = "26">For example  consider the phrase number of major cities  and suppose that number corresponds to the count predicate.</S><S sid ="113" ssid = "89">Formally  let O(  ') be the objective function O() with ZL(x) ZL I(x).</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'", "'134'", "'154'", "'50'", "'113'"]
'22'
'134'
'154'
'50'
'113'
['22', '134', '154', '50', '113']
parsed_discourse_facet ['results_citation']
<S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="126" ssid = "11">During development  we further held out a random 30% of the training sets for validation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'134'", "'3'", "'12'", "'126'"]
'6'
'134'
'3'
'12'
'126'
['6', '134', '3', '12', '126']
parsed_discourse_facet ['aim_citation']
<S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S><S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="172" ssid = "57">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms  we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus  and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'132'", "'9'", "'10'", "'172'", "'134'"]
'132'
'9'
'10'
'172'
'134'
['132', '9', '10', '172', '134']
parsed_discourse_facet ['method_citation']
['Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem.']
['Table 2 shows that our system using lexical triggers L (henceforth  DCS) outperforms SEMRESP (78.9% over 73.2%).', 'However  we still model the logical form (now as a latent variable) to capture the complexities of language.', 'Feedback from the context; for example  the lexical entry for borders world has been used to guide both syntactic parsing is S\\NP/NP : Ay.Ax.border(x  y)  which means (Schuler  2003) and semantic parsing (Popescu et borders looks right for the first argument and left al.  2003; Clarke et al.  2010).', 'Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.', 'In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:120', 'F:0']
['It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent&#8217;s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.']
['As a running example  consider x = city that is in California and z = hcity; 11:hloc; 21:hCAiii  where city triggers city and California triggers CA.', 'The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.', u'For JOBS  if we use the standard Jobs database  close to half the y\u2019s are empty  which makes it uninteresting.', u'Our first question is: given notation of the DCS tree before execution is an utterance x  what trees z \u2208 Z are permissible?', 'We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.']
['system', 'ROUGE-S*', 'Average_R:', '0.00754', '(95%-conf.int.', '0.00754', '-', '0.00754)']
['system', 'ROUGE-S*', 'Average_P:', '0.02463', '(95%-conf.int.', '0.02463', '-', '0.02463)']
['system', 'ROUGE-S*', 'Average_F:', '0.01155', '(95%-conf.int.', '0.01155', '-', '0.01155)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:406', 'F:10']
['Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.']
[u"Join The join of two denotations d and d' with respect to components j and j' (\xe2\u02c6\u2014 means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.", 'The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.', 'Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.', 'The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.', 'The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.']
['system', 'ROUGE-S*', 'Average_R:', '0.00047', '(95%-conf.int.', '0.00047', '-', '0.00047)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00089', '(95%-conf.int.', '0.00089', '-', '0.00089)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:105', 'F:1']
['Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.']
['Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.', 'For each data predicate p (e.g.  language)  we add each possible tuple (e.g.  (job37  Java)) to w(p) independently with probability 0.8.', 'We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.', 'The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.', 'To California cities)  and it also allows us to underspecify L. In particular  our L will not include verbs or prepositions; rather  we rely on the predicates corresponding to those words to be triggered by traces.']
['system', 'ROUGE-S*', 'Average_R:', '0.00067', '(95%-conf.int.', '0.00067', '-', '0.00067)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00130', '(95%-conf.int.', '0.00130', '-', '0.00130)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:55', 'F:1']
['This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.']
['As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.', 'Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.', 'In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.', 'Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms  we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus  and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.', 'However  we still model the logical form (now as a latent variable) to capture the complexities of language.']
['system', 'ROUGE-S*', 'Average_R:', '0.00316', '(95%-conf.int.', '0.00316', '-', '0.00316)']
['system', 'ROUGE-S*', 'Average_P:', '0.05263', '(95%-conf.int.', '0.05263', '-', '0.05263)']
['system', 'ROUGE-S*', 'Average_F:', '0.00597', '(95%-conf.int.', '0.00597', '-', '0.00597)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:190', 'F:10']
["The CSP has two types of constraints: (i) x &#8712; w(p) for each node x labeled with predicate p &#8712; P; and (ii) xj = yj0 (the j-th component of x must equal the j'-th component of y) for each edge (x, y) labeled with j0j &#8712; R. A solution to the CSP is an assignment of nodes to values that satisfies all the constraints."]
['We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.', 'In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.', 'In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.', 'During development  we further held out a random 30% of the training sets for validation.', 'Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:300', 'F:0']
['After training, given a new utterance x, our system outputs the most likely y, summing out the latent logical form z: argmaxy p&#952;(T)(y  |x, z &#8712; &#732;ZL,&#952;(T)).']
[u"Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z \u2208 Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the child\u2019s denotation (t \u2208 JciKw).", u'However  in order to learn  we need to sum over {z \u2208 ZL(x) : JzKw = y}  and unfortunately  the additional constraint JzKw = y does not factorize.', 'In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.', 'There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.', 'In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00212', '(95%-conf.int.', '0.00212', '-', '0.00212)']
['system', 'ROUGE-S*', 'Average_P:', '0.04396', '(95%-conf.int.', '0.04396', '-', '0.04396)']
['system', 'ROUGE-S*', 'Average_F:', '0.00404', '(95%-conf.int.', '0.00404', '-', '0.00404)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:91', 'F:4']
['On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.']
[u"Join The join of two denotations d and d' with respect to components j and j' (\u2217 means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.", 'We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x  y)  which is much cheaper to obtain than (x  z) pairs.', 'Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms  we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus  and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.', u'Let P be a set of predicates (e.g.  state  count \u2208 P)  which are just symbols.', 'In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00034', '(95%-conf.int.', '0.00034', '-', '0.00034)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00066', '(95%-conf.int.', '0.00066', '-', '0.00066)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:105', 'F:1']
['In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries.']
['The tree structure still enables us to compute denotations efficiently based on (1) and (2).', 'However  we still model the logical form (now as a latent variable) to capture the complexities of language.', 'In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.', 'The full definition of join is as follows: Aggregate The aggregate operation takes a denotation and forms a set out of the tuples in the first column for each setting of the rest of the columns: Now we turn to the mark (M) and execute (Xi) operations  which handles the divergence between syntactic and semantic scope.', 'In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:45', 'F:0']
['On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.']
['Then higher up in the tree  we invoke it with an execute relation Xi to create the desired semantic scope.2 This mark-execute construct acts non-locally  so to maintain compositionality  we must augment the denotation d = JzKw to include any information about the marked nodes in z that can be accessed by an execute relation later on.', 'But consider Figure 4: (a) is headed by borders  but states needs to be extracted; in (b)  the quantifier no is syntactically dominated by the head verb borders but needs to take wider scope.', 'In DCS  we start with lexical triggers  which are 6 Conclusion more basic than CCG lexical entries.', 'Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.', 'The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:105', 'F:0']
['Next, we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).']
['For example  consider the phrase number of major cities  and suppose that number corresponds to the count predicate.', u"Formally  let \u02dcO(\u03b8  \u03b8') be the objective function O(\u03b8) with ZL(x) \u02dcZL \u03b8I(x).", 'The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).', 'The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.', 'In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00075', '(95%-conf.int.', '0.00075', '-', '0.00075)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00141', '(95%-conf.int.', '0.00141', '-', '0.00141)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:91', 'F:1']
['Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language&#8212;think grounded al., 2010). compositional semantics.']
['In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.', 'The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.', 'There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.', 'The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.', 'Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).']
['system', 'ROUGE-S*', 'Average_R:', '0.00322', '(95%-conf.int.', '0.00322', '-', '0.00322)']
['system', 'ROUGE-S*', 'Average_P:', '0.03162', '(95%-conf.int.', '0.03162', '-', '0.03162)']
['system', 'ROUGE-S*', 'Average_F:', '0.00584', '(95%-conf.int.', '0.00584', '-', '0.00584)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:253', 'F:8']
0.016754166527 0.00152249998731 0.00263833331135





input/ref/Task1/W06-3114_aakansha.csv
input/res/Task1/W06-3114.csv
parsing: input/ref/Task1/W06-3114_aakansha.csv
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="2">Training and testing is based on the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="2">Training and testing is based on the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'"]
'102'
['102']
parsed_discourse_facet ['method_citation']
<S sid="84" ssid="23">The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'"]
'84'
['84']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="4">To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="19">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="6">Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="3">&#8226; We evaluated translation from English, in addition to into English.</S>
    <S sid="6" ssid="4">English was again paired with German, French, and Spanish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'6'"]
'5'
'6'
['5', '6']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="22" ssid = "15">The text type are editorials instead of speech transcripts.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S><S sid ="11" ssid = "4">To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.</S><S sid ="167" ssid = "60">One annotator suggested that this was the case for as much as 10% of our test sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'22'", "'155'", "'11'", "'167'"]
'59'
'22'
'155'
'11'
'167'
['59', '22', '155', '11', '167']
parsed_discourse_facet ['implication_citation']
<S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="34" ssid = "27">For more on the participating systems  please refer to the respective system description in the proceedings of the workshop.</S><S sid ="89" ssid = "5">In words  the judgements are normalized  so that the average normalized judgement per judge is 3.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S><S sid ="160" ssid = "53">Annotators suggested that long sentences are almost impossible to judge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'34'", "'89'", "'155'", "'160'"]
'136'
'34'
'89'
'155'
'160'
['136', '34', '89', '155', '160']
parsed_discourse_facet ['implication_citation']
<S sid ="27" ssid = "20">Microsofts approach uses dependency trees  others use hierarchical phrase models.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="115" ssid = "8">Often  two systems can not be distinguished with a confidence of over 95%  so there are ranked the same.</S><S sid ="15" ssid = "8">Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.</S><S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'", "'1'", "'115'", "'15'", "'59'"]
'27'
'1'
'115'
'15'
'59'
['27', '1', '115', '15', '59']
parsed_discourse_facet ['results_citation']
<S sid ="140" ssid = "33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid ="22" ssid = "15">The text type are editorials instead of speech transcripts.</S><S sid ="26" ssid = "19">Most of these groups follow a phrase-based statistical approach to machine translation.</S><S sid ="82" ssid = "21">This decreases the statistical significance of our results compared to those studies.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'22'", "'26'", "'82'", "'136'"]
'140'
'22'
'26'
'82'
'136'
['140', '22', '26', '82', '136']
parsed_discourse_facet ['method_citation']
<S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="131" ssid = "24">The manual scores are averages over the raw unnormalized scores.</S><S sid ="119" ssid = "12">There may be occasionally a system clearly at the top or at the bottom  but most systems are so close that it is hard to distinguish them.</S><S sid ="44" ssid = "10">We computed BLEU scores for each submission with a single reference translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'84'", "'131'", "'119'", "'44'"]
'125'
'84'
'131'
'119'
'44'
['125', '84', '131', '119', '44']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "12">We aligned the texts at a sentence level across all four languages  resulting in 1064 sentence per language.</S><S sid ="63" ssid = "2">Many human evaluation metrics have been proposed.</S><S sid ="135" ssid = "28">The differences in difficulty are better reflected in the BLEU scores than in the raw un-normalized manual judgements.</S><S sid ="11" ssid = "4">To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'63'", "'135'", "'11'", "'1'"]
'19'
'63'
'135'
'11'
'1'
['19', '63', '135', '11', '1']
parsed_discourse_facet ['method_citation']
<S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="51" ssid = "17">When dropping the top and bottom 2.5% the remaining BLEU scores define the range of the confidence interval.</S><S sid ="146" ssid = "39">This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.</S><S sid ="139" ssid = "32">Given the closeness of most systems and the wide over-lapping confidence intervals it is hard to make strong statements about the correlation between human judgements and automatic scoring methods such as BLEU.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'51'", "'146'", "'139'", "'1'"]
'163'
'51'
'146'
'139'
'1'
['163', '51', '146', '139', '1']
parsed_discourse_facet ['method_citation']
<S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="67" ssid = "6">Participants and other volunteers contributed about 180 hours of labor in the manual evaluation.</S><S sid ="26" ssid = "19">Most of these groups follow a phrase-based statistical approach to machine translation.</S><S sid ="55" ssid = "21">If one system is better in 95% of the sample sets  we conclude that its higher BLEU score is statistically significantly better.</S><S sid ="89" ssid = "5">In words  the judgements are normalized  so that the average normalized judgement per judge is 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'67'", "'26'", "'55'", "'89'"]
'163'
'67'
'26'
'55'
'89'
['163', '67', '26', '55', '89']
parsed_discourse_facet ['method_citation']
<S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="121" ssid = "14">For the automatic scoring method BLEU  we can distinguish three quarters of the systems.</S><S sid ="161" ssid = "54">Since all long sentence translation are somewhat muddled  even a contrastive evaluation between systems was difficult.</S><S sid ="30" ssid = "23">The other half was replaced by other participants  so we ended up with roughly the same number.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'136'", "'121'", "'161'", "'30'"]
'84'
'136'
'121'
'161'
'30'
['84', '136', '121', '161', '30']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="50" ssid = "16">Following this method  we repeatedly  say  1000 times  sample sets of sentences from the output of each system  measure their BLEU score  and use these 1000 BLEU scores as basis for estimating a confidence interval.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="83" ssid = "22">The number of judgements is additionally fragmented by our breakup of sentences into in-domain and out-of-domain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'163'", "'50'", "'84'", "'83'"]
'136'
'163'
'50'
'84'
'83'
['136', '163', '50', '84', '83']
parsed_discourse_facet ['implication_citation']
<S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="119" ssid = "12">There may be occasionally a system clearly at the top or at the bottom  but most systems are so close that it is hard to distinguish them.</S><S sid ="82" ssid = "21">This decreases the statistical significance of our results compared to those studies.</S><S sid ="43" ssid = "9">At the very least  we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.</S><S sid ="10" ssid = "3">Figure 1 provides some statistics about this corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'119'", "'82'", "'43'", "'10'"]
'59'
'119'
'82'
'43'
'10'
['59', '119', '82', '43', '10']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "8">We settled on contrastive evaluations of 5 system outputs for a single test sentence.</S><S sid ="99" ssid = "15">Systems that generally do worse than others will receive a negative one.</S><S sid ="112" ssid = "5">The normalization on a per-judge basis gave very similar ranking  only slightly less consistent with the ranking from the pairwise comparisons.</S><S sid ="62" ssid = "1">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems  they are only a imperfect substitute for human assessment of translation quality  or as the acronym BLEU puts it  a bilingual evaluation understudy.</S><S sid ="98" ssid = "14">Systems that generally do better than others will receive a positive average normalizedjudgement per sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'99'", "'112'", "'62'", "'98'"]
'69'
'99'
'112'
'62'
'98'
['69', '99', '112', '62', '98']
parsed_discourse_facet ['method_citation']
<S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="110" ssid = "3">In the graphs  system scores are indicated by a point  the confidence intervals by shaded areas around the point.</S><S sid ="146" ssid = "39">This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.</S><S sid ="90" ssid = "6">Another way to view the judgements is that they are less quality judgements of machine translation systems per se  but rankings of machine translation systems.</S><S sid ="33" ssid = "26">While building a machine translation system is a serious undertaking  in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'110'", "'146'", "'90'", "'33'"]
'125'
'110'
'146'
'90'
'33'
['125', '110', '146', '90', '33']
parsed_discourse_facet ['method_citation']
<S sid ="98" ssid = "14">Systems that generally do better than others will receive a positive average normalizedjudgement per sentence.</S><S sid ="148" ssid = "41">The best answer to this is: many research labs have very competitive systems whose performance is hard to tell apart.</S><S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S><S sid ="37" ssid = "3">It rewards matches of n-gram sequences  but measures only at most indirectly overall grammatical coherence.</S><S sid ="102" ssid = "18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems  we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'98'", "'148'", "'6'", "'37'", "'102'"]
'98'
'148'
'6'
'37'
'102'
['98', '148', '6', '37', '102']
parsed_discourse_facet ['results_citation']
<S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="177" ssid = "1">This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency  Contract No.</S><S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="3" ssid = "1">was done by the participants.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'177'", "'59'", "'1'", "'3'"]
'125'
'177'
'59'
'1'
'3'
['125', '177', '59', '1', '3']
parsed_discourse_facet ['implication_citation']
<S sid ="90" ssid = "6">Another way to view the judgements is that they are less quality judgements of machine translation systems per se  but rankings of machine translation systems.</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="135" ssid = "28">The differences in difficulty are better reflected in the BLEU scores than in the raw un-normalized manual judgements.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'0'", "'125'", "'126'", "'135'"]
'90'
'0'
'125'
'126'
'135'
['90', '0', '125', '126', '135']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "17">When dropping the top and bottom 2.5% the remaining BLEU scores define the range of the confidence interval.</S><S sid ="39" ssid = "5">However  a recent study (Callison-Burch et al.  2006)  pointed out that this correlation may not always be strong.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S><S sid ="26" ssid = "19">Most of these groups follow a phrase-based statistical approach to machine translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'39'", "'1'", "'162'", "'26'"]
'51'
'39'
'1'
'162'
'26'
['51', '39', '1', '162', '26']
parsed_discourse_facet ['results_citation']
<S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="112" ssid = "5">The normalization on a per-judge basis gave very similar ranking  only slightly less consistent with the ranking from the pairwise comparisons.</S><S sid ="174" ssid = "5">The manual evaluation of scoring translation on a graded scale from 15 seems to be very hard to perform.</S><S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'112'", "'174'", "'59'", "'125'"]
'136'
'112'
'174'
'59'
'125'
['136', '112', '174', '59', '125']
parsed_discourse_facet ['aim_citation']
['&#8226; We evaluated translation from English, in addition to into English.', 'English was again paired with German, French, and Spanish.']
[u'The manual evaluation of scoring translation on a graded scale from 1\xe2\u20ac\u201c5 seems to be very hard to perform.', 'The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.', 'The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).', 'We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.', 'The normalization on a per-judge basis gave very similar ranking  only slightly less consistent with the ranking from the pairwise comparisons.']
['system', 'ROUGE-S*', 'Average_R:', '0.00899', '(95%-conf.int.', '0.00899', '-', '0.00899)']
['system', 'ROUGE-S*', 'Average_P:', '0.30909', '(95%-conf.int.', '0.30909', '-', '0.30909)']
['system', 'ROUGE-S*', 'Average_F:', '0.01747', '(95%-conf.int.', '0.01747', '-', '0.01747)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:55', 'F:17']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.', 'For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.', 'One annotator suggested that this was the case for as much as 10% of our test sentences.', 'The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.', 'The text type are editorials instead of speech transcripts.']
['system', 'ROUGE-S*', 'Average_R:', '0.00097', '(95%-conf.int.', '0.00097', '-', '0.00097)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00183', '(95%-conf.int.', '0.00183', '-', '0.00183)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:55', 'F:1']
['We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.']
['For the automatic scoring method BLEU  we can distinguish three quarters of the systems.', 'The other half was replaced by other participants  so we ended up with roughly the same number.', 'The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'Since all long sentence translation are somewhat muddled  even a contrastive evaluation between systems was difficult.']
['system', 'ROUGE-S*', 'Average_R:', '0.00097', '(95%-conf.int.', '0.00097', '-', '0.00097)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00180', '(95%-conf.int.', '0.00180', '-', '0.00180)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:78', 'F:1']
['Training and testing is based on the Europarl corpus.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'Often  two systems can not be distinguished with a confidence of over 95%  so there are ranked the same.', u'Microsoft\u2019s approach uses dependency trees  others use hierarchical phrase models.', 'Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.', 'The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:961191', 'P:10', 'F:0']
['The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.']
['This decreases the statistical significance of our results compared to those studies.', 'Most of these groups follow a phrase-based statistical approach to machine translation.', 'The text type are editorials instead of speech transcripts.', 'We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.', 'The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).']
['system', 'ROUGE-S*', 'Average_R:', '0.00595', '(95%-conf.int.', '0.00595', '-', '0.00595)']
['system', 'ROUGE-S*', 'Average_P:', '0.08974', '(95%-conf.int.', '0.08974', '-', '0.08974)']
['system', 'ROUGE-S*', 'Average_F:', '0.01116', '(95%-conf.int.', '0.01116', '-', '0.01116)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:78', 'F:7']
['We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.']
['This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.', 'Another way to view the judgements is that they are less quality judgements of machine translation systems per se  but rankings of machine translation systems.', 'While building a machine translation system is a serious undertaking  in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.', 'We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.', 'In the graphs  system scores are indicated by a point  the confidence intervals by shaded areas around the point.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:78', 'F:0']
['Training and testing is based on the Europarl corpus.']
['The manual scores are averages over the raw unnormalized scores.', 'There may be occasionally a system clearly at the top or at the bottom  but most systems are so close that it is hard to distinguish them.', 'We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'We computed BLEU scores for each submission with a single reference translation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:10', 'F:0']
['To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources.']
['Systems that generally do worse than others will receive a negative one.', 'We settled on contrastive evaluations of 5 system outputs for a single test sentence.', 'While automatic measures are an invaluable tool for the day-to-day development of machine translation systems  they are only a imperfect substitute for human assessment of translation quality  or as the acronym BLEU puts it  a bilingual evaluation understudy.', 'Systems that generally do better than others will receive a positive average normalizedjudgement per sentence.', 'The normalization on a per-judge basis gave very similar ranking  only slightly less consistent with the ranking from the pairwise comparisons.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:55', 'F:0']
['Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary.']
['This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency  Contract No.', u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'was done by the participants.', 'The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.', 'We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:965355', 'P:45', 'F:3']
['The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.']
['Another way to view the judgements is that they are less quality judgements of machine translation systems per se  but rankings of machine translation systems.', 'The differences in difficulty are better reflected in the BLEU scores than in the raw un-normalized manual judgements.', 'We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'Manual and Automatic Evaluation of Machine Translation between European Languages']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:45', 'F:0']
['The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:']
['This decreases the statistical significance of our results compared to those studies.', 'There may be occasionally a system clearly at the top or at the bottom  but most systems are so close that it is hard to distinguish them.', 'The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.', 'At the very least  we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.', 'Figure 1 provides some statistics about this corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:28', 'F:0']
['The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.']
['Annotators suggested that long sentences are almost impossible to judge.', 'In words  the judgements are normalized  so that the average normalized judgement per judge is 3.', 'For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.', 'The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).', 'For more on the participating systems  please refer to the respective system description in the proceedings of the workshop.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:45', 'F:0']
['Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'However  a recent study (Callison-Burch et al.  2006)  pointed out that this correlation may not always be strong.', 'When dropping the top and bottom 2.5% the remaining BLEU scores define the range of the confidence interval.', 'Most of these groups follow a phrase-based statistical approach to machine translation.', 'A few annotators suggested to break up long sentences into clauses and evaluate these separately.']
['system', 'ROUGE-S*', 'Average_R:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:966745', 'P:66', 'F:6']
0.0451853842678 0.00129923075924 0.00248307690398





input/ref/Task1/P11-1061_sweta.csv
input/res/Task1/P11-1061.csv
parsing: input/ref/Task1/P11-1061_sweta.csv
 <S sid="9" ssid="5">Unfortunately, the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al., 2010), making its practical usability questionable at best.</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="47" ssid="13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["47'"]
47'
['47']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["10'"]
10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="1">Given the bilingual graph described in the previous section, we can use label propagation to project the English POS labels to the foreign language.</S>
original cit marker offset is 0
new cit marker offset is 0



["70'"]
70'
['70']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="18">This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
 <S sid="83" ssid="14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value &#964;: We describe how we choose &#964; in &#167;6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["83'"]
83'
['83']
parsed_discourse_facet ['method_citation']
<S sid="113" ssid="13">For each language under consideration, Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["113'"]
113'
['113']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
 <S sid="13" ssid="9">(2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available.</S>
original cit marker offset is 0
new cit marker offset is 0



["13'"]
13'
['13']
parsed_discourse_facet ['method_citation']
 <S sid="3" ssid="3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="20">We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Syntactic universals are a well studied concept in linguistics (Carnie, 2002; Newmeyer, 2005), and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["19'"]
19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="16">Figure 2 shows an excerpt of a sentence from the Italian test set and the tags assigned by four different models, as well as the gold tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



["161'"]
161'
['161']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["23'"]
23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["23'"]
23'
['23']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1061.csv
<S sid ="14" ssid = "10">Our work is closest to that of Yarowsky and Ngai (2001)  but differs in two important ways.</S><S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S><S sid ="103" ssid = "3">The availability of these resources guided our selection of foreign languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'7'", "'56'", "'135'", "'103'"]
'14'
'7'
'56'
'135'
'103'
['14', '7', '56', '135', '103']
parsed_discourse_facet ['implication_citation']
<S sid ="75" ssid = "6">We use a squared loss to penalize neighboring vertices that have different label distributions: kqi  qjk2 = Ey(qi(y)  qj(y))2  and additionally regularize the label distributions towards the uniform distribution U over all possible labels Y.</S><S sid ="53" ssid = "19">Finally  note that while most feature concepts are lexicalized  others  such as the suffix concept  are not.</S><S sid ="34" ssid = "11">The following three sections elaborate these different stages is more detail.</S><S sid ="129" ssid = "29">Fortunately  performance was stable across various values  and we were able to use the same hyperparameters for all languages.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'53'", "'34'", "'129'", "'26'"]
'75'
'53'
'34'
'129'
'26'
['75', '53', '34', '129', '26']
parsed_discourse_facet ['implication_citation']
<S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="59" ssid = "25">So far the graph has been completely unlabeled.</S><S sid ="56" ssid = "22">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'107'", "'59'", "'56'", "'39'"]
'1'
'107'
'59'
'56'
'39'
['1', '107', '59', '56', '39']
parsed_discourse_facet ['results_citation']
<S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : We describe how we choose  in 6.4.</S><S sid ="66" ssid = "32">In general  the neighborhoods can be more diverse and we allow a soft label distribution over the vertices.</S><S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'66'", "'107'", "'117'", "'135'"]
'83'
'66'
'107'
'117'
'135'
['83', '66', '107', '117', '135']
parsed_discourse_facet ['method_citation']
<S sid ="104" ssid = "4">For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi  2006; Nivre et al.  2007).</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="155" ssid = "18">Examining the word fidanzato for the No LP and With LP models is particularly instructive.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="98" ssid = "29">7).</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'", "'1'", "'155'", "'39'", "'98'"]
'104'
'1'
'155'
'39'
'98'
['104', '1', '155', '39', '98']
parsed_discourse_facet ['method_citation']
<S sid ="143" ssid = "6">Overall  it gives improvements ranging from 1.1% for German to 14.7% for Italian  for an average improvement of 8.3% over the unsupervised feature-HMM model.</S><S sid ="96" ssid = "27">The function A : F * C maps from the language specific fine-grained tagset F to the coarser universal tagset C and is described in detail in 6.2: Note that when tx(y) = 1 the feature value is 0 and has no effect on the model  while its value is oc when tx(y) = 0 and constrains the HMMs state space.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="68" ssid = "34">In the label propagation stage  we propagate the automatic English tags to the aligned Italian trigram types  followed by further propagation solely among the Italian vertices. the Italian vertices are connected to an automatically labeled English vertex.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'96'", "'97'", "'68'", "'26'"]
'143'
'96'
'97'
'68'
'26'
['143', '96', '97', '68', '26']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="129" ssid = "29">Fortunately  performance was stable across various values  and we were able to use the same hyperparameters for all languages.</S><S sid ="108" ssid = "8">However  we needed to restrict ourselves to these languages in order to be able to evaluate the performance of our approach.</S><S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'26'", "'129'", "'108'", "'37'"]
'7'
'26'
'129'
'108'
'37'
['7', '26', '129', '108', '37']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S><S sid ="122" ssid = "22">For each language  we took the same number of sentences from the bitext as there are in its treebank  and trained a supervised feature-HMM.</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="31" ssid = "8">By aggregating the POS labels of the English tokens to types  we can generate label distributions for the English vertices.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'110'", "'122'", "'19'", "'31'"]
'39'
'110'
'122'
'19'
'31'
['39', '110', '122', '19', '31']
parsed_discourse_facet ['method_citation']
<S sid ="73" ssid = "4">Note that because we extracted only high-confidence alignments  many foreign vertices will not be connected to any English vertices.</S><S sid ="24" ssid = "1">The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.</S><S sid ="150" ssid = "13">For all languages  the vocabulary sizes increase by several thousand words.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="139" ssid = "2">As expected  the vanilla HMM trained with EM performs the worst.</S>
original cit marker offset is 0
new cit marker offset is 0



["'73'", "'24'", "'150'", "'39'", "'139'"]
'73'
'24'
'150'
'39'
'139'
['73', '24', '150', '39', '139']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "7">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid ="45" ssid = "11">Furthermore  we do not connect the English vertices to each other  but only to foreign language vertices.4 The graph vertices are extracted from the different sides of a parallel corpus (De  Df) and an additional unlabeled monolingual foreign corpus Ff  which will be used later for training.</S><S sid ="69" ssid = "35">Label propagation is used to propagate these tags inwards and results in tag distributions for the middle word of each Italian trigram.</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'45'", "'69'", "'135'", "'110'"]
'107'
'45'
'69'
'135'
'110'
['107', '45', '69', '135', '110']
parsed_discourse_facet ['implication_citation']
<S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="1" ssid = "1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : We describe how we choose  in 6.4.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'83'", "'25'", "'4'"]
'37'
'1'
'83'
'25'
'4'
['37', '1', '83', '25', '4']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="120" ssid = "20">We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="44" ssid = "10">Because all English vertices are going to be labeled  we do not need to disambiguate them by embedding them in trigrams.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'120'", "'117'", "'44'", "'26'"]
'54'
'120'
'117'
'44'
'26'
['54', '120', '117', '44', '26']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="47" ssid = "13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S><S sid ="161" ssid = "4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections  and bridge the gap between purely supervised and unsupervised POS tagging models.</S><S sid ="7" ssid = "3">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'117'", "'47'", "'161'", "'7'"]
'39'
'117'
'47'
'161'
'7'
['39', '117', '47', '161', '7']
parsed_discourse_facet ['method_citation']
<S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="32" ssid = "9">Label propagation can then be used to transfer the labels to the peripheral foreign vertices (i.e. the ones adjacent to the English vertices) first  and then among all of the foreign vertices (4).</S><S sid ="86" ssid = "17">For a sentence x and a state sequence z  a first order Markov model defines a distribution: (9) where Val(X) corresponds to the entire vocabulary.</S><S sid ="132" ssid = "32">When extracting the vector t  used to compute the constraint feature from the graph  we tried three threshold values for r (see Eq.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'117'", "'32'", "'86'", "'132'"]
'37'
'117'
'32'
'86'
'132'
['37', '117', '32', '86', '132']
parsed_discourse_facet ['results_citation']
<S sid ="163" ssid = "2">We would also like to thank Amarnag Subramanya for helping us with the implementation of label propagation and Shankar Kumar for access to the parallel data.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S><S sid ="139" ssid = "2">As expected  the vanilla HMM trained with EM performs the worst.</S><S sid ="58" ssid = "24">Based on these high-confidence alignments we can extract tuples of the form [u H v]  where u is a foreign trigram type  whose middle word aligns to an English word type v. Our bilingual similarity function then sets the edge weights in proportion to these tuple counts.</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : We describe how we choose  in 6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'110'", "'139'", "'58'", "'83'"]
'163'
'110'
'139'
'58'
'83'
['163', '110', '139', '58', '83']
parsed_discourse_facet ['implication_citation']
<S sid ="37" ssid = "3">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="134" ssid = "34">Because we dont have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="110" ssid = "10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S><S sid ="72" ssid = "3">In the first stage  we run a single step of label propagation  which transfers the label distributions from the English vertices to the connected foreign language vertices (say  Vf) at the periphery of the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'117'", "'134'", "'110'", "'72'"]
'37'
'117'
'134'
'110'
'72'
['37', '117', '134', '110', '72']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="39" ssid = "5">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid ="26" ssid = "3">As discussed in more detail in 3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="104" ssid = "4">For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi  2006; Nivre et al.  2007).</S><S sid ="135" ssid = "35">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'39'", "'26'", "'104'", "'135'"]
'54'
'39'
'26'
'104'
'135'
['54', '39', '26', '104', '135']
parsed_discourse_facet ['results_citation']
<S sid ="33" ssid = "10">The POS distributions over the foreign trigram types are used as features to learn a better unsupervised POS tagger (5).</S><S sid ="117" ssid = "17">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid ="4" ssid = "4">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S><S sid ="90" ssid = "21">All features were conjoined with the state z.</S><S sid ="96" ssid = "27">The function A : F * C maps from the language specific fine-grained tagset F to the coarser universal tagset C and is described in detail in 6.2: Note that when tx(y) = 1 the feature value is 0 and has no effect on the model  while its value is oc when tx(y) = 0 and constrains the HMMs state space.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'117'", "'4'", "'90'", "'96'"]
'33'
'117'
'4'
'90'
'96'
['33', '117', '4', '90', '96']
parsed_discourse_facet ['aim_citation']
['To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).']
['Based on these high-confidence alignments we can extract tuples of the form [u H v]  where u is a foreign trigram type  whose middle word aligns to an English word type v. Our bilingual similarity function then sets the edge weights in proportion to these tuple counts.', u'We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value \u03c4: We describe how we choose \u03c4 in \xa76.4.', 'As expected  the vanilla HMM trained with EM performs the worst.', 'We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.', 'We would also like to thank Amarnag Subramanya for helping us with the implementation of label propagation and Shankar Kumar for access to the parallel data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:66', 'F:0']
['Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).']
['The availability of these resources guided our selection of foreign languages.', 'For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.', 'To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.', 'Our work is closest to that of Yarowsky and Ngai (2001)  but differs in two important ways.', 'However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.']
['system', 'ROUGE-S*', 'Average_R:', '0.00266', '(95%-conf.int.', '0.00266', '-', '0.00266)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00507', '(95%-conf.int.', '0.00507', '-', '0.00507)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:55', 'F:3']
['Syntactic universals are a well studied concept in linguistics (Carnie, 2002; Newmeyer, 2005), and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.']
['Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).', 'They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.', 'In other words  the set of hidden states F was chosen to be the fine set of treebank tags.', 'However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.', 'Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections  and bridge the gap between purely supervised and unsupervised POS tagging models.']
['system', 'ROUGE-S*', 'Average_R:', '0.00132', '(95%-conf.int.', '0.00132', '-', '0.00132)']
['system', 'ROUGE-S*', 'Average_P:', '0.01961', '(95%-conf.int.', '0.01961', '-', '0.01961)']
['system', 'ROUGE-S*', 'Average_F:', '0.00247', '(95%-conf.int.', '0.00247', '-', '0.00247)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:153', 'F:3']
['(2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available.']
['The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.', 'For all languages  the vocabulary sizes increase by several thousand words.', 'They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.', 'Note that because we extracted only high-confidence alignments  many foreign vertices will not be connected to any English vertices.', 'As expected  the vanilla HMM trained with EM performs the worst.']
['system', 'ROUGE-S*', 'Average_R:', '0.00060', '(95%-conf.int.', '0.00060', '-', '0.00060)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00117', '(95%-conf.int.', '0.00117', '-', '0.00117)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:55', 'F:1']
['Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.']
['We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.', 'Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.', 'In other words  the set of hidden states F was chosen to be the fine set of treebank tags.', u'As discussed in more detail in \u0e22\u0e073  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', 'Because all English vertices are going to be labeled  we do not need to disambiguate them by embedding them in trigrams.']
['system', 'ROUGE-S*', 'Average_R:', '0.00055', '(95%-conf.int.', '0.00055', '-', '0.00055)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00103', '(95%-conf.int.', '0.00103', '-', '0.00103)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:105', 'F:1']
['We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).']
['Label propagation is used to propagate these tags inwards and results in tag distributions for the middle word of each Italian trigram.', 'Furthermore  we do not connect the English vertices to each other  but only to foreign language vertices.4 The graph vertices are extracted from the different sides of a parallel corpus (De  Df) and an additional unlabeled monolingual foreign corpus Ff  which will be used later for training.', 'Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.', 'For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.', 'We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.']
['system', 'ROUGE-S*', 'Average_R:', '0.00437', '(95%-conf.int.', '0.00437', '-', '0.00437)']
['system', 'ROUGE-S*', 'Average_P:', '0.05882', '(95%-conf.int.', '0.05882', '-', '0.05882)']
['system', 'ROUGE-S*', 'Average_F:', '0.00814', '(95%-conf.int.', '0.00814', '-', '0.00814)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:136', 'F:8']
['We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value &#964;: We describe how we choose &#964; in &#167;6.4.']
['We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.', '7).', 'They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.', u'Examining the word fidanzato for the \u201cNo LP\u201d and \u201cWith LP\u201d models is particularly instructive.', 'For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi  2006; Nivre et al.  2007).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:105', 'F:0']
['To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.']
['Fortunately  performance was stable across various values  and we were able to use the same hyperparameters for all languages.', u'We use a squared loss to penalize neighboring vertices that have different label distributions: kqi \u2212 qjk2 = Ey(qi(y) \u2212 qj(y))2  and additionally regularize the label distributions towards the uniform distribution U over all possible labels Y.', u'As discussed in more detail in \xa73  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', 'Finally  note that while most feature concepts are lexicalized  others  such as the suffix concept  are not.', 'The following three sections elaborate these different stages is more detail.']
['system', 'ROUGE-S*', 'Average_R:', '0.00584', '(95%-conf.int.', '0.00584', '-', '0.00584)']
['system', 'ROUGE-S*', 'Average_P:', '0.01705', '(95%-conf.int.', '0.01705', '-', '0.01705)']
['system', 'ROUGE-S*', 'Average_F:', '0.00870', '(95%-conf.int.', '0.00870', '-', '0.00870)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:528', 'F:9']
['Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).']
[u'The POS distributions over the foreign trigram types are used as features to learn a better unsupervised POS tagger (\xa75).', 'All features were conjoined with the state z.', 'In other words  the set of hidden states F was chosen to be the fine set of treebank tags.', 'Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.', u'The function A : F \u2014* C maps from the language specific fine-grained tagset F to the coarser universal tagset C and is described in detail in \xa76.2: Note that when tx(y) = 1 the feature value is 0 and has no effect on the model  while its value is \u2212oc when tx(y) = 0 and constrains the HMM\u2019s state space.']
['system', 'ROUGE-S*', 'Average_R:', '0.01956', '(95%-conf.int.', '0.01956', '-', '0.01956)']
['system', 'ROUGE-S*', 'Average_P:', '0.10081', '(95%-conf.int.', '0.10081', '-', '0.10081)']
['system', 'ROUGE-S*', 'Average_F:', '0.03277', '(95%-conf.int.', '0.03277', '-', '0.03277)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:496', 'F:50']
['To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).']
['Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.', 'By aggregating the POS labels of the English tokens to types  we can generate label distributions for the English vertices.', 'They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.', 'For each language  we took the same number of sentences from the bitext as there are in its treebank  and trained a supervised feature-HMM.', 'We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.']
['system', 'ROUGE-S*', 'Average_R:', '0.00093', '(95%-conf.int.', '0.00093', '-', '0.00093)']
['system', 'ROUGE-S*', 'Average_P:', '0.03030', '(95%-conf.int.', '0.03030', '-', '0.03030)']
['system', 'ROUGE-S*', 'Average_F:', '0.00181', '(95%-conf.int.', '0.00181', '-', '0.00181)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:66', 'F:2']
['We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).']
['Fortunately  performance was stable across various values  and we were able to use the same hyperparameters for all languages.', u'As discussed in more detail in \u0e22\u0e073  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', 'Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.', 'However  we needed to restrict ourselves to these languages in order to be able to evaluate the performance of our approach.', 'However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.']
['system', 'ROUGE-S*', 'Average_R:', '0.00395', '(95%-conf.int.', '0.00395', '-', '0.00395)']
['system', 'ROUGE-S*', 'Average_P:', '0.05147', '(95%-conf.int.', '0.05147', '-', '0.05147)']
['system', 'ROUGE-S*', 'Average_F:', '0.00735', '(95%-conf.int.', '0.00735', '-', '0.00735)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:136', 'F:7']
0.0327554542477 0.00361636360349 0.00622818176156





input/ref/Task1/E03-1005_aakansha.csv
input/res/Task1/E03-1005.csv
parsing: input/ref/Task1/E03-1005_aakansha.csv
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'"]
'105'
['105']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'"]
'145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'"]
'80'
['80']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'"]
'143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid="141" ssid="6">This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'141'"]
'140'
'141'
['140', '141']
parsed_discourse_facet ['result_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="102" ssid="5">In case the shortest derivation is not unique, Bod (2000b) proposes to back off to a frequency ordering of the subtrees.</S>
    <S sid="103" ssid="6">That is, all subtrees of each root label are assigned a rank according to their frequency in the treebank: the most frequent subtree (or subtrees) of each root label gets rank 1, the second most frequent subtree gets rank 2, etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'", "'103'"]
'102'
'103'
['102', '103']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="46" ssid="43">Most previous notions of best parse tree in DOP1 were based on a probabilistic metric, with Bod (2000b) as a notable exception, who used a simplicity metric based on the shortest derivation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'"]
'46'
['46']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="37">For example, Bod (2001) samples a fixed number of subtrees of each depth, which has the effect of assigning roughly equal weight to each node in the training data, and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S>
    <S sid="86" ssid="38">Bod reports state-of-the-art results with this method, and observes no decrease in parse accuracy when larger subtrees are included (using subtrees up to depth 14).</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'", "'86'"]
'85'
'86'
['85', '86']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="115" ssid="18">The only thing that needs to be changed for Simplicity-DOP is that all subtrees should be assigned equal probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="95" ssid = "47">By using these PCFG-reductions we can thus parse with all subtrees in polynomial time.</S><S sid ="25" ssid = "22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="87" ssid = "39">Yet  his grammar contains more than 5 million subtrees and processing times of over 200 seconds per WSJ sentence are reported (Bod 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'95'", "'25'", "'22'", "'87'"]
'130'
'95'
'25'
'22'
'87'
['130', '95', '25', '22', '87']
parsed_discourse_facet ['implication_citation']
<S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S><S sid ="58" ssid = "10">The notation A@k denotes the node at address k where A is the nonterminal labeling that node.</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="112" ssid = "15">Moreover  if n gets large  SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'", "'58'", "'40'", "'79'", "'112'"]
'85'
'58'
'40'
'79'
'112'
['85', '58', '40', '79', '112']
parsed_discourse_facet ['implication_citation']
<S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="25" ssid = "22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="105" ssid = "8">The derivation with the smallest sum  or highest rank  is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP  its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'25'", "'40'", "'108'", "'105'"]
'130'
'25'
'40'
'108'
'105'
['130', '25', '40', '108', '105']
parsed_discourse_facet ['results_citation']
<S sid ="30" ssid = "27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S><S sid ="139" ssid = "4">But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP  the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.</S><S sid ="32" ssid = "29">However  ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'139'", "'32'", "'22'", "'40'"]
'30'
'139'
'32'
'22'
'40'
['30', '139', '32', '22', '40']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S><S sid ="112" ssid = "15">Moreover  if n gets large  SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="71" ssid = "23">For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.</S><S sid ="139" ssid = "4">But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP  the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'112'", "'79'", "'71'", "'139'"]
'80'
'112'
'79'
'71'
'139'
['80', '112', '79', '71', '139']
parsed_discourse_facet ['method_citation']
<S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="92" ssid = "44">Tested on the OVIS corpus  Bonnema et al. 's proposal obtains results that are comparable to Sima'an (1999) -- see Bonnema et al.</S><S sid ="51" ssid = "3">That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S sid ="101" ssid = "4">We will refer to this model as Simplicity-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'82'", "'140'", "'92'", "'51'", "'101'"]
'82'
'140'
'92'
'51'
'101'
['82', '140', '92', '51', '101']
parsed_discourse_facet ['method_citation']
<S sid ="94" ssid = "46">This paper presents the first published results with this estimator on the WSJ.</S><S sid ="42" ssid = "39">We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.</S><S sid ="12" ssid = "9">This approach has now gained wide usage  as exemplified by the work of Collins (1996  1999)  Charniak (1996  1997)  Johnson (1998)  Chiang (2000)  and many others.</S><S sid ="51" ssid = "3">That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'", "'42'", "'12'", "'51'", "'45'"]
'94'
'42'
'12'
'51'
'45'
['94', '42', '12', '51', '45']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "4">We will refer to this model as Simplicity-DOP.</S><S sid ="51" ssid = "3">That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S sid ="139" ssid = "4">But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP  the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.</S><S sid ="67" ssid = "19">We can create a subtree by choosing any possible left subtree and any possible right subtree.</S><S sid ="126" ssid = "7">We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores  as these are commonly used to rank parsing systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'51'", "'139'", "'67'", "'126'"]
'101'
'51'
'139'
'67'
'126'
['101', '51', '139', '67', '126']
parsed_discourse_facet ['method_citation']
<S sid ="111" ssid = "14">Note that for n=1  SL-DOP is equal to Likelihood-DOP  since there is only one most probable tree to select from  and LS-DOP is equal to Simplicity-DOP  since there is only one simplest tree to select from.</S><S sid ="0" ssid = "0">An Efficient Implementation of a New DOP Model</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="71" ssid = "23">For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'111'", "'0'", "'82'", "'71'", "'140'"]
'111'
'0'
'82'
'71'
'140'
['111', '0', '82', '71', '140']
parsed_discourse_facet ['method_citation']
<S sid ="112" ssid = "15">Moreover  if n gets large  SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.</S><S sid ="145" ssid = "10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S><S sid ="106" ssid = "9">What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.</S><S sid ="101" ssid = "4">We will refer to this model as Simplicity-DOP.</S><S sid ="68" ssid = "20">Thus  there are aj= (bk+ 1)(ci + 1) possible subtrees headed by A @j. Goodman then gives a simple small PCFG with the following property: for every subtree in the training corpus headed by A  the grammar will generate an isomorphic subderivation with probability 1/a.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'145'", "'106'", "'101'", "'68'"]
'112'
'145'
'106'
'101'
'68'
['112', '145', '106', '101', '68']
parsed_discourse_facet ['implication_citation']
<S sid ="58" ssid = "10">The notation A@k denotes the node at address k where A is the nonterminal labeling that node.</S><S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S><S sid ="38" ssid = "35">Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task  the parsing time was reported to be over 200 seconds per sentence (Bod 2003).</S><S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="106" ssid = "9">What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'28'", "'38'", "'13'", "'106'"]
'58'
'28'
'38'
'13'
'106'
['58', '28', '38', '13', '106']
parsed_discourse_facet ['method_citation']
<S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="29" ssid = "26">Johnson (1998b  2002) showed that DOP1's subtree estimation method is statistically biased and inconsistent.</S><S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'", "'40'", "'39'", "'29'", "'130'"]
'28'
'40'
'39'
'29'
'130'
['28', '40', '39', '29', '130']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S><S sid ="44" ssid = "41">But while Bod's estimator obtains state-of-the-art results on the WSJ  comparable to Charniak (2000) and Collins (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'22'", "'45'", "'44'", "'82'"]
'39'
'22'
'45'
'44'
'82'
['39', '22', '45', '44', '82']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="0" ssid = "0">An Efficient Implementation of a New DOP Model</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'22'", "'0'", "'79'", "'15'"]
'108'
'22'
'0'
'79'
'15'
['108', '22', '0', '79', '15']
parsed_discourse_facet ['results_citation']
<S sid ="25" ssid = "22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S><S sid ="114" ssid = "17">Note that Goodman's PCFG-reduction method summarized in Section 2 applies not only to Likelihood-DOP but also to Simplicity-DOP.</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="89" ssid = "41">Let a be the number of times nonterminals of type A occur in the training data.</S><S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'114'", "'82'", "'89'", "'28'"]
'25'
'114'
'82'
'89'
'28'
['25', '114', '82', '89', '28']
parsed_discourse_facet ['implication_citation']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.', "Tested on the OVIS corpus  Bonnema et al. 's proposal obtains results that are comparable to Sima'an (1999) -- see Bonnema et al.", "That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.", 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'We will refer to this model as Simplicity-DOP.']
['system', 'ROUGE-S*', 'Average_R:', '0.02532', '(95%-conf.int.', '0.02532', '-', '0.02532)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.04938', '(95%-conf.int.', '0.04938', '-', '0.04938)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:78', 'F:78']
['The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.']
["Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).", "While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).", 'By using these PCFG-reductions we can thus parse with all subtrees in polynomial time.', 'Yet  his grammar contains more than 5 million subtrees and processing times of over 200 seconds per WSJ sentence are reported (Bod 2003).', 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).']
['system', 'ROUGE-S*', 'Average_R:', '0.00185', '(95%-conf.int.', '0.00185', '-', '0.00185)']
['system', 'ROUGE-S*', 'Average_P:', '0.02000', '(95%-conf.int.', '0.02000', '-', '0.02000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00339', '(95%-conf.int.', '0.00339', '-', '0.00339)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3240', 'P:300', 'F:6']
['Bod reports state-of-the-art results with this method, and observes no decrease in parse accuracy when larger subtrees are included (using subtrees up to depth 14).', 'For example, Bod (2001) samples a fixed number of subtrees of each depth, which has the effect of assigning roughly equal weight to each node in the training data, and roughly exponentially less probability for larger trees (see Goodman 2002: 12).']
['Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.', "Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.", 'In the second part of this paper  we extend our experiments with a new notion of the best parse tree.', "But while Bod's estimator obtains state-of-the-art results on the WSJ  comparable to Charniak (2000) and Collins (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).", 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).']
['system', 'ROUGE-S*', 'Average_R:', '0.02994', '(95%-conf.int.', '0.02994', '-', '0.02994)']
['system', 'ROUGE-S*', 'Average_P:', '0.13090', '(95%-conf.int.', '0.13090', '-', '0.13090)']
['system', 'ROUGE-S*', 'Average_F:', '0.04873', '(95%-conf.int.', '0.04873', '-', '0.04873)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3240', 'P:741', 'F:97']
['While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.']
['However  ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.', 'But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP  the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.', 'Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.', "Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.", 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).']
['system', 'ROUGE-S*', 'Average_R:', '0.01407', '(95%-conf.int.', '0.01407', '-', '0.01407)']
['system', 'ROUGE-S*', 'Average_P:', '0.09402', '(95%-conf.int.', '0.09402', '-', '0.09402)']
['system', 'ROUGE-S*', 'Average_F:', '0.02447', '(95%-conf.int.', '0.02447', '-', '0.02447)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:351', 'F:33']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['An Efficient Implementation of a New DOP Model', 'The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.', 'However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', "While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.", 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:78', 'F:0']
['The only thing that needs to be changed for Simplicity-DOP is that all subtrees should be assigned equal probabilities.']
['Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.', "Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).", 'Let a be the number of times nonterminals of type A occur in the training data.', "While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.", "Note that Goodman's PCFG-reduction method summarized in Section 2 applies not only to Likelihood-DOP but also to Simplicity-DOP."]
['system', 'ROUGE-S*', 'Average_R:', '0.00063', '(95%-conf.int.', '0.00063', '-', '0.00063)']
['system', 'ROUGE-S*', 'Average_P:', '0.07143', '(95%-conf.int.', '0.07143', '-', '0.07143)']
['system', 'ROUGE-S*', 'Average_F:', '0.00125', '(95%-conf.int.', '0.00125', '-', '0.00125)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:28', 'F:2']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['Thus  there are aj= (bk+ 1)(ci + 1) possible subtrees headed by A @j. Goodman then gives a simple small PCFG with the following property: for every subtree in the training corpus headed by A  the grammar will generate an isomorphic subderivation with probability 1/a.', 'This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.', 'What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.', 'We will refer to this model as Simplicity-DOP.', 'Moreover  if n gets large  SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00136', '(95%-conf.int.', '0.00136', '-', '0.00136)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00262', '(95%-conf.int.', '0.00262', '-', '0.00262)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:78', 'F:3']
['DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).']
["Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).", 'The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.', "While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).", "Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.", 'The derivation with the smallest sum  or highest rank  is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP  its results are still rather impressive for such a simple model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00547', '(95%-conf.int.', '0.00547', '-', '0.00547)']
['system', 'ROUGE-S*', 'Average_P:', '0.57778', '(95%-conf.int.', '0.57778', '-', '0.57778)']
['system', 'ROUGE-S*', 'Average_F:', '0.01084', '(95%-conf.int.', '0.01084', '-', '0.01084)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4753', 'P:45', 'F:26']
['In case the shortest derivation is not unique, Bod (2000b) proposes to back off to a frequency ordering of the subtrees.', 'That is, all subtrees of each root label are assigned a rank according to their frequency in the treebank: the most frequent subtree (or subtrees) of each root label gets rank 1, the second most frequent subtree gets rank 2, etc.']
['In the second part of this paper  we extend our experiments with a new notion of the best parse tree.', "That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.", 'This approach has now gained wide usage  as exemplified by the work of Collins (1996  1999)  Charniak (1996  1997)  Johnson (1998)  Chiang (2000)  and many others.', 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', 'This paper presents the first published results with this estimator on the WSJ.']
['system', 'ROUGE-S*', 'Average_R:', '0.00837', '(95%-conf.int.', '0.00837', '-', '0.00837)']
['system', 'ROUGE-S*', 'Average_P:', '0.05419', '(95%-conf.int.', '0.05419', '-', '0.05419)']
['system', 'ROUGE-S*', 'Average_F:', '0.01450', '(95%-conf.int.', '0.01450', '-', '0.01450)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:406', 'F:22']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.', 'An Efficient Implementation of a New DOP Model', 'Note that for n=1  SL-DOP is equal to Likelihood-DOP  since there is only one most probable tree to select from  and LS-DOP is equal to Simplicity-DOP  since there is only one simplest tree to select from.', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.']
['system', 'ROUGE-S*', 'Average_R:', '0.04407', '(95%-conf.int.', '0.04407', '-', '0.04407)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.08442', '(95%-conf.int.', '0.08442', '-', '0.08442)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:78', 'F:78']
0.298677997013 0.0131079998689 0.0239599997604





input/ref/Task1/E03-1005_sweta.csv
input/res/Task1/E03-1005.csv
parsing: input/ref/Task1/E03-1005_sweta.csv
<S sid="20" ssid="17">Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).</S>
original cit marker offset is 0
new cit marker offset is 0



["20'"]
20'
['20']
parsed_discourse_facet ['method_citation']
 <S sid="74" ssid="26">Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.</S>
original cit marker offset is 0
new cit marker offset is 0



["74'"]
74'
['74']
parsed_discourse_facet ['method_citation']
<S sid="44" ssid="41">But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["44'"]
44'
['44']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["145'"]
145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S>
original cit marker offset is 0
new cit marker offset is 0



["22'"]
22'
['22']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="14">It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).</S>
original cit marker offset is 0
new cit marker offset is 0



["133'"]
133'
['133']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier &amp; Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="35">Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
 <S sid="100" ssid="3">In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="29">However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
 <S sid="130" ssid="11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["130'"]
130'
['130']
parsed_discourse_facet ['method_citation']
 <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["140'"]
140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="24">Goodman (1996, 1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'"]
27'
['27']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="95" ssid = "47">By using these PCFG-reductions we can thus parse with all subtrees in polynomial time.</S><S sid ="25" ssid = "22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="87" ssid = "39">Yet  his grammar contains more than 5 million subtrees and processing times of over 200 seconds per WSJ sentence are reported (Bod 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'95'", "'25'", "'22'", "'87'"]
'130'
'95'
'25'
'22'
'87'
['130', '95', '25', '22', '87']
parsed_discourse_facet ['implication_citation']
<S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S><S sid ="58" ssid = "10">The notation A@k denotes the node at address k where A is the nonterminal labeling that node.</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="112" ssid = "15">Moreover  if n gets large  SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'", "'58'", "'40'", "'79'", "'112'"]
'85'
'58'
'40'
'79'
'112'
['85', '58', '40', '79', '112']
parsed_discourse_facet ['implication_citation']
<S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="25" ssid = "22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="105" ssid = "8">The derivation with the smallest sum  or highest rank  is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP  its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'25'", "'40'", "'108'", "'105'"]
'130'
'25'
'40'
'108'
'105'
['130', '25', '40', '108', '105']
parsed_discourse_facet ['results_citation']
<S sid ="30" ssid = "27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S><S sid ="139" ssid = "4">But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP  the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.</S><S sid ="32" ssid = "29">However  ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'139'", "'32'", "'22'", "'40'"]
'30'
'139'
'32'
'22'
'40'
['30', '139', '32', '22', '40']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S><S sid ="112" ssid = "15">Moreover  if n gets large  SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="71" ssid = "23">For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.</S><S sid ="139" ssid = "4">But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP  the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'112'", "'79'", "'71'", "'139'"]
'80'
'112'
'79'
'71'
'139'
['80', '112', '79', '71', '139']
parsed_discourse_facet ['method_citation']
<S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="92" ssid = "44">Tested on the OVIS corpus  Bonnema et al. 's proposal obtains results that are comparable to Sima'an (1999) -- see Bonnema et al.</S><S sid ="51" ssid = "3">That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S sid ="101" ssid = "4">We will refer to this model as Simplicity-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'82'", "'140'", "'92'", "'51'", "'101'"]
'82'
'140'
'92'
'51'
'101'
['82', '140', '92', '51', '101']
parsed_discourse_facet ['method_citation']
<S sid ="94" ssid = "46">This paper presents the first published results with this estimator on the WSJ.</S><S sid ="42" ssid = "39">We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.</S><S sid ="12" ssid = "9">This approach has now gained wide usage  as exemplified by the work of Collins (1996  1999)  Charniak (1996  1997)  Johnson (1998)  Chiang (2000)  and many others.</S><S sid ="51" ssid = "3">That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'", "'42'", "'12'", "'51'", "'45'"]
'94'
'42'
'12'
'51'
'45'
['94', '42', '12', '51', '45']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "4">We will refer to this model as Simplicity-DOP.</S><S sid ="51" ssid = "3">That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S sid ="139" ssid = "4">But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP  the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.</S><S sid ="67" ssid = "19">We can create a subtree by choosing any possible left subtree and any possible right subtree.</S><S sid ="126" ssid = "7">We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores  as these are commonly used to rank parsing systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'51'", "'139'", "'67'", "'126'"]
'101'
'51'
'139'
'67'
'126'
['101', '51', '139', '67', '126']
parsed_discourse_facet ['method_citation']
<S sid ="111" ssid = "14">Note that for n=1  SL-DOP is equal to Likelihood-DOP  since there is only one most probable tree to select from  and LS-DOP is equal to Simplicity-DOP  since there is only one simplest tree to select from.</S><S sid ="0" ssid = "0">An Efficient Implementation of a New DOP Model</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="71" ssid = "23">For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'111'", "'0'", "'82'", "'71'", "'140'"]
'111'
'0'
'82'
'71'
'140'
['111', '0', '82', '71', '140']
parsed_discourse_facet ['method_citation']
<S sid ="112" ssid = "15">Moreover  if n gets large  SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.</S><S sid ="145" ssid = "10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S><S sid ="106" ssid = "9">What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.</S><S sid ="101" ssid = "4">We will refer to this model as Simplicity-DOP.</S><S sid ="68" ssid = "20">Thus  there are aj= (bk+ 1)(ci + 1) possible subtrees headed by A @j. Goodman then gives a simple small PCFG with the following property: for every subtree in the training corpus headed by A  the grammar will generate an isomorphic subderivation with probability 1/a.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'145'", "'106'", "'101'", "'68'"]
'112'
'145'
'106'
'101'
'68'
['112', '145', '106', '101', '68']
parsed_discourse_facet ['implication_citation']
<S sid ="58" ssid = "10">The notation A@k denotes the node at address k where A is the nonterminal labeling that node.</S><S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S><S sid ="38" ssid = "35">Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task  the parsing time was reported to be over 200 seconds per sentence (Bod 2003).</S><S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="106" ssid = "9">What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'28'", "'38'", "'13'", "'106'"]
'58'
'28'
'38'
'13'
'106'
['58', '28', '38', '13', '106']
parsed_discourse_facet ['method_citation']
<S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="29" ssid = "26">Johnson (1998b  2002) showed that DOP1's subtree estimation method is statistically biased and inconsistent.</S><S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'", "'40'", "'39'", "'29'", "'130'"]
'28'
'40'
'39'
'29'
'130'
['28', '40', '39', '29', '130']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="45" ssid = "42">In the second part of this paper  we extend our experiments with a new notion of the best parse tree.</S><S sid ="44" ssid = "41">But while Bod's estimator obtains state-of-the-art results on the WSJ  comparable to Charniak (2000) and Collins (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'22'", "'45'", "'44'", "'82'"]
'39'
'22'
'45'
'44'
'82'
['39', '22', '45', '44', '82']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "11">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.</S><S sid ="22" ssid = "19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S><S sid ="0" ssid = "0">An Efficient Implementation of a New DOP Model</S><S sid ="79" ssid = "31">While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'22'", "'0'", "'79'", "'15'"]
'108'
'22'
'0'
'79'
'15'
['108', '22', '0', '79', '15']
parsed_discourse_facet ['results_citation']
<S sid ="25" ssid = "22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S><S sid ="114" ssid = "17">Note that Goodman's PCFG-reduction method summarized in Section 2 applies not only to Likelihood-DOP but also to Simplicity-DOP.</S><S sid ="82" ssid = "34">Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.</S><S sid ="89" ssid = "41">Let a be the number of times nonterminals of type A occur in the training data.</S><S sid ="28" ssid = "25">While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'114'", "'82'", "'89'", "'28'"]
'25'
'114'
'82'
'89'
'28'
['25', '114', '82', '89', '28']
parsed_discourse_facet ['implication_citation']
['This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.']
['Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.', "Tested on the OVIS corpus  Bonnema et al. 's proposal obtains results that are comparable to Sima'an (1999) -- see Bonnema et al.", "That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.", 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'We will refer to this model as Simplicity-DOP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00779', '(95%-conf.int.', '0.00779', '-', '0.00779)']
['system', 'ROUGE-S*', 'Average_P:', '0.17647', '(95%-conf.int.', '0.17647', '-', '0.17647)']
['system', 'ROUGE-S*', 'Average_F:', '0.01492', '(95%-conf.int.', '0.01492', '-', '0.01492)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:136', 'F:24']
['Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).']
["Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).", "While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).", 'By using these PCFG-reductions we can thus parse with all subtrees in polynomial time.', 'Yet  his grammar contains more than 5 million subtrees and processing times of over 200 seconds per WSJ sentence are reported (Bod 2003).', 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).']
['system', 'ROUGE-S*', 'Average_R:', '0.00093', '(95%-conf.int.', '0.00093', '-', '0.00093)']
['system', 'ROUGE-S*', 'Average_P:', '0.01961', '(95%-conf.int.', '0.01961', '-', '0.01961)']
['system', 'ROUGE-S*', 'Average_F:', '0.00177', '(95%-conf.int.', '0.00177', '-', '0.00177)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3240', 'P:153', 'F:3']
["While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996)."]
['Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.', "Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.", 'In the second part of this paper  we extend our experiments with a new notion of the best parse tree.', "But while Bod's estimator obtains state-of-the-art results on the WSJ  comparable to Charniak (2000) and Collins (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).", 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).']
['system', 'ROUGE-S*', 'Average_R:', '0.04722', '(95%-conf.int.', '0.04722', '-', '0.04722)']
['system', 'ROUGE-S*', 'Average_P:', '0.80526', '(95%-conf.int.', '0.80526', '-', '0.80526)']
['system', 'ROUGE-S*', 'Average_F:', '0.08921', '(95%-conf.int.', '0.08921', '-', '0.08921)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3240', 'P:190', 'F:153']
['While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.']
['However  ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.', 'But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP  the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.', 'Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.', "Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.", 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).']
['system', 'ROUGE-S*', 'Average_R:', '0.01407', '(95%-conf.int.', '0.01407', '-', '0.01407)']
['system', 'ROUGE-S*', 'Average_P:', '0.09402', '(95%-conf.int.', '0.09402', '-', '0.09402)']
['system', 'ROUGE-S*', 'Average_F:', '0.02447', '(95%-conf.int.', '0.02447', '-', '0.02447)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:351', 'F:33']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['An Efficient Implementation of a New DOP Model', 'The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.', 'However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', "While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this  Goodman's method can do the same job with a more compact grammar.", 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:78', 'F:0']
['Goodman (1996, 1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.']
['Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.', "Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).", 'Let a be the number of times nonterminals of type A occur in the training data.', "While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1  it does efficiently compute the &quot;maximum constituents parse&quot;  i.e. the parse tree which is most likely to have the largest number of correct constituents.", "Note that Goodman's PCFG-reduction method summarized in Section 2 applies not only to Likelihood-DOP but also to Simplicity-DOP."]
['system', 'ROUGE-S*', 'Average_R:', '0.00791', '(95%-conf.int.', '0.00791', '-', '0.00791)']
['system', 'ROUGE-S*', 'Average_P:', '0.13158', '(95%-conf.int.', '0.13158', '-', '0.13158)']
['system', 'ROUGE-S*', 'Average_F:', '0.01493', '(95%-conf.int.', '0.01493', '-', '0.01493)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:190', 'F:25']
["Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003)."]
['Thus  there are aj= (bk+ 1)(ci + 1) possible subtrees headed by A @j. Goodman then gives a simple small PCFG with the following property: for every subtree in the training corpus headed by A  the grammar will generate an isomorphic subderivation with probability 1/a.', 'This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.', 'What is more important  is  that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.', 'We will refer to this model as Simplicity-DOP.', 'Moreover  if n gets large  SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00724', '(95%-conf.int.', '0.00724', '-', '0.00724)']
['system', 'ROUGE-S*', 'Average_P:', '0.10458', '(95%-conf.int.', '0.10458', '-', '0.10458)']
['system', 'ROUGE-S*', 'Average_F:', '0.01354', '(95%-conf.int.', '0.01354', '-', '0.01354)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:153', 'F:16']
["But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996)."]
["Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000)  or by Viterbi n-best search (Bod 2001)  or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).", 'The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees  where n is a free parameter.', "While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).", "Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.", 'The derivation with the smallest sum  or highest rank  is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP  its results are still rather impressive for such a simple model.']
['system', 'ROUGE-S*', 'Average_R:', '0.03535', '(95%-conf.int.', '0.03535', '-', '0.03535)']
['system', 'ROUGE-S*', 'Average_P:', '0.88421', '(95%-conf.int.', '0.88421', '-', '0.88421)']
['system', 'ROUGE-S*', 'Average_F:', '0.06797', '(95%-conf.int.', '0.06797', '-', '0.06797)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4753', 'P:190', 'F:168']
['DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).']
['In the second part of this paper  we extend our experiments with a new notion of the best parse tree.', "That is  the probability of a subtree t is taken as the number of occurrences of t in the training set  I t I  divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.", 'This approach has now gained wide usage  as exemplified by the work of Collins (1996  1999)  Charniak (1996  1997)  Johnson (1998)  Chiang (2000)  and many others.', 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', 'This paper presents the first published results with this estimator on the WSJ.']
['system', 'ROUGE-S*', 'Average_R:', '0.00419', '(95%-conf.int.', '0.00419', '-', '0.00419)']
['system', 'ROUGE-S*', 'Average_P:', '0.08088', '(95%-conf.int.', '0.08088', '-', '0.08088)']
['system', 'ROUGE-S*', 'Average_F:', '0.00796', '(95%-conf.int.', '0.00796', '-', '0.00796)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:136', 'F:11']
["Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002)."]
['Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees  and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.', 'An Efficient Implementation of a New DOP Model', 'Note that for n=1  SL-DOP is equal to Likelihood-DOP  since there is only one most probable tree to select from  and LS-DOP is equal to Simplicity-DOP  since there is only one simplest tree to select from.', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.']
['system', 'ROUGE-S*', 'Average_R:', '0.00226', '(95%-conf.int.', '0.00226', '-', '0.00226)']
['system', 'ROUGE-S*', 'Average_P:', '0.01333', '(95%-conf.int.', '0.01333', '-', '0.01333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00386', '(95%-conf.int.', '0.00386', '-', '0.00386)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:300', 'F:4']
0.23099399769 0.012695999873 0.0238629997614





input/ref/Task1/A00-2018_sweta.csv
input/res/Task1/A00-2018.csv
parsing: input/ref/Task1/A00-2018_sweta.csv
 <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["17'"]
17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
 <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["17'"]
17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="11">Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &amp;quot;correct&amp;quot;, and statistics were collected on the resulting parses.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="10">(It is &amp;quot;soft&amp;quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.)</S>
original cit marker offset is 0
new cit marker offset is 0



["119'"]
119'
['119']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.</S>
original cit marker offset is 0
new cit marker offset is 0



["95'"]
95'
['95']
parsed_discourse_facet ['method_citation']
 <S sid="175" ssid="2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["175'"]
175'
['175']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
original cit marker offset is 0
new cit marker offset is 0



["92'"]
92'
['92']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="17">This is indicated in Figure 2, where the model labeled &amp;quot;Best&amp;quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["126'"]
126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="1">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c, t(c) (t for &amp;quot;tag&amp;quot;), then the lexical head of c, h(c), and then the expansion of c into further constituents e(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["174'"]
174'
['174']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="32">As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &amp;quot;before&amp;quot;), and the label of the grandparent of c (la).</S>
original cit marker offset is 0
new cit marker offset is 0



["63'"]
63'
['63']
parsed_discourse_facet ['method_citation']
<S sid="78" ssid="47">With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.</S>
original cit marker offset is 0
new cit marker offset is 0



["78'"]
78'
['78']
parsed_discourse_facet ['method_citation']
 <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="7">From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.</S>
original cit marker offset is 0
new cit marker offset is 0



["180'"]
180'
['180']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2018.csv
<S sid ="114" ssid = "5">That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.</S><S sid ="159" ssid = "50">Something very much like this is done in [15].</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="165" ssid = "56">Note that we also tried including this information using a standard deleted-interpolation model.</S><S sid ="163" ssid = "54">Next we add the less obvious conditioning events noted in our previous discussion of the final model  grandparent label lg and left sibling label /b.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'159'", "'2'", "'165'", "'163'"]
'114'
'159'
'2'
'165'
'163'
['114', '159', '2', '165', '163']
parsed_discourse_facet ['implication_citation']
<S sid ="114" ssid = "5">That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.</S><S sid ="3" ssid = "3">The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.</S><S sid ="99" ssid = "10">Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S><S sid ="129" ssid = "20">It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments)  it does not guess the pre-terminal before guessing the lexical head  and it uses a tree-bank grammar rather than a Markov grammar.</S><S sid ="92" ssid = "3">For runs with the generative model based upon Markov grammar statistics  the first pass uses the same statistics  but conditioned only on standard PCFG information.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'3'", "'99'", "'129'", "'92'"]
'114'
'3'
'99'
'129'
'92'
['114', '3', '99', '129', '92']
parsed_discourse_facet ['implication_citation']
<S sid ="132" ssid = "23">Between the Old model and the Best model  Figure 2 gives precision/recall measurements for several different versions of our parser.</S><S sid ="69" ssid = "38">For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.</S><S sid ="184" ssid = "11">The first is the slight  but important  improvement achieved by using this model over conventional deleted interpolation  as indicated in Figure 2.</S><S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="85" ssid = "54">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models  this simplifies the model significantly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'132'", "'69'", "'184'", "'157'", "'85'"]
'132'
'69'
'184'
'157'
'85'
['132', '69', '184', '157', '85']
parsed_discourse_facet ['results_citation']
<S sid ="137" ssid = "28">However  Collins in [10] does not stress the decision to guess the head's pre-terminal first  and it might be lost on the casual reader.</S><S sid ="51" ssid = "20">Second  and this is a point we have not yet mentioned  the features used in these models need have no particular independence of one another.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="58" ssid = "27">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid ="150" ssid = "41">The second modification is the explicit marking of noun and verb-phrase coordination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'51'", "'2'", "'58'", "'150'"]
'137'
'51'
'2'
'58'
'150'
['137', '51', '2', '58', '150']
parsed_discourse_facet ['method_citation']
<S sid ="49" ssid = "18">First  as already implicit in our discussion  factoring the probability computation into a sequence of values corresponding to various &quot;features&quot; suggests that the probability model should be easily changeable  just change the set of features used.</S><S sid ="4" ssid = "4">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c  information outside c that our probability model deems important in determining the probability in question.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="165" ssid = "56">Note that we also tried including this information using a standard deleted-interpolation model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'4'", "'13'", "'2'", "'165'"]
'49'
'4'
'13'
'2'
'165'
['49', '4', '13', '2', '165']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "3">Also  remember that H is a pla ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.</S><S sid ="11" ssid = "7">What fundamentally distinguishes probabilistic generative parsers is how they compute p(r)  and it is to that topic we turn next.</S><S sid ="96" ssid = "7">As noted above  the probability model uses five smoothed probability distributions  one each for Li  M Ri t  and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7.</S><S sid ="99" ssid = "10">Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S><S sid ="19" ssid = "8">The method we use follows that of [10].</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'11'", "'96'", "'99'", "'19'"]
'34'
'11'
'96'
'99'
'19'
['34', '11', '96', '99', '19']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.</S><S sid ="185" ssid = "12">We expect that as we experiment with other  more semantic conditioning information  the importance of this aspect of the model will increase.</S><S sid ="80" ssid = "49">(Our experience is that rather than requiring 50 or so iterations  three suffice.)</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="144" ssid = "35">This quantity is a relatively intuitive one (as  for example  it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it  in effect  as the unsmoothed probability upon which all smoothing of p(h) is based.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'185'", "'80'", "'54'", "'144'"]
'3'
'185'
'80'
'54'
'144'
['3', '185', '80', '54', '144']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "16">The intuitive idea is that each factor gi is larger than one if the feature in question makes the probability more likely  one if the feature has no effect  and smaller than one if it makes the probability less likely.</S><S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S><S sid ="85" ssid = "54">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models  this simplifies the model significantly.</S><S sid ="22" ssid = "11">For us the non-terminal symbols are those of the tree-bank  augmented by the symbols aux and auxg  which have been assigned deterministically to certain auxiliary verbs such as &quot;have&quot; or &quot;having&quot;.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'2'", "'41'", "'85'", "'22'"]
'47'
'2'
'41'
'85'
'22'
['47', '2', '41', '85', '22']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="55" ssid = "24">This method is known as &quot;deleted interpolation&quot; smoothing.</S><S sid ="66" ssid = "35">In many cases this is clearly warranted.</S><S sid ="4" ssid = "4">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid ="121" ssid = "12">Without these enhancements Char97 performs at the 86.6% level for sentences of length < 40.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'55'", "'66'", "'4'", "'121'"]
'157'
'55'
'66'
'4'
'121'
['157', '55', '66', '4', '121']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid ="157" ssid = "48">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid ="0" ssid = "0">A Maximum-Entropy-Inspired Parser *</S><S sid ="37" ssid = "6">Rather  we concentrate on the aspects of these models that most directly influenced the model presented here.</S><S sid ="21" ssid = "10">(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'157'", "'0'", "'37'", "'21'"]
'2'
'157'
'0'
'37'
'21'
['2', '157', '0', '37', '21']
parsed_discourse_facet ['implication_citation']
<S sid ="48" ssid = "17">Maximum-entropy models have two benefits for a parser builder.</S><S sid ="21" ssid = "10">(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="177" ssid = "4">The results reported here disprove this conjecture.</S><S sid ="159" ssid = "50">Something very much like this is done in [15].</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'21'", "'54'", "'177'", "'159'"]
'48'
'21'
'54'
'177'
'159'
['48', '21', '54', '177', '159']
parsed_discourse_facet ['method_citation']
<S sid ="35" ssid = "4">In the past few years the maximum entropy  or log-linear  approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing [1 17].</S><S sid ="121" ssid = "12">Without these enhancements Char97 performs at the 86.6% level for sentences of length < 40.</S><S sid ="69" ssid = "38">For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.</S><S sid ="138" ssid = "29">Indeed  it was lost on the present author until he went back after the fact and found it there.</S><S sid ="114" ssid = "5">That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'", "'121'", "'69'", "'138'", "'114'"]
'35'
'121'
'69'
'138'
'114'
['35', '121', '69', '138', '114']
parsed_discourse_facet ['method_citation']
<S sid ="169" ssid = "60">Up to this point all the models considered in this section are tree-bank grammar models.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="134" ssid = "25">As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.</S><S sid ="99" ssid = "10">Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S><S sid ="155" ssid = "46">For example  in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'", "'125'", "'134'", "'99'", "'155'"]
'169'
'125'
'134'
'99'
'155'
['169', '125', '134', '99', '155']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid ="34" ssid = "3">Also  remember that H is a pla ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.</S><S sid ="165" ssid = "56">Note that we also tried including this information using a standard deleted-interpolation model.</S><S sid ="69" ssid = "38">For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.</S><S sid ="0" ssid = "0">A Maximum-Entropy-Inspired Parser *</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'34'", "'165'", "'69'", "'0'"]
'7'
'34'
'165'
'69'
'0'
['7', '34', '165', '69', '0']
parsed_discourse_facet ['results_citation']
<S sid ="41" ssid = "10">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S><S sid ="37" ssid = "6">Rather  we concentrate on the aspects of these models that most directly influenced the model presented here.</S><S sid ="169" ssid = "60">Up to this point all the models considered in this section are tree-bank grammar models.</S><S sid ="7" ssid = "3">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'", "'37'", "'169'", "'7'", "'125'"]
'41'
'37'
'169'
'7'
'125'
['41', '37', '169', '7', '125']
parsed_discourse_facet ['implication_citation']
<S sid ="88" ssid = "57">While we could have smoothed in the same fashion  we choose instead to use standard deleted interpolation.</S><S sid ="124" ssid = "15">We note here that this corpus is somewhat more difficult than the &quot;official&quot; test corpus.</S><S sid ="54" ssid = "23">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid ="21" ssid = "10">(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)</S><S sid ="63" ssid = "32">As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'124'", "'54'", "'21'", "'63'"]
'88'
'124'
'54'
'21'
'63'
['88', '124', '54', '21', '63']
parsed_discourse_facet ['method_citation']
['In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.']
['Something very much like this is done in [15].', 'That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.', u'Next we add the less obvious conditioning events noted in our previous discussion of the final model \xe2\u20ac\u201d grandparent label lg and left sibling label /b.', 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].', 'Note that we also tried including this information using a standard deleted-interpolation model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00332', '(95%-conf.int.', '0.00332', '-', '0.00332)']
['system', 'ROUGE-S*', 'Average_P:', '0.02857', '(95%-conf.int.', '0.02857', '-', '0.02857)']
['system', 'ROUGE-S*', 'Average_F:', '0.00595', '(95%-conf.int.', '0.00595', '-', '0.00595)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:105', 'F:3']
['We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank.']
['This method is known as &quot;deleted interpolation&quot; smoothing.', 'Without these enhancements Char97 performs at the 86.6% level for sentences of length ', 'In many cases this is clearly warranted.', 'Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.', "We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head."]
['system', 'ROUGE-S*', 'Average_R:', '0.00211', '(95%-conf.int.', '0.00211', '-', '0.00211)']
['system', 'ROUGE-S*', 'Average_P:', '0.00317', '(95%-conf.int.', '0.00317', '-', '0.00317)']
['system', 'ROUGE-S*', 'Average_F:', '0.00254', '(95%-conf.int.', '0.00254', '-', '0.00254)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:630', 'F:2']
['Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &quot;correct&quot;, and statistics were collected on the resulting parses.']
['Second  and this is a point we have not yet mentioned  the features used in these models need have no particular independence of one another.', 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].', "However  Collins in [10] does not stress the decision to guess the head's pre-terminal first  and it might be lost on the casual reader.", 'Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.', 'The second modification is the explicit marking of noun and verb-phrase coordination.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:171', 'F:0']
['This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].']
['This quantity is a relatively intuitive one (as  for example  it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it  in effect  as the unsmoothed probability upon which all smoothing of p(h) is based.', "The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).", 'The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.', '(Our experience is that rather than requiring 50 or so iterations  three suffice.)', 'We expect that as we experiment with other  more semantic conditioning information  the importance of this aspect of the model will increase.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:78', 'F:0']
['As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.']
['For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.', 'Rather  we concentrate on the aspects of these models that most directly influenced the model presented here.', 'Up to this point all the models considered in this section are tree-bank grammar models.', 'Following [5 10]  our parser is based upon a probabilistic generative model.', "For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise."]
['system', 'ROUGE-S*', 'Average_R:', '0.01077', '(95%-conf.int.', '0.01077', '-', '0.01077)']
['system', 'ROUGE-S*', 'Average_P:', '0.06926', '(95%-conf.int.', '0.06926', '-', '0.06926)']
['system', 'ROUGE-S*', 'Average_F:', '0.01865', '(95%-conf.int.', '0.01865', '-', '0.01865)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:231', 'F:16']
['We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.']
['What fundamentally distinguishes probabilistic generative parsers is how they compute p(r)  and it is to that topic we turn next.', 'Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.', 'As noted above  the probability model uses five smoothed probability distributions  one each for Li  M Ri t  and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7.', 'The method we use follows that of [10].', 'Also  remember that H is a pla ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:190', 'F:0']
['In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.']
['Between the Old model and the Best model  Figure 2 gives precision/recall measurements for several different versions of our parser.', 'As partition-function calculation is typically the major on-line computational problem for maximum-entropy models  this simplifies the model significantly.', 'Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.', 'The first is the slight  but important  improvement achieved by using this model over conventional deleted interpolation  as indicated in Figure 2.', "For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label."]
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:105', 'F:0']
['(It is &quot;soft&quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.)']
['Note that we also tried including this information using a standard deleted-interpolation model.', u'Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c \u2014 information outside c that our probability model deems important in determining the probability in question.', 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].', "We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.", u'First  as already implicit in our discussion  factoring the probability computation into a sequence of values corresponding to various &quot;features&quot; suggests that the probability model should be easily changeable \u2014 just change the set of features used.']
['system', 'ROUGE-S*', 'Average_R:', '0.00105', '(95%-conf.int.', '0.00105', '-', '0.00105)']
['system', 'ROUGE-S*', 'Average_P:', '0.02500', '(95%-conf.int.', '0.02500', '-', '0.02500)']
['system', 'ROUGE-S*', 'Average_F:', '0.00202', '(95%-conf.int.', '0.00202', '-', '0.00202)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:120', 'F:3']
['We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.']
['Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.', 'That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.', 'The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.', 'For runs with the generative model based upon Markov grammar statistics  the first pass uses the same statistics  but conditioned only on standard PCFG information.', 'It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments)  it does not guess the pre-terminal before guessing the lexical head  and it uses a tree-bank grammar rather than a Markov grammar.']
['system', 'ROUGE-S*', 'Average_R:', '0.03154', '(95%-conf.int.', '0.03154', '-', '0.03154)']
['system', 'ROUGE-S*', 'Average_P:', '0.07822', '(95%-conf.int.', '0.07822', '-', '0.07822)']
['system', 'ROUGE-S*', 'Average_F:', '0.04496', '(95%-conf.int.', '0.04496', '-', '0.04496)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:946', 'F:74']
["From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head."]
["The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).", 'While we could have smoothed in the same fashion  we choose instead to use standard deleted interpolation.', "As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).", 'We note here that this corpus is somewhat more difficult than the &quot;official&quot; test corpus.', '(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)']
['system', 'ROUGE-S*', 'Average_R:', '0.00565', '(95%-conf.int.', '0.00565', '-', '0.00565)']
['system', 'ROUGE-S*', 'Average_P:', '0.03623', '(95%-conf.int.', '0.03623', '-', '0.03623)']
['system', 'ROUGE-S*', 'Average_F:', '0.00978', '(95%-conf.int.', '0.00978', '-', '0.00978)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:276', 'F:10']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.']
['Without these enhancements Char97 performs at the 86.6% level for sentences of length ', 'In the past few years the maximum entropy  or log-linear  approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing [1 17].']
['system', 'ROUGE-S*', 'Average_R:', '0.00725', '(95%-conf.int.', '0.00725', '-', '0.00725)']
['system', 'ROUGE-S*', 'Average_P:', '0.00493', '(95%-conf.int.', '0.00493', '-', '0.00493)']
['system', 'ROUGE-S*', 'Average_F:', '0.00587', '(95%-conf.int.', '0.00587', '-', '0.00587)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:276', 'P:406', 'F:2']
['With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.']
['Note that we also tried including this information using a standard deleted-interpolation model.', "For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.", 'A Maximum-Entropy-Inspired Parser *', 'Following [5 10]  our parser is based upon a probabilistic generative model.', 'Also  remember that H is a pla ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:45', 'F:0']
0.0204483331629 0.00514083329049 0.00748083327099





input/ref/Task1/W11-2123_swastika.csv
input/res/Task1/W11-2123.csv
parsing: input/ref/Task1/W11-2123_swastika.csv
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



['52']
52
['52']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="177" ssid="49">However, TRIE partitions storage by n-gram length, so walking the trie reads N disjoint pages.</S>
original cit marker offset is 0
new cit marker offset is 0



['177']
177
['177']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="278" ssid="5">We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns.</S>
original cit marker offset is 0
new cit marker offset is 0



['278']
278
['278']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/W11-2123.csv
<S sid ="270" ssid = "12">This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S>
original cit marker offset is 0
new cit marker offset is 0



["'270'", "'276'", "'265'", "'286'", "'284'"]
'270'
'276'
'265'
'286'
'284'
['270', '276', '265', '286', '284']
parsed_discourse_facet ['implication_citation']
<S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S>
original cit marker offset is 0
new cit marker offset is 0



["'262'", "'280'", "'265'", "'277'", "'284'"]
'262'
'280'
'265'
'277'
'284'
['262', '280', '265', '277', '284']
parsed_discourse_facet ['implication_citation']
<S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="285" ssid = "6">Juri Ganitkevitch answered questions about Joshua.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'265'", "'284'", "'278'", "'285'", "'276'"]
'265'
'284'
'278'
'285'
'276'
['265', '284', '278', '285', '276']
parsed_discourse_facet ['results_citation']
<S sid ="260" ssid = "2">For speed  we plan to implement the direct-mapped cache from BerkeleyLM.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>
original cit marker offset is 0
new cit marker offset is 0



["'260'", "'262'", "'280'", "'283'", "'266'"]
'260'
'262'
'280'
'283'
'266'
['260', '262', '280', '283', '266']
parsed_discourse_facet ['method_citation']
<S sid ="272" ssid = "14">Generalizing state minimization  the model could also provide explicit bounds on probability for both backward and forward extension.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="268" ssid = "10">For example  syntactic decoders (Koehn et al.  2007; Dyer et al.  2010; Li et al.  2009) perform dynamic programming parametrized by both backward- and forward-looking state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'272'", "'265'", "'283'", "'286'", "'268'"]
'272'
'265'
'283'
'286'
'268'
['272', '265', '283', '286', '268']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="264" ssid = "6">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'284'", "'265'", "'264'", "'267'"]
'256'
'284'
'265'
'264'
'267'
['256', '284', '265', '264', '267']
parsed_discourse_facet ['method_citation']
<S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S>
original cit marker offset is 0
new cit marker offset is 0



["'283'", "'284'", "'280'", "'265'", "'275'"]
'283'
'284'
'280'
'265'
'275'
['283', '284', '280', '265', '275']
parsed_discourse_facet ['method_citation']
<S sid ="261" ssid = "3">Much could be done to further reduce memory consumption.</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'261'", "'277'", "'286'", "'284'", "'283'"]
'261'
'277'
'286'
'284'
'283'
['261', '277', '286', '284', '283']
parsed_discourse_facet ['method_citation']
<S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S><S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'286'", "'280'", "'267'", "'279'", "'283'"]
'286'
'280'
'267'
'279'
'283'
['286', '280', '267', '279', '283']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'287'", "'266'", "'262'", "'265'"]
'256'
'287'
'266'
'262'
'265'
['256', '287', '266', '262', '265']
parsed_discourse_facet ['implication_citation']
<S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="264" ssid = "6">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'262'", "'278'", "'277'", "'265'", "'264'"]
'262'
'278'
'277'
'265'
'264'
['262', '278', '277', '265', '264']
parsed_discourse_facet ['method_citation']
<S sid ="261" ssid = "3">Much could be done to further reduce memory consumption.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S><S sid ="269" ssid = "11">If they knew that the first four words in a hypergraph node would never extend to the left and form a 5-gram  then three or even fewer words could be kept in the backward state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'261'", "'265'", "'266'", "'267'", "'269'"]
'261'
'265'
'266'
'267'
'269'
['261', '265', '266', '267', '269']
parsed_discourse_facet ['method_citation']
<S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'287'", "'266'", "'286'", "'262'", "'274'"]
'287'
'266'
'286'
'262'
'274'
['287', '266', '286', '262', '274']
parsed_discourse_facet ['method_citation']
<S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'286'", "'256'", "'265'", "'284'", "'276'"]
'286'
'256'
'265'
'284'
'276'
['286', '256', '265', '284', '276']
parsed_discourse_facet ['results_citation']
<S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S><S sid ="270" ssid = "12">This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.</S>
original cit marker offset is 0
new cit marker offset is 0



["'278'", "'262'", "'265'", "'275'", "'270'"]
'278'
'262'
'265'
'275'
'270'
['278', '262', '265', '275', '270']
parsed_discourse_facet ['implication_citation']
<S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'280'", "'262'", "'265'", "'279'", "'256'"]
'280'
'262'
'265'
'279'
'256'
['280', '262', '265', '279', '256']
parsed_discourse_facet ['method_citation']
<S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'279'", "'256'", "'262'", "'265'", "'276'"]
'279'
'256'
'262'
'265'
'276'
['279', '256', '262', '265', '276']
parsed_discourse_facet ['results_citation']
<S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="264" ssid = "6">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'277'", "'286'", "'287'", "'264'"]
'256'
'277'
'286'
'287'
'264'
['256', '277', '286', '287', '264']
parsed_discourse_facet ['aim_citation']
<S sid ="263" ssid = "5">Quantization can be improved by jointly encoding probability and backoff.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="258" ssid = "77">However  the point of RandLM is to scale to even larger data  compensating for this loss in quality.</S>
original cit marker offset is 0
new cit marker offset is 0



["'263'", "'265'", "'287'", "'283'", "'258'"]
'263'
'265'
'287'
'283'
'258'
['263', '265', '287', '283', '258']
parsed_discourse_facet ['method_citation']
<S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'265'", "'280'", "'256'", "'278'", "'267'"]
'265'
'280'
'256'
'278'
'267'
['265', '280', '256', '278', '267']
parsed_discourse_facet ['aim_citation']
['The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.']
['Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', 'We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.', 'Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.', '0750271 and by the DARPA GALE program.', 'This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:120', 'F:0']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['If they knew that the first four words in a hypergraph node would never extend to the left and form a 5-gram  then three or even fewer words could be kept in the backward state.', 'Much could be done to further reduce memory consumption.', 'While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.', 'Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).']
['system', 'ROUGE-S*', 'Average_R:', '0.00490', '(95%-conf.int.', '0.00490', '-', '0.00490)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.00921', '(95%-conf.int.', '0.00921', '-', '0.00921)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:78', 'F:6']
['We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns.']
['For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.', 'This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.', u'We elected run Moses single-threaded to minimize the impact of RandLM\xe2\u20ac\u2122s cache on memory use.', '0750271 and by the DARPA GALE program.', 'These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:91', 'F:0']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['Generalizing state minimization  the model could also provide explicit bounds on probability for both backward and forward extension.', 'For example  syntactic decoders (Koehn et al.  2007; Dyer et al.  2010; Li et al.  2009) perform dynamic programming parametrized by both backward- and forward-looking state.', 'Nicola Bertoldi and Marcello Federico assisted with IRSTLM.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.']
['system', 'ROUGE-S*', 'Average_R:', '0.00065', '(95%-conf.int.', '0.00065', '-', '0.00065)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00124', '(95%-conf.int.', '0.00124', '-', '0.00124)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:78', 'F:1']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.', 'Juri Ganitkevitch answered questions about Joshua.', 'Chris Dyer integrated the code into cdec.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.']
['system', 'ROUGE-S*', 'Average_R:', '0.00116', '(95%-conf.int.', '0.00116', '-', '0.00116)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00218', '(95%-conf.int.', '0.00218', '-', '0.00218)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:55', 'F:1']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', 'Nicola Bertoldi and Marcello Federico assisted with IRSTLM.', 'For speed  we plan to implement the direct-mapped cache from BerkeleyLM.', 'Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.', 'Alon Lavie advised on this work.']
['system', 'ROUGE-S*', 'Average_R:', '0.00476', '(95%-conf.int.', '0.00476', '-', '0.00476)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00847', '(95%-conf.int.', '0.00847', '-', '0.00847)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:78', 'F:3']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', 'For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.']
['system', 'ROUGE-S*', 'Average_R:', '0.00055', '(95%-conf.int.', '0.00055', '-', '0.00055)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00106', '(95%-conf.int.', '0.00106', '-', '0.00106)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:55', 'F:1']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.', u'We elected run Moses single-threaded to minimize the impact of RandLM\xe2\u20ac\u2122s cache on memory use.', 'The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:55', 'F:0']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', 'These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.', 'Chris Dyer integrated the code into cdec.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'Alon Lavie advised on this work.']
['system', 'ROUGE-S*', 'Average_R:', '0.00348', '(95%-conf.int.', '0.00348', '-', '0.00348)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00639', '(95%-conf.int.', '0.00639', '-', '0.00639)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:78', 'F:3']
['The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.']
['The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.', 'Alon Lavie advised on this work.', u'We elected run Moses single-threaded to minimize the impact of RandLM\xe2\u20ac\u2122s cache on memory use.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:120', 'F:0']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.', 'Chris Dyer integrated the code into cdec.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.', 'This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.']
['system', 'ROUGE-S*', 'Average_R:', '0.00332', '(95%-conf.int.', '0.00332', '-', '0.00332)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00612', '(95%-conf.int.', '0.00612', '-', '0.00612)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:78', 'F:3']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['Alon Lavie advised on this work.', 'Nicola Bertoldi and Marcello Federico assisted with IRSTLM.', 'Chris Dyer integrated the code into cdec.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.']
['system', 'ROUGE-S*', 'Average_R:', '0.01008', '(95%-conf.int.', '0.01008', '-', '0.01008)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.01783', '(95%-conf.int.', '0.01783', '-', '0.01783)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:78', 'F:6']
['Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.']
['While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.', u'We elected run Moses single-threaded to minimize the impact of RandLM\xe2\u20ac\u2122s cache on memory use.', 'Chris Dyer integrated the code into cdec.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.']
['system', 'ROUGE-S*', 'Average_R:', '0.00075', '(95%-conf.int.', '0.00075', '-', '0.00075)']
['system', 'ROUGE-S*', 'Average_P:', '0.01515', '(95%-conf.int.', '0.01515', '-', '0.01515)']
['system', 'ROUGE-S*', 'Average_F:', '0.00144', '(95%-conf.int.', '0.00144', '-', '0.00144)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:66', 'F:1']
['The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.']
['We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', 'This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.', 'The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:120', 'F:0']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.', 'Alon Lavie advised on this work.', 'While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.', u'We elected run Moses single-threaded to minimize the impact of RandLM\xe2\u20ac\u2122s cache on memory use.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).']
['system', 'ROUGE-S*', 'Average_R:', '0.00078', '(95%-conf.int.', '0.00078', '-', '0.00078)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00150', '(95%-conf.int.', '0.00150', '-', '0.00150)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:55', 'F:1']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['Nicola Bertoldi and Marcello Federico assisted with IRSTLM.', 'Much could be done to further reduce memory consumption.', 'This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.', 'Chris Dyer integrated the code into cdec.', 'These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:78', 'F:0']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.', u'We elected run Moses single-threaded to minimize the impact of RandLM\xe2\u20ac\u2122s cache on memory use.', 'Chris Dyer integrated the code into cdec.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.']
['system', 'ROUGE-S*', 'Average_R:', '0.00106', '(95%-conf.int.', '0.00106', '-', '0.00106)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00200', '(95%-conf.int.', '0.00200', '-', '0.00200)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:55', 'F:1']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['Quantization can be improved by jointly encoding probability and backoff.', 'Nicola Bertoldi and Marcello Federico assisted with IRSTLM.', 'However  the point of RandLM is to scale to even larger data  compensating for this loss in quality.', '0750271 and by the DARPA GALE program.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:55', 'F:0']
0.0205505554414 0.00174944443473 0.00319111109338





input/ref/Task1/P08-1043_sweta.csv
input/res/Task1/P08-1043.csv
parsing: input/ref/Task1/P08-1043_sweta.csv
<S sid="156" ssid="34">To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["4'"]
4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="2">Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.</S>
original cit marker offset is 0
new cit marker offset is 0



["70'"]
70'
['70']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="3">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="9">Tsarfaty (2006) used a morphological analyzer (Segal, 2000), a PoS tagger (Bar-Haim et al., 2005), and a general purpose parser (Schmid, 2000) in an integrated framework in which morphological and syntactic components interact to share information, leading to improved performance on the joint task.</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="107" ssid="39">Firstly, Hebrew unknown tokens are doubly unknown: each unknown token may correspond to several segmentation possibilities, and each segment in such sequences may be able to admit multiple PoS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["107'"]
107'
['107']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="10">The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["14'"]
14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="12">The current work treats both segmental and super-segmental phenomena, yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg, 2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="53" ssid="11">Both (Tsarfaty, 2006; Cohen and Smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has shown a single generative model which handles both tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["53'"]
53'
['53']
parsed_discourse_facet ['method_citation']
 <S sid="48" ssid="6">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["48'"]
48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="141" ssid="19">This analyzer setting is similar to that of (Cohen and Smith, 2007), and models using it are denoted nohsp, Parser and Grammar We used BitPar (Schmid, 2004), an efficient general purpose parser,10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["141'"]
141'
['141']
parsed_discourse_facet ['method_citation']
<S sid="188" ssid="2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



["188'"]
188'
['188']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["94'"]
94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="12">Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training data similar to (Cohen and Smith, 2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
    <S sid="156" ssid="34">To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth, &#8216;tokens&#8217;) that constitute the unanalyzed surface forms (utterances).</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S sid ="54" ssid = "1">A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.</S><S sid ="89" ssid = "21">Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree  s.t.</S><S sid ="179" ssid = "17">On the surface  our model may seem as a special case of Cohen and Smith in which  = 0.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the  hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="148" ssid = "26">Removing the leaves from the resulting tree yields a parse for L under G  with the desired probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'89'", "'179'", "'183'", "'148'"]
'54'
'89'
'179'
'183'
'148'
['54', '89', '179', '183', '148']
parsed_discourse_facet ['implication_citation']
<S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="33" ssid = "12">The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="169" ssid = "7">Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.</S><S sid ="188" ssid = "2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'33'", "'156'", "'169'", "'188'"]
'80'
'33'
'156'
'169'
'188'
['80', '33', '156', '169', '188']
parsed_discourse_facet ['implication_citation']
<S sid ="130" ssid = "8">To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and malformed7 were removed.</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="48" ssid = "6">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S><S sid ="176" ssid = "14">Furthermore  the combination of pruning and vertical markovization of the grammar outperforms the Oracle results reported by Cohen and Smith.</S><S sid ="97" ssid = "29">Thus our proposed model is a proper model assigning probability mass to all (7r  L) pairs  where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'80'", "'48'", "'176'", "'97'"]
'130'
'80'
'48'
'176'
'97'
['130', '80', '48', '176', '97']
parsed_discourse_facet ['results_citation']
<S sid ="49" ssid = "7">Tsarfaty and Simaan (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S><S sid ="163" ssid = "1">The accuracy results for segmentation  tagging and parsing using our different models and our standard data split are summarized in Table 1.</S><S sid ="71" ssid = "3">This is by now a fairly standard representation for multiple morphological segmentation of Hebrew utterances (Adler  2001; Bar-Haim et al.  2005; Smith et al.  2005; Cohen and Smith  2007; Adler  2007).</S><S sid ="33" ssid = "12">The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).</S><S sid ="148" ssid = "26">Removing the leaves from the resulting tree yields a parse for L under G  with the desired probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'163'", "'71'", "'33'", "'148'"]
'49'
'163'
'71'
'33'
'148'
['49', '163', '71', '33', '148']
parsed_discourse_facet ['method_citation']
<S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="45" ssid = "3">Morphological disambiguators that consider a token in context (an utterance) and propose the most likely morphological analysis of an utterance (including segmentation) were presented by Bar-Haim et al. (2005)  Adler and Elhadad (2006)  Shacham and Wintner (2007)  and achieved good results (the best segmentation result so far is around 98%).</S><S sid ="157" ssid = "35">SEGTok(noH) is the segmentation accuracy ignoring mistakes involving the implicit definite article h.11 To evaluate our performance on the tagging task we report CPOS and FPOS corresponding to coarse- and fine-grained PoS tagging results (F1) measure.</S><S sid ="169" ssid = "7">Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'156'", "'45'", "'157'", "'169'"]
'180'
'156'
'45'
'157'
'169'
['180', '156', '45', '157', '169']
parsed_discourse_facet ['method_citation']
<S sid ="169" ssid = "7">Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="161" ssid = "39">We report the F1 value of both measures.</S><S sid ="67" ssid = "14">Hence  we take the probability of the event fmnh analyzed as REL VB to be This means that we generate f and mnh independently depending on their corresponding PoS tags  and the context (as well as the syntactic relation between the two) is modeled via the derivation resulting in a sequence REL VB spanning the form fmnh. based on linear context.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'", "'156'", "'80'", "'161'", "'67'"]
'169'
'156'
'80'
'161'
'67'
['169', '156', '80', '161', '67']
parsed_discourse_facet ['method_citation']
<S sid ="88" ssid = "20">M(wi) = Li).</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="36" ssid = "15">Furthermore  the systematic way in which particles are prefixed to one another and onto an open-class category gives rise to a distinct sort of morphological ambiguity: space-delimited tokens may be ambiguous between several different segmentation possibilities.</S><S sid ="26" ssid = "5">The relativizer f(that) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.</S><S sid ="73" ssid = "5">We use double-circles to indicate the space-delimited token boundaries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'156'", "'36'", "'26'", "'73'"]
'88'
'156'
'36'
'26'
'73'
['88', '156', '36', '26', '73']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="38" ssid = "17">The form mnh itself can be read as at least three different verbs (counted  appointed  was appointed)  a noun (a portion)  and a possessed noun (her kind).</S><S sid ="108" ssid = "40">Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p * (s  p)) > 0  while other segments have never been observed as a lexical event before.</S><S sid ="49" ssid = "7">Tsarfaty and Simaan (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S><S sid ="158" ssid = "36">Evaluating parsing results in our joint framework  as argued by Tsarfaty (2006)  is not trivial under the joint disambiguation task  as the hypothesized yield need not coincide with the correct one.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'38'", "'108'", "'49'", "'158'"]
'80'
'38'
'108'
'49'
'158'
['80', '38', '108', '49', '158']
parsed_discourse_facet ['method_citation']
<S sid ="182" ssid = "20">In addition  as the CRF and PCFG look at similar sorts of information from within two inherently different models  they are far from independent and optimizing their product is meaningless.</S><S sid ="101" ssid = "33">The possible analyses of a surface token pose constraints on the analyses of specific segments.</S><S sid ="163" ssid = "1">The accuracy results for segmentation  tagging and parsing using our different models and our standard data split are summarized in Table 1.</S><S sid ="58" ssid = "5">Such tag sequences are often treated as complex tags (e.g.</S><S sid ="44" ssid = "2">Such analyzers propose multiple segmentation possibilities and their corresponding analyses for a token in isolation but have no means to determine the most likely ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["'182'", "'101'", "'163'", "'58'", "'44'"]
'182'
'101'
'163'
'58'
'44'
['182', '101', '163', '58', '44']
parsed_discourse_facet ['method_citation']
<S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="195" ssid = "9">We further thank Khalil Simaan (ILLCUvA) for his careful advise concerning the formal details of the proposal.</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="98" ssid = "30">The Grammar Our parser looks for the most likely tree spanning a single path through the lattice of which the yield is a sequence of lexemes.</S><S sid ="191" ssid = "5">In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'195'", "'80'", "'98'", "'191'"]
'180'
'195'
'80'
'98'
'191'
['180', '195', '80', '98', '191']
parsed_discourse_facet ['implication_citation']
<S sid ="120" ssid = "52">From now on all lattice arcs are tagged segments and the assignment of probability P(p * (s  p)) to lattice arcs proceeds as usual.4 A rather pathological case is when our lexical heuristics prune away all segmentation possibilities and we remain with an empty lattice.</S><S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="158" ssid = "36">Evaluating parsing results in our joint framework  as argued by Tsarfaty (2006)  is not trivial under the joint disambiguation task  as the hypothesized yield need not coincide with the correct one.</S><S sid ="157" ssid = "35">SEGTok(noH) is the segmentation accuracy ignoring mistakes involving the implicit definite article h.11 To evaluate our performance on the tagging task we report CPOS and FPOS corresponding to coarse- and fine-grained PoS tagging results (F1) measure.</S><S sid ="49" ssid = "7">Tsarfaty and Simaan (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'", "'180'", "'158'", "'157'", "'49'"]
'120'
'180'
'158'
'157'
'49'
['120', '180', '158', '157', '49']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="26" ssid = "5">The relativizer f(that) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.</S><S sid ="54" ssid = "1">A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.</S><S sid ="130" ssid = "8">To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and malformed7 were removed.</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'26'", "'54'", "'130'", "'80'"]
'14'
'26'
'54'
'130'
'80'
['14', '26', '54', '130', '80']
parsed_discourse_facet ['method_citation']
<S sid ="154" ssid = "32">For all grammars  we use fine-grained PoS tags indicating various morphological features annotated therein.</S><S sid ="194" ssid = "8">Acknowledgments We thank Meni Adler and Michael Elhadad (BGU) for helpful comments and discussion.</S><S sid ="97" ssid = "29">Thus our proposed model is a proper model assigning probability mass to all (7r  L) pairs  where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S><S sid ="130" ssid = "8">To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and malformed7 were removed.</S><S sid ="149" ssid = "27">We use a patched version of BitPar allowing for direct input of probabilities instead of counts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'154'", "'194'", "'97'", "'130'", "'149'"]
'154'
'194'
'97'
'130'
'149'
['154', '194', '97', '130', '149']
parsed_discourse_facet ['method_citation']
<S sid ="5" ssid = "1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  tokens) that constitute the unanalyzed surface forms (utterances).</S><S sid ="76" ssid = "8">Furthermore  some of the arcs represent lexemes not present in the input tokens (e.g. h/DT  fl/POS)  however these are parts of valid analyses of the token (cf. super-segmental morphology section 2).</S><S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="108" ssid = "40">Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p * (s  p)) > 0  while other segments have never been observed as a lexical event before.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'76'", "'180'", "'108'", "'133'"]
'5'
'76'
'180'
'108'
'133'
['5', '76', '180', '108', '133']
parsed_discourse_facet ['results_citation']
<S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="48" ssid = "6">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S><S sid ="181" ssid = "19">Many morphological decisions are based on long distance dependencies  and when the global syntactic evidence disagrees with evidence based on local linear context  the two models compete with one another  despite the fact that the PCFG takes also local context into account.</S><S sid ="95" ssid = "27">A compatible view is presented by Charniak et al. (1996) who consider the kind of probabilities a generative parser should get from a PoS tagger  and concludes that these should be P(w|t) and nothing fancier.3 In our setting  therefore  the Lattice is not used to induce a probability distribution on a linear context  but rather  it is used as a common-denominator of state-indexation of all segmentations possibilities of a surface form.</S><S sid ="54" ssid = "1">A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'48'", "'181'", "'95'", "'54'"]
'80'
'48'
'181'
'95'
'54'
['80', '48', '181', '95', '54']
parsed_discourse_facet ['implication_citation']
<S sid ="164" ssid = "2">In addition we report for each model its performance on goldsegmented input (GS) to indicate the upper bound 11Overt definiteness errors may be seen as a wrong feature rather than as wrong constituent and it is by now an accepted standard to report accuracy with and without such errors. for the grammars performance on the parsing task.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S><S sid ="22" ssid = "1">Segmental morphology Hebrew consists of seven particles m(from) f(when/who/that) h(the) w(and) k(like) l(to) and b(in). which may never appear in isolation and must always attach as prefixes to the following open-class category item we refer to as stem.</S><S sid ="126" ssid = "4">When a comparison against previous results requires additional pre-processing  we state it explicitly to allow for the reader to replicate the reported results.</S><S sid ="173" ssid = "11">The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter () which is tuned separately for each of the tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'164'", "'133'", "'22'", "'126'", "'173'"]
'164'
'133'
'22'
'126'
'173'
['164', '133', '22', '126', '173']
parsed_discourse_facet ['method_citation']
['The current work treats both segmental and super-segmental phenomena, yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg, 2008).']
['Evaluating parsing results in our joint framework  as argued by Tsarfaty (2006)  is not trivial under the joint disambiguation task  as the hypothesized yield need not coincide with the correct one.', 'In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.', u'The form mnh itself can be read as at least three different verbs (\u201ccounted\u201d  \u201cappointed\u201d  \u201cwas appointed\u201d)  a noun (\u201ca portion\u201d)  and a possessed noun (\u201cher kind\u201d).', u'Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p \u2014* (s  p)) > 0  while other segments have never been observed as a lexical event before.', u'Tsarfaty and Sima\u2019an (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00341', '(95%-conf.int.', '0.00341', '-', '0.00341)']
['system', 'ROUGE-S*', 'Average_P:', '0.03810', '(95%-conf.int.', '0.03810', '-', '0.03810)']
['system', 'ROUGE-S*', 'Average_F:', '0.00626', '(95%-conf.int.', '0.00626', '-', '0.00626)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:210', 'F:8']
['To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).']
['A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.', u'Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree \u03c0 s.t.', 'Removing the leaves from the resulting tree yields a parse for L under G  with the desired probabilities.', u'On the surface  our model may seem as a special case of Cohen and Smith in which \u03b1 = 0.', u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.01508', '(95%-conf.int.', '0.01508', '-', '0.01508)']
['system', 'ROUGE-S*', 'Average_P:', '0.03175', '(95%-conf.int.', '0.03175', '-', '0.03175)']
['system', 'ROUGE-S*', 'Average_F:', '0.02045', '(95%-conf.int.', '0.02045', '-', '0.02045)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:630', 'F:20']
['Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.']
['We further thank Khalil Simaan (ILLCUvA) for his careful advise concerning the formal details of the proposal.', 'The Grammar Our parser looks for the most likely tree spanning a single path through the lattice of which the yield is a sequence of lexemes.', 'However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.', 'In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.', 'In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:78', 'F:0']
['Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.']
['The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).', 'The accuracy results for segmentation  tagging and parsing using our different models and our standard data split are summarized in Table 1.', 'Removing the leaves from the resulting tree yields a parse for L under G  with the desired probabilities.', 'This is by now a fairly standard representation for multiple morphological segmentation of Hebrew utterances (Adler  2001; Bar-Haim et al.  2005; Smith et al.  2005; Cohen and Smith  2007; Adler  2007).', u'Tsarfaty and Sima\xe2\u20ac\u2122an (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00095', '(95%-conf.int.', '0.00095', '-', '0.00095)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00187', '(95%-conf.int.', '0.00187', '-', '0.00187)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:55', 'F:3']
['Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.']
['Furthermore  the combination of pruning and vertical markovization of the grammar outperforms the Oracle results reported by Cohen and Smith.', 'In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.', u'To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and \u201cmalformed\u201d7 were removed.', 'Thus our proposed model is a proper model assigning probability mass to all (7r  L) pairs  where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.', 'Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.']
['system', 'ROUGE-S*', 'Average_R:', '0.00181', '(95%-conf.int.', '0.00181', '-', '0.00181)']
['system', 'ROUGE-S*', 'Average_P:', '0.05128', '(95%-conf.int.', '0.05128', '-', '0.05128)']
['system', 'ROUGE-S*', 'Average_F:', '0.00349', '(95%-conf.int.', '0.00349', '-', '0.00349)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:78', 'F:4']
['Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).', 'In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.', 'The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.', 'Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.', 'To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).']
['system', 'ROUGE-S*', 'Average_R:', '0.00708', '(95%-conf.int.', '0.00708', '-', '0.00708)']
['system', 'ROUGE-S*', 'Average_P:', '0.11640', '(95%-conf.int.', '0.11640', '-', '0.11640)']
['system', 'ROUGE-S*', 'Average_F:', '0.01335', '(95%-conf.int.', '0.01335', '-', '0.01335)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6216', 'P:378', 'F:44']
['The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006).']
['M(wi) = Li).', u'The relativizer f(\u201cthat\u201d) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.', 'We use double-circles to indicate the space-delimited token boundaries.', 'Furthermore  the systematic way in which particles are prefixed to one another and onto an open-class category gives rise to a distinct sort of morphological ambiguity: space-delimited tokens may be ambiguous between several different segmentation possibilities.', 'To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).']
['system', 'ROUGE-S*', 'Average_R:', '0.00895', '(95%-conf.int.', '0.00895', '-', '0.00895)']
['system', 'ROUGE-S*', 'Average_P:', '0.12281', '(95%-conf.int.', '0.12281', '-', '0.12281)']
['system', 'ROUGE-S*', 'Average_F:', '0.01669', '(95%-conf.int.', '0.01669', '-', '0.01669)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:171', 'F:21']
['Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth, &#8216;tokens&#8217;) that constitute the unanalyzed surface forms (utterances).']
[u'In addition we report for each model its performance on goldsegmented input (GS) to indicate the upper bound 11Overt definiteness errors may be seen as a wrong feature rather than as wrong constituent and it is by now an accepted standard to report accuracy with and without such errors. for the grammars\u2019 performance on the parsing task.', 'Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.', u'The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter (\u03b1) which is tuned separately for each of the tasks.', 'When a comparison against previous results requires additional pre-processing  we state it explicitly to allow for the reader to replicate the reported results.', u'Segmental morphology Hebrew consists of seven particles m(\u201cfrom\u201d) f(\u201cwhen\u201d/\u201cwho\u201d/\u201cthat\u201d) h(\u201cthe\u201d) w(\u201cand\u201d) k(\u201clike\u201d) l(\u201cto\u201d) and b(\u201cin\u201d). which may never appear in isolation and must always attach as prefixes to the following open-class category item we refer to as stem.']
['system', 'ROUGE-S*', 'Average_R:', '0.00065', '(95%-conf.int.', '0.00065', '-', '0.00065)']
['system', 'ROUGE-S*', 'Average_P:', '0.00460', '(95%-conf.int.', '0.00460', '-', '0.00460)']
['system', 'ROUGE-S*', 'Average_F:', '0.00114', '(95%-conf.int.', '0.00114', '-', '0.00114)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:435', 'F:2']
['Tsarfaty (2006) used a morphological analyzer (Segal, 2000), a PoS tagger (Bar-Haim et al., 2005), and a general purpose parser (Schmid, 2000) in an integrated framework in which morphological and syntactic components interact to share information, leading to improved performance on the joint task.']
['However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.', 'Morphological disambiguators that consider a token in context (an utterance) and propose the most likely morphological analysis of an utterance (including segmentation) were presented by Bar-Haim et al. (2005)  Adler and Elhadad (2006)  Shacham and Wintner (2007)  and achieved good results (the best segmentation result so far is around 98%).', 'Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.', 'SEGTok(noH) is the segmentation accuracy ignoring mistakes involving the implicit definite article h.11 To evaluate our performance on the tagging task we report CPOS and FPOS corresponding to coarse- and fine-grained PoS tagging results (F1) measure.', 'To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).']
['system', 'ROUGE-S*', 'Average_R:', '0.00627', '(95%-conf.int.', '0.00627', '-', '0.00627)']
['system', 'ROUGE-S*', 'Average_P:', '0.08966', '(95%-conf.int.', '0.08966', '-', '0.08966)']
['system', 'ROUGE-S*', 'Average_F:', '0.01173', '(95%-conf.int.', '0.01173', '-', '0.01173)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6216', 'P:435', 'F:39']
['Both (Tsarfaty, 2006; Cohen and Smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has shown a single generative model which handles both tasks.']
['In addition  as the CRF and PCFG look at similar sorts of information from within two inherently different models  they are far from independent and optimizing their product is meaningless.', 'Such analyzers propose multiple segmentation possibilities and their corresponding analyses for a token in isolation but have no means to determine the most likely ones.', 'The possible analyses of a surface token pose constraints on the analyses of specific segments.', u'Such tag sequences are often treated as \u201ccomplex tags\u201d (e.g.', 'The accuracy results for segmentation  tagging and parsing using our different models and our standard data split are summarized in Table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:171', 'F:0']
0.0509149994909 0.0044199999558 0.00749799992502





input/ref/Task1/P04-1036_sweta.csv
input/res/Task1/P04-1036.csv
parsing: input/ref/Task1/P04-1036_sweta.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="82" ssid="11">We also calculate the WSD accuracy that would be obtained on SemCor, when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["82'"]
82'
['82']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="20">We briefly summarise the two measures here; for a more detailed summary see (Patwardhan et al., 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="172" ssid="20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="1">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="165" ssid="13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus, whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S>
original cit marker offset is 0
new cit marker offset is 0



["165'"]
165'
['165']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["171'"]
171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["89'"]
89'
['89']
parsed_discourse_facet ['method_citation']
<S sid="172" ssid="20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="12">Additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["189'"]
189'
['189']
parsed_discourse_facet ['method_citation']
<S sid="87" ssid="16">Again, the automatic ranking outperforms this by a large margin.</S>
original cit marker offset is 0
new cit marker offset is 0



["87'"]
87'
['87']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["89'"]
89'
['89']
parsed_discourse_facet ['method_citation']
 <S sid="137" ssid="14">Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["137'"]
137'
['137']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="19">We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline, but because of space limitations give results for the two which perform the best.</S>
original cit marker offset is 0
new cit marker offset is 0



["63'"]
63'
['63']
parsed_discourse_facet ['method_citation']
<S sid="159" ssid="7">Magnini and Cavagli`a (2000) have identified WordNet word senses with particular domains, and this has proven useful for high precision WSD (Magnini et al., 2001); indeed in section 5 we used these domain labels for evaluation.</S>
original cit marker offset is 0
new cit marker offset is 0



["159'"]
159'
['159']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="98" ssid = "27">This seems intuitive given our expected relative usage of these senses in modern British English.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="27" ssid = "20">We use WordNet as our sense inventory for this work.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'98'", "'124'", "'5'", "'27'", "'65'"]
'98'
'124'
'5'
'27'
'65'
['98', '124', '5', '27', '65']
parsed_discourse_facet ['implication_citation']
<S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="109" ssid = "7">We generated a thesaurus entry for all polysemous nouns in WordNet as described in section 2.1 above.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S><S sid ="132" ssid = "9">The FINANCE corpus consists of 117734 documents (about 32.5 million words).</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'109'", "'95'", "'55'", "'132'"]
'124'
'109'
'95'
'55'
'132'
['124', '109', '95', '55', '132']
parsed_discourse_facet ['implication_citation']
<S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="32" ssid = "25">We describe some related work in section 6 and conclude in section 7. are therefore investigating a method of automatically ranking WordNet senses from raw text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'137'", "'5'", "'110'", "'32'"]
'44'
'137'
'5'
'110'
'32'
['44', '137', '5', '110', '32']
parsed_discourse_facet ['results_citation']
<S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="96" ssid = "25">Another example where the ranking is intuitive  is soil.</S><S sid ="33" ssid = "26">Many researchers are developing thesauruses from automatically parsed data.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'95'", "'96'", "'33'", "'44'"]
'124'
'95'
'96'
'33'
'44'
['124', '95', '96', '33', '44']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "5">To disambiguate senses a system should take context into account.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="67" ssid = "23">Each 2We use this version of WordNet since it allows us to map information to WordNets of other languages more accurately.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'124'", "'153'", "'147'", "'67'"]
'107'
'124'
'153'
'147'
'67'
['107', '124', '153', '147', '67']
parsed_discourse_facet ['method_citation']
<S sid ="94" ssid = "23">This seems quite reasonable given the nearest neighbours: tube  cable  wire  tank  hole  cylinder  fitting  tap  cistern  plate....</S><S sid ="162" ssid = "10">It only requires raw text from the given domain and because of this it can easily be applied to a new domain  or sense inventory  given sufficient text.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="99" ssid = "28">Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'", "'162'", "'60'", "'95'", "'99'"]
'94'
'162'
'60'
'95'
'99'
['94', '162', '60', '95', '99']
parsed_discourse_facet ['method_citation']
<S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="101" ssid = "30">Thus  if we used the sense ranking as a heuristic for an all nouns task we would expect to get precision in the region of 60%.</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'165'", "'101'", "'169'", "'108'", "'44'"]
'165'
'101'
'169'
'108'
'44'
['165', '101', '169', '108', '44']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="127" ssid = "4">We chose the domains of SPORTS and FINANCE since there is sufficient material for these domains in this publically available corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'60'", "'169'", "'91'", "'127'"]
'136'
'60'
'169'
'91'
'127'
['136', '60', '169', '91', '127']
parsed_discourse_facet ['method_citation']
<S sid ="193" ssid = "2">This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.</S><S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="151" ssid = "28">This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="167" ssid = "15">In this work the lists of neighbours are themselves clustered to bring out the various senses of the word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'193'", "'136'", "'151'", "'123'", "'167'"]
'193'
'136'
'151'
'123'
'167'
['193', '136', '151', '123', '167']
parsed_discourse_facet ['method_citation']
<S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="63" ssid = "19">We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline  but because of space limitations give results for the two which perform the best.</S>
original cit marker offset is 0
new cit marker offset is 0



["'173'", "'5'", "'108'", "'95'", "'63'"]
'173'
'5'
'108'
'95'
'63'
['173', '5', '108', '95', '63']
parsed_discourse_facet ['implication_citation']
<S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="76" ssid = "5">The jcn measure uses corpus data for the calculation of IC.</S><S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'184'", "'110'", "'60'", "'76'", "'55'"]
'184'
'110'
'60'
'76'
'55'
['184', '110', '60', '76', '55']
parsed_discourse_facet ['method_citation']
<S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="97" ssid = "26">The first ranked sense according to SemCor is the filth  stain: state of being unclean sense whereas the automatic ranking lists dirt  ground  earth as the first sense  which is the second ranked sense according to SemCor.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'123'", "'169'", "'147'", "'97'", "'8'"]
'123'
'169'
'147'
'97'
'8'
['123', '169', '147', '97', '8']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="28" ssid = "21">The paper is structured as follows.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of nearest neighbours.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'28'", "'124'", "'60'", "'34'"]
'61'
'28'
'124'
'60'
'34'
['61', '28', '124', '60', '34']
parsed_discourse_facet ['method_citation']
<S sid ="163" ssid = "11">Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="9" ssid = "2">This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).</S><S sid ="0" ssid = "0">Finding Predominant Word Senses in Untagged Text</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'124'", "'9'", "'0'", "'142'"]
'163'
'124'
'9'
'0'
'142'
['163', '124', '9', '0', '142']
parsed_discourse_facet ['results_citation']
<S sid ="138" ssid = "15">The SFC contains an economy label and a sports label.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of nearest neighbours.</S><S sid ="22" ssid = "15">More importantly  when working within a specific domain one would wish to tune the first sense heuristic to the domain at hand.</S><S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="92" ssid = "21">For example  in WordNet the first listed sense ofpipe is tobacco pipe  and this is ranked joint first according to the Brown files in SemCor with the second sense tube made of metal or plastic used to carry water  oil or gas etc....</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'", "'34'", "'22'", "'184'", "'92'"]
'138'
'34'
'22'
'184'
'92'
['138', '34', '22', '184', '92']
parsed_discourse_facet ['implication_citation']
<S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="185" ssid = "8">In the future  we will perform a large scale evaluation on domain specific corpora.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'173'", "'95'", "'185'", "'65'"]
'108'
'173'
'95'
'185'
'65'
['108', '173', '95', '185', '65']
parsed_discourse_facet ['method_citation']
<S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="151" ssid = "28">This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'173'", "'110'", "'151'", "'95'", "'5'"]
'173'
'110'
'151'
'95'
'5'
['173', '110', '151', '95', '5']
parsed_discourse_facet ['results_citation']
['We briefly summarise the two measures here; for a more detailed summary see (Patwardhan et al., 2003).']
['The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.', 'We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within', 'Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.', 'We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.', 'We describe some related work in section 6 and conclude in section 7. are therefore investigating a method of automatically ranking WordNet senses from raw text.']
['system', 'ROUGE-S*', 'Average_R:', '0.00136', '(95%-conf.int.', '0.00136', '-', '0.00136)']
['system', 'ROUGE-S*', 'Average_P:', '0.10714', '(95%-conf.int.', '0.10714', '-', '0.10714)']
['system', 'ROUGE-S*', 'Average_F:', '0.00268', '(95%-conf.int.', '0.00268', '-', '0.00268)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:28', 'F:3']
['We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline, but because of space limitations give results for the two which perform the best.']
['Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.', 'However  it is important to know the performance of this heuristic for any systems that use it.', 'It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.', 'The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.', 'In the future  we will perform a large scale evaluation on domain specific corpora.']
['system', 'ROUGE-S*', 'Average_R:', '0.00043', '(95%-conf.int.', '0.00043', '-', '0.00043)']
['system', 'ROUGE-S*', 'Average_P:', '0.01515', '(95%-conf.int.', '0.01515', '-', '0.01515)']
['system', 'ROUGE-S*', 'Average_F:', '0.00083', '(95%-conf.int.', '0.00083', '-', '0.00083)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:66', 'F:1']
['In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.']
['If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .', 'This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.', 'We chose the domains of SPORTS and FINANCE since there is sufficient material for these domains in this publically available corpus.', 'The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.', 'This method obtains precision of 61% and recall 51%.']
['system', 'ROUGE-S*', 'Average_R:', '0.00226', '(95%-conf.int.', '0.00226', '-', '0.00226)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00434', '(95%-conf.int.', '0.00434', '-', '0.00434)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:55', 'F:3']
['Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.']
['This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).', 'Finding Predominant Word Senses in Untagged Text', 'A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.', 'The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.', 'Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD.']
['system', 'ROUGE-S*', 'Average_R:', '0.00614', '(95%-conf.int.', '0.00614', '-', '0.00614)']
['system', 'ROUGE-S*', 'Average_P:', '0.07018', '(95%-conf.int.', '0.07018', '-', '0.07018)']
['system', 'ROUGE-S*', 'Average_F:', '0.01130', '(95%-conf.int.', '0.01130', '-', '0.01130)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:171', 'F:12']
['Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.']
['For example  in WordNet the first listed sense ofpipe is tobacco pipe  and this is ranked joint first according to the Brown files in SemCor with the second sense tube made of metal or plastic used to carry water  oil or gas etc....', 'The SFC contains an economy label and a sports label.', 'We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.', 'More importantly  when working within a specific domain one would wish to tune the first sense heuristic to the domain at hand.', u'In these each target word is entered with an ordered list of \u201cnearest neighbours\u201d.']
['system', 'ROUGE-S*', 'Average_R:', '0.00280', '(95%-conf.int.', '0.00280', '-', '0.00280)']
['system', 'ROUGE-S*', 'Average_P:', '0.02941', '(95%-conf.int.', '0.02941', '-', '0.02941)']
['system', 'ROUGE-S*', 'Average_F:', '0.00511', '(95%-conf.int.', '0.00511', '-', '0.00511)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:136', 'F:4']
['word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.']
['If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .', 'Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.', 'Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.', 'This seems quite reasonable given the nearest neighbours: tube  cable  wire  tank  hole  cylinder  fitting  tap  cistern  plate....', 'It only requires raw text from the given domain and because of this it can easily be applied to a new domain  or sense inventory  given sufficient text.']
['system', 'ROUGE-S*', 'Average_R:', '0.00043', '(95%-conf.int.', '0.00043', '-', '0.00043)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00083', '(95%-conf.int.', '0.00083', '-', '0.00083)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:78', 'F:1']
['Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.']
['This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.', 'In this work the lists of neighbours are themselves clustered to bring out the various senses of the word.', 'This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.', 'The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.', 'This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.']
['system', 'ROUGE-S*', 'Average_R:', '0.00132', '(95%-conf.int.', '0.00132', '-', '0.00132)']
['system', 'ROUGE-S*', 'Average_P:', '0.01754', '(95%-conf.int.', '0.01754', '-', '0.01754)']
['system', 'ROUGE-S*', 'Average_F:', '0.00245', '(95%-conf.int.', '0.00245', '-', '0.00245)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:171', 'F:3']
['Magnini and Cavagli`a (2000) have identified WordNet word senses with particular domains, and this has proven useful for high precision WSD (Magnini et al., 2001); indeed in section 5 we used these domain labels for evaluation.']
['This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.', 'Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.', 'The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.', 'It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.', 'We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.']
['system', 'ROUGE-S*', 'Average_R:', '0.00577', '(95%-conf.int.', '0.00577', '-', '0.00577)']
['system', 'ROUGE-S*', 'Average_P:', '0.08421', '(95%-conf.int.', '0.08421', '-', '0.08421)']
['system', 'ROUGE-S*', 'Average_F:', '0.01079', '(95%-conf.int.', '0.01079', '-', '0.01079)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:190', 'F:16']
['word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.']
['A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.', 'We use the WordNet Similarity Package 0.05 and WordNet version 1.6.', 'The paper is structured as follows.', 'If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .', u'In these each target word is entered with an ordered list of \u201cnearest neighbours\u201d.']
['system', 'ROUGE-S*', 'Average_R:', '0.00122', '(95%-conf.int.', '0.00122', '-', '0.00122)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00223', '(95%-conf.int.', '0.00223', '-', '0.00223)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:78', 'F:1']
['Again, the automatic ranking outperforms this by a large margin.']
['The word share is among the words whose predominant sense remained the same for all three corpora.', 'This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.', 'This method obtains precision of 61% and recall 51%.', 'The first ranked sense according to SemCor is the filth  stain: state of being unclean sense whereas the automatic ranking lists dirt  ground  earth as the first sense  which is the second ranked sense according to SemCor.', 'The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['system', 'ROUGE-S*', 'Average_R:', '0.00202', '(95%-conf.int.', '0.00202', '-', '0.00202)']
['system', 'ROUGE-S*', 'Average_P:', '0.30000', '(95%-conf.int.', '0.30000', '-', '0.30000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00401', '(95%-conf.int.', '0.00401', '-', '0.00401)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:10', 'F:3']
0.0703819992962 0.00237499997625 0.00445699995543





input/ref/Task1/P87-1015_sweta.csv
input/res/Task1/P87-1015.csv
parsing: input/ref/Task1/P87-1015_sweta.csv
<S sid="205" ssid="11">Independence of paths at this level reflects context freeness of rewriting and suggests why they can be recognized efficiently.</S>
original cit marker offset is 0
new cit marker offset is 0



["205'"]
205'
['205']
parsed_discourse_facet ['method_citation']
<S sid="229" ssid="35">In addition, the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars, and the hierarchy defined by Weir).</S>
original cit marker offset is 0
new cit marker offset is 0



["229'"]
229'
['229']
parsed_discourse_facet ['method_citation']
<S sid="146" ssid="31">Since every CFL is known to be semilinear (Parikh, 1966), in order to show semilinearity of some language, we need only show the existence of a letter equivalent CFL Our definition of LCFRS's insists that the composition operations are linear and nonerasing.</S>
original cit marker offset is 0
new cit marker offset is 0



["146'"]
146'
['146']
parsed_discourse_facet ['method_citation']
<S sid="201" ssid="7">It is interesting to note, however, that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG, FUG and IG's.</S>
original cit marker offset is 0
new cit marker offset is 0



["201'"]
201'
['201']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
 <S sid="222" ssid="28">However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



["222'"]
222'
['222']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="7">Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.</S>
    <S sid="23" ssid="8">He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["22'", "'23'"]
22'
'23'
['22', '23']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="221" ssid="27">Members of LCFRS whose operations have this property can be translated into the ILFP notation (Rounds, 1985).</S>
original cit marker offset is 0
new cit marker offset is 0



["221'"]
221'
['221']
parsed_discourse_facet ['method_citation']
<S sid="54" ssid="39">An IG can be viewed as a CFG in which each nonterminal is associated with a stack.</S>
original cit marker offset is 0
new cit marker offset is 0



["54'"]
54'
['54']
parsed_discourse_facet ['method_citation']
<S sid="128" ssid="13">As in the case of the derivation trees of CFG's, nodes are labeled by a member of some finite set of symbols (perhaps only implicit in the grammar as in TAG's) used to denote derived structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["128'"]
128'
['128']
parsed_discourse_facet ['method_citation']
<S sid="217" ssid="23">In considering the recognition of these languages, we were forced to be more specific regarding the relationship between the structures derived by these formalisms and the substrings they span.</S>
original cit marker offset is 0
new cit marker offset is 0



[";217'"]
;217'
[';217']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="1">From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="1">From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="214" ssid="20">LCFRS's share several properties possessed by the class of mildly context-sensitive formalisms discussed by Joshi (1983/85).</S>
original cit marker offset is 0
new cit marker offset is 0



["214'"]
214'
['214']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="20">TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish, for cross serial dependencies in Dutch subordinate clauses, and for the nestings of paired English complementizers.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P87-1015.csv
<S sid ="191" ssid = "76">In addition to the tapes required to store the indices  M requires one work tape for splitting the substrings.</S><S sid ="206" ssid = "12">As suggested in Section 4.3.2  a derivation with independent paths can be divided into subcomputations with limited sharing of information.</S><S sid ="186" ssid = "71">To do this  the x's and y's are stored in the next 2ni + 2n2 tapes  and M goes to a universal state.</S><S sid ="104" ssid = "10">Pumping t2 will change only one branch and leave the other branch unaffected.</S><S sid ="192" ssid = "77">Thus  the ATM has no more than 6km&quot; + 1 work tapes  where km&quot; is the maximum number of substrings spanned by a derived structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'191'", "'206'", "'186'", "'104'", "'192'"]
'191'
'206'
'186'
'104'
'192'
['191', '206', '186', '104', '192']
parsed_discourse_facet ['implication_citation']
<S sid ="84" ssid = "69">((fii  Q2  Pa)    (01  i32  03)   e) (02 e) Oa  en) The path complexity of the tee set generated by a MCTAG is not necessarily context-free.</S><S sid ="54" ssid = "39">An IG can be viewed as a CFG in which each nonterminal is associated with a stack.</S><S sid ="101" ssid = "7">The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently.</S><S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="71" ssid = "56">0n0'i'i0'2&quot;bin242bn I n = 711 + n2 } On the other hand  no linguistic use is made of this general form of composition and Steedman (personal communication) and Steedman (1986) argues that a more limited definition of composition is more natural.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'54'", "'101'", "'59'", "'71'"]
'84'
'54'
'101'
'59'
'71'
['84', '54', '101', '59', '71']
parsed_discourse_facet ['implication_citation']
<S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="44" ssid = "29">The edge from the root to the subtree for the derivation of 7i is labeled by the address ni.</S><S sid ="136" ssid = "21">These two restrictions impose the constraint that the result of composing any two structures should be a structure whose &quot;size&quot; is the sum of its constituents plus some constant For example  the operation 4  discussed in the case of CFG's (in Section 4.1) adds the constant equal to the sum of the length of the strings VI  un+r Since we are considering formalisms with arbitrary structures it is difficult to precisely specify all of the restrictions on the composition operations that we believe would appropriately generalize the concatenation operation for the particular structures used by the formalism.</S><S sid ="28" ssid = "13">A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'44'", "'136'", "'28'", "'153'"]
'151'
'44'
'136'
'28'
'153'
['151', '44', '136', '28', '153']
parsed_discourse_facet ['results_citation']
<S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="134" ssid = "19">These systems are similar to those described by Pollard (1984) as Generalized Context-Free Grammars (GCFG's).</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="112" ssid = "18">.t The path set of tree sets at level k +1 have the complexity of the string language of level k. The independence of paths in a tree set appears to be an important property.</S><S sid ="20" ssid = "5">As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'134'", "'153'", "'112'", "'20'"]
'59'
'134'
'153'
'112'
'20'
['59', '134', '153', '112', '20']
parsed_discourse_facet ['method_citation']
<S sid ="106" ssid = "12">We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths.</S><S sid ="54" ssid = "39">An IG can be viewed as a CFG in which each nonterminal is associated with a stack.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="101" ssid = "7">The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently.</S><S sid ="182" ssid = "67">Since each zi is a contiguous substring of the input (say ai )  and no two substrings overlap  we can represent zi by the pair of integers (i2  i2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'54'", "'143'", "'101'", "'182'"]
'106'
'54'
'143'
'101'
'182'
['106', '54', '143', '101', '182']
parsed_discourse_facet ['method_citation']
<S sid ="55" ssid = "40">Each production can push or pop symbols on the stack as can be seen in the following productions that generate tree of the form shown in Figure 4b.</S><S sid ="117" ssid = "2">Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.</S><S sid ="200" ssid = "6">The importance of this property becomes clear in contrasting theories underlying GPSG (Gazdar  Klein  Pulluna  and Sag  1985)  and GB (as described by Berwick  1984) with those underlying LFG and FUG.</S><S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="106" ssid = "12">We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'117'", "'200'", "'59'", "'106'"]
'55'
'117'
'200'
'59'
'106'
['55', '117', '200', '59', '106']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "17">When 7' is adjoined at ?I in the tree 7 we obtain a tree v&quot;.</S><S sid ="121" ssid = "6">First  any grammar must involve a finite number of elementary structures  composed using a finite number of composition operations.</S><S sid ="19" ssid = "4">It can be easily shown from Thatcher's result that the path set of every local set is a regular set.</S><S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="138" ssid = "23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'121'", "'19'", "'151'", "'138'"]
'32'
'121'
'19'
'151'
'138'
['32', '121', '19', '151', '138']
parsed_discourse_facet ['method_citation']
<S sid ="188" ssid = "73">Thus  for example  one successor process will be have M to be in the existential state qa with the indices encoding xi     xn  in the first 2n i tapes.</S><S sid ="118" ssid = "3">In the remainder of the paper  we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S><S sid ="89" ssid = "74">Hence  trees shown in Figure 8 can not be generated by any MCTAG (but can be generated by an IG) because the number of pairs of dependent paths grows with n. Since the derivation tees of TAG's  MCTAG's  and HG's are local sets  the choice of the structure used at each point in a derivation in these systems does not depend on the context at that point within the derivation.</S><S sid ="96" ssid = "2">A tree set may be said to have dependencies between paths if some &quot;appropriate&quot; subset can be shown to have dependent paths as defined above.</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'188'", "'118'", "'89'", "'96'", "'227'"]
'188'
'118'
'89'
'96'
'227'
['188', '118', '89', '96', '227']
parsed_discourse_facet ['method_citation']
<S sid ="84" ssid = "69">((fii  Q2  Pa)    (01  i32  03)   e) (02 e) Oa  en) The path complexity of the tee set generated by a MCTAG is not necessarily context-free.</S><S sid ="98" ssid = "4">Thatcher (1973) describes a tee pumping lemma for recognizable sets related to the string pumping lemma for regular sets.</S><S sid ="12" ssid = "10">We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.</S><S sid ="179" ssid = "64">It can be seen that M performs a top-down recognition of the input al ... nin logspace.</S><S sid ="185" ssid = "70">Each spawned process must check if xi     xn  and   yn  can be derived from B and C  respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'98'", "'12'", "'179'", "'185'"]
'84'
'98'
'12'
'179'
'185'
['84', '98', '12', '179', '185']
parsed_discourse_facet ['method_citation']
<S sid ="207" ssid = "13">We outlined the definition of a family of constrained grammatical formalisms  called Linear Context-Free Rewriting Systems.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="26" ssid = "11">The productions of HG's are very similar to those of CFG's except that the operation used must be made explicit.</S><S sid ="178" ssid = "63">We define an ATM  M  recognizing a language generated by a grammar  G  having the properties discussed in Section 43.</S><S sid ="155" ssid = "40">CFG's  TAG's  MCTAG's and HG's are all members of this class since they satisfy these restrictions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'153'", "'26'", "'178'", "'155'"]
'207'
'153'
'26'
'178'
'155'
['207', '153', '26', '178', '155']
parsed_discourse_facet ['implication_citation']
<S sid ="38" ssid = "23">Thus  the derivation trees for TAG's have the same structure as local sets.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="165" ssid = "50">This class of formalisms have the properties that their derivation trees are local sets  and manipulate objects  using a finite number of composition operations that use a finite number of symbols.</S><S sid ="62" ssid = "47">TAG's can be shown to be equivalent to this restricted system.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'153'", "'165'", "'62'", "'143'"]
'38'
'153'
'165'
'62'
'143'
['38', '153', '165', '62', '143']
parsed_discourse_facet ['method_citation']
<S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="132" ssid = "17">In TAG's the elementary tree and addresses where adjunction takes place are used to instantiate the operation.</S><S sid ="101" ssid = "7">The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently.</S><S sid ="96" ssid = "2">A tree set may be said to have dependencies between paths if some &quot;appropriate&quot; subset can be shown to have dependent paths as defined above.</S><S sid ="94" ssid = "79">The semilinearity of Tree Adjoining Languages (TAL's)  MCTAL's  and Head Languages (HL's) can be proved using this property  with suitable restrictions on the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'132'", "'101'", "'96'", "'94'"]
'143'
'132'
'101'
'96'
'94'
['143', '132', '101', '96', '94']
parsed_discourse_facet ['method_citation']
<S sid ="89" ssid = "74">Hence  trees shown in Figure 8 can not be generated by any MCTAG (but can be generated by an IG) because the number of pairs of dependent paths grows with n. Since the derivation tees of TAG's  MCTAG's  and HG's are local sets  the choice of the structure used at each point in a derivation in these systems does not depend on the context at that point within the derivation.</S><S sid ="164" ssid = "49">Although embedding this version of LCFRS's in the framework of ILFP developed by Rounds (1985) is straightforward  our motivation was to capture properties shared by a family of grammatical systems and generalize them defining a class of related formalisms.</S><S sid ="17" ssid = "2">We define the path set of a tree 1 as the set of strings that label a path from the root to frontier of 7.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="26" ssid = "11">The productions of HG's are very similar to those of CFG's except that the operation used must be made explicit.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'164'", "'17'", "'143'", "'26'"]
'89'
'164'
'17'
'143'
'26'
['89', '164', '17', '143', '26']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">For example  Gazdar (1985) discusses the applicability of Indexed Grammars (IG's) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar (LFG) and Government and Bindings grammars (GB).</S><S sid ="91" ssid = "76">We characterize a class of formalisms that have this property in Section 4.</S><S sid ="68" ssid = "53">This kind of dependency arises from the use of the composition operation to compose two arbitrarily large categories.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="189" ssid = "74">For rules p : A fpo such that fp is constant function  giving an elementary structure  fp is defined such that fp() = (Si ... xi() where each z is a constant string.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'91'", "'68'", "'143'", "'189'"]
'6'
'91'
'68'
'143'
'189'
['6', '91', '68', '143', '189']
parsed_discourse_facet ['results_citation']
<S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="90" ssid = "75">Thus  as in CFG's  at any point in the derivation  the set of structures that can be applied is determined only by a finite set of rules encapsulated by the grammar.</S><S sid ="185" ssid = "70">Each spawned process must check if xi     xn  and   yn  can be derived from B and C  respectively.</S><S sid ="141" ssid = "26">Thus  the length of any string in L is a linear combination of the length of strings in some fixed finite subset of L  and thus L is said to have the constant growth property.</S><S sid ="158" ssid = "43">We can represent any derived tree of a TAG by the two substrings that appear in its frontier  and then define how the adjunction operation concatenates the substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'90'", "'185'", "'141'", "'158'"]
'151'
'90'
'185'
'141'
'158'
['151', '90', '185', '141', '158']
parsed_discourse_facet ['implication_citation']
<S sid ="156" ssid = "41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S><S sid ="106" ssid = "12">We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths.</S><S sid ="32" ssid = "17">When 7' is adjoined at ?I in the tree 7 we obtain a tree v&quot;.</S><S sid ="12" ssid = "10">We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.</S><S sid ="55" ssid = "40">Each production can push or pop symbols on the stack as can be seen in the following productions that generate tree of the form shown in Figure 4b.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'", "'106'", "'32'", "'12'", "'55'"]
'156'
'106'
'32'
'12'
'55'
['156', '106', '32', '12', '55']
parsed_discourse_facet ['method_citation']
['Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.', 'He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.']
["It can be easily shown from Thatcher's result that the path set of every local set is a regular set.", "When 7' is adjoined at ?I in the tree 7 we obtain a tree v&quot;.", "We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).", "We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.", 'First  any grammar must involve a finite number of elementary structures  composed using a finite number of composition operations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00170', '(95%-conf.int.', '0.00170', '-', '0.00170)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00289', '(95%-conf.int.', '0.00289', '-', '0.00289)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:210', 'F:2']
['Independence of paths at this level reflects context freeness of rewriting and suggests why they can be recognized efficiently.']
["To do this  the x's and y's are stored in the next 2ni + 2n2 tapes  and M goes to a universal state.", 'Thus  the ATM has no more than 6km&quot; + 1 work tapes  where km&quot; is the maximum number of substrings spanned by a derived structure.', 'In addition to the tapes required to store the indices  M requires one work tape for splitting the substrings.', 'Pumping t2 will change only one branch and leave the other branch unaffected.', 'As suggested in Section 4.3.2  a derivation with independent paths can be divided into subcomputations with limited sharing of information.']
['system', 'ROUGE-S*', 'Average_R:', '0.00082', '(95%-conf.int.', '0.00082', '-', '0.00082)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00157', '(95%-conf.int.', '0.00157', '-', '0.00157)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:45', 'F:1']
["In addition, the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars, and the hierarchy defined by Weir)."]
['An IG can be viewed as a CFG in which each nonterminal is associated with a stack.', "The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently.", 'Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.', "0n0'i'i0'2&quot;bin242bn I n = 711 + n2 } On the other hand  no linguistic use is made of this general form of composition and Steedman (personal communication) and Steedman (1986) argues that a more limited definition of composition is more natural.", u'((fii  Q2  Pa)   \u2014\u25a0 (01  i32  03)   e) (02 e) Oa  en) The path complexity of the tee set generated by a MCTAG is not necessarily context-free.']
['system', 'ROUGE-S*', 'Average_R:', '0.00577', '(95%-conf.int.', '0.00577', '-', '0.00577)']
['system', 'ROUGE-S*', 'Average_P:', '0.03678', '(95%-conf.int.', '0.03678', '-', '0.03678)']
['system', 'ROUGE-S*', 'Average_F:', '0.00997', '(95%-conf.int.', '0.00997', '-', '0.00997)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:435', 'F:16']
["TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish, for cross serial dependencies in Dutch subordinate clauses, and for the nestings of paired English complementizers."]
['Each production can push or pop symbols on the stack as can be seen in the following productions that generate tree of the form shown in Figure 4b.', "When 7' is adjoined at ?I in the tree 7 we obtain a tree v&quot;.", 'We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.', "We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths.", "Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings."]
['system', 'ROUGE-S*', 'Average_R:', '0.00154', '(95%-conf.int.', '0.00154', '-', '0.00154)']
['system', 'ROUGE-S*', 'Average_P:', '0.01429', '(95%-conf.int.', '0.01429', '-', '0.01429)']
['system', 'ROUGE-S*', 'Average_F:', '0.00277', '(95%-conf.int.', '0.00277', '-', '0.00277)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:210', 'F:3']
["LCFRS's share several properties possessed by the class of mildly context-sensitive formalisms discussed by Joshi (1983/85)."]
["We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).", "Thus  as in CFG's  at any point in the derivation  the set of structures that can be applied is determined only by a finite set of rules encapsulated by the grammar.", 'We can represent any derived tree of a TAG by the two substrings that appear in its frontier  and then define how the adjunction operation concatenates the substrings.', 'Thus  the length of any string in L is a linear combination of the length of strings in some fixed finite subset of L  and thus L is said to have the constant growth property.', 'Each spawned process must check if xi     xn  and   yn  can be derived from B and C  respectively.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:78', 'F:0']
["From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'."]
['We characterize a class of formalisms that have this property in Section 4.', 'This kind of dependency arises from the use of the composition operation to compose two arbitrarily large categories.', 'For rules p : A fpo such that fp is constant function  giving an elementary structure  fp is defined such that fp() = (Si ... xi() where each z is a constant string.', 'The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.', "For example  Gazdar (1985) discusses the applicability of Indexed Grammars (IG's) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar (LFG) and Government and Bindings grammars (GB)."]
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:120', 'F:0']
['However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.']
['Each production can push or pop symbols on the stack as can be seen in the following productions that generate tree of the form shown in Figure 4b.', 'The importance of this property becomes clear in contrasting theories underlying GPSG (Gazdar  Klein  Pulluna  and Sag  1985)  and GB (as described by Berwick  1984) with those underlying LFG and FUG.', 'Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.', 'Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.', "We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths."]
['system', 'ROUGE-S*', 'Average_R:', '0.00156', '(95%-conf.int.', '0.00156', '-', '0.00156)']
['system', 'ROUGE-S*', 'Average_P:', '0.02105', '(95%-conf.int.', '0.02105', '-', '0.02105)']
['system', 'ROUGE-S*', 'Average_F:', '0.00291', '(95%-conf.int.', '0.00291', '-', '0.00291)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:190', 'F:4']
["As in the case of the derivation trees of CFG's, nodes are labeled by a member of some finite set of symbols (perhaps only implicit in the grammar as in TAG's) used to denote derived structures."]
['This class of formalisms have the properties that their derivation trees are local sets  and manipulate objects  using a finite number of composition operations that use a finite number of symbols.', 'In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.', "Thus  the derivation trees for TAG's have the same structure as local sets.", "TAG's can be shown to be equivalent to this restricted system.", 'The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.']
['system', 'ROUGE-S*', 'Average_R:', '0.02036', '(95%-conf.int.', '0.02036', '-', '0.02036)']
['system', 'ROUGE-S*', 'Average_P:', '0.22500', '(95%-conf.int.', '0.22500', '-', '0.22500)']
['system', 'ROUGE-S*', 'Average_F:', '0.03734', '(95%-conf.int.', '0.03734', '-', '0.03734)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:120', 'F:27']
["Since every CFL is known to be semilinear (Parikh, 1966), in order to show semilinearity of some language, we need only show the existence of a letter equivalent CFL Our definition of LCFRS's insists that the composition operations are linear and nonerasing."]
["We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).", 'The edge from the root to the subtree for the derivation of 7i is labeled by the address ni.', 'A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.', u"These two restrictions impose the constraint that the result of composing any two structures should be a structure whose &quot;size&quot; is the sum of its constituents plus some constant For example  the operation 4  discussed in the case of CFG's (in Section 4.1) adds the constant equal to the sum of the length of the strings VI  un+r\xe2\u20ac\xa2 Since we are considering formalisms with arbitrary structures it is difficult to precisely specify all of the restrictions on the composition operations that we believe would appropriately generalize the concatenation operation for the particular structures used by the formalism.", 'In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.']
['system', 'ROUGE-S*', 'Average_R:', '0.00109', '(95%-conf.int.', '0.00109', '-', '0.00109)']
['system', 'ROUGE-S*', 'Average_P:', '0.02105', '(95%-conf.int.', '0.02105', '-', '0.02105)']
['system', 'ROUGE-S*', 'Average_F:', '0.00208', '(95%-conf.int.', '0.00208', '-', '0.00208)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3655', 'P:190', 'F:4']
["It is interesting to note, however, that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG, FUG and IG's."]
["As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.", 'In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.', '.t The path set of tree sets at level k +1 have the complexity of the string language of level k. The independence of paths in a tree set appears to be an important property.', 'Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.', "These systems are similar to those described by Pollard (1984) as Generalized Context-Free Grammars (GCFG's)."]
['system', 'ROUGE-S*', 'Average_R:', '0.00384', '(95%-conf.int.', '0.00384', '-', '0.00384)']
['system', 'ROUGE-S*', 'Average_P:', '0.04737', '(95%-conf.int.', '0.04737', '-', '0.04737)']
['system', 'ROUGE-S*', 'Average_F:', '0.00710', '(95%-conf.int.', '0.00710', '-', '0.00710)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:190', 'F:9']
['Members of LCFRS whose operations have this property can be translated into the ILFP notation (Rounds, 1985).']
['Thatcher (1973) describes a tee pumping lemma for recognizable sets related to the string pumping lemma for regular sets.', 'We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.', 'It can be seen that M performs a top-down recognition of the input al ... nin logspace.', u'((fii  Q2  Pa)   \u2014\u25a0 (01  i32  03)   e) (02 e) Oa  en) The path complexity of the tee set generated by a MCTAG is not necessarily context-free.', 'Each spawned process must check if xi     xn  and   yn  can be derived from B and C  respectively.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:36', 'F:0']
0.036116363308 0.00333454542423 0.00605727267221





input/ref/Task1/D10-1044_sweta.csv
input/res/Task1/D10-1044.csv
parsing: input/ref/Task1/D10-1044_sweta.csv
<S sid="4" ssid="1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S>
original cit marker offset is 0
new cit marker offset is 0



["4'"]
4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="1">We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="4">For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components (word-alignment model, language model, translation model, etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["7'"]
7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="14">Linear weights are difficult to incorporate into the standard MERT procedure because they are &#8220;hidden&#8221; within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn, 2007), we circumvent this problem by choosing weights to optimize corpus loglikelihood, which is roughly speaking the training criterion used by the LM and TM themselves.</S>
original cit marker offset is 0
new cit marker offset is 0



["50'"]
50'
['50']
parsed_discourse_facet ['method_citation']
<S sid="152" ssid="9">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["152'"]
152'
['152']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
 <S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="97" ssid="1">We carried out translation experiments in two different settings.</S>
original cit marker offset is 0
new cit marker offset is 0



["97'"]
97'
['97']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="12">Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="10">Finally, we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="141" ssid="10">Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L&#168;u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L&#168;u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["141'"]
141'
['141']
parsed_discourse_facet ['method_citation']
 <S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="37" ssid="1">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S>
original cit marker offset is 0
new cit marker offset is 0



["37'"]
37'
['37']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D10-1044.csv
<S sid ="32" ssid = "29">This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.</S><S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="101" ssid = "5">The second setting uses the news-related subcorpora for the NIST09 MT Chinese to English evaluation8 as IN  and the remaining NIST parallel Chinese/English corpora (UN  Hong Kong Laws  and Hong Kong Hansard) as OUT.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'2'", "'114'", "'101'", "'104'"]
'32'
'2'
'114'
'101'
'104'
['32', '2', '114', '101', '104']
parsed_discourse_facet ['implication_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="138" ssid = "7">However  for multinomial models like our LMs and TMs  there is a one to one correspondence between instances and features  eg the correspondence between a phrase pair (s  t) and its conditional multinomial probability p(s1t).</S><S sid ="144" ssid = "1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S><S sid ="133" ssid = "2">It is difficult to directly compare the Matsoukas et al results with ours  since our out-of-domain corpus is homogeneous; given heterogeneous training data  however  it would be trivial to include Matsoukas-style identity features in our instance-weighting model.</S><S sid ="99" ssid = "3">The dev and test sets were randomly chosen from the EMEA corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'138'", "'144'", "'133'", "'99'"]
'67'
'138'
'144'
'133'
'99'
['67', '138', '144', '133', '99']
parsed_discourse_facet ['implication_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="142" ssid = "11">There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).</S><S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="2" ssid = "2">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'142'", "'8'", "'2'", "'38'"]
'67'
'142'
'8'
'2'
'38'
['67', '142', '8', '2', '38']
parsed_discourse_facet ['results_citation']
<S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="32" ssid = "29">This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.</S><S sid ="83" ssid = "20">We have not yet tried this.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daumes (2007) work that splits each feature into domain-specific and general copies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'32'", "'83'", "'78'", "'136'"]
'8'
'32'
'83'
'78'
'136'
['8', '32', '83', '78', '136']
parsed_discourse_facet ['method_citation']
<S sid ="133" ssid = "2">It is difficult to directly compare the Matsoukas et al results with ours  since our out-of-domain corpus is homogeneous; given heterogeneous training data  however  it would be trivial to include Matsoukas-style identity features in our instance-weighting model.</S><S sid ="118" ssid = "22">Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.</S><S sid ="5" ssid = "2">Even when there is training data available in the domain of interest  there is often additional data from other domains that could in principle be used to improve performance.</S><S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'118'", "'5'", "'1'", "'10'"]
'133'
'118'
'5'
'1'
'10'
['133', '118', '5', '1', '10']
parsed_discourse_facet ['method_citation']
<S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="5" ssid = "2">Even when there is training data available in the domain of interest  there is often additional data from other domains that could in principle be used to improve performance.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daumes (2007) work that splits each feature into domain-specific and general copies.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S><S sid ="14" ssid = "11">There is a fairly large body of work on SMT adaptation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'5'", "'136'", "'104'", "'14'"]
'10'
'5'
'136'
'104'
'14'
['10', '5', '136', '104', '14']
parsed_discourse_facet ['method_citation']
<S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="95" ssid = "32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="125" ssid = "29">Somewhat surprisingly  there do not appear to be large systematic differences between linear and MAP combinations.</S><S sid ="20" ssid = "17">Daume (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'", "'95'", "'78'", "'125'", "'20'"]
'64'
'95'
'78'
'125'
'20'
['64', '95', '78', '125', '20']
parsed_discourse_facet ['method_citation']
<S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="36" ssid = "33">Section 5 covers relevant previous work on SMT adaptation  and section 6 concludes.</S><S sid ="19" ssid = "16">The idea of distinguishing between general and domain-specific examples is due to Daume and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.</S><S sid ="79" ssid = "16">This combination generalizes (2) and (3): we use either at = a to obtain a fixed-weight linear combination  or at = cI(t)/(cI(t) + 0) to obtain a MAP combination.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'36'", "'19'", "'79'", "'114'"]
'151'
'36'
'19'
'79'
'114'
['151', '36', '19', '79', '114']
parsed_discourse_facet ['method_citation']
<S sid ="118" ssid = "22">Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.</S><S sid ="50" ssid = "14">Linear weights are difficult to incorporate into the standard MERT procedure because they are hidden within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.</S><S sid ="88" ssid = "25">We have not explored this strategy.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S><S sid ="19" ssid = "16">The idea of distinguishing between general and domain-specific examples is due to Daume and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'", "'50'", "'88'", "'104'", "'19'"]
'118'
'50'
'88'
'104'
'19'
['118', '50', '88', '104', '19']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "15">For the LM  adaptive weights are set as follows: where  is a weight vector containing an element i for each domain (just IN and OUT in our case)  pi are the corresponding domain-specific models  and p(w  h) is an empirical distribution from a targetlanguage training corpuswe used the IN dev set for this.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the flattened variant described in section 3.2.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="118" ssid = "22">Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'114'", "'127'", "'67'", "'118'"]
'51'
'114'
'127'
'67'
'118'
['51', '114', '127', '67', '118']
parsed_discourse_facet ['implication_citation']
<S sid ="1" ssid = "1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid ="96" ssid = "33">We used it to score all phrase pairs in the OUT table  in order to provide a feature for the instance-weighting model.</S><S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="106" ssid = "10">The corpora for both settings are summarized in table 1.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'96'", "'128'", "'106'", "'111'"]
'1'
'96'
'128'
'106'
'111'
['1', '96', '128', '106', '111']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "7">Its success depends on the two domains being relatively close  and on the OUT corpus not being so large as to overwhelm the contribution of IN.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="41" ssid = "5">We do not adapt the alignment procedure for generating the phrase table from which the TM distributions are derived.</S><S sid ="60" ssid = "24">The matching sentence pairs are then added to the IN corpus  and the system is re-trained.</S><S sid ="0" ssid = "0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'64'", "'41'", "'60'", "'0'"]
'43'
'64'
'41'
'60'
'0'
['43', '64', '41', '60', '0']
parsed_discourse_facet ['method_citation']
<S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="15" ssid = "12">We introduce several new ideas.</S><S sid ="71" ssid = "8">Finally  we incorporate the instance-weighting model into a general linear combination  and learn weights and mixing parameters simultaneously. where c(s  t) is a modified count for pair (s  t) in OUT  u(s|t) is a prior distribution  and y is a prior weight.</S><S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'67'", "'15'", "'71'", "'104'"]
'8'
'67'
'15'
'71'
'104'
['8', '67', '15', '71', '104']
parsed_discourse_facet ['method_citation']
<S sid ="104" ssid = "8">(Thus the domain of the dev and test corpora matches IN.)</S><S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="43" ssid = "7">Its success depends on the two domains being relatively close  and on the OUT corpus not being so large as to overwhelm the contribution of IN.</S><S sid ="135" ssid = "4">Finally  we note that Jiangs instance-weighting framework is broader than we have presented above  encompassing among other possibilities the use of unlabelled IN data  which is applicable to SMT settings where source-only IN corpora are available.</S><S sid ="146" ssid = "3">The features are weighted within a logistic model to give an overall weight that is applied to the phrase pairs frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'", "'151'", "'43'", "'135'", "'146'"]
'104'
'151'
'43'
'135'
'146'
['104', '151', '43', '135', '146']
parsed_discourse_facet ['results_citation']
<S sid ="16" ssid = "13">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S><S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="83" ssid = "20">We have not yet tried this.</S><S sid ="28" ssid = "25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S><S sid ="17" ssid = "14">Previous approaches have tried to find examples that are similar to the target domain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'114'", "'83'", "'28'", "'17'"]
'16'
'114'
'83'
'28'
'17'
['16', '114', '83', '28', '17']
parsed_discourse_facet ['implication_citation']
<S sid ="89" ssid = "26">We used 22 features for the logistic weighting model  divided into two groups: one intended to reflect the degree to which a phrase pair belongs to general language  and one intended to capture similarity to the IN domain.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="110" ssid = "14">Je voudrais preciser  a` ladresse du commissaire Liikanen  quil nest pas aise de recourir aux tribunaux nationaux.</S><S sid ="109" ssid = "13"> I would also like to point out to commissioner Liikanen that it is not easy to take a matter to a national court.</S><S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'78'", "'110'", "'109'", "'67'"]
'89'
'78'
'110'
'109'
'67'
['89', '78', '110', '109', '67']
parsed_discourse_facet ['method_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="64" ssid = "1">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid ="36" ssid = "33">Section 5 covers relevant previous work on SMT adaptation  and section 6 concludes.</S><S sid ="131" ssid = "35">The general-language features have a slight advantage over the similarity features  and both are better than the SVM feature.</S><S sid ="47" ssid = "11">Apart from MERT difficulties  a conceptual problem with log-linear combination is that it multiplies feature probabilities  essentially forcing different features to agree on high-scoring candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'64'", "'36'", "'131'", "'47'"]
'67'
'64'
'36'
'131'
'47'
['67', '64', '36', '131', '47']
parsed_discourse_facet ['results_citation']
<S sid ="114" ssid = "18">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daumes (2007) work that splits each feature into domain-specific and general copies.</S><S sid ="8" ssid = "5">)  which precludes a single universal approach to adaptation.</S><S sid ="11" ssid = "8">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'136'", "'8'", "'11'", "'111'"]
'114'
'136'
'8'
'11'
'111'
['114', '136', '8', '11', '111']
parsed_discourse_facet ['aim_citation']
<S sid ="67" ssid = "4">We extend the Matsoukas et al approach in several ways.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S><S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="92" ssid = "29">6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.</S><S sid ="95" ssid = "32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'111'", "'128'", "'92'", "'95'"]
'67'
'111'
'128'
'92'
'95'
['67', '111', '128', '92', '95']
parsed_discourse_facet ['method_citation']
['In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.']
['We have not yet tried this.', 'It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.', 'We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.', 'Previous approaches have tried to find examples that are similar to the target domain.', 'First  we aim to explicitly characterize examples from OUT as belonging to general language or not.']
['system', 'ROUGE-S*', 'Average_R:', '0.00580', '(95%-conf.int.', '0.00580', '-', '0.00580)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.01078', '(95%-conf.int.', '0.01078', '-', '0.01078)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:78', 'F:6']
['To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.']
['We have not yet tried this.', ')  which precludes a single universal approach to adaptation.', 'This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.', u'It is also worth pointing out a connection with Daum\xb4e\u2019s (2007) work that splits each feature into domain-specific and general copies.', 'This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:120', 'F:0']
['We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.']
['It is difficult to directly compare the Matsoukas et al results with ours  since our out-of-domain corpus is homogeneous; given heterogeneous training data  however  it would be trivial to include Matsoukas-style identity features in our instance-weighting model.', 'The dev and test sets were randomly chosen from the EMEA corpus.', 'In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.', 'However  for multinomial models like our LMs and TMs  there is a one to one correspondence between instances and features  eg the correspondence between a phrase pair (s  t) and its conditional multinomial probability p(s1t).', 'We extend the Matsoukas et al approach in several ways.']
['system', 'ROUGE-S*', 'Average_R:', '0.00847', '(95%-conf.int.', '0.00847', '-', '0.00847)']
['system', 'ROUGE-S*', 'Average_P:', '0.12500', '(95%-conf.int.', '0.12500', '-', '0.12500)']
['system', 'ROUGE-S*', 'Average_F:', '0.01587', '(95%-conf.int.', '0.01587', '-', '0.01587)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:120', 'F:15']
['Domain adaptation is a common concern when optimizing empirical NLP applications.']
['This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.', 'This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.', '(Thus the domain of the dev and test corpora matches IN.)', 'The second setting uses the news-related subcorpora for the NIST09 MT Chinese to English evaluation8 as IN  and the remaining NIST parallel Chinese/English corpora (UN  Hong Kong Laws  and Hong Kong Hansard) as OUT.', 'It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:28', 'F:0']
['Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.']
['Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.', 'Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.', '6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.', 'We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.', 'We extend the Matsoukas et al approach in several ways.']
['system', 'ROUGE-S*', 'Average_R:', '0.00348', '(95%-conf.int.', '0.00348', '-', '0.00348)']
['system', 'ROUGE-S*', 'Average_P:', '0.04762', '(95%-conf.int.', '0.04762', '-', '0.04762)']
['system', 'ROUGE-S*', 'Average_F:', '0.00649', '(95%-conf.int.', '0.00649', '-', '0.00649)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:231', 'F:11']
['However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.']
[u'The idea of distinguishing between general and domain-specific examples is due to Daum\xb4e and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.', 'We have not explored this strategy.', 'Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.', '(Thus the domain of the dev and test corpora matches IN.)', u'Linear weights are difficult to incorporate into the standard MERT procedure because they are \u201chidden\u201d within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:136', 'F:0']
['Finally, we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.']
['In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.', u'The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair\u2019s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).', u'Finally  we note that Jiang\u2019s instance-weighting framework is broader than we have presented above  encompassing among other possibilities the use of unlabelled IN data  which is applicable to SMT settings where source-only IN corpora are available.', 'Its success depends on the two domains being relatively close  and on the OUT corpus not being so large as to overwhelm the contribution of IN.', '(Thus the domain of the dev and test corpora matches IN.)']
['system', 'ROUGE-S*', 'Average_R:', '0.00124', '(95%-conf.int.', '0.00124', '-', '0.00124)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00243', '(95%-conf.int.', '0.00243', '-', '0.00243)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:55', 'F:3']
['We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.']
['It is difficult when IN and OUT are dissimilar  as they are in the cases we study.', 'It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.', 'We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.', u'It is also worth pointing out a connection with Daum\xb4e\u2019s (2007) work that splits each feature into domain-specific and general copies.', ')  which precludes a single universal approach to adaptation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00395', '(95%-conf.int.', '0.00395', '-', '0.00395)']
['system', 'ROUGE-S*', 'Average_P:', '0.04575', '(95%-conf.int.', '0.04575', '-', '0.04575)']
['system', 'ROUGE-S*', 'Average_F:', '0.00728', '(95%-conf.int.', '0.00728', '-', '0.00728)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:153', 'F:7']
['For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components (word-alignment model, language model, translation model, etc.']
[')  which precludes a single universal approach to adaptation.', 'The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.', 'This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.', 'There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).', 'We extend the Matsoukas et al approach in several ways.']
['system', 'ROUGE-S*', 'Average_R:', '0.00584', '(95%-conf.int.', '0.00584', '-', '0.00584)']
['system', 'ROUGE-S*', 'Average_P:', '0.05848', '(95%-conf.int.', '0.05848', '-', '0.05848)']
['system', 'ROUGE-S*', 'Average_F:', '0.01063', '(95%-conf.int.', '0.01063', '-', '0.01063)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:171', 'F:10']
0.0453688883848 0.00319777774225 0.0059422221562





input/ref/Task1/P08-1028_swastika.csv
input/res/Task1/P08-1028.csv
parsing: input/ref/Task1/P08-1028_swastika.csv
<S sid="21" ssid="17">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="23">Our results show that the multiplicative models are superior and correlate significantly with behavioral data.</S>
original cit marker offset is 0
new cit marker offset is 0



['27']
27
['27']
parsed_discourse_facet ['result_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['189']
189
['189']
parsed_discourse_facet ['aim_citation']
<S sid="53" ssid="1">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.</S>
original cit marker offset is 0
new cit marker offset is 0



['53']
53
['53']
parsed_discourse_facet ['method_citation']
<S sid="76" ssid="24">The models considered so far assume that components do not &#8216;interfere&#8217; with each other, i.e., that It is also possible to re-introduce the dependence on K into the model of vector composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['76']
76
['76']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['57']
57
['57']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['57']
57
['57']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="60" ssid="8">To derive specific models from this general framework requires the identification of appropriate constraints to narrow the space of functions being considered.</S>
original cit marker offset is 0
new cit marker offset is 0



['60']
60
['60']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="73" ssid="21">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware, since semantically important constituents can participate more actively in the composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['73']
73
['73']
parsed_discourse_facet ['result_citation']
<S sid="99" ssid="12">In order to establish an independent measure of sentence similarity, we assembled a set of experimental materials and elicited similarity ratings from human subjects.</S>
original cit marker offset is 0
new cit marker offset is 0



['99']
99
['99']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['189']
189
['189']
parsed_discourse_facet ['aim_citation']
<S sid="191" ssid="3">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S>
original cit marker offset is 0
new cit marker offset is 0



['191']
191
['191']
parsed_discourse_facet ['result_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P08-1028.csv
<S sid ="200" ssid = "12">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid ="136" ssid = "49">We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine-grained semantic distinctions (see Table 1).</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants similarity judgments with those of the models.6 We used Spearmans p for our correlation analyses.</S><S sid ="135" ssid = "48">The average inter-subject agreement5 was  = 0.40.</S><S sid ="149" ssid = "62">Our composition models have no additional parameters beyond the semantic space just described  with three exceptions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'200'", "'136'", "'164'", "'135'", "'149'"]
'200'
'136'
'164'
'135'
'149'
['200', '136', '164', '135', '149']
parsed_discourse_facet ['implication_citation']
<S sid ="107" ssid = "20">Landmarks were taken from WordNet (Fellbaum  1998).</S><S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="45" ssid = "18">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid ="192" ssid = "4">We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'63'", "'42'", "'45'", "'192'"]
'107'
'63'
'42'
'45'
'192'
['107', '63', '42', '45', '192']
parsed_discourse_facet ['implication_citation']
<S sid ="30" ssid = "3">For the hierarchical structure of natural language this binding problem becomes particularly acute.</S><S sid ="78" ssid = "26">These vectors are not arbitrary and ideally they must exhibit some relation to the words of the construction under consideration.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants similarity judgments with those of the models.6 We used Spearmans p for our correlation analyses.</S><S sid ="171" ssid = "5">For comparison  we also show the human ratings for these items (UpperBound).</S><S sid ="176" ssid = "10">The multiplicative and combined models yield means closer to the human ratings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'78'", "'164'", "'171'", "'176'"]
'30'
'78'
'164'
'171'
'176'
['30', '78', '164', '171', '176']
parsed_discourse_facet ['results_citation']
<S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S><S sid ="85" ssid = "33">One potential drawback of multiplicative models is the effect of components with value zero.</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'", "'103'", "'85'", "'101'", "'63'"]
'42'
'103'
'85'
'101'
'63'
['42', '103', '85', '101', '63']
parsed_discourse_facet ['method_citation']
<S sid ="97" ssid = "10">Moreover by using a minimal structure we factor out inessential degrees of freedom and are able to assess the merits of different models on an equal footing.</S><S sid ="126" ssid = "39">49 unpaid volunteers completed the experiment  all native speakers of English.</S><S sid ="192" ssid = "4">We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials.</S><S sid ="120" ssid = "33">Examples of our items are given in Table 1.</S><S sid ="165" ssid = "78">Again  better models should correlate better with the experimental data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'126'", "'192'", "'120'", "'165'"]
'97'
'126'
'192'
'120'
'165'
['97', '126', '192', '120', '165']
parsed_discourse_facet ['method_citation']
<S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="45" ssid = "18">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="68" ssid = "16">Although the composition model in (5) is commonly used in the literature  from a linguistic perspective  the model in (6) is more appealing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'", "'121'", "'45'", "'116'", "'68'"]
'42'
'121'
'45'
'116'
'68'
['42', '121', '45', '116', '68']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "13">In the following we describe our data collection procedure and give details on how our composition models were constructed and evaluated.</S><S sid ="123" ssid = "36">Sentence pairs were presented serially in random order.</S><S sid ="112" ssid = "25">The stimuli were administered to four separate groups; each group saw one set of 100 sentences.</S><S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'123'", "'112'", "'25'", "'63'"]
'100'
'123'
'112'
'25'
'63'
['100', '123', '112', '25', '63']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S><S sid ="85" ssid = "33">One potential drawback of multiplicative models is the effect of components with value zero.</S><S sid ="145" ssid = "58">The latter were the most common context words (excluding a stop list of function words).</S><S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'85'", "'145'", "'121'", "'103'"]
'4'
'85'
'145'
'121'
'103'
['4', '85', '145', '121', '103']
parsed_discourse_facet ['method_citation']
<S sid ="25" ssid = "21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid ="45" ssid = "18">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid ="95" ssid = "8">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'45'", "'95'", "'150'", "'42'"]
'25'
'45'
'95'
'150'
'42'
['25', '45', '95', '150', '42']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "49">We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine-grained semantic distinctions (see Table 1).</S><S sid ="132" ssid = "45">We also measured how well humans agree in their ratings.</S><S sid ="158" ssid = "71">The m neighbors most similar to the predicate  and the k of m neighbors closest to its argument.</S><S sid ="200" ssid = "12">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid ="17" ssid = "13">It was not the sales manager who hit the bottle that day  but the office worker with the serious drinking problem. b.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'132'", "'158'", "'200'", "'17'"]
'136'
'132'
'158'
'200'
'17'
['136', '132', '158', '200', '17']
parsed_discourse_facet ['implication_citation']
<S sid ="95" ssid = "8">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="190" ssid = "2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid ="46" ssid = "19">However  since it is order independent  it cannot capture meaning differences that are modulated by differences in syntactic structure.</S><S sid ="29" ssid = "2">While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'150'", "'190'", "'46'", "'29'"]
'95'
'150'
'190'
'46'
'29'
['95', '150', '190', '46', '29']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="156" ssid = "69">This yielded a weighted sum consisting of 95% verb  0% noun and 5% of their multiplicative combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'103'", "'116'", "'150'", "'156'"]
'34'
'103'
'116'
'150'
'156'
['34', '103', '116', '150', '156']
parsed_discourse_facet ['method_citation']
<S sid ="176" ssid = "10">The multiplicative and combined models yield means closer to the human ratings.</S><S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="157" ssid = "70">Finally  Kintschs (2001) additive model has two extra parameters.</S><S sid ="88" ssid = "1">We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).</S><S sid ="38" ssid = "11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'", "'42'", "'157'", "'88'", "'38'"]
'176'
'42'
'157'
'88'
'38'
['176', '42', '157', '88', '38']
parsed_discourse_facet ['method_citation']
<S sid ="63" ssid = "11">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="150" ssid = "63">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="114" ssid = "27">For each reference verb  the subjects responses were entered into a contingency table  whose rows corresponded to nouns and columns to each possible answer (i.e.  one of the two landmarks).</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'116'", "'150'", "'59'", "'114'"]
'63'
'116'
'150'
'59'
'114'
['63', '116', '150', '59', '114']
parsed_discourse_facet ['results_citation']
<S sid ="42" ssid = "15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="116" ssid = "29">We used Fishers exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid ="64" ssid = "12">Now  if we assume that p lies in the same space as u and v  avoiding the issues of dimensionality associated with tensor products  and that f is a linear function  for simplicity  of the cartesian product of u and v  then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast  if we assume that f is a linear function of the tensor product of u and v  then we obtain multiplicative models: where C is a tensor of rank 3  which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'", "'121'", "'116'", "'64'", "'103'"]
'42'
'121'
'116'
'64'
'103'
['42', '121', '116', '64', '103']
parsed_discourse_facet ['implication_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="77" ssid = "25">For additive models  a natural way to achieve this is to include further vectors into the summation.</S><S sid ="70" ssid = "18">Instead  it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v  and vice versa.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="85" ssid = "33">One potential drawback of multiplicative models is the effect of components with value zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'77'", "'70'", "'59'", "'85'"]
'194'
'77'
'70'
'59'
'85'
['194', '77', '70', '59', '85']
parsed_discourse_facet ['method_citation']
<S sid ="121" ssid = "34">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid ="103" ssid = "16">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S><S sid ="130" ssid = "43">As we can see sentences with high similarity landmarks are perceived as more similar to the reference sentence.</S><S sid ="82" ssid = "30">In contrast to the simple additive model  this extended model is sensitive to syntactic structure  since n is chosen from among the neighbors of the predicate  distinguishing it from the argument.</S><S sid ="73" ssid = "21">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware  since semantically important constituents can participate more actively in the composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'121'", "'103'", "'130'", "'82'", "'73'"]
'121'
'103'
'130'
'82'
'73'
['121', '103', '130', '82', '73']
parsed_discourse_facet ['results_citation']
['Our results show that the multiplicative models are superior and correlate significantly with behavioral data.']
[u'A more scrupulous evaluation requires directly correlating all the individual participants\u2019 similarity judgments with those of the models.6 We used Spearman\u2019s p for our correlation analyses.', 'The applications of the framework discussed here are many and varied both for cognitive science and NLP.', 'We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine-grained semantic distinctions (see Table 1).', u'The average inter-subject agreement5 was \u03c1 = 0.40.', 'Our composition models have no additional parameters beyond the semantic space just described  with three exceptions.']
['system', 'ROUGE-S*', 'Average_R:', '0.00089', '(95%-conf.int.', '0.00089', '-', '0.00089)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00172', '(95%-conf.int.', '0.00172', '-', '0.00172)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:36', 'F:1']
['We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.']
['This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.', 'All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).', 'Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.', 'This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.', 'One potential drawback of multiplicative models is the effect of components with value zero.']
['system', 'ROUGE-S*', 'Average_R:', '0.00966', '(95%-conf.int.', '0.00966', '-', '0.00966)']
['system', 'ROUGE-S*', 'Average_P:', '0.03077', '(95%-conf.int.', '0.03077', '-', '0.03077)']
['system', 'ROUGE-S*', 'Average_F:', '0.01471', '(95%-conf.int.', '0.01471', '-', '0.01471)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:325', 'F:10']
['In order to establish an independent measure of sentence similarity, we assembled a set of experimental materials and elicited similarity ratings from human subjects.']
[u'For each reference verb  the subjects\u2019 responses were entered into a contingency table  whose rows corresponded to nouns and columns to each possible answer (i.e.  one of the two landmarks).', 'First  the additive model in (7) weighs differentially the contribution of the two constituents.', 'This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.', u'We used Fisher\u2019s exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.', 'It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:105', 'F:0']
['The models considered so far assume that components do not &#8216;interfere&#8217; with each other, i.e., that It is also possible to re-introduce the dependence on K into the model of vector composition.']
['Examples of our items are given in Table 1.', 'Again  better models should correlate better with the experimental data.', '49 unpaid volunteers completed the experiment  all native speakers of English.', 'Moreover by using a minimal structure we factor out inessential degrees of freedom and are able to assess the merits of different models on an equal footing.', 'We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials.']
['system', 'ROUGE-S*', 'Average_R:', '0.00150', '(95%-conf.int.', '0.00150', '-', '0.00150)']
['system', 'ROUGE-S*', 'Average_P:', '0.01515', '(95%-conf.int.', '0.01515', '-', '0.01515)']
['system', 'ROUGE-S*', 'Average_F:', '0.00273', '(95%-conf.int.', '0.00273', '-', '0.00273)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:66', 'F:1']
['We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['We present a general framework for vector-based composition which allows us to consider different classes of models.', 'Vector addition does not increase the dimensionality of the resulting vector.', 'Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.', 'This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.', 'First  the additive model in (7) weighs differentially the contribution of the two constituents.']
['system', 'ROUGE-S*', 'Average_R:', '0.00808', '(95%-conf.int.', '0.00808', '-', '0.00808)']
['system', 'ROUGE-S*', 'Average_P:', '0.22222', '(95%-conf.int.', '0.22222', '-', '0.22222)']
['system', 'ROUGE-S*', 'Average_F:', '0.01559', '(95%-conf.int.', '0.01559', '-', '0.01559)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:36', 'F:8']
['We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['In contrast to the simple additive model  this extended model is sensitive to syntactic structure  since n is chosen from among the neighbors of the predicate  distinguishing it from the argument.', 'Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).', 'As we can see sentences with high similarity landmarks are perceived as more similar to the reference sentence.', 'Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware  since semantically important constituents can participate more actively in the composition.', 'All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).']
['system', 'ROUGE-S*', 'Average_R:', '0.00038', '(95%-conf.int.', '0.00038', '-', '0.00038)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00075', '(95%-conf.int.', '0.00075', '-', '0.00075)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:36', 'F:1']
['We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['However  since it is order independent  it cannot capture meaning differences that are modulated by differences in syntactic structure.', 'First  the additive model in (7) weighs differentially the contribution of the two constituents.', 'Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.', 'We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.', 'While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.']
['system', 'ROUGE-S*', 'Average_R:', '0.02338', '(95%-conf.int.', '0.02338', '-', '0.02338)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.04569', '(95%-conf.int.', '0.04569', '-', '0.04569)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:36', 'F:36']
['In this paper we presented a general framework for vector-based semantic composition.']
['Vector addition does not increase the dimensionality of the resulting vector.', 'This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.', 'This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.', 'Landmarks were taken from WordNet (Fellbaum  1998).', 'We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:28', 'F:0']
['Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.']
['This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.', 'We present a general framework for vector-based composition which allows us to consider different classes of models.', 'In the following we describe our data collection procedure and give details on how our composition models were constructed and evaluated.', 'Sentence pairs were presented serially in random order.', 'The stimuli were administered to four separate groups; each group saw one set of 100 sentences.']
['system', 'ROUGE-S*', 'Average_R:', '0.00810', '(95%-conf.int.', '0.00810', '-', '0.00810)']
['system', 'ROUGE-S*', 'Average_P:', '0.04412', '(95%-conf.int.', '0.04412', '-', '0.04412)']
['system', 'ROUGE-S*', 'Average_F:', '0.01368', '(95%-conf.int.', '0.01368', '-', '0.01368)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:136', 'F:6']
0.151979998311 0.00577666660248 0.010541110994





input/ref/Task1/P11-1060_aakansha.csv
input/res/Task1/P11-1060.csv
parsing: input/ref/Task1/P11-1060_aakansha.csv
<S sid="11" ssid="7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'"]
'2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="5">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'"]
'112'
['112']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="11">Although a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">However, we still model the logical form (now as a latent variable) to capture the complexities of language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="12">It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="21">The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="88" ssid="64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets, a restrictor A and a nuclear scope B.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'"]
'88'
['88']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S sid ="57" ssid = "33">But consider Figure 4: (a) is headed by borders  but states needs to be extracted; in (b)  the quantifier no is syntactically dominated by the head verb borders but needs to take wider scope.</S><S sid ="39" ssid = "15">Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.</S><S sid ="60" ssid = "36">Then higher up in the tree  we invoke it with an execute relation Xi to create the desired semantic scope.2 This mark-execute construct acts non-locally  so to maintain compositionality  we must augment the denotation d = JzKw to include any information about the marked nodes in z that can be accessed by an execute relation later on.</S><S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="167" ssid = "52">In DCS  we start with lexical triggers  which are 6 Conclusion more basic than CCG lexical entries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'", "'39'", "'60'", "'22'", "'167'"]
'57'
'39'
'60'
'22'
'167'
['57', '39', '60', '22', '167']
parsed_discourse_facet ['implication_citation']
<S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="71" ssid = "47">Let z be a DCS tree.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="120" ssid = "5">GEO has 48 non-value predicates and JOBS has 26.</S><S sid ="85" ssid = "61">Extraction allows us to return the set of consistent values of a marked non-root node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'71'", "'141'", "'120'", "'85'"]
'15'
'71'
'141'
'120'
'85'
['15', '71', '141', '120', '85']
parsed_discourse_facet ['implication_citation']
<S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S><S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parents denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="39" ssid = "15">Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.</S><S sid ="147" ssid = "32">However  training on just these examples is enough to improve the parameters  and this 29% increases to 66% and then to 95% over the next few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'51'", "'3'", "'39'", "'147'"]
'46'
'51'
'3'
'39'
'147'
['46', '51', '3', '39', '147']
parsed_discourse_facet ['results_citation']
<S sid ="51" ssid = "27">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parents denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid ="84" ssid = "60">There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.</S><S sid ="170" ssid = "55">Our features as soft preferences.</S><S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'84'", "'170'", "'46'", "'98'"]
'51'
'84'
'170'
'46'
'98'
['51', '84', '170', '46', '98']
parsed_discourse_facet ['method_citation']
<S sid ="122" ssid = "7">For JOBS  if we use the standard Jobs database  close to half the ys are empty  which makes it uninteresting.</S><S sid ="94" ssid = "70">We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.</S><S sid ="102" ssid = "78">As a running example  consider x = city that is in California and z = hcity; 11:hloc; 21:hCAiii  where city triggers city and California triggers CA.</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S><S sid ="95" ssid = "71">Our first question is: given notation of the DCS tree before execution is an utterance x  what trees z  Z are permissible?</S>
original cit marker offset is 0
new cit marker offset is 0



["'122'", "'94'", "'102'", "'98'", "'95'"]
'122'
'94'
'102'
'98'
'95'
['122', '94', '102', '98', '95']
parsed_discourse_facet ['method_citation']
<S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="84" ssid = "60">There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'", "'3'", "'6'", "'84'", "'162'"]
'22'
'3'
'6'
'84'
'162'
['22', '3', '6', '84', '162']
parsed_discourse_facet ['method_citation']
<S sid ="13" ssid = "9">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x  y)  which is much cheaper to obtain than (x  z) pairs.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="29" ssid = "5">Let P be a set of predicates (e.g.  state  count  P)  which are just symbols.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="172" ssid = "57">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms  we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus  and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'134'", "'29'", "'77'", "'172'"]
'13'
'134'
'29'
'77'
'172'
['13', '134', '29', '77', '172']
parsed_discourse_facet ['method_citation']
<S sid ="96" ssid = "72">To California cities)  and it also allows us to underspecify L. In particular  our L will not include verbs or prepositions; rather  we rely on the predicates corresponding to those words to be triggered by traces.</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S><S sid ="94" ssid = "70">We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.</S><S sid ="124" ssid = "9">For each data predicate p (e.g.  language)  we add each possible tuple (e.g.  (job37  Java)) to w(p) independently with probability 0.8.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S>
original cit marker offset is 0
new cit marker offset is 0



["'96'", "'88'", "'94'", "'124'", "'162'"]
'96'
'88'
'94'
'124'
'162'
['96', '88', '94', '124', '162']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "84">However  in order to learn  we need to sum over {z  ZL(x) : JzKw = y}  and unfortunately  the additional constraint JzKw = y does not factorize.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="84" ssid = "60">There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'3'", "'84'", "'134'", "'46'"]
'108'
'3'
'84'
'134'
'46'
['108', '3', '84', '134', '46']
parsed_discourse_facet ['method_citation']
<S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'", "'98'", "'77'", "'162'", "'88'"]
'22'
'98'
'77'
'162'
'88'
['22', '98', '77', '162', '88']
parsed_discourse_facet ['implication_citation']
<S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="98" ssid = "74">The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'134'", "'3'", "'141'", "'98'"]
'12'
'134'
'3'
'141'
'98'
['12', '134', '3', '141', '98']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="29" ssid = "5">Let P be a set of predicates (e.g.  state  count  P)  which are just symbols.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="129" ssid = "14">We also define an augmented lexicon L+ which includes a prototype word x for each predicate appearing in (iii) above (e.g.  (large  size))  which cancels the predicates triggered by xs POS tag.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'29'", "'77'", "'129'", "'8'"]
'3'
'29'
'77'
'129'
'8'
['3', '29', '77', '129', '8']
parsed_discourse_facet ['method_citation']
<S sid ="46" ssid = "22">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z  Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the childs denotation (t  JciKw).</S><S sid ="142" ssid = "27">This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.</S><S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="144" ssid = "29">Intuitions How is our system learning?</S><S sid ="112" ssid = "88">Our learning algorithm alternates between (i) using the current parameters  to generate the K-best set ZL (x) for each training example x  and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'142'", "'10'", "'144'", "'112'"]
'46'
'142'
'10'
'144'
'112'
['46', '142', '10', '144', '112']
parsed_discourse_facet ['method_citation']
<S sid ="133" ssid = "18">Table 2 shows that our system using lexical triggers L (henceforth  DCS) outperforms SEMRESP (78.9% over 73.2%).</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="163" ssid = "48">Feedback from the context; for example  the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x  y)  which means (Schuler  2003) and semantic parsing (Popescu et borders looks right for the first argument and left al.  2003; Clarke et al.  2010).</S><S sid ="141" ssid = "26">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'134'", "'163'", "'141'", "'10'"]
'133'
'134'
'163'
'141'
'10'
['133', '134', '163', '141', '10']
parsed_discourse_facet ['results_citation']
<S sid ="86" ssid = "62">Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  (i  )]{1 = }.</S><S sid ="162" ssid = "47">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S><S sid ="77" ssid = "53">Join The join of two denotations d and d' with respect to components j and j' ( means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="113" ssid = "89">Formally  let O(  ') be the objective function O() with ZL(x) ZL I(x).</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'162'", "'77'", "'3'", "'113'"]
'86'
'162'
'77'
'3'
'113'
['86', '162', '77', '3', '113']
parsed_discourse_facet ['implication_citation']
<S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="55" ssid = "31">The tree structure still enables us to compute denotations efficiently based on (1) and (2).</S><S sid ="80" ssid = "56">The full definition of join is as follows: Aggregate The aggregate operation takes a denotation and forms a set out of the tuples in the first column for each setting of the rest of the columns: Now we turn to the mark (M) and execute (Xi) operations  which handles the divergence between syntactic and semantic scope.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'", "'134'", "'3'", "'55'", "'80'"]
'10'
'134'
'3'
'55'
'80'
['10', '134', '3', '55', '80']
parsed_discourse_facet ['method_citation']
<S sid ="22" ssid = "18">The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="50" ssid = "26">For example  consider the phrase number of major cities  and suppose that number corresponds to the count predicate.</S><S sid ="113" ssid = "89">Formally  let O(  ') be the objective function O() with ZL(x) ZL I(x).</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'", "'134'", "'154'", "'50'", "'113'"]
'22'
'134'
'154'
'50'
'113'
['22', '134', '154', '50', '113']
parsed_discourse_facet ['results_citation']
<S sid ="6" ssid = "2">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="3" ssid = "3">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid ="12" ssid = "8">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid ="126" ssid = "11">During development  we further held out a random 30% of the training sets for validation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'134'", "'3'", "'12'", "'126'"]
'6'
'134'
'3'
'12'
'126'
['6', '134', '3', '12', '126']
parsed_discourse_facet ['aim_citation']
<S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S><S sid ="9" ssid = "5">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid ="10" ssid = "6">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid ="172" ssid = "57">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms  we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus  and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'132'", "'9'", "'10'", "'172'", "'134'"]
'132'
'9'
'10'
'172'
'134'
['132', '9', '10', '172', '134']
parsed_discourse_facet ['method_citation']
['The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).']
['Table 2 shows that our system using lexical triggers L (henceforth  DCS) outperforms SEMRESP (78.9% over 73.2%).', 'However  we still model the logical form (now as a latent variable) to capture the complexities of language.', 'Feedback from the context; for example  the lexical entry for borders world has been used to guide both syntactic parsing is S\\NP/NP : Ay.Ax.border(x  y)  which means (Schuler  2003) and semantic parsing (Popescu et borders looks right for the first argument and left al.  2003; Clarke et al.  2010).', 'Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.', 'In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:45', 'F:0']
['The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['As a running example  consider x = city that is in California and z = hcity; 11:hloc; 21:hCAiii  where city triggers city and California triggers CA.', 'The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.', u'For JOBS  if we use the standard Jobs database  close to half the y\u2019s are empty  which makes it uninteresting.', u'Our first question is: given notation of the DCS tree before execution is an utterance x  what trees z \u2208 Z are permissible?', 'We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:105', 'F:0']
['The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
[u"Join The join of two denotations d and d' with respect to components j and j' (\xe2\u02c6\u2014 means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.", 'The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.', 'Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.', 'The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.', 'The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:105', 'F:0']
['Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.']
['Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.', 'For each data predicate p (e.g.  language)  we add each possible tuple (e.g.  (job37  Java)) to w(p) independently with probability 0.8.', 'We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.', 'The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.', 'To California cities)  and it also allows us to underspecify L. In particular  our L will not include verbs or prepositions; rather  we rely on the predicates corresponding to those words to be triggered by traces.']
['system', 'ROUGE-S*', 'Average_R:', '0.00471', '(95%-conf.int.', '0.00471', '-', '0.00471)']
['system', 'ROUGE-S*', 'Average_P:', '0.02333', '(95%-conf.int.', '0.02333', '-', '0.02333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00784', '(95%-conf.int.', '0.00784', '-', '0.00784)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:300', 'F:7']
['This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.']
['As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.', 'Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.', 'In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.', 'Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms  we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus  and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.', 'However  we still model the logical form (now as a latent variable) to capture the complexities of language.']
['system', 'ROUGE-S*', 'Average_R:', '0.00316', '(95%-conf.int.', '0.00316', '-', '0.00316)']
['system', 'ROUGE-S*', 'Average_P:', '0.05263', '(95%-conf.int.', '0.05263', '-', '0.05263)']
['system', 'ROUGE-S*', 'Average_F:', '0.00597', '(95%-conf.int.', '0.00597', '-', '0.00597)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:190', 'F:10']
['Although a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words.']
[u"Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z \u2208 Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the child\u2019s denotation (t \u2208 JciKw).", u'However  in order to learn  we need to sum over {z \u2208 ZL(x) : JzKw = y}  and unfortunately  the additional constraint JzKw = y does not factorize.', 'In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.', 'There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.', 'In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00846', '(95%-conf.int.', '0.00846', '-', '0.00846)']
['system', 'ROUGE-S*', 'Average_P:', '0.29091', '(95%-conf.int.', '0.29091', '-', '0.29091)']
['system', 'ROUGE-S*', 'Average_F:', '0.01644', '(95%-conf.int.', '0.01644', '-', '0.01644)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:55', 'F:16']
['The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.']
['The tree structure still enables us to compute denotations efficiently based on (1) and (2).', 'However  we still model the logical form (now as a latent variable) to capture the complexities of language.', 'In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.', 'The full definition of join is as follows: Aggregate The aggregate operation takes a denotation and forms a set out of the tuples in the first column for each setting of the rest of the columns: Now we turn to the mark (M) and execute (Xi) operations  which handles the divergence between syntactic and semantic scope.', 'In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00280', '(95%-conf.int.', '0.00280', '-', '0.00280)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.00543', '(95%-conf.int.', '0.00543', '-', '0.00543)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:66', 'F:6']
['Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.']
['Then higher up in the tree  we invoke it with an execute relation Xi to create the desired semantic scope.2 This mark-execute construct acts non-locally  so to maintain compositionality  we must augment the denotation d = JzKw to include any information about the marked nodes in z that can be accessed by an execute relation later on.', 'But consider Figure 4: (a) is headed by borders  but states needs to be extracted; in (b)  the quantifier no is syntactically dominated by the head verb borders but needs to take wider scope.', 'In DCS  we start with lexical triggers  which are 6 Conclusion more basic than CCG lexical entries.', 'Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.', 'The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.']
['system', 'ROUGE-S*', 'Average_R:', '0.00033', '(95%-conf.int.', '0.00033', '-', '0.00033)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00065', '(95%-conf.int.', '0.00065', '-', '0.00065)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:55', 'F:1']
['Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets, a restrictor A and a nuclear scope B.']
['For example  consider the phrase number of major cities  and suppose that number corresponds to the count predicate.', u"Formally  let \u02dcO(\u03b8  \u03b8') be the objective function O(\u03b8) with ZL(x) \u02dcZL \u03b8I(x).", 'The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).', 'The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.', 'In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:45', 'F:0']
0.0528844438568 0.0021622221982 0.00403666662181





input/ref/Task1/W06-3114_swastika.csv
input/res/Task1/W06-3114.csv
parsing: input/ref/Task1/W06-3114_swastika.csv
<S sid="47" ssid="13">Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data.</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
    <S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="37">Our suspicion is that BLEU is very sensitive to jargon, to selecting exactly the right words, and not synonyms that human judges may appreciate as equally good.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
<S sid="145" ssid="38">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S>
original cit marker offset is 0
new cit marker offset is 0



['145']
145
['145']
parsed_discourse_facet ['result_citation']
    <S sid="103" ssid="19">Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['103']
103
['103']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="16">Following this method, we repeatedly &#8212; say, 1000 times &#8212; sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.</S>
original cit marker offset is 0
new cit marker offset is 0



['50']
50
['50']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="7">We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.</S>
original cit marker offset is 0
new cit marker offset is 0



['68']
68
['68']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="8">The way judgements are collected, human judges tend to use the scores to rank systems against each other.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['result_citation']
<S sid="145" ssid="38">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S>
original cit marker offset is 0
new cit marker offset is 0



['145']
145
['145']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="22" ssid = "15">The text type are editorials instead of speech transcripts.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S><S sid ="11" ssid = "4">To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.</S><S sid ="167" ssid = "60">One annotator suggested that this was the case for as much as 10% of our test sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'22'", "'155'", "'11'", "'167'"]
'59'
'22'
'155'
'11'
'167'
['59', '22', '155', '11', '167']
parsed_discourse_facet ['implication_citation']
<S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="34" ssid = "27">For more on the participating systems  please refer to the respective system description in the proceedings of the workshop.</S><S sid ="89" ssid = "5">In words  the judgements are normalized  so that the average normalized judgement per judge is 3.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S><S sid ="160" ssid = "53">Annotators suggested that long sentences are almost impossible to judge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'34'", "'89'", "'155'", "'160'"]
'136'
'34'
'89'
'155'
'160'
['136', '34', '89', '155', '160']
parsed_discourse_facet ['implication_citation']
<S sid ="27" ssid = "20">Microsofts approach uses dependency trees  others use hierarchical phrase models.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="115" ssid = "8">Often  two systems can not be distinguished with a confidence of over 95%  so there are ranked the same.</S><S sid ="15" ssid = "8">Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.</S><S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'", "'1'", "'115'", "'15'", "'59'"]
'27'
'1'
'115'
'15'
'59'
['27', '1', '115', '15', '59']
parsed_discourse_facet ['results_citation']
<S sid ="140" ssid = "33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid ="22" ssid = "15">The text type are editorials instead of speech transcripts.</S><S sid ="26" ssid = "19">Most of these groups follow a phrase-based statistical approach to machine translation.</S><S sid ="82" ssid = "21">This decreases the statistical significance of our results compared to those studies.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'22'", "'26'", "'82'", "'136'"]
'140'
'22'
'26'
'82'
'136'
['140', '22', '26', '82', '136']
parsed_discourse_facet ['method_citation']
<S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="131" ssid = "24">The manual scores are averages over the raw unnormalized scores.</S><S sid ="119" ssid = "12">There may be occasionally a system clearly at the top or at the bottom  but most systems are so close that it is hard to distinguish them.</S><S sid ="44" ssid = "10">We computed BLEU scores for each submission with a single reference translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'84'", "'131'", "'119'", "'44'"]
'125'
'84'
'131'
'119'
'44'
['125', '84', '131', '119', '44']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "12">We aligned the texts at a sentence level across all four languages  resulting in 1064 sentence per language.</S><S sid ="63" ssid = "2">Many human evaluation metrics have been proposed.</S><S sid ="135" ssid = "28">The differences in difficulty are better reflected in the BLEU scores than in the raw un-normalized manual judgements.</S><S sid ="11" ssid = "4">To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'63'", "'135'", "'11'", "'1'"]
'19'
'63'
'135'
'11'
'1'
['19', '63', '135', '11', '1']
parsed_discourse_facet ['method_citation']
<S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="51" ssid = "17">When dropping the top and bottom 2.5% the remaining BLEU scores define the range of the confidence interval.</S><S sid ="146" ssid = "39">This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.</S><S sid ="139" ssid = "32">Given the closeness of most systems and the wide over-lapping confidence intervals it is hard to make strong statements about the correlation between human judgements and automatic scoring methods such as BLEU.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'51'", "'146'", "'139'", "'1'"]
'163'
'51'
'146'
'139'
'1'
['163', '51', '146', '139', '1']
parsed_discourse_facet ['method_citation']
<S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="67" ssid = "6">Participants and other volunteers contributed about 180 hours of labor in the manual evaluation.</S><S sid ="26" ssid = "19">Most of these groups follow a phrase-based statistical approach to machine translation.</S><S sid ="55" ssid = "21">If one system is better in 95% of the sample sets  we conclude that its higher BLEU score is statistically significantly better.</S><S sid ="89" ssid = "5">In words  the judgements are normalized  so that the average normalized judgement per judge is 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'67'", "'26'", "'55'", "'89'"]
'163'
'67'
'26'
'55'
'89'
['163', '67', '26', '55', '89']
parsed_discourse_facet ['method_citation']
<S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="121" ssid = "14">For the automatic scoring method BLEU  we can distinguish three quarters of the systems.</S><S sid ="161" ssid = "54">Since all long sentence translation are somewhat muddled  even a contrastive evaluation between systems was difficult.</S><S sid ="30" ssid = "23">The other half was replaced by other participants  so we ended up with roughly the same number.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'136'", "'121'", "'161'", "'30'"]
'84'
'136'
'121'
'161'
'30'
['84', '136', '121', '161', '30']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="163" ssid = "56">Not every annotator was fluent in both the source and the target language.</S><S sid ="50" ssid = "16">Following this method  we repeatedly  say  1000 times  sample sets of sentences from the output of each system  measure their BLEU score  and use these 1000 BLEU scores as basis for estimating a confidence interval.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="83" ssid = "22">The number of judgements is additionally fragmented by our breakup of sentences into in-domain and out-of-domain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'163'", "'50'", "'84'", "'83'"]
'136'
'163'
'50'
'84'
'83'
['136', '163', '50', '84', '83']
parsed_discourse_facet ['implication_citation']
<S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="119" ssid = "12">There may be occasionally a system clearly at the top or at the bottom  but most systems are so close that it is hard to distinguish them.</S><S sid ="82" ssid = "21">This decreases the statistical significance of our results compared to those studies.</S><S sid ="43" ssid = "9">At the very least  we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.</S><S sid ="10" ssid = "3">Figure 1 provides some statistics about this corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'119'", "'82'", "'43'", "'10'"]
'59'
'119'
'82'
'43'
'10'
['59', '119', '82', '43', '10']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "8">We settled on contrastive evaluations of 5 system outputs for a single test sentence.</S><S sid ="99" ssid = "15">Systems that generally do worse than others will receive a negative one.</S><S sid ="112" ssid = "5">The normalization on a per-judge basis gave very similar ranking  only slightly less consistent with the ranking from the pairwise comparisons.</S><S sid ="62" ssid = "1">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems  they are only a imperfect substitute for human assessment of translation quality  or as the acronym BLEU puts it  a bilingual evaluation understudy.</S><S sid ="98" ssid = "14">Systems that generally do better than others will receive a positive average normalizedjudgement per sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'99'", "'112'", "'62'", "'98'"]
'69'
'99'
'112'
'62'
'98'
['69', '99', '112', '62', '98']
parsed_discourse_facet ['method_citation']
<S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="110" ssid = "3">In the graphs  system scores are indicated by a point  the confidence intervals by shaded areas around the point.</S><S sid ="146" ssid = "39">This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.</S><S sid ="90" ssid = "6">Another way to view the judgements is that they are less quality judgements of machine translation systems per se  but rankings of machine translation systems.</S><S sid ="33" ssid = "26">While building a machine translation system is a serious undertaking  in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'110'", "'146'", "'90'", "'33'"]
'125'
'110'
'146'
'90'
'33'
['125', '110', '146', '90', '33']
parsed_discourse_facet ['method_citation']
<S sid ="98" ssid = "14">Systems that generally do better than others will receive a positive average normalizedjudgement per sentence.</S><S sid ="148" ssid = "41">The best answer to this is: many research labs have very competitive systems whose performance is hard to tell apart.</S><S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S><S sid ="37" ssid = "3">It rewards matches of n-gram sequences  but measures only at most indirectly overall grammatical coherence.</S><S sid ="102" ssid = "18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems  we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'98'", "'148'", "'6'", "'37'", "'102'"]
'98'
'148'
'6'
'37'
'102'
['98', '148', '6', '37', '102']
parsed_discourse_facet ['results_citation']
<S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="177" ssid = "1">This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency  Contract No.</S><S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="3" ssid = "1">was done by the participants.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'177'", "'59'", "'1'", "'3'"]
'125'
'177'
'59'
'1'
'3'
['125', '177', '59', '1', '3']
parsed_discourse_facet ['implication_citation']
<S sid ="90" ssid = "6">Another way to view the judgements is that they are less quality judgements of machine translation systems per se  but rankings of machine translation systems.</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="135" ssid = "28">The differences in difficulty are better reflected in the BLEU scores than in the raw un-normalized manual judgements.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'0'", "'125'", "'126'", "'135'"]
'90'
'0'
'125'
'126'
'135'
['90', '0', '125', '126', '135']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "17">When dropping the top and bottom 2.5% the remaining BLEU scores define the range of the confidence interval.</S><S sid ="39" ssid = "5">However  a recent study (Callison-Burch et al.  2006)  pointed out that this correlation may not always be strong.</S><S sid ="1" ssid = "1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3  0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt  rali upc-jmc  cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upv -0.5 systran upv upc -jmc  Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6    td t cc upc-  rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 upv -0.4 upv -0.3 In Domain upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain upc-jmc nrc ntt Adequacy upc-jmc   lcc  rali  rali -0.7 -0.5 -0.6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28  rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt  upc-mr lcc utd upc-jg rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 upc-jmc  uedin-birch -0.5 -0.5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc  upc-jmc systran upv Fluency ula upc-mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 systran upv uedin-phi -jmc rali systran -0.3 -0.4 -0.5 -0.6 upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi   utd upc-jmc upc-mr 0.4 rali -0.3 -0.4 -0.5 upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid ="162" ssid = "55">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S><S sid ="26" ssid = "19">Most of these groups follow a phrase-based statistical approach to machine translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'39'", "'1'", "'162'", "'26'"]
'51'
'39'
'1'
'162'
'26'
['51', '39', '1', '162', '26']
parsed_discourse_facet ['results_citation']
<S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="112" ssid = "5">The normalization on a per-judge basis gave very similar ranking  only slightly less consistent with the ranking from the pairwise comparisons.</S><S sid ="174" ssid = "5">The manual evaluation of scoring translation on a graded scale from 15 seems to be very hard to perform.</S><S sid ="59" ssid = "25">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid ="125" ssid = "18">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'112'", "'174'", "'59'", "'125'"]
'136'
'112'
'174'
'59'
'125'
['136', '112', '174', '59', '125']
parsed_discourse_facet ['aim_citation']
['This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.']
[u'The manual evaluation of scoring translation on a graded scale from 1\xe2\u20ac\u201c5 seems to be very hard to perform.', 'The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.', 'The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).', 'We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.', 'The normalization on a per-judge basis gave very similar ranking  only slightly less consistent with the ranking from the pairwise comparisons.']
['system', 'ROUGE-S*', 'Average_R:', '0.01163', '(95%-conf.int.', '0.01163', '-', '0.01163)']
['system', 'ROUGE-S*', 'Average_P:', '0.16176', '(95%-conf.int.', '0.16176', '-', '0.16176)']
['system', 'ROUGE-S*', 'Average_F:', '0.02171', '(95%-conf.int.', '0.02171', '-', '0.02171)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:136', 'F:22']
['Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data.']
['To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.', 'For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.', 'One annotator suggested that this was the case for as much as 10% of our test sentences.', 'The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.', 'The text type are editorials instead of speech transcripts.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:36', 'F:0']
['Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.']
['For the automatic scoring method BLEU  we can distinguish three quarters of the systems.', 'The other half was replaced by other participants  so we ended up with roughly the same number.', 'The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'Since all long sentence translation are somewhat muddled  even a contrastive evaluation between systems was difficult.']
['system', 'ROUGE-S*', 'Average_R:', '0.00966', '(95%-conf.int.', '0.00966', '-', '0.00966)']
['system', 'ROUGE-S*', 'Average_P:', '0.01587', '(95%-conf.int.', '0.01587', '-', '0.01587)']
['system', 'ROUGE-S*', 'Average_F:', '0.01201', '(95%-conf.int.', '0.01201', '-', '0.01201)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:630', 'F:10']
['In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'Often  two systems can not be distinguished with a confidence of over 95%  so there are ranked the same.', u'Microsoft\u2019s approach uses dependency trees  others use hierarchical phrase models.', 'Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.', 'The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.']
['system', 'ROUGE-S*', 'Average_R:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Average_P:', '0.06593', '(95%-conf.int.', '0.06593', '-', '0.06593)']
['system', 'ROUGE-S*', 'Average_F:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:961191', 'P:91', 'F:6']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency  Contract No.', u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'was done by the participants.', 'The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.', 'We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.']
['system', 'ROUGE-S*', 'Average_R:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Average_P:', '0.10909', '(95%-conf.int.', '0.10909', '-', '0.10909)']
['system', 'ROUGE-S*', 'Average_F:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:965355', 'P:55', 'F:6']
['In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.']
['The manual scores are averages over the raw unnormalized scores.', 'There may be occasionally a system clearly at the top or at the bottom  but most systems are so close that it is hard to distinguish them.', 'We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'We computed BLEU scores for each submission with a single reference translation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:91', 'F:0']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['Systems that generally do worse than others will receive a negative one.', 'We settled on contrastive evaluations of 5 system outputs for a single test sentence.', 'While automatic measures are an invaluable tool for the day-to-day development of machine translation systems  they are only a imperfect substitute for human assessment of translation quality  or as the acronym BLEU puts it  a bilingual evaluation understudy.', 'Systems that generally do better than others will receive a positive average normalizedjudgement per sentence.', 'The normalization on a per-judge basis gave very similar ranking  only slightly less consistent with the ranking from the pairwise comparisons.']
['system', 'ROUGE-S*', 'Average_R:', '0.00419', '(95%-conf.int.', '0.00419', '-', '0.00419)']
['system', 'ROUGE-S*', 'Average_P:', '0.10909', '(95%-conf.int.', '0.10909', '-', '0.10909)']
['system', 'ROUGE-S*', 'Average_F:', '0.00808', '(95%-conf.int.', '0.00808', '-', '0.00808)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:55', 'F:6']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['Another way to view the judgements is that they are less quality judgements of machine translation systems per se  but rankings of machine translation systems.', 'The differences in difficulty are better reflected in the BLEU scores than in the raw un-normalized manual judgements.', 'We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'Manual and Automatic Evaluation of Machine Translation between European Languages']
['system', 'ROUGE-S*', 'Average_R:', '0.01524', '(95%-conf.int.', '0.01524', '-', '0.01524)']
['system', 'ROUGE-S*', 'Average_P:', '0.38182', '(95%-conf.int.', '0.38182', '-', '0.38182)']
['system', 'ROUGE-S*', 'Average_F:', '0.02931', '(95%-conf.int.', '0.02931', '-', '0.02931)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:55', 'F:21']
['We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.']
['This decreases the statistical significance of our results compared to those studies.', 'There may be occasionally a system clearly at the top or at the bottom  but most systems are so close that it is hard to distinguish them.', 'The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.', 'At the very least  we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.', 'Figure 1 provides some statistics about this corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00405', '(95%-conf.int.', '0.00405', '-', '0.00405)']
['system', 'ROUGE-S*', 'Average_P:', '0.03297', '(95%-conf.int.', '0.03297', '-', '0.03297)']
['system', 'ROUGE-S*', 'Average_F:', '0.00721', '(95%-conf.int.', '0.00721', '-', '0.00721)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:91', 'F:3']
['The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.']
['Annotators suggested that long sentences are almost impossible to judge.', 'In words  the judgements are normalized  so that the average normalized judgement per judge is 3.', 'For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.', 'The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).', 'For more on the participating systems  please refer to the respective system description in the proceedings of the workshop.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:45', 'F:0']
['The way judgements are collected, human judges tend to use the scores to rank systems against each other.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'However  a recent study (Callison-Burch et al.  2006)  pointed out that this correlation may not always be strong.', 'When dropping the top and bottom 2.5% the remaining BLEU scores define the range of the confidence interval.', 'Most of these groups follow a phrase-based statistical approach to machine translation.', 'A few annotators suggested to break up long sentences into clauses and evaluate these separately.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:966745', 'P:28', 'F:0']
0.0796845447301 0.0040718181448 0.00712181811707





input/ref/Task1/W99-0623_sweta.csv
input/res/Task1/W99-0623.csv
parsing: input/ref/Task1/W99-0623_sweta.csv
 <S sid="144" ssid="6">Combining multiple highly-accurate independent parsers yields promising results.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
 <S sid="125" ssid="54">The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["125'"]
125'
['125']
parsed_discourse_facet ['method_citation']
<S sid="125" ssid="54">The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["125'"]
125'
['125']
parsed_discourse_facet ['method_citation']
    <S sid="48" ssid="34">&#8226; Similarly, when the na&#239;ve Bayes classifier is configured such that the constituents require estimated probabilities strictly larger than 0.5 to be accepted, there is not enough probability mass remaining on crossing brackets for them to be included in the hypothesis.</S>
original cit marker offset is 0
new cit marker offset is 0



["48'"]
48'
['48']
parsed_discourse_facet ['method_citation']
 <S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="63">As seen by the drop in average individual parser performance baseline, the introduced parser does not perform very well.</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
    <S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="13" ssid="9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993).</S>
original cit marker offset is 0
new cit marker offset is 0



["13'"]
13'
['13']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="37">From this we see that a finer-grained model for parser combination, at least for the features we have examined, will not give us any additional power.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="98" ssid="27">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["98'"]
98'
['98']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="13">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'"]
27'
['27']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="9">For our experiments we also report the mean of precision and recall, which we denote by (P + R)I2 and F-measure.</S>
    <S sid="81" ssid="10">F-measure is the harmonic mean of precision and recall, 2PR/(P + R).</S>
    <S sid="82" ssid="11">It is closer to the smaller value of precision and recall when there is a large skew in their values.</S>
original cit marker offset is 0
new cit marker offset is 0



["80'", "'81'", "'82'"]
80'
'81'
'82'
['80', '81', '82']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="35">In general, the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["49'"]
49'
['49']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="7">Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["11'"]
11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="98" ssid="27">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["98'"]
98'
['98']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0623.csv
<S sid ="34" ssid = "20">Mi(c) is a binary function returning t when parser i (from among the k parsers) suggests constituent c should be in the parse.</S><S sid ="106" ssid = "35">In each figure the upper graph shows the isolated constituent precision and the bottom graph shows the corresponding number of hypothesized constituents.</S><S sid ="66" ssid = "52">Each decision determines the inclusion or exclusion of a candidate constituent.</S><S sid ="65" ssid = "51">We model each parse as the decisions made to create it  and model those decisions as independent events.</S><S sid ="2" ssid = "2">Two general approaches are presented and two combination techniques are described for each approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'106'", "'66'", "'65'", "'2'"]
'34'
'106'
'66'
'65'
'2'
['34', '106', '66', '65', '2']
Error in Discourse Facet
<S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'15'", "'130'", "'57'", "'50'"]
'18'
'15'
'130'
'57'
'50'
['18', '15', '130', '57', '50']
Error in Discourse Facet
<S sid ="122" ssid = "51">All of these systems were run on data that was not seen during their development.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="24" ssid = "10">We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="107" ssid = "36">Again we notice that the isolated constituent precision is larger than 0.5 only in those partitions that contain very few samples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'122'", "'11'", "'24'", "'61'", "'107'"]
'122'
'11'
'24'
'61'
'107'
['122', '11', '24', '61', '107']
Error in Discourse Facet
<S sid ="92" ssid = "21">While we cannot prove there are no such useful features on which one should condition trust  we can give some insight into why the features we explored offered no gain.</S><S sid ="111" ssid = "40">The first row represents the average accuracy of the three parsers we combine.</S><S sid ="104" ssid = "33">In the cases where isolated constituent precision is larger than 0.5 the affected portion of the hypotheses is negligible.</S><S sid ="42" ssid = "28">Call the crossing constituents A and B.</S><S sid ="117" ssid = "46">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'111'", "'104'", "'42'", "'117'"]
'92'
'111'
'104'
'42'
'117'
['92', '111', '104', '42', '117']
Error in Discourse Facet
<S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="39" ssid = "25">There are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.</S><S sid ="63" ssid = "49">The probabilistic version of this procedure is straightforward: We once again assume independence among our various member parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'139'", "'11'", "'39'", "'63'"]
'15'
'139'
'11'
'39'
'63'
['15', '139', '11', '39', '63']
Error in Discourse Facet
<S sid ="39" ssid = "25">There are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.</S><S sid ="86" ssid = "15">Finally we show the combining techniques degrade very little when a poor parser is added to the set.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a nave Bayes classifier to determine which constituents to include in the parse.</S><S sid ="17" ssid = "3">The substructures that are unanimously hypothesized by the parsers should be preserved after combination  and the combination technique should not foolishly create substructures for which there is no supporting evidence.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'86'", "'27'", "'17'", "'61'"]
'39'
'86'
'27'
'17'
'61'
['39', '86', '27', '17', '61']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="24" ssid = "10">We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'72'", "'21'", "'24'", "'139'"]
'112'
'72'
'21'
'24'
'139'
['112', '72', '21', '24', '139']
Error in Discourse Facet
<S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S><S sid ="96" ssid = "25">We call such a constituent an isolated constituent.</S><S sid ="132" ssid = "61">The results of this experiment can be seen in Table 5.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and nave Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="125" ssid = "54">The constituent voting and nave Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'139'", "'96'", "'132'", "'38'", "'125'"]
'139'
'96'
'132'
'38'
'125'
['139', '96', '132', '38', '125']
Error in Discourse Facet
<S sid ="43" ssid = "29">A receives a votes  and B receives b votes.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="84" ssid = "13">The first shows how constituent features and context do not help in deciding which parser to trust.</S><S sid ="93" ssid = "22">Because we are working with only three parsers  the only situation in which context will help us is when it can indicate we should choose to believe a single parser that disagrees with the majority hypothesis instead of the majority hypothesis itself.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'15'", "'52'", "'84'", "'93'"]
'43'
'15'
'52'
'84'
'93'
['43', '15', '52', '84', '93']
Error in Discourse Facet
<S sid ="101" ssid = "30">We show the results of three of the experiments we conducted to measure isolated constituent precision under various partitioning schemes.</S><S sid ="126" ssid = "55">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a nave Bayes classifier to determine which constituents to include in the parse.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="43" ssid = "29">A receives a votes  and B receives b votes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'126'", "'27'", "'52'", "'43'"]
'101'
'126'
'27'
'52'
'43'
['101', '126', '27', '52', '43']
Error in Discourse Facet
<S sid ="126" ssid = "55">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S><S sid ="142" ssid = "4">Both of the switching techniques  as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.</S><S sid ="111" ssid = "40">The first row represents the average accuracy of the three parsers we combine.</S><S sid ="43" ssid = "29">A receives a votes  and B receives b votes.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'142'", "'111'", "'43'", "'52'"]
'126'
'142'
'111'
'43'
'52'
['126', '142', '111', '43', '52']
Error in Discourse Facet
<S sid ="101" ssid = "30">We show the results of three of the experiments we conducted to measure isolated constituent precision under various partitioning schemes.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="106" ssid = "35">In each figure the upper graph shows the isolated constituent precision and the bottom graph shows the corresponding number of hypothesized constituents.</S><S sid ="22" ssid = "8">If enough parsers suggest that a particular constituent belongs in the parse  we include it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'50'", "'15'", "'106'", "'22'"]
'101'
'50'
'15'
'106'
'22'
['101', '50', '15', '106', '22']
Error in Discourse Facet
<S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S><S sid ="111" ssid = "40">The first row represents the average accuracy of the three parsers we combine.</S><S sid ="30" ssid = "16">This is equivalent to the assumption used in probability estimation for nave Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.</S><S sid ="28" ssid = "14">The development of a nave Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a nave Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'111'", "'30'", "'28'", "'27'"]
'50'
'111'
'30'
'28'
'27'
['50', '111', '30', '28', '27']
Error in Discourse Facet
<S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="140" ssid = "2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S><S sid ="131" ssid = "60">It was then tested on section 22 of the Treebank in conjunction with the other parsers.</S><S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="34" ssid = "20">Mi(c) is a binary function returning t when parser i (from among the k parsers) suggests constituent c should be in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'140'", "'131'", "'112'", "'34'"]
'61'
'140'
'131'
'112'
'34'
['61', '140', '131', '112', '34']
Error in Discourse Facet
<S sid ="87" ssid = "16">It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S><S sid ="37" ssid = "23">Here NO counts the number of hypothesized constituents in the development set that match the binary predicate specified as an argument.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="100" ssid = "29">When this metric is less than 0.5  we expect to incur more errors' than we will remove by adding those constituents to the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'", "'57'", "'37'", "'11'", "'100'"]
'87'
'57'
'37'
'11'
'100'
['87', '57', '37', '11', '100']
Error in Discourse Facet
<S sid ="83" ssid = "12">We performed three experiments to evaluate our techniques.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="108" ssid = "37">From this we see that a finer-grained model for parser combination  at least for the features we have examined  will not give us any additional power.</S><S sid ="107" ssid = "36">Again we notice that the isolated constituent precision is larger than 0.5 only in those partitions that contain very few samples.</S><S sid ="118" ssid = "47">The maximum precision oracle is an upper bound on the possible gain we can achieve by parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'11'", "'108'", "'107'", "'118'"]
'83'
'11'
'108'
'107'
'118'
['83', '11', '108', '107', '118']
Error in Discourse Facet
<S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="30" ssid = "16">This is equivalent to the assumption used in probability estimation for nave Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="113" ssid = "42">The next two rows are results of oracle experiments.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'30'", "'52'", "'113'", "'50'"]
'18'
'30'
'52'
'113'
'50'
['18', '30', '52', '113', '50']
Error in Discourse Facet
['Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.']
['These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.', u'This is equivalent to the assumption used in probability estimation for na\xc3\xafve Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.', 'There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.', 'The next two rows are results of oracle experiments.', 'This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.']
['system', 'ROUGE-S*', 'Average_R:', '0.00106', '(95%-conf.int.', '0.00106', '-', '0.00106)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00190', '(95%-conf.int.', '0.00190', '-', '0.00190)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:105', 'F:1']
['It is closer to the smaller value of precision and recall when there is a large skew in their values.', 'For our experiments we also report the mean of precision and recall, which we denote by (P + R)I2 and F-measure.', 'F-measure is the harmonic mean of precision and recall, 2PR/(P + R).']
['Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).', "When this metric is less than 0.5  we expect to incur more errors' than we will remove by adding those constituents to the parse.", 'The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.', 'Here NO counts the number of hypothesized constituents in the development set that match the binary predicate specified as an argument.', 'It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:171', 'F:0']
['We have presented two general approaches to studying parser combination: parser switching and parse hybridization.']
['In each figure the upper graph shows the isolated constituent precision and the bottom graph shows the corresponding number of hypothesized constituents.', 'We show the results of three of the experiments we conducted to measure isolated constituent precision under various partitioning schemes.', 'We are interested in combining the substructures of the input parses to produce a better parse.', 'There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.', 'If enough parsers suggest that a particular constituent belongs in the parse  we include it.']
['system', 'ROUGE-S*', 'Average_R:', '0.00303', '(95%-conf.int.', '0.00303', '-', '0.00303)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00580', '(95%-conf.int.', '0.00580', '-', '0.00580)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:45', 'F:3']
['In general, the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.']
['Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).', 'We performed three experiments to evaluate our techniques.', 'From this we see that a finer-grained model for parser combination  at least for the features we have examined  will not give us any additional power.', 'Again we notice that the isolated constituent precision is larger than 0.5 only in those partitions that contain very few samples.', 'The maximum precision oracle is an upper bound on the possible gain we can achieve by parse hybridization.']
['system', 'ROUGE-S*', 'Average_R:', '0.00082', '(95%-conf.int.', '0.00082', '-', '0.00082)']
['system', 'ROUGE-S*', 'Average_P:', '0.01515', '(95%-conf.int.', '0.01515', '-', '0.01515)']
['system', 'ROUGE-S*', 'Average_F:', '0.00155', '(95%-conf.int.', '0.00155', '-', '0.00155)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:66', 'F:1']
['The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.']
['These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.', 'The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.', 'The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.', 'We are interested in combining the substructures of the input parses to produce a better parse.', 'There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.']
['system', 'ROUGE-S*', 'Average_R:', '0.00332', '(95%-conf.int.', '0.00332', '-', '0.00332)']
['system', 'ROUGE-S*', 'Average_P:', '0.02857', '(95%-conf.int.', '0.02857', '-', '0.02857)']
['system', 'ROUGE-S*', 'Average_F:', '0.00595', '(95%-conf.int.', '0.00595', '-', '0.00595)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:105', 'F:3']
['Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.']
['We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.', 'We have presented two general approaches to studying parser combination: parser switching and parse hybridization.', 'The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.', "One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.", "The second row is the accuracy of the best of the three parsers.'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00870', '(95%-conf.int.', '0.00870', '-', '0.00870)']
['system', 'ROUGE-S*', 'Average_P:', '0.07500', '(95%-conf.int.', '0.07500', '-', '0.07500)']
['system', 'ROUGE-S*', 'Average_F:', '0.01558', '(95%-conf.int.', '0.01558', '-', '0.01558)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:120', 'F:9']
['The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.']
['The results of this experiment can be seen in Table 5.', 'We have presented two general approaches to studying parser combination: parser switching and parse hybridization.', u'Under certain conditions the constituent voting and na\xefve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.', u'The constituent voting and na\xefve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.', 'We call such a constituent an isolated constituent.']
['system', 'ROUGE-S*', 'Average_R:', '0.03330', '(95%-conf.int.', '0.03330', '-', '0.03330)']
['system', 'ROUGE-S*', 'Average_P:', '0.12000', '(95%-conf.int.', '0.12000', '-', '0.12000)']
['system', 'ROUGE-S*', 'Average_F:', '0.05214', '(95%-conf.int.', '0.05214', '-', '0.05214)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:300', 'F:36']
['The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.']
['Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).', 'We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.', 'All of these systems were run on data that was not seen during their development.', 'Again we notice that the isolated constituent precision is larger than 0.5 only in those partitions that contain very few samples.', 'We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.']
['system', 'ROUGE-S*', 'Average_R:', '0.00089', '(95%-conf.int.', '0.00089', '-', '0.00089)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00162', '(95%-conf.int.', '0.00162', '-', '0.00162)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:105', 'F:1']
['From this we see that a finer-grained model for parser combination, at least for the features we have examined, will not give us any additional power.']
['The first row represents the average accuracy of the three parsers we combine.', 'Both of the switching techniques  as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.', 'A receives a votes  and B receives b votes.', 'Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.', 'This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.']
['system', 'ROUGE-S*', 'Average_R:', '0.00142', '(95%-conf.int.', '0.00142', '-', '0.00142)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00267', '(95%-conf.int.', '0.00267', '-', '0.00267)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:45', 'F:1']
0.0385166662387 0.00583777771291 0.00968999989233





input/ref/Task1/P87-1015_swastika.csv
input/res/Task1/P87-1015.csv
parsing: input/ref/Task1/P87-1015_swastika.csv
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="149" ssid="34">We can obtain a letter equivalent CFL defined by a CFG in which the for each rule as above, we have the production A &#8212;* A1 Anup where tk (up) = cp.</S>
original cit marker offset is 0
new cit marker offset is 0



['149']
149
['149']
parsed_discourse_facet ['method_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
    <S sid="2" ssid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['2']
2
['2']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="222" ssid="28">However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



['222']
222
['222']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['???']
???
['???']
parsed_discourse_facet ['aim_citation']
<S sid="207" ssid="13">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['207']
207
['207']
parsed_discourse_facet ['result_citation']
<S sid="207" ssid="13">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['207']
207
['207']
parsed_discourse_facet ['result_citation']
<S sid="19" ssid="4">It can be easily shown from Thatcher's result that the path set of every local set is a regular set.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/P87-1015.csv
<S sid ="191" ssid = "76">In addition to the tapes required to store the indices  M requires one work tape for splitting the substrings.</S><S sid ="206" ssid = "12">As suggested in Section 4.3.2  a derivation with independent paths can be divided into subcomputations with limited sharing of information.</S><S sid ="186" ssid = "71">To do this  the x's and y's are stored in the next 2ni + 2n2 tapes  and M goes to a universal state.</S><S sid ="104" ssid = "10">Pumping t2 will change only one branch and leave the other branch unaffected.</S><S sid ="192" ssid = "77">Thus  the ATM has no more than 6km&quot; + 1 work tapes  where km&quot; is the maximum number of substrings spanned by a derived structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'191'", "'206'", "'186'", "'104'", "'192'"]
'191'
'206'
'186'
'104'
'192'
['191', '206', '186', '104', '192']
parsed_discourse_facet ['implication_citation']
<S sid ="84" ssid = "69">((fii  Q2  Pa)    (01  i32  03)   e) (02 e) Oa  en) The path complexity of the tee set generated by a MCTAG is not necessarily context-free.</S><S sid ="54" ssid = "39">An IG can be viewed as a CFG in which each nonterminal is associated with a stack.</S><S sid ="101" ssid = "7">The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently.</S><S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="71" ssid = "56">0n0'i'i0'2&quot;bin242bn I n = 711 + n2 } On the other hand  no linguistic use is made of this general form of composition and Steedman (personal communication) and Steedman (1986) argues that a more limited definition of composition is more natural.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'54'", "'101'", "'59'", "'71'"]
'84'
'54'
'101'
'59'
'71'
['84', '54', '101', '59', '71']
parsed_discourse_facet ['implication_citation']
<S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="44" ssid = "29">The edge from the root to the subtree for the derivation of 7i is labeled by the address ni.</S><S sid ="136" ssid = "21">These two restrictions impose the constraint that the result of composing any two structures should be a structure whose &quot;size&quot; is the sum of its constituents plus some constant For example  the operation 4  discussed in the case of CFG's (in Section 4.1) adds the constant equal to the sum of the length of the strings VI  un+r Since we are considering formalisms with arbitrary structures it is difficult to precisely specify all of the restrictions on the composition operations that we believe would appropriately generalize the concatenation operation for the particular structures used by the formalism.</S><S sid ="28" ssid = "13">A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'44'", "'136'", "'28'", "'153'"]
'151'
'44'
'136'
'28'
'153'
['151', '44', '136', '28', '153']
parsed_discourse_facet ['results_citation']
<S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="134" ssid = "19">These systems are similar to those described by Pollard (1984) as Generalized Context-Free Grammars (GCFG's).</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="112" ssid = "18">.t The path set of tree sets at level k +1 have the complexity of the string language of level k. The independence of paths in a tree set appears to be an important property.</S><S sid ="20" ssid = "5">As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'134'", "'153'", "'112'", "'20'"]
'59'
'134'
'153'
'112'
'20'
['59', '134', '153', '112', '20']
parsed_discourse_facet ['method_citation']
<S sid ="106" ssid = "12">We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths.</S><S sid ="54" ssid = "39">An IG can be viewed as a CFG in which each nonterminal is associated with a stack.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="101" ssid = "7">The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently.</S><S sid ="182" ssid = "67">Since each zi is a contiguous substring of the input (say ai )  and no two substrings overlap  we can represent zi by the pair of integers (i2  i2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'54'", "'143'", "'101'", "'182'"]
'106'
'54'
'143'
'101'
'182'
['106', '54', '143', '101', '182']
parsed_discourse_facet ['method_citation']
<S sid ="55" ssid = "40">Each production can push or pop symbols on the stack as can be seen in the following productions that generate tree of the form shown in Figure 4b.</S><S sid ="117" ssid = "2">Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.</S><S sid ="200" ssid = "6">The importance of this property becomes clear in contrasting theories underlying GPSG (Gazdar  Klein  Pulluna  and Sag  1985)  and GB (as described by Berwick  1984) with those underlying LFG and FUG.</S><S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="106" ssid = "12">We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'117'", "'200'", "'59'", "'106'"]
'55'
'117'
'200'
'59'
'106'
['55', '117', '200', '59', '106']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "17">When 7' is adjoined at ?I in the tree 7 we obtain a tree v&quot;.</S><S sid ="121" ssid = "6">First  any grammar must involve a finite number of elementary structures  composed using a finite number of composition operations.</S><S sid ="19" ssid = "4">It can be easily shown from Thatcher's result that the path set of every local set is a regular set.</S><S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="138" ssid = "23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'121'", "'19'", "'151'", "'138'"]
'32'
'121'
'19'
'151'
'138'
['32', '121', '19', '151', '138']
parsed_discourse_facet ['method_citation']
<S sid ="188" ssid = "73">Thus  for example  one successor process will be have M to be in the existential state qa with the indices encoding xi     xn  in the first 2n i tapes.</S><S sid ="118" ssid = "3">In the remainder of the paper  we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S><S sid ="89" ssid = "74">Hence  trees shown in Figure 8 can not be generated by any MCTAG (but can be generated by an IG) because the number of pairs of dependent paths grows with n. Since the derivation tees of TAG's  MCTAG's  and HG's are local sets  the choice of the structure used at each point in a derivation in these systems does not depend on the context at that point within the derivation.</S><S sid ="96" ssid = "2">A tree set may be said to have dependencies between paths if some &quot;appropriate&quot; subset can be shown to have dependent paths as defined above.</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'188'", "'118'", "'89'", "'96'", "'227'"]
'188'
'118'
'89'
'96'
'227'
['188', '118', '89', '96', '227']
parsed_discourse_facet ['method_citation']
<S sid ="84" ssid = "69">((fii  Q2  Pa)    (01  i32  03)   e) (02 e) Oa  en) The path complexity of the tee set generated by a MCTAG is not necessarily context-free.</S><S sid ="98" ssid = "4">Thatcher (1973) describes a tee pumping lemma for recognizable sets related to the string pumping lemma for regular sets.</S><S sid ="12" ssid = "10">We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.</S><S sid ="179" ssid = "64">It can be seen that M performs a top-down recognition of the input al ... nin logspace.</S><S sid ="185" ssid = "70">Each spawned process must check if xi     xn  and   yn  can be derived from B and C  respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'98'", "'12'", "'179'", "'185'"]
'84'
'98'
'12'
'179'
'185'
['84', '98', '12', '179', '185']
parsed_discourse_facet ['method_citation']
<S sid ="207" ssid = "13">We outlined the definition of a family of constrained grammatical formalisms  called Linear Context-Free Rewriting Systems.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="26" ssid = "11">The productions of HG's are very similar to those of CFG's except that the operation used must be made explicit.</S><S sid ="178" ssid = "63">We define an ATM  M  recognizing a language generated by a grammar  G  having the properties discussed in Section 43.</S><S sid ="155" ssid = "40">CFG's  TAG's  MCTAG's and HG's are all members of this class since they satisfy these restrictions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'153'", "'26'", "'178'", "'155'"]
'207'
'153'
'26'
'178'
'155'
['207', '153', '26', '178', '155']
parsed_discourse_facet ['implication_citation']
<S sid ="38" ssid = "23">Thus  the derivation trees for TAG's have the same structure as local sets.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="165" ssid = "50">This class of formalisms have the properties that their derivation trees are local sets  and manipulate objects  using a finite number of composition operations that use a finite number of symbols.</S><S sid ="62" ssid = "47">TAG's can be shown to be equivalent to this restricted system.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'153'", "'165'", "'62'", "'143'"]
'38'
'153'
'165'
'62'
'143'
['38', '153', '165', '62', '143']
parsed_discourse_facet ['method_citation']
<S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="132" ssid = "17">In TAG's the elementary tree and addresses where adjunction takes place are used to instantiate the operation.</S><S sid ="101" ssid = "7">The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently.</S><S sid ="96" ssid = "2">A tree set may be said to have dependencies between paths if some &quot;appropriate&quot; subset can be shown to have dependent paths as defined above.</S><S sid ="94" ssid = "79">The semilinearity of Tree Adjoining Languages (TAL's)  MCTAL's  and Head Languages (HL's) can be proved using this property  with suitable restrictions on the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'132'", "'101'", "'96'", "'94'"]
'143'
'132'
'101'
'96'
'94'
['143', '132', '101', '96', '94']
parsed_discourse_facet ['method_citation']
<S sid ="89" ssid = "74">Hence  trees shown in Figure 8 can not be generated by any MCTAG (but can be generated by an IG) because the number of pairs of dependent paths grows with n. Since the derivation tees of TAG's  MCTAG's  and HG's are local sets  the choice of the structure used at each point in a derivation in these systems does not depend on the context at that point within the derivation.</S><S sid ="164" ssid = "49">Although embedding this version of LCFRS's in the framework of ILFP developed by Rounds (1985) is straightforward  our motivation was to capture properties shared by a family of grammatical systems and generalize them defining a class of related formalisms.</S><S sid ="17" ssid = "2">We define the path set of a tree 1 as the set of strings that label a path from the root to frontier of 7.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="26" ssid = "11">The productions of HG's are very similar to those of CFG's except that the operation used must be made explicit.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'164'", "'17'", "'143'", "'26'"]
'89'
'164'
'17'
'143'
'26'
['89', '164', '17', '143', '26']
parsed_discourse_facet ['method_citation']
<S sid ="6" ssid = "4">For example  Gazdar (1985) discusses the applicability of Indexed Grammars (IG's) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar (LFG) and Government and Bindings grammars (GB).</S><S sid ="91" ssid = "76">We characterize a class of formalisms that have this property in Section 4.</S><S sid ="68" ssid = "53">This kind of dependency arises from the use of the composition operation to compose two arbitrarily large categories.</S><S sid ="143" ssid = "28">The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.</S><S sid ="189" ssid = "74">For rules p : A fpo such that fp is constant function  giving an elementary structure  fp is defined such that fp() = (Si ... xi() where each z is a constant string.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'91'", "'68'", "'143'", "'189'"]
'6'
'91'
'68'
'143'
'189'
['6', '91', '68', '143', '189']
parsed_discourse_facet ['results_citation']
<S sid ="151" ssid = "36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S sid ="90" ssid = "75">Thus  as in CFG's  at any point in the derivation  the set of structures that can be applied is determined only by a finite set of rules encapsulated by the grammar.</S><S sid ="185" ssid = "70">Each spawned process must check if xi     xn  and   yn  can be derived from B and C  respectively.</S><S sid ="141" ssid = "26">Thus  the length of any string in L is a linear combination of the length of strings in some fixed finite subset of L  and thus L is said to have the constant growth property.</S><S sid ="158" ssid = "43">We can represent any derived tree of a TAG by the two substrings that appear in its frontier  and then define how the adjunction operation concatenates the substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'90'", "'185'", "'141'", "'158'"]
'151'
'90'
'185'
'141'
'158'
['151', '90', '185', '141', '158']
parsed_discourse_facet ['implication_citation']
<S sid ="156" ssid = "41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S><S sid ="106" ssid = "12">We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths.</S><S sid ="32" ssid = "17">When 7' is adjoined at ?I in the tree 7 we obtain a tree v&quot;.</S><S sid ="12" ssid = "10">We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.</S><S sid ="55" ssid = "40">Each production can push or pop symbols on the stack as can be seen in the following productions that generate tree of the form shown in Figure 4b.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'", "'106'", "'32'", "'12'", "'55'"]
'156'
'106'
'32'
'12'
'55'
['156', '106', '32', '12', '55']
parsed_discourse_facet ['method_citation']
['In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.']
["It can be easily shown from Thatcher's result that the path set of every local set is a regular set.", "When 7' is adjoined at ?I in the tree 7 we obtain a tree v&quot;.", "We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).", "We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.", 'First  any grammar must involve a finite number of elementary structures  composed using a finite number of composition operations.']
['system', 'ROUGE-S*', 'Average_R:', '0.02976', '(95%-conf.int.', '0.02976', '-', '0.02976)']
['system', 'ROUGE-S*', 'Average_P:', '0.03700', '(95%-conf.int.', '0.03700', '-', '0.03700)']
['system', 'ROUGE-S*', 'Average_F:', '0.03299', '(95%-conf.int.', '0.03299', '-', '0.03299)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:946', 'F:35']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
["To do this  the x's and y's are stored in the next 2ni + 2n2 tapes  and M goes to a universal state.", 'Thus  the ATM has no more than 6km&quot; + 1 work tapes  where km&quot; is the maximum number of substrings spanned by a derived structure.', 'In addition to the tapes required to store the indices  M requires one work tape for splitting the substrings.', 'Pumping t2 will change only one branch and leave the other branch unaffected.', 'As suggested in Section 4.3.2  a derivation with independent paths can be divided into subcomputations with limited sharing of information.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:120', 'F:0']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
['An IG can be viewed as a CFG in which each nonterminal is associated with a stack.', "The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently.", 'Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.', "0n0'i'i0'2&quot;bin242bn I n = 711 + n2 } On the other hand  no linguistic use is made of this general form of composition and Steedman (personal communication) and Steedman (1986) argues that a more limited definition of composition is more natural.", u'((fii  Q2  Pa)   \u2014\u25a0 (01  i32  03)   e) (02 e) Oa  en) The path complexity of the tee set generated by a MCTAG is not necessarily context-free.']
['system', 'ROUGE-S*', 'Average_R:', '0.00036', '(95%-conf.int.', '0.00036', '-', '0.00036)']
['system', 'ROUGE-S*', 'Average_P:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00069', '(95%-conf.int.', '0.00069', '-', '0.00069)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:120', 'F:1']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
['Each production can push or pop symbols on the stack as can be seen in the following productions that generate tree of the form shown in Figure 4b.', "When 7' is adjoined at ?I in the tree 7 we obtain a tree v&quot;.", 'We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.', "We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths.", "Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings."]
['system', 'ROUGE-S*', 'Average_R:', '0.00256', '(95%-conf.int.', '0.00256', '-', '0.00256)']
['system', 'ROUGE-S*', 'Average_P:', '0.04167', '(95%-conf.int.', '0.04167', '-', '0.04167)']
['system', 'ROUGE-S*', 'Average_F:', '0.00482', '(95%-conf.int.', '0.00482', '-', '0.00482)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:120', 'F:5']
["In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs."]
["We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).", "Thus  as in CFG's  at any point in the derivation  the set of structures that can be applied is determined only by a finite set of rules encapsulated by the grammar.", 'We can represent any derived tree of a TAG by the two substrings that appear in its frontier  and then define how the adjunction operation concatenates the substrings.', 'Thus  the length of any string in L is a linear combination of the length of strings in some fixed finite subset of L  and thus L is said to have the constant growth property.', 'Each spawned process must check if xi     xn  and   yn  can be derived from B and C  respectively.']
['system', 'ROUGE-S*', 'Average_R:', '0.01224', '(95%-conf.int.', '0.01224', '-', '0.01224)']
['system', 'ROUGE-S*', 'Average_P:', '0.22727', '(95%-conf.int.', '0.22727', '-', '0.22727)']
['system', 'ROUGE-S*', 'Average_F:', '0.02324', '(95%-conf.int.', '0.02324', '-', '0.02324)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:66', 'F:15']
["It can be easily shown from Thatcher's result that the path set of every local set is a regular set."]
['We characterize a class of formalisms that have this property in Section 4.', 'This kind of dependency arises from the use of the composition operation to compose two arbitrarily large categories.', 'For rules p : A fpo such that fp is constant function  giving an elementary structure  fp is defined such that fp() = (Si ... xi() where each z is a constant string.', 'The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.', "For example  Gazdar (1985) discusses the applicability of Indexed Grammars (IG's) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar (LFG) and Government and Bindings grammars (GB)."]
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:45', 'F:0']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
['Each production can push or pop symbols on the stack as can be seen in the following productions that generate tree of the form shown in Figure 4b.', 'The importance of this property becomes clear in contrasting theories underlying GPSG (Gazdar  Klein  Pulluna  and Sag  1985)  and GB (as described by Berwick  1984) with those underlying LFG and FUG.', 'Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.', 'Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.', "We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths."]
['system', 'ROUGE-S*', 'Average_R:', '0.00196', '(95%-conf.int.', '0.00196', '-', '0.00196)']
['system', 'ROUGE-S*', 'Average_P:', '0.04167', '(95%-conf.int.', '0.04167', '-', '0.04167)']
['system', 'ROUGE-S*', 'Average_F:', '0.00374', '(95%-conf.int.', '0.00374', '-', '0.00374)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:120', 'F:5']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
['This class of formalisms have the properties that their derivation trees are local sets  and manipulate objects  using a finite number of composition operations that use a finite number of symbols.', 'In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.', "Thus  the derivation trees for TAG's have the same structure as local sets.", "TAG's can be shown to be equivalent to this restricted system.", 'The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.']
['system', 'ROUGE-S*', 'Average_R:', '0.00603', '(95%-conf.int.', '0.00603', '-', '0.00603)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01107', '(95%-conf.int.', '0.01107', '-', '0.01107)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:120', 'F:8']
['We can obtain a letter equivalent CFL defined by a CFG in which the for each rule as above, we have the production A &#8212;* A1 Anup where tk (up) = cp.']
["We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).", 'The edge from the root to the subtree for the derivation of 7i is labeled by the address ni.', 'A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.', u"These two restrictions impose the constraint that the result of composing any two structures should be a structure whose &quot;size&quot; is the sum of its constituents plus some constant For example  the operation 4  discussed in the case of CFG's (in Section 4.1) adds the constant equal to the sum of the length of the strings VI  un+r\xe2\u20ac\xa2 Since we are considering formalisms with arbitrary structures it is difficult to precisely specify all of the restrictions on the composition operations that we believe would appropriately generalize the concatenation operation for the particular structures used by the formalism.", 'In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3655', 'P:78', 'F:0']
['However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.']
['Thatcher (1973) describes a tee pumping lemma for recognizable sets related to the string pumping lemma for regular sets.', 'We are very grateful to Tony Kroc.h  Michael Pails  Sunil Shende  and Mark Steedman for valuable discussions. formalisms.', 'It can be seen that M performs a top-down recognition of the input al ... nin logspace.', u'((fii  Q2  Pa)   \u2014\u25a0 (01  i32  03)   e) (02 e) Oa  en) The path complexity of the tee set generated by a MCTAG is not necessarily context-free.', 'Each spawned process must check if xi     xn  and   yn  can be derived from B and C  respectively.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:190', 'F:0']
0.0422609995774 0.00529099994709 0.00765499992345





input/ref/Task1/P04-1036_swastika.csv
input/res/Task1/P04-1036.csv
parsing: input/ref/Task1/P04-1036_swastika.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="24">We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.</S>
original cit marker offset is 0
new cit marker offset is 0



['68']
68
['68']
parsed_discourse_facet ['result_citation']
<S sid="83" ssid="12">The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.</S>
original cit marker offset is 0
new cit marker offset is 0



['83']
83
['83']
parsed_discourse_facet ['result_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
<S sid="101" ssid="30">Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.</S>
original cit marker offset is 0
new cit marker offset is 0



['101']
101
['101']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="83" ssid="12">The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.</S>
original cit marker offset is 0
new cit marker offset is 0



['83']
83
['83']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="46" ssid="2">This provides the nearest neighbours to each target word, along with the distributional similarity score between the target word and its neighbour.</S>
original cit marker offset is 0
new cit marker offset is 0



['46']
46
['46']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents, rather than a lexical sample task, where the target words are manually determined and the results will depend on the skew of the words in the sample.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
<S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['method_citation']
<S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="98" ssid = "27">This seems intuitive given our expected relative usage of these senses in modern British English.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="27" ssid = "20">We use WordNet as our sense inventory for this work.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'98'", "'124'", "'5'", "'27'", "'65'"]
'98'
'124'
'5'
'27'
'65'
['98', '124', '5', '27', '65']
parsed_discourse_facet ['implication_citation']
<S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="109" ssid = "7">We generated a thesaurus entry for all polysemous nouns in WordNet as described in section 2.1 above.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S><S sid ="132" ssid = "9">The FINANCE corpus consists of 117734 documents (about 32.5 million words).</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'109'", "'95'", "'55'", "'132'"]
'124'
'109'
'95'
'55'
'132'
['124', '109', '95', '55', '132']
parsed_discourse_facet ['implication_citation']
<S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="32" ssid = "25">We describe some related work in section 6 and conclude in section 7. are therefore investigating a method of automatically ranking WordNet senses from raw text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'137'", "'5'", "'110'", "'32'"]
'44'
'137'
'5'
'110'
'32'
['44', '137', '5', '110', '32']
parsed_discourse_facet ['results_citation']
<S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="96" ssid = "25">Another example where the ranking is intuitive  is soil.</S><S sid ="33" ssid = "26">Many researchers are developing thesauruses from automatically parsed data.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'95'", "'96'", "'33'", "'44'"]
'124'
'95'
'96'
'33'
'44'
['124', '95', '96', '33', '44']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "5">To disambiguate senses a system should take context into account.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="67" ssid = "23">Each 2We use this version of WordNet since it allows us to map information to WordNets of other languages more accurately.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'124'", "'153'", "'147'", "'67'"]
'107'
'124'
'153'
'147'
'67'
['107', '124', '153', '147', '67']
parsed_discourse_facet ['method_citation']
<S sid ="94" ssid = "23">This seems quite reasonable given the nearest neighbours: tube  cable  wire  tank  hole  cylinder  fitting  tap  cistern  plate....</S><S sid ="162" ssid = "10">It only requires raw text from the given domain and because of this it can easily be applied to a new domain  or sense inventory  given sufficient text.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="99" ssid = "28">Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'", "'162'", "'60'", "'95'", "'99'"]
'94'
'162'
'60'
'95'
'99'
['94', '162', '60', '95', '99']
parsed_discourse_facet ['method_citation']
<S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="101" ssid = "30">Thus  if we used the sense ranking as a heuristic for an all nouns task we would expect to get precision in the region of 60%.</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'165'", "'101'", "'169'", "'108'", "'44'"]
'165'
'101'
'169'
'108'
'44'
['165', '101', '169', '108', '44']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="127" ssid = "4">We chose the domains of SPORTS and FINANCE since there is sufficient material for these domains in this publically available corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'60'", "'169'", "'91'", "'127'"]
'136'
'60'
'169'
'91'
'127'
['136', '60', '169', '91', '127']
parsed_discourse_facet ['method_citation']
<S sid ="193" ssid = "2">This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.</S><S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="151" ssid = "28">This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="167" ssid = "15">In this work the lists of neighbours are themselves clustered to bring out the various senses of the word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'193'", "'136'", "'151'", "'123'", "'167'"]
'193'
'136'
'151'
'123'
'167'
['193', '136', '151', '123', '167']
parsed_discourse_facet ['method_citation']
<S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="63" ssid = "19">We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline  but because of space limitations give results for the two which perform the best.</S>
original cit marker offset is 0
new cit marker offset is 0



["'173'", "'5'", "'108'", "'95'", "'63'"]
'173'
'5'
'108'
'95'
'63'
['173', '5', '108', '95', '63']
parsed_discourse_facet ['implication_citation']
<S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="76" ssid = "5">The jcn measure uses corpus data for the calculation of IC.</S><S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'184'", "'110'", "'60'", "'76'", "'55'"]
'184'
'110'
'60'
'76'
'55'
['184', '110', '60', '76', '55']
parsed_discourse_facet ['method_citation']
<S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="97" ssid = "26">The first ranked sense according to SemCor is the filth  stain: state of being unclean sense whereas the automatic ranking lists dirt  ground  earth as the first sense  which is the second ranked sense according to SemCor.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'123'", "'169'", "'147'", "'97'", "'8'"]
'123'
'169'
'147'
'97'
'8'
['123', '169', '147', '97', '8']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="28" ssid = "21">The paper is structured as follows.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of nearest neighbours.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'28'", "'124'", "'60'", "'34'"]
'61'
'28'
'124'
'60'
'34'
['61', '28', '124', '60', '34']
parsed_discourse_facet ['method_citation']
<S sid ="163" ssid = "11">Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="9" ssid = "2">This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).</S><S sid ="0" ssid = "0">Finding Predominant Word Senses in Untagged Text</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'124'", "'9'", "'0'", "'142'"]
'163'
'124'
'9'
'0'
'142'
['163', '124', '9', '0', '142']
parsed_discourse_facet ['results_citation']
<S sid ="138" ssid = "15">The SFC contains an economy label and a sports label.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of nearest neighbours.</S><S sid ="22" ssid = "15">More importantly  when working within a specific domain one would wish to tune the first sense heuristic to the domain at hand.</S><S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="92" ssid = "21">For example  in WordNet the first listed sense ofpipe is tobacco pipe  and this is ranked joint first according to the Brown files in SemCor with the second sense tube made of metal or plastic used to carry water  oil or gas etc....</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'", "'34'", "'22'", "'184'", "'92'"]
'138'
'34'
'22'
'184'
'92'
['138', '34', '22', '184', '92']
parsed_discourse_facet ['implication_citation']
<S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="185" ssid = "8">In the future  we will perform a large scale evaluation on domain specific corpora.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'173'", "'95'", "'185'", "'65'"]
'108'
'173'
'95'
'185'
'65'
['108', '173', '95', '185', '65']
parsed_discourse_facet ['method_citation']
<S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="151" ssid = "28">This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'173'", "'110'", "'151'", "'95'", "'5'"]
'173'
'110'
'151'
'95'
'5'
['173', '110', '151', '95', '5']
parsed_discourse_facet ['results_citation']
['We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.']
['The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.', 'We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within', 'Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.', 'We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.', 'We describe some related work in section 6 and conclude in section 7. are therefore investigating a method of automatically ranking WordNet senses from raw text.']
['system', 'ROUGE-S*', 'Average_R:', '0.00543', '(95%-conf.int.', '0.00543', '-', '0.00543)']
['system', 'ROUGE-S*', 'Average_P:', '0.11429', '(95%-conf.int.', '0.11429', '-', '0.11429)']
['system', 'ROUGE-S*', 'Average_F:', '0.01036', '(95%-conf.int.', '0.01036', '-', '0.01036)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:105', 'F:12']
['The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.']
['Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.', 'However  it is important to know the performance of this heuristic for any systems that use it.', 'It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.', 'The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.', 'In the future  we will perform a large scale evaluation on domain specific corpora.']
['system', 'ROUGE-S*', 'Average_R:', '0.00426', '(95%-conf.int.', '0.00426', '-', '0.00426)']
['system', 'ROUGE-S*', 'Average_P:', '0.18182', '(95%-conf.int.', '0.18182', '-', '0.18182)']
['system', 'ROUGE-S*', 'Average_F:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:55', 'F:10']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .', 'This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.', 'We chose the domains of SPORTS and FINANCE since there is sufficient material for these domains in this publically available corpus.', 'The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.', 'This method obtains precision of 61% and recall 51%.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:66', 'F:0']
['We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents, rather than a lexical sample task, where the target words are manually determined and the results will depend on the skew of the words in the sample.']
['This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).', 'Finding Predominant Word Senses in Untagged Text', 'A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.', 'The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.', 'Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD.']
['system', 'ROUGE-S*', 'Average_R:', '0.01485', '(95%-conf.int.', '0.01485', '-', '0.01485)']
['system', 'ROUGE-S*', 'Average_P:', '0.13810', '(95%-conf.int.', '0.13810', '-', '0.13810)']
['system', 'ROUGE-S*', 'Average_F:', '0.02681', '(95%-conf.int.', '0.02681', '-', '0.02681)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:210', 'F:29']
['The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.']
['For example  in WordNet the first listed sense ofpipe is tobacco pipe  and this is ranked joint first according to the Brown files in SemCor with the second sense tube made of metal or plastic used to carry water  oil or gas etc....', 'The SFC contains an economy label and a sports label.', 'We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.', 'More importantly  when working within a specific domain one would wish to tune the first sense heuristic to the domain at hand.', u'In these each target word is entered with an ordered list of \u201cnearest neighbours\u201d.']
['system', 'ROUGE-S*', 'Average_R:', '0.00349', '(95%-conf.int.', '0.00349', '-', '0.00349)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.00673', '(95%-conf.int.', '0.00673', '-', '0.00673)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:55', 'F:5']
['Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.']
['If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .', 'Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.', 'Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.', 'This seems quite reasonable given the nearest neighbours: tube  cable  wire  tank  hole  cylinder  fitting  tap  cistern  plate....', 'It only requires raw text from the given domain and because of this it can easily be applied to a new domain  or sense inventory  given sufficient text.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:55', 'F:0']
['The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.']
['This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.', 'In this work the lists of neighbours are themselves clustered to bring out the various senses of the word.', 'This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.', 'The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.', 'This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.']
['system', 'ROUGE-S*', 'Average_R:', '0.00088', '(95%-conf.int.', '0.00088', '-', '0.00088)']
['system', 'ROUGE-S*', 'Average_P:', '0.01170', '(95%-conf.int.', '0.01170', '-', '0.01170)']
['system', 'ROUGE-S*', 'Average_F:', '0.00163', '(95%-conf.int.', '0.00163', '-', '0.00163)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:171', 'F:2']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.', 'Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.', 'The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.', 'It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.', 'We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:66', 'F:0']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.', 'We use the WordNet Similarity Package 0.05 and WordNet version 1.6.', 'The paper is structured as follows.', 'If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .', u'In these each target word is entered with an ordered list of \u201cnearest neighbours\u201d.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:66', 'F:0']
['We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.']
['The word share is among the words whose predominant sense remained the same for all three corpora.', 'This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.', 'This method obtains precision of 61% and recall 51%.', 'The first ranked sense according to SemCor is the filth  stain: state of being unclean sense whereas the automatic ranking lists dirt  ground  earth as the first sense  which is the second ranked sense according to SemCor.', 'The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['system', 'ROUGE-S*', 'Average_R:', '0.00404', '(95%-conf.int.', '0.00404', '-', '0.00404)']
['system', 'ROUGE-S*', 'Average_P:', '0.21429', '(95%-conf.int.', '0.21429', '-', '0.21429)']
['system', 'ROUGE-S*', 'Average_F:', '0.00793', '(95%-conf.int.', '0.00793', '-', '0.00793)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:28', 'F:6']
0.0751109992489 0.00329499996705 0.00617899993821





input/ref/Task1/W06-2932_vardha.csv
input/res/Task1/W06-2932.csv
parsing: input/ref/Task1/W06-2932_vardha.csv
    <S sid="19" ssid="1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="5">However, in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="76" ssid="14">For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'"]
'76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="54" ssid="2">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'"]
'54'
['54']
parsed_discourse_facet ['method_citation']
    <S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="58" ssid="6">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'"]
'58'
['58']
parsed_discourse_facet ['method_citation']
    <S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'"]
'22'
['22']
parsed_discourse_facet ['method_citation']
    <S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
    <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'"]
'64'
['64']
parsed_discourse_facet ['method_citation']
    <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'"]
'43'
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S><S sid ="6" ssid = "2">With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'cuk  1988).</S><S sid ="71" ssid = "9">Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S>
original cit marker offset is nan
new cit marker offset is 0



["'64'", "'6'", "'7'", "'71'", "'2'"]
'64'
'6'
'7'
'71'
'2'
['64', '6', '7', '71', '2']
parsed_discourse_facet ['implication_citation']
<S sid ="71" ssid = "9">Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'cuk  1988).</S><S sid ="78" ssid = "16">Even with this improvement  the labeling of verb dependents remains the highest source of error.</S><S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'71'", "'20'", "'7'", "'78'", "'61'"]
'71'
'20'
'7'
'78'
'61'
['71', '20', '7', '78', '61']
parsed_discourse_facet ['implication_citation']
<S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="92" ssid = "14">For example  in the test sentence Lo que decia Mae West de si misma podriamos decirlo tambien los hombres:...  decias head is given as decirlo  although the main verbs of relative clauses are normally dependent on what the relative modifies  in this case the article Lo.</S><S sid ="28" ssid = "10">Consider a proposed dependency of a dependent xj on the head xi  each with morphological features Mj and Mi respectively.</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'92'", "'28'", "'88'", "'10'"]
'20'
'92'
'28'
'88'
'10'
['20', '92', '28', '88', '10']
parsed_discourse_facet ['results_citation']
<S sid ="13" ssid = "9">We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajic et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; Bohmova et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S><S sid ="30" ssid = "12">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'36'", "'64'", "'30'", "'72'"]
'13'
'36'
'64'
'30'
'72'
['13', '36', '64', '30', '72']
parsed_discourse_facet ['method_citation']
<S sid ="109" ssid = "6">The current system simply includes all morphological bi-gram features.</S><S sid ="60" ssid = "8">Furthermore  these results show that a twostage system can achieve a relatively high performance.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S><S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="101" ssid = "23">For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'60'", "'107'", "'86'", "'101'"]
'109'
'60'
'107'
'86'
'101'
['109', '60', '107', '86', '101']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="101" ssid = "23">For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.</S><S sid ="100" ssid = "22">The fact that Arabic has only 1500 training instances might also be problematic.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'72'", "'101'", "'100'", "'64'"]
'86'
'72'
'101'
'100'
'64'
['86', '72', '101', '100', '64']
parsed_discourse_facet ['method_citation']
<S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S><S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216220  New York City  June 2006. c2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="30" ssid = "12">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.</S><S sid ="82" ssid = "4">These weaknesses are not surprising  since these decisions encode the more global aspects of sentence structure: arrangement of clauses and adverbial dependents in multi-clause sentences  and prepositional phrase attachment.</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'66'", "'18'", "'30'", "'82'", "'41'"]
'66'
'18'
'30'
'82'
'41'
['66', '18', '30', '82', '41']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="51" ssid = "20">Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.</S><S sid ="91" ssid = "13">In doing this preliminary analysis  we noticed some inconsistencies in the reference dependency structures.</S><S sid ="60" ssid = "8">Furthermore  these results show that a twostage system can achieve a relatively high performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'72'", "'51'", "'91'", "'60'"]
'107'
'72'
'51'
'91'
'60'
['107', '72', '51', '91', '60']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="90" ssid = "12">We need to look more carefully at verb features that may be useful here  in particular features that distinguish finite and non-finite forms.</S><S sid ="69" ssid = "7">However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).</S><S sid ="71" ssid = "9">Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'72'", "'90'", "'69'", "'71'"]
'86'
'72'
'90'
'69'
'71'
['86', '72', '90', '69', '71']
parsed_discourse_facet ['method_citation']
<S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216220  New York City  June 2006. c2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="93" ssid = "15">A quick look at unlabeled attachment accuracies indicate that errors in Arabic parsing are the most common across all languages: prepositions (62%)  conjunctions (69%) and to a lesser extent verbs (73%).</S><S sid ="100" ssid = "22">The fact that Arabic has only 1500 training instances might also be problematic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'", "'18'", "'86'", "'93'", "'100'"]
'72'
'18'
'86'
'93'
'100'
['72', '18', '86', '93', '100']
parsed_discourse_facet ['implication_citation']
<S sid ="109" ssid = "6">The current system simply includes all morphological bi-gram features.</S><S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S><S sid ="0" ssid = "0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S sid ="48" ssid = "17">Is this the left/rightmost dependent for the head?</S><S sid ="93" ssid = "15">A quick look at unlabeled attachment accuracies indicate that errors in Arabic parsing are the most common across all languages: prepositions (62%)  conjunctions (69%) and to a lesser extent verbs (73%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'61'", "'0'", "'48'", "'93'"]
'109'
'61'
'0'
'48'
'93'
['109', '61', '0', '48', '93']
parsed_discourse_facet ['method_citation']
<S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S><S sid ="82" ssid = "4">These weaknesses are not surprising  since these decisions encode the more global aspects of sentence structure: arrangement of clauses and adverbial dependents in multi-clause sentences  and prepositional phrase attachment.</S><S sid ="9" ssid = "5">Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.</S><S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S><S sid ="100" ssid = "22">The fact that Arabic has only 1500 training instances might also be problematic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'66'", "'82'", "'9'", "'61'", "'100'"]
'66'
'82'
'9'
'61'
'100'
['66', '82', '9', '61', '100']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="69" ssid = "7">However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).</S><S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216220  New York City  June 2006. c2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'2'", "'69'", "'18'", "'66'"]
'32'
'2'
'69'
'18'
'66'
['32', '2', '69', '18', '66']
parsed_discourse_facet ['method_citation']
['Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.']
['Furthermore  these results show that a twostage system can achieve a relatively high performance.', 'The current system simply includes all morphological bi-gram features.', 'Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.', 'It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.', 'For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.']
['system', 'ROUGE-S*', 'Average_R:', '0.00177', '(95%-conf.int.', '0.00177', '-', '0.00177)']
['system', 'ROUGE-S*', 'Average_P:', '0.00667', '(95%-conf.int.', '0.00667', '-', '0.00667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00280', '(95%-conf.int.', '0.00280', '-', '0.00280)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:300', 'F:2']
['An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.']
['In doing this preliminary analysis  we noticed some inconsistencies in the reference dependency structures.', 'Furthermore  these results show that a twostage system can achieve a relatively high performance.', 'Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.', 'It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.', 'Derived morphological features improved accuracy in all these languages by 1-3% absolute.']
['system', 'ROUGE-S*', 'Average_R:', '0.00581', '(95%-conf.int.', '0.00581', '-', '0.00581)']
['system', 'ROUGE-S*', 'Average_P:', '0.03676', '(95%-conf.int.', '0.03676', '-', '0.03676)']
['system', 'ROUGE-S*', 'Average_F:', '0.01003', '(95%-conf.int.', '0.01003', '-', '0.01003)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:136', 'F:5']
['Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.']
['Consider a proposed dependency of a dependent xj on the head xi  each with morphological features Mj and Mi respectively.', 'Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.', u'For example  in the test sentence Lo que decia Mae West de si misma podriamos decirlo tambi\xb4en los hombres:...  decia\u2019s head is given as decirlo  although the main verbs of relative clauses are normally dependent on what the relative modifies  in this case the article Lo.', 'This system is primarily based on the parsing models described by McDonald and Pereira (2006).', 'Dependency graphs also encode much of the deep syntactic information needed for further processing.']
['system', 'ROUGE-S*', 'Average_R:', '0.00744', '(95%-conf.int.', '0.00744', '-', '0.00744)']
['system', 'ROUGE-S*', 'Average_P:', '0.05929', '(95%-conf.int.', '0.05929', '-', '0.05929)']
['system', 'ROUGE-S*', 'Average_F:', '0.01322', '(95%-conf.int.', '0.01322', '-', '0.01322)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:253', 'F:15']
['N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.']
['The fact that Arabic has only 1500 training instances might also be problematic.', u'We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216\u2013220  New York City  June 2006. c\ufffd2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.', 'A quick look at unlabeled attachment accuracies indicate that errors in Arabic parsing are the most common across all languages: prepositions (62%)  conjunctions (69%) and to a lesser extent verbs (73%).', 'Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.', 'Derived morphological features improved accuracy in all these languages by 1-3% absolute.']
['system', 'ROUGE-S*', 'Average_R:', '0.00219', '(95%-conf.int.', '0.00219', '-', '0.00219)']
['system', 'ROUGE-S*', 'Average_P:', '0.01976', '(95%-conf.int.', '0.01976', '-', '0.01976)']
['system', 'ROUGE-S*', 'Average_F:', '0.00395', '(95%-conf.int.', '0.00395', '-', '0.00395)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:253', 'F:5']
['For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.']
['The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).', 'The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.', 'These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.', u'We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216\u2013220  New York City  June 2006. c\ufffd2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.', 'However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).']
['system', 'ROUGE-S*', 'Average_R:', '0.00200', '(95%-conf.int.', '0.00200', '-', '0.00200)']
['system', 'ROUGE-S*', 'Average_P:', '0.02597', '(95%-conf.int.', '0.02597', '-', '0.02597)']
['system', 'ROUGE-S*', 'Average_F:', '0.00371', '(95%-conf.int.', '0.00371', '-', '0.00371)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:231', 'F:6']
['In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['These weaknesses are not surprising  since these decisions encode the more global aspects of sentence structure: arrangement of clauses and adverbial dependents in multi-clause sentences  and prepositional phrase attachment.', u'We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216\u2013220  New York City  June 2006. c\ufffd2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.', 'These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.', 'These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.', u'To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm\ufffd1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm\u22121) in the tree y.']
['system', 'ROUGE-S*', 'Average_R:', '0.00129', '(95%-conf.int.', '0.00129', '-', '0.00129)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.00254', '(95%-conf.int.', '0.00254', '-', '0.00254)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4656', 'P:66', 'F:6']
['We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.']
['N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.', 'The fact that Arabic has only 1500 training instances might also be problematic.', 'For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.', 'Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.', 'Derived morphological features improved accuracy in all these languages by 1-3% absolute.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:171', 'F:0']
['To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.']
['We need to look more carefully at verb features that may be useful here  in particular features that distinguish finite and non-finite forms.', 'Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.', 'Derived morphological features improved accuracy in all these languages by 1-3% absolute.', 'Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.', 'However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).']
['system', 'ROUGE-S*', 'Average_R:', '0.00339', '(95%-conf.int.', '0.00339', '-', '0.00339)']
['system', 'ROUGE-S*', 'Average_P:', '0.01587', '(95%-conf.int.', '0.01587', '-', '0.01587)']
['system', 'ROUGE-S*', 'Average_F:', '0.00559', '(95%-conf.int.', '0.00559', '-', '0.00559)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:378', 'F:6']
['For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.']
[u'We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Haji\u02c7c et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B\xa8ohmov\xb4a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; D\u02c7zeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).', 'These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.', 'N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.', 'However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.', 'Derived morphological features improved accuracy in all these languages by 1-3% absolute.']
['system', 'ROUGE-S*', 'Average_R:', '0.00266', '(95%-conf.int.', '0.00266', '-', '0.00266)']
['system', 'ROUGE-S*', 'Average_P:', '0.03016', '(95%-conf.int.', '0.03016', '-', '0.03016)']
['system', 'ROUGE-S*', 'Average_F:', '0.00489', '(95%-conf.int.', '0.00489', '-', '0.00489)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:7140', 'P:630', 'F:19']
0.0317099996477 0.00294999996722 0.00519222216453





input/ref/Task1/W06-2932_sweta.csv
input/res/Task1/W06-2932.csv
parsing: input/ref/Task1/W06-2932_sweta.csv
 <S sid="5" ssid="1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="5">However, in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["36'"]
36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="9">In fact, for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="76" ssid="14">For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["76'"]
76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="45" ssid="14">Furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["45'"]
45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="3">First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["106'"]
106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="58" ssid="6">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["58'"]
58'
['58']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
 <S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="2">Ideally one would like to make all parsing and labeling decisions jointly so that the shared knowledge of both decisions will help resolve any ambiguities.</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
 <S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["43'"]
43'
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S><S sid ="6" ssid = "2">With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'cuk  1988).</S><S sid ="71" ssid = "9">Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S>
original cit marker offset is nan
new cit marker offset is 0



["'64'", "'6'", "'7'", "'71'", "'2'"]
'64'
'6'
'7'
'71'
'2'
['64', '6', '7', '71', '2']
parsed_discourse_facet ['implication_citation']
<S sid ="71" ssid = "9">Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'cuk  1988).</S><S sid ="78" ssid = "16">Even with this improvement  the labeling of verb dependents remains the highest source of error.</S><S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'71'", "'20'", "'7'", "'78'", "'61'"]
'71'
'20'
'7'
'78'
'61'
['71', '20', '7', '78', '61']
parsed_discourse_facet ['implication_citation']
<S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="92" ssid = "14">For example  in the test sentence Lo que decia Mae West de si misma podriamos decirlo tambien los hombres:...  decias head is given as decirlo  although the main verbs of relative clauses are normally dependent on what the relative modifies  in this case the article Lo.</S><S sid ="28" ssid = "10">Consider a proposed dependency of a dependent xj on the head xi  each with morphological features Mj and Mi respectively.</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'92'", "'28'", "'88'", "'10'"]
'20'
'92'
'28'
'88'
'10'
['20', '92', '28', '88', '10']
parsed_discourse_facet ['results_citation']
<S sid ="13" ssid = "9">We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajic et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; Bohmova et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S><S sid ="30" ssid = "12">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'36'", "'64'", "'30'", "'72'"]
'13'
'36'
'64'
'30'
'72'
['13', '36', '64', '30', '72']
parsed_discourse_facet ['method_citation']
<S sid ="109" ssid = "6">The current system simply includes all morphological bi-gram features.</S><S sid ="60" ssid = "8">Furthermore  these results show that a twostage system can achieve a relatively high performance.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S><S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="101" ssid = "23">For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'60'", "'107'", "'86'", "'101'"]
'109'
'60'
'107'
'86'
'101'
['109', '60', '107', '86', '101']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="101" ssid = "23">For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.</S><S sid ="100" ssid = "22">The fact that Arabic has only 1500 training instances might also be problematic.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'72'", "'101'", "'100'", "'64'"]
'86'
'72'
'101'
'100'
'64'
['86', '72', '101', '100', '64']
parsed_discourse_facet ['method_citation']
<S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S><S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216220  New York City  June 2006. c2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="30" ssid = "12">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.</S><S sid ="82" ssid = "4">These weaknesses are not surprising  since these decisions encode the more global aspects of sentence structure: arrangement of clauses and adverbial dependents in multi-clause sentences  and prepositional phrase attachment.</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'66'", "'18'", "'30'", "'82'", "'41'"]
'66'
'18'
'30'
'82'
'41'
['66', '18', '30', '82', '41']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="51" ssid = "20">Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.</S><S sid ="91" ssid = "13">In doing this preliminary analysis  we noticed some inconsistencies in the reference dependency structures.</S><S sid ="60" ssid = "8">Furthermore  these results show that a twostage system can achieve a relatively high performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'72'", "'51'", "'91'", "'60'"]
'107'
'72'
'51'
'91'
'60'
['107', '72', '51', '91', '60']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="90" ssid = "12">We need to look more carefully at verb features that may be useful here  in particular features that distinguish finite and non-finite forms.</S><S sid ="69" ssid = "7">However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).</S><S sid ="71" ssid = "9">Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'72'", "'90'", "'69'", "'71'"]
'86'
'72'
'90'
'69'
'71'
['86', '72', '90', '69', '71']
parsed_discourse_facet ['method_citation']
<S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216220  New York City  June 2006. c2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="93" ssid = "15">A quick look at unlabeled attachment accuracies indicate that errors in Arabic parsing are the most common across all languages: prepositions (62%)  conjunctions (69%) and to a lesser extent verbs (73%).</S><S sid ="100" ssid = "22">The fact that Arabic has only 1500 training instances might also be problematic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'", "'18'", "'86'", "'93'", "'100'"]
'72'
'18'
'86'
'93'
'100'
['72', '18', '86', '93', '100']
parsed_discourse_facet ['implication_citation']
<S sid ="109" ssid = "6">The current system simply includes all morphological bi-gram features.</S><S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S><S sid ="0" ssid = "0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S sid ="48" ssid = "17">Is this the left/rightmost dependent for the head?</S><S sid ="93" ssid = "15">A quick look at unlabeled attachment accuracies indicate that errors in Arabic parsing are the most common across all languages: prepositions (62%)  conjunctions (69%) and to a lesser extent verbs (73%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'61'", "'0'", "'48'", "'93'"]
'109'
'61'
'0'
'48'
'93'
['109', '61', '0', '48', '93']
parsed_discourse_facet ['method_citation']
<S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S><S sid ="82" ssid = "4">These weaknesses are not surprising  since these decisions encode the more global aspects of sentence structure: arrangement of clauses and adverbial dependents in multi-clause sentences  and prepositional phrase attachment.</S><S sid ="9" ssid = "5">Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.</S><S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S><S sid ="100" ssid = "22">The fact that Arabic has only 1500 training instances might also be problematic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'66'", "'82'", "'9'", "'61'", "'100'"]
'66'
'82'
'9'
'61'
'100'
['66', '82', '9', '61', '100']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="69" ssid = "7">However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).</S><S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216220  New York City  June 2006. c2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'2'", "'69'", "'18'", "'66'"]
'32'
'2'
'69'
'18'
'66'
['32', '2', '69', '18', '66']
parsed_discourse_facet ['method_citation']
['Furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira, 2006).']
['Furthermore  these results show that a twostage system can achieve a relatively high performance.', 'The current system simply includes all morphological bi-gram features.', 'Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.', 'It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.', 'For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.']
['system', 'ROUGE-S*', 'Average_R:', '0.00177', '(95%-conf.int.', '0.00177', '-', '0.00177)']
['system', 'ROUGE-S*', 'Average_P:', '0.03030', '(95%-conf.int.', '0.03030', '-', '0.03030)']
['system', 'ROUGE-S*', 'Average_F:', '0.00335', '(95%-conf.int.', '0.00335', '-', '0.00335)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:66', 'F:2']
['N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.']
['In doing this preliminary analysis  we noticed some inconsistencies in the reference dependency structures.', 'Furthermore  these results show that a twostage system can achieve a relatively high performance.', 'Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.', 'It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.', 'Derived morphological features improved accuracy in all these languages by 1-3% absolute.']
['system', 'ROUGE-S*', 'Average_R:', '0.02091', '(95%-conf.int.', '0.02091', '-', '0.02091)']
['system', 'ROUGE-S*', 'Average_P:', '0.07115', '(95%-conf.int.', '0.07115', '-', '0.07115)']
['system', 'ROUGE-S*', 'Average_F:', '0.03232', '(95%-conf.int.', '0.03232', '-', '0.03232)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:253', 'F:18']
['In fact, for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).']
['Consider a proposed dependency of a dependent xj on the head xi  each with morphological features Mj and Mi respectively.', 'Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.', u'For example  in the test sentence Lo que decia Mae West de si misma podriamos decirlo tambi\xb4en los hombres:...  decia\u2019s head is given as decirlo  although the main verbs of relative clauses are normally dependent on what the relative modifies  in this case the article Lo.', 'This system is primarily based on the parsing models described by McDonald and Pereira (2006).', 'Dependency graphs also encode much of the deep syntactic information needed for further processing.']
['system', 'ROUGE-S*', 'Average_R:', '0.00099', '(95%-conf.int.', '0.00099', '-', '0.00099)']
['system', 'ROUGE-S*', 'Average_P:', '0.02564', '(95%-conf.int.', '0.02564', '-', '0.02564)']
['system', 'ROUGE-S*', 'Average_F:', '0.00191', '(95%-conf.int.', '0.00191', '-', '0.00191)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:78', 'F:2']
['N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.']
['The fact that Arabic has only 1500 training instances might also be problematic.', u'We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216\u2013220  New York City  June 2006. c\ufffd2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.', 'A quick look at unlabeled attachment accuracies indicate that errors in Arabic parsing are the most common across all languages: prepositions (62%)  conjunctions (69%) and to a lesser extent verbs (73%).', 'Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.', 'Derived morphological features improved accuracy in all these languages by 1-3% absolute.']
['system', 'ROUGE-S*', 'Average_R:', '0.00219', '(95%-conf.int.', '0.00219', '-', '0.00219)']
['system', 'ROUGE-S*', 'Average_P:', '0.01976', '(95%-conf.int.', '0.01976', '-', '0.01976)']
['system', 'ROUGE-S*', 'Average_F:', '0.00395', '(95%-conf.int.', '0.00395', '-', '0.00395)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:253', 'F:5']
['For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.']
['The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).', 'The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.', 'These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.', u'We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216\u2013220  New York City  June 2006. c\ufffd2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.', 'However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).']
['system', 'ROUGE-S*', 'Average_R:', '0.00200', '(95%-conf.int.', '0.00200', '-', '0.00200)']
['system', 'ROUGE-S*', 'Average_P:', '0.02597', '(95%-conf.int.', '0.02597', '-', '0.02597)']
['system', 'ROUGE-S*', 'Average_F:', '0.00371', '(95%-conf.int.', '0.00371', '-', '0.00371)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:231', 'F:6']
['In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['These weaknesses are not surprising  since these decisions encode the more global aspects of sentence structure: arrangement of clauses and adverbial dependents in multi-clause sentences  and prepositional phrase attachment.', u'We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216\u2013220  New York City  June 2006. c\ufffd2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.', 'These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.', 'These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.', u'To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm\ufffd1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm\u22121) in the tree y.']
['system', 'ROUGE-S*', 'Average_R:', '0.00129', '(95%-conf.int.', '0.00129', '-', '0.00129)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.00254', '(95%-conf.int.', '0.00254', '-', '0.00254)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4656', 'P:66', 'F:6']
['First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.']
['N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.', 'The fact that Arabic has only 1500 training instances might also be problematic.', 'For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.', 'Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.', 'Derived morphological features improved accuracy in all these languages by 1-3% absolute.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:66', 'F:0']
['To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.']
['We need to look more carefully at verb features that may be useful here  in particular features that distinguish finite and non-finite forms.', 'Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.', 'Derived morphological features improved accuracy in all these languages by 1-3% absolute.', 'Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.', 'However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).']
['system', 'ROUGE-S*', 'Average_R:', '0.00339', '(95%-conf.int.', '0.00339', '-', '0.00339)']
['system', 'ROUGE-S*', 'Average_P:', '0.01587', '(95%-conf.int.', '0.01587', '-', '0.01587)']
['system', 'ROUGE-S*', 'Average_F:', '0.00559', '(95%-conf.int.', '0.00559', '-', '0.00559)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:378', 'F:6']
['For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.']
[u'We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Haji\u02c7c et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B\xa8ohmov\xb4a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; D\u02c7zeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).', 'These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.', 'N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.', 'However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.', 'Derived morphological features improved accuracy in all these languages by 1-3% absolute.']
['system', 'ROUGE-S*', 'Average_R:', '0.00266', '(95%-conf.int.', '0.00266', '-', '0.00266)']
['system', 'ROUGE-S*', 'Average_P:', '0.03016', '(95%-conf.int.', '0.03016', '-', '0.03016)']
['system', 'ROUGE-S*', 'Average_F:', '0.00489', '(95%-conf.int.', '0.00489', '-', '0.00489)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:7140', 'P:630', 'F:19']
['Ideally one would like to make all parsing and labeling decisions jointly so that the shared knowledge of both decisions will help resolve any ambiguities.']
['In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).', 'A quick look at unlabeled attachment accuracies indicate that errors in Arabic parsing are the most common across all languages: prepositions (62%)  conjunctions (69%) and to a lesser extent verbs (73%).', 'The current system simply includes all morphological bi-gram features.', 'Is this the left/rightmost dependent for the head?', 'Multilingual Dependency Analysis with a Two-Stage Discriminative Parser']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:55', 'F:0']
0.0309759996902 0.0035199999648 0.00582599994174





input/ref/Task1/P04-1036_aakansha.csv
input/res/Task1/P04-1036.csv
parsing: input/ref/Task1/P04-1036_aakansha.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="34">In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
    <S sid="181" ssid="4">This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'181'"]
'180'
'181'
['180', '181']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="4">To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'"]
'48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="169" ssid="17">This method obtains precision of 61% and recall 51%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'"]
'169'
['169']
parsed_discourse_facet ['result_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="115" ssid="13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
    <S sid="181" ssid="4">This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'181'"]
'180'
'181'
['180', '181']
parsed_discourse_facet ['result_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'"]
'180'
['180']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="34">In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="98" ssid = "27">This seems intuitive given our expected relative usage of these senses in modern British English.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="27" ssid = "20">We use WordNet as our sense inventory for this work.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'98'", "'124'", "'5'", "'27'", "'65'"]
'98'
'124'
'5'
'27'
'65'
['98', '124', '5', '27', '65']
parsed_discourse_facet ['implication_citation']
<S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="109" ssid = "7">We generated a thesaurus entry for all polysemous nouns in WordNet as described in section 2.1 above.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S><S sid ="132" ssid = "9">The FINANCE corpus consists of 117734 documents (about 32.5 million words).</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'109'", "'95'", "'55'", "'132'"]
'124'
'109'
'95'
'55'
'132'
['124', '109', '95', '55', '132']
parsed_discourse_facet ['implication_citation']
<S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="32" ssid = "25">We describe some related work in section 6 and conclude in section 7. are therefore investigating a method of automatically ranking WordNet senses from raw text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'137'", "'5'", "'110'", "'32'"]
'44'
'137'
'5'
'110'
'32'
['44', '137', '5', '110', '32']
parsed_discourse_facet ['results_citation']
<S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="96" ssid = "25">Another example where the ranking is intuitive  is soil.</S><S sid ="33" ssid = "26">Many researchers are developing thesauruses from automatically parsed data.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'95'", "'96'", "'33'", "'44'"]
'124'
'95'
'96'
'33'
'44'
['124', '95', '96', '33', '44']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "5">To disambiguate senses a system should take context into account.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="67" ssid = "23">Each 2We use this version of WordNet since it allows us to map information to WordNets of other languages more accurately.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'124'", "'153'", "'147'", "'67'"]
'107'
'124'
'153'
'147'
'67'
['107', '124', '153', '147', '67']
parsed_discourse_facet ['method_citation']
<S sid ="94" ssid = "23">This seems quite reasonable given the nearest neighbours: tube  cable  wire  tank  hole  cylinder  fitting  tap  cistern  plate....</S><S sid ="162" ssid = "10">It only requires raw text from the given domain and because of this it can easily be applied to a new domain  or sense inventory  given sufficient text.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="99" ssid = "28">Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'", "'162'", "'60'", "'95'", "'99'"]
'94'
'162'
'60'
'95'
'99'
['94', '162', '60', '95', '99']
parsed_discourse_facet ['method_citation']
<S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="101" ssid = "30">Thus  if we used the sense ranking as a heuristic for an all nouns task we would expect to get precision in the region of 60%.</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'165'", "'101'", "'169'", "'108'", "'44'"]
'165'
'101'
'169'
'108'
'44'
['165', '101', '169', '108', '44']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="127" ssid = "4">We chose the domains of SPORTS and FINANCE since there is sufficient material for these domains in this publically available corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'60'", "'169'", "'91'", "'127'"]
'136'
'60'
'169'
'91'
'127'
['136', '60', '169', '91', '127']
parsed_discourse_facet ['method_citation']
<S sid ="193" ssid = "2">This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.</S><S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="151" ssid = "28">This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="167" ssid = "15">In this work the lists of neighbours are themselves clustered to bring out the various senses of the word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'193'", "'136'", "'151'", "'123'", "'167'"]
'193'
'136'
'151'
'123'
'167'
['193', '136', '151', '123', '167']
parsed_discourse_facet ['method_citation']
<S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="63" ssid = "19">We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline  but because of space limitations give results for the two which perform the best.</S>
original cit marker offset is 0
new cit marker offset is 0



["'173'", "'5'", "'108'", "'95'", "'63'"]
'173'
'5'
'108'
'95'
'63'
['173', '5', '108', '95', '63']
parsed_discourse_facet ['implication_citation']
<S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="76" ssid = "5">The jcn measure uses corpus data for the calculation of IC.</S><S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'184'", "'110'", "'60'", "'76'", "'55'"]
'184'
'110'
'60'
'76'
'55'
['184', '110', '60', '76', '55']
parsed_discourse_facet ['method_citation']
<S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="97" ssid = "26">The first ranked sense according to SemCor is the filth  stain: state of being unclean sense whereas the automatic ranking lists dirt  ground  earth as the first sense  which is the second ranked sense according to SemCor.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'123'", "'169'", "'147'", "'97'", "'8'"]
'123'
'169'
'147'
'97'
'8'
['123', '169', '147', '97', '8']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="28" ssid = "21">The paper is structured as follows.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of nearest neighbours.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'28'", "'124'", "'60'", "'34'"]
'61'
'28'
'124'
'60'
'34'
['61', '28', '124', '60', '34']
parsed_discourse_facet ['method_citation']
<S sid ="163" ssid = "11">Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="9" ssid = "2">This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).</S><S sid ="0" ssid = "0">Finding Predominant Word Senses in Untagged Text</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'124'", "'9'", "'0'", "'142'"]
'163'
'124'
'9'
'0'
'142'
['163', '124', '9', '0', '142']
parsed_discourse_facet ['results_citation']
<S sid ="138" ssid = "15">The SFC contains an economy label and a sports label.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of nearest neighbours.</S><S sid ="22" ssid = "15">More importantly  when working within a specific domain one would wish to tune the first sense heuristic to the domain at hand.</S><S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="92" ssid = "21">For example  in WordNet the first listed sense ofpipe is tobacco pipe  and this is ranked joint first according to the Brown files in SemCor with the second sense tube made of metal or plastic used to carry water  oil or gas etc....</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'", "'34'", "'22'", "'184'", "'92'"]
'138'
'34'
'22'
'184'
'92'
['138', '34', '22', '184', '92']
parsed_discourse_facet ['implication_citation']
<S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="185" ssid = "8">In the future  we will perform a large scale evaluation on domain specific corpora.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'173'", "'95'", "'185'", "'65'"]
'108'
'173'
'95'
'185'
'65'
['108', '173', '95', '185', '65']
parsed_discourse_facet ['method_citation']
<S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="151" ssid = "28">This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'173'", "'110'", "'151'", "'95'", "'5'"]
'173'
'110'
'151'
'95'
'5'
['173', '110', '151', '95', '5']
parsed_discourse_facet ['results_citation']
['In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.']
['The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.', 'We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within', 'Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.', 'We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.', 'We describe some related work in section 6 and conclude in section 7. are therefore investigating a method of automatically ranking WordNet senses from raw text.']
['system', 'ROUGE-S*', 'Average_R:', '0.01945', '(95%-conf.int.', '0.01945', '-', '0.01945)']
['system', 'ROUGE-S*', 'Average_P:', '0.40952', '(95%-conf.int.', '0.40952', '-', '0.40952)']
['system', 'ROUGE-S*', 'Average_F:', '0.03713', '(95%-conf.int.', '0.03713', '-', '0.03713)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:105', 'F:43']
['The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.']
['Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.', 'However  it is important to know the performance of this heuristic for any systems that use it.', 'It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.', 'The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.', 'In the future  we will perform a large scale evaluation on domain specific corpora.']
['system', 'ROUGE-S*', 'Average_R:', '0.00597', '(95%-conf.int.', '0.00597', '-', '0.00597)']
['system', 'ROUGE-S*', 'Average_P:', '0.07368', '(95%-conf.int.', '0.07368', '-', '0.07368)']
['system', 'ROUGE-S*', 'Average_F:', '0.01104', '(95%-conf.int.', '0.01104', '-', '0.01104)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:190', 'F:14']
['To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.']
['If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .', 'This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.', 'We chose the domains of SPORTS and FINANCE since there is sufficient material for these domains in this publically available corpus.', 'The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.', 'This method obtains precision of 61% and recall 51%.']
['system', 'ROUGE-S*', 'Average_R:', '0.00452', '(95%-conf.int.', '0.00452', '-', '0.00452)']
['system', 'ROUGE-S*', 'Average_P:', '0.10909', '(95%-conf.int.', '0.10909', '-', '0.10909)']
['system', 'ROUGE-S*', 'Average_F:', '0.00869', '(95%-conf.int.', '0.00869', '-', '0.00869)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:55', 'F:6']
['We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.']
['This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).', 'Finding Predominant Word Senses in Untagged Text', 'A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.', 'The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.', 'Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD.']
['system', 'ROUGE-S*', 'Average_R:', '0.00256', '(95%-conf.int.', '0.00256', '-', '0.00256)']
['system', 'ROUGE-S*', 'Average_P:', '0.17857', '(95%-conf.int.', '0.17857', '-', '0.17857)']
['system', 'ROUGE-S*', 'Average_F:', '0.00505', '(95%-conf.int.', '0.00505', '-', '0.00505)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:28', 'F:5']
['The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.', 'This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.']
['For example  in WordNet the first listed sense ofpipe is tobacco pipe  and this is ranked joint first according to the Brown files in SemCor with the second sense tube made of metal or plastic used to carry water  oil or gas etc....', 'The SFC contains an economy label and a sports label.', 'We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.', 'More importantly  when working within a specific domain one would wish to tune the first sense heuristic to the domain at hand.', u'In these each target word is entered with an ordered list of \u201cnearest neighbours\u201d.']
['system', 'ROUGE-S*', 'Average_R:', '0.02096', '(95%-conf.int.', '0.02096', '-', '0.02096)']
['system', 'ROUGE-S*', 'Average_P:', '0.05348', '(95%-conf.int.', '0.05348', '-', '0.05348)']
['system', 'ROUGE-S*', 'Average_F:', '0.03012', '(95%-conf.int.', '0.03012', '-', '0.03012)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:561', 'F:30']
['The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.', 'This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.']
['If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .', 'Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.', 'Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.', 'This seems quite reasonable given the nearest neighbours: tube  cable  wire  tank  hole  cylinder  fitting  tap  cistern  plate....', 'It only requires raw text from the given domain and because of this it can easily be applied to a new domain  or sense inventory  given sufficient text.']
['system', 'ROUGE-S*', 'Average_R:', '0.01151', '(95%-conf.int.', '0.01151', '-', '0.01151)']
['system', 'ROUGE-S*', 'Average_P:', '0.04813', '(95%-conf.int.', '0.04813', '-', '0.04813)']
['system', 'ROUGE-S*', 'Average_F:', '0.01858', '(95%-conf.int.', '0.01858', '-', '0.01858)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:561', 'F:27']
['This method obtains precision of 61% and recall 51%.']
['This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.', 'In this work the lists of neighbours are themselves clustered to bring out the various senses of the word.', 'This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.', 'The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.', 'This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:15', 'F:0']
['In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.']
['This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.', 'Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.', 'The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.', 'It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.', 'We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.']
['system', 'ROUGE-S*', 'Average_R:', '0.01405', '(95%-conf.int.', '0.01405', '-', '0.01405)']
['system', 'ROUGE-S*', 'Average_P:', '0.37143', '(95%-conf.int.', '0.37143', '-', '0.37143)']
['system', 'ROUGE-S*', 'Average_F:', '0.02708', '(95%-conf.int.', '0.02708', '-', '0.02708)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:105', 'F:39']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.', 'We use the WordNet Similarity Package 0.05 and WordNet version 1.6.', 'The paper is structured as follows.', 'If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .', u'In these each target word is entered with an ordered list of \u201cnearest neighbours\u201d.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:66', 'F:0']
['Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.']
['The word share is among the words whose predominant sense remained the same for all three corpora.', 'This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.', 'This method obtains precision of 61% and recall 51%.', 'The first ranked sense according to SemCor is the filth  stain: state of being unclean sense whereas the automatic ranking lists dirt  ground  earth as the first sense  which is the second ranked sense according to SemCor.', 'The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['system', 'ROUGE-S*', 'Average_R:', '0.02357', '(95%-conf.int.', '0.02357', '-', '0.02357)']
['system', 'ROUGE-S*', 'Average_P:', '0.33333', '(95%-conf.int.', '0.33333', '-', '0.33333)']
['system', 'ROUGE-S*', 'Average_F:', '0.04403', '(95%-conf.int.', '0.04403', '-', '0.04403)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:105', 'F:35']
0.157722998423 0.0102589998974 0.0181719998183





input/ref/Task1/P04-1036_vardha.csv
input/res/Task1/P04-1036.csv
parsing: input/ref/Task1/P04-1036_vardha.csv
    <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
    <S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="66" ssid="22">It uses the glosses of semantically related (according to WordNet) senses too. jcn (Jiang and Conrath, 1997) This score uses corpus data to populate classes (synsets) in the WordNet hierarchy with frequency counts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'66'"]
'66'
['66']
parsed_discourse_facet ['method_citation']
 <S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="101" ssid="30">Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'"]
'101'
['101']
parsed_discourse_facet ['method_citation']
 <S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
 <S sid="115" ssid="13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
  <S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'"]
'89'
['89']
parsed_discourse_facet ['method_citation']
 <S sid="137" ssid="14">Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'"]
'137'
['137']
parsed_discourse_facet ['method_citation']
75 ssid="4">We generated a thesaurus entry for all polysemous nouns which occurred in SemCor with a frequency 2, and in the BNC with a frequency 10 in the grammatical relations listed in section 2.1 above.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'"]
'75'
['75']
Error in Reference Offset
 <S sid="155" ssid="3">A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
  <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
 <S sid="155" ssid="3">A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="24">We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'"]
'68'
['68']
parsed_discourse_facet ['method_citation']
    <S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'"]
'13'
['13']
parsed_discourse_facet ['method_citation']
  <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="98" ssid = "27">This seems intuitive given our expected relative usage of these senses in modern British English.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="27" ssid = "20">We use WordNet as our sense inventory for this work.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'98'", "'124'", "'5'", "'27'", "'65'"]
'98'
'124'
'5'
'27'
'65'
['98', '124', '5', '27', '65']
parsed_discourse_facet ['implication_citation']
<S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="109" ssid = "7">We generated a thesaurus entry for all polysemous nouns in WordNet as described in section 2.1 above.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S><S sid ="132" ssid = "9">The FINANCE corpus consists of 117734 documents (about 32.5 million words).</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'109'", "'95'", "'55'", "'132'"]
'124'
'109'
'95'
'55'
'132'
['124', '109', '95', '55', '132']
parsed_discourse_facet ['implication_citation']
<S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="32" ssid = "25">We describe some related work in section 6 and conclude in section 7. are therefore investigating a method of automatically ranking WordNet senses from raw text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'137'", "'5'", "'110'", "'32'"]
'44'
'137'
'5'
'110'
'32'
['44', '137', '5', '110', '32']
parsed_discourse_facet ['results_citation']
<S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="96" ssid = "25">Another example where the ranking is intuitive  is soil.</S><S sid ="33" ssid = "26">Many researchers are developing thesauruses from automatically parsed data.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'95'", "'96'", "'33'", "'44'"]
'124'
'95'
'96'
'33'
'44'
['124', '95', '96', '33', '44']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "5">To disambiguate senses a system should take context into account.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="153" ssid = "1">Most research in WSD concentrates on using contextual features  typically neighbouring words  to help determine the correct sense of a target word.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="67" ssid = "23">Each 2We use this version of WordNet since it allows us to map information to WordNets of other languages more accurately.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'124'", "'153'", "'147'", "'67'"]
'107'
'124'
'153'
'147'
'67'
['107', '124', '153', '147', '67']
parsed_discourse_facet ['method_citation']
<S sid ="94" ssid = "23">This seems quite reasonable given the nearest neighbours: tube  cable  wire  tank  hole  cylinder  fitting  tap  cistern  plate....</S><S sid ="162" ssid = "10">It only requires raw text from the given domain and because of this it can easily be applied to a new domain  or sense inventory  given sufficient text.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="99" ssid = "28">Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'", "'162'", "'60'", "'95'", "'99'"]
'94'
'162'
'60'
'95'
'99'
['94', '162', '60', '95', '99']
parsed_discourse_facet ['method_citation']
<S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="101" ssid = "30">Thus  if we used the sense ranking as a heuristic for an all nouns task we would expect to get precision in the region of 60%.</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'165'", "'101'", "'169'", "'108'", "'44'"]
'165'
'101'
'169'
'108'
'44'
['165', '101', '169', '108', '44']
parsed_discourse_facet ['method_citation']
<S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="127" ssid = "4">We chose the domains of SPORTS and FINANCE since there is sufficient material for these domains in this publically available corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'", "'60'", "'169'", "'91'", "'127'"]
'136'
'60'
'169'
'91'
'127'
['136', '60', '169', '91', '127']
parsed_discourse_facet ['method_citation']
<S sid ="193" ssid = "2">This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.</S><S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="151" ssid = "28">This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="167" ssid = "15">In this work the lists of neighbours are themselves clustered to bring out the various senses of the word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'193'", "'136'", "'151'", "'123'", "'167'"]
'193'
'136'
'151'
'123'
'167'
['193', '136', '151', '123', '167']
parsed_discourse_facet ['method_citation']
<S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S><S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="63" ssid = "19">We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline  but because of space limitations give results for the two which perform the best.</S>
original cit marker offset is 0
new cit marker offset is 0



["'173'", "'5'", "'108'", "'95'", "'63'"]
'173'
'5'
'108'
'95'
'63'
['173', '5', '108', '95', '63']
parsed_discourse_facet ['implication_citation']
<S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="76" ssid = "5">The jcn measure uses corpus data for the calculation of IC.</S><S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'184'", "'110'", "'60'", "'76'", "'55'"]
'184'
'110'
'60'
'76'
'55'
['184', '110', '60', '76', '55']
parsed_discourse_facet ['method_citation']
<S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="169" ssid = "17">This method obtains precision of 61% and recall 51%.</S><S sid ="147" ssid = "24">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid ="97" ssid = "26">The first ranked sense according to SemCor is the filth  stain: state of being unclean sense whereas the automatic ranking lists dirt  ground  earth as the first sense  which is the second ranked sense according to SemCor.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'123'", "'169'", "'147'", "'97'", "'8'"]
'123'
'169'
'147'
'97'
'8'
['123', '169', '147', '97', '8']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="28" ssid = "21">The paper is structured as follows.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="60" ssid = "16">If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of nearest neighbours.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'28'", "'124'", "'60'", "'34'"]
'61'
'28'
'124'
'60'
'34'
['61', '28', '124', '60', '34']
parsed_discourse_facet ['method_citation']
<S sid ="163" ssid = "11">Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD.</S><S sid ="124" ssid = "1">A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.</S><S sid ="9" ssid = "2">This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).</S><S sid ="0" ssid = "0">Finding Predominant Word Senses in Untagged Text</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S>
original cit marker offset is 0
new cit marker offset is 0



["'163'", "'124'", "'9'", "'0'", "'142'"]
'163'
'124'
'9'
'0'
'142'
['163', '124', '9', '0', '142']
parsed_discourse_facet ['results_citation']
<S sid ="138" ssid = "15">The SFC contains an economy label and a sports label.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of nearest neighbours.</S><S sid ="22" ssid = "15">More importantly  when working within a specific domain one would wish to tune the first sense heuristic to the domain at hand.</S><S sid ="184" ssid = "7">We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.</S><S sid ="92" ssid = "21">For example  in WordNet the first listed sense ofpipe is tobacco pipe  and this is ranked joint first according to the Brown files in SemCor with the second sense tube made of metal or plastic used to carry water  oil or gas etc....</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'", "'34'", "'22'", "'184'", "'92'"]
'138'
'34'
'22'
'184'
'92'
['138', '34', '22', '184', '92']
parsed_discourse_facet ['implication_citation']
<S sid ="108" ssid = "6">However  it is important to know the performance of this heuristic for any systems that use it.</S><S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="185" ssid = "8">In the future  we will perform a large scale evaluation on domain specific corpora.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'173'", "'95'", "'185'", "'65'"]
'108'
'173'
'95'
'185'
'65'
['108', '173', '95', '185', '65']
parsed_discourse_facet ['method_citation']
<S sid ="173" ssid = "21">It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.</S><S sid ="110" ssid = "8">We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.</S><S sid ="151" ssid = "28">This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.</S><S sid ="95" ssid = "24">Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S><S sid ="5" ssid = "5">The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'173'", "'110'", "'151'", "'95'", "'5'"]
'173'
'110'
'151'
'95'
'5'
['173', '110', '151', '95', '5']
parsed_discourse_facet ['results_citation']
['In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).']
['The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.', 'We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within', 'Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.', 'We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.', 'We describe some related work in section 6 and conclude in section 7. are therefore investigating a method of automatically ranking WordNet senses from raw text.']
['system', 'ROUGE-S*', 'Average_R:', '0.00724', '(95%-conf.int.', '0.00724', '-', '0.00724)']
['system', 'ROUGE-S*', 'Average_P:', '0.15238', '(95%-conf.int.', '0.15238', '-', '0.15238)']
['system', 'ROUGE-S*', 'Average_F:', '0.01382', '(95%-conf.int.', '0.01382', '-', '0.01382)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:105', 'F:16']
['The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.']
['Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.', 'However  it is important to know the performance of this heuristic for any systems that use it.', 'It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.', 'The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.', 'In the future  we will perform a large scale evaluation on domain specific corpora.']
['system', 'ROUGE-S*', 'Average_R:', '0.00426', '(95%-conf.int.', '0.00426', '-', '0.00426)']
['system', 'ROUGE-S*', 'Average_P:', '0.18182', '(95%-conf.int.', '0.18182', '-', '0.18182)']
['system', 'ROUGE-S*', 'Average_F:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:55', 'F:10']
['Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.']
['If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .', 'This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.', 'We chose the domains of SPORTS and FINANCE since there is sufficient material for these domains in this publically available corpus.', 'The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.', 'This method obtains precision of 61% and recall 51%.']
['system', 'ROUGE-S*', 'Average_R:', '0.00528', '(95%-conf.int.', '0.00528', '-', '0.00528)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00978', '(95%-conf.int.', '0.00978', '-', '0.00978)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:105', 'F:7']
['A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.']
['This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).', 'Finding Predominant Word Senses in Untagged Text', 'A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.', 'The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.', 'Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD.']
['system', 'ROUGE-S*', 'Average_R:', '0.00512', '(95%-conf.int.', '0.00512', '-', '0.00512)']
['system', 'ROUGE-S*', 'Average_P:', '0.06536', '(95%-conf.int.', '0.06536', '-', '0.06536)']
['system', 'ROUGE-S*', 'Average_F:', '0.00950', '(95%-conf.int.', '0.00950', '-', '0.00950)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:153', 'F:10']
['We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.']
['For example  in WordNet the first listed sense ofpipe is tobacco pipe  and this is ranked joint first according to the Brown files in SemCor with the second sense tube made of metal or plastic used to carry water  oil or gas etc....', 'The SFC contains an economy label and a sports label.', 'We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.', 'More importantly  when working within a specific domain one would wish to tune the first sense heuristic to the domain at hand.', u'In these each target word is entered with an ordered list of \u201cnearest neighbours\u201d.']
['system', 'ROUGE-S*', 'Average_R:', '0.00070', '(95%-conf.int.', '0.00070', '-', '0.00070)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00130', '(95%-conf.int.', '0.00130', '-', '0.00130)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:105', 'F:1']
['Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.']
['If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .', 'Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.', 'Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.', 'This seems quite reasonable given the nearest neighbours: tube  cable  wire  tank  hole  cylinder  fitting  tap  cistern  plate....', 'It only requires raw text from the given domain and because of this it can easily be applied to a new domain  or sense inventory  given sufficient text.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:55', 'F:0']
['Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.']
['This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.', 'In this work the lists of neighbours are themselves clustered to bring out the various senses of the word.', 'This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.', 'The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.', 'This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.']
['system', 'ROUGE-S*', 'Average_R:', '0.00132', '(95%-conf.int.', '0.00132', '-', '0.00132)']
['system', 'ROUGE-S*', 'Average_P:', '0.01754', '(95%-conf.int.', '0.01754', '-', '0.01754)']
['system', 'ROUGE-S*', 'Average_F:', '0.00245', '(95%-conf.int.', '0.00245', '-', '0.00245)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:171', 'F:3']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.', 'Since SemCor is derived from the Brown corpus  which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6  the high ranking for the tobacco pipe sense according to SemCor seems plausible.', 'The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.', 'It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.', 'We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:66', 'F:0']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.', 'We use the WordNet Similarity Package 0.05 and WordNet version 1.6.', 'The paper is structured as follows.', 'If is the set of co-occurrence types such that is positive then the similarity between two nouns  and   can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .', u'In these each target word is entered with an ordered list of \u201cnearest neighbours\u201d.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:66', 'F:0']
['A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.']
['The word share is among the words whose predominant sense remained the same for all three corpora.', 'This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.', 'This method obtains precision of 61% and recall 51%.', 'The first ranked sense according to SemCor is the filth  stain: state of being unclean sense whereas the automatic ranking lists dirt  ground  earth as the first sense  which is the second ranked sense according to SemCor.', 'The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['system', 'ROUGE-S*', 'Average_R:', '0.00875', '(95%-conf.int.', '0.00875', '-', '0.00875)']
['system', 'ROUGE-S*', 'Average_P:', '0.08497', '(95%-conf.int.', '0.08497', '-', '0.08497)']
['system', 'ROUGE-S*', 'Average_F:', '0.01587', '(95%-conf.int.', '0.01587', '-', '0.01587)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:153', 'F:13']
0.0578259994217 0.00326699996733 0.00610499993895





input/ref/Task1/W99-0623_swastika.csv
input/res/Task1/W99-0623.csv
parsing: input/ref/Task1/W99-0623_swastika.csv
<S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['aim_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['result_citation']
<S sid="91" ssid="20">Features and context were initially introduced into the models, but they refused to offer any gains in performance.</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



['139']
139
['139']
parsed_discourse_facet ['result_citation']
<S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['method_citation']
<S sid="103" ssid="32">The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.</S>
original cit marker offset is 0
new cit marker offset is 0



['103']
103
['103']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



['139']
139
['139']
parsed_discourse_facet ['result_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
<S sid="140" ssid="2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
<S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['result_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/W99-0623.csv
<S sid ="34" ssid = "20">Mi(c) is a binary function returning t when parser i (from among the k parsers) suggests constituent c should be in the parse.</S><S sid ="106" ssid = "35">In each figure the upper graph shows the isolated constituent precision and the bottom graph shows the corresponding number of hypothesized constituents.</S><S sid ="66" ssid = "52">Each decision determines the inclusion or exclusion of a candidate constituent.</S><S sid ="65" ssid = "51">We model each parse as the decisions made to create it  and model those decisions as independent events.</S><S sid ="2" ssid = "2">Two general approaches are presented and two combination techniques are described for each approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'106'", "'66'", "'65'", "'2'"]
'34'
'106'
'66'
'65'
'2'
['34', '106', '66', '65', '2']
Error in Discourse Facet
<S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'15'", "'130'", "'57'", "'50'"]
'18'
'15'
'130'
'57'
'50'
['18', '15', '130', '57', '50']
Error in Discourse Facet
<S sid ="122" ssid = "51">All of these systems were run on data that was not seen during their development.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="24" ssid = "10">We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="107" ssid = "36">Again we notice that the isolated constituent precision is larger than 0.5 only in those partitions that contain very few samples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'122'", "'11'", "'24'", "'61'", "'107'"]
'122'
'11'
'24'
'61'
'107'
['122', '11', '24', '61', '107']
Error in Discourse Facet
<S sid ="92" ssid = "21">While we cannot prove there are no such useful features on which one should condition trust  we can give some insight into why the features we explored offered no gain.</S><S sid ="111" ssid = "40">The first row represents the average accuracy of the three parsers we combine.</S><S sid ="104" ssid = "33">In the cases where isolated constituent precision is larger than 0.5 the affected portion of the hypotheses is negligible.</S><S sid ="42" ssid = "28">Call the crossing constituents A and B.</S><S sid ="117" ssid = "46">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'111'", "'104'", "'42'", "'117'"]
'92'
'111'
'104'
'42'
'117'
['92', '111', '104', '42', '117']
Error in Discourse Facet
<S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="39" ssid = "25">There are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.</S><S sid ="63" ssid = "49">The probabilistic version of this procedure is straightforward: We once again assume independence among our various member parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'139'", "'11'", "'39'", "'63'"]
'15'
'139'
'11'
'39'
'63'
['15', '139', '11', '39', '63']
Error in Discourse Facet
<S sid ="39" ssid = "25">There are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.</S><S sid ="86" ssid = "15">Finally we show the combining techniques degrade very little when a poor parser is added to the set.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a nave Bayes classifier to determine which constituents to include in the parse.</S><S sid ="17" ssid = "3">The substructures that are unanimously hypothesized by the parsers should be preserved after combination  and the combination technique should not foolishly create substructures for which there is no supporting evidence.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'86'", "'27'", "'17'", "'61'"]
'39'
'86'
'27'
'17'
'61'
['39', '86', '27', '17', '61']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="72" ssid = "1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="24" ssid = "10">We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'72'", "'21'", "'24'", "'139'"]
'112'
'72'
'21'
'24'
'139'
['112', '72', '21', '24', '139']
Error in Discourse Facet
<S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S><S sid ="96" ssid = "25">We call such a constituent an isolated constituent.</S><S sid ="132" ssid = "61">The results of this experiment can be seen in Table 5.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and nave Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="125" ssid = "54">The constituent voting and nave Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'139'", "'96'", "'132'", "'38'", "'125'"]
'139'
'96'
'132'
'38'
'125'
['139', '96', '132', '38', '125']
Error in Discourse Facet
<S sid ="43" ssid = "29">A receives a votes  and B receives b votes.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="84" ssid = "13">The first shows how constituent features and context do not help in deciding which parser to trust.</S><S sid ="93" ssid = "22">Because we are working with only three parsers  the only situation in which context will help us is when it can indicate we should choose to believe a single parser that disagrees with the majority hypothesis instead of the majority hypothesis itself.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'15'", "'52'", "'84'", "'93'"]
'43'
'15'
'52'
'84'
'93'
['43', '15', '52', '84', '93']
Error in Discourse Facet
<S sid ="101" ssid = "30">We show the results of three of the experiments we conducted to measure isolated constituent precision under various partitioning schemes.</S><S sid ="126" ssid = "55">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a nave Bayes classifier to determine which constituents to include in the parse.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="43" ssid = "29">A receives a votes  and B receives b votes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'126'", "'27'", "'52'", "'43'"]
'101'
'126'
'27'
'52'
'43'
['101', '126', '27', '52', '43']
Error in Discourse Facet
<S sid ="126" ssid = "55">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S><S sid ="142" ssid = "4">Both of the switching techniques  as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.</S><S sid ="111" ssid = "40">The first row represents the average accuracy of the three parsers we combine.</S><S sid ="43" ssid = "29">A receives a votes  and B receives b votes.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'142'", "'111'", "'43'", "'52'"]
'126'
'142'
'111'
'43'
'52'
['126', '142', '111', '43', '52']
Error in Discourse Facet
<S sid ="101" ssid = "30">We show the results of three of the experiments we conducted to measure isolated constituent precision under various partitioning schemes.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="106" ssid = "35">In each figure the upper graph shows the isolated constituent precision and the bottom graph shows the corresponding number of hypothesized constituents.</S><S sid ="22" ssid = "8">If enough parsers suggest that a particular constituent belongs in the parse  we include it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'50'", "'15'", "'106'", "'22'"]
'101'
'50'
'15'
'106'
'22'
['101', '50', '15', '106', '22']
Error in Discourse Facet
<S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S><S sid ="111" ssid = "40">The first row represents the average accuracy of the three parsers we combine.</S><S sid ="30" ssid = "16">This is equivalent to the assumption used in probability estimation for nave Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.</S><S sid ="28" ssid = "14">The development of a nave Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a nave Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'111'", "'30'", "'28'", "'27'"]
'50'
'111'
'30'
'28'
'27'
['50', '111', '30', '28', '27']
Error in Discourse Facet
<S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="140" ssid = "2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S><S sid ="131" ssid = "60">It was then tested on section 22 of the Treebank in conjunction with the other parsers.</S><S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="34" ssid = "20">Mi(c) is a binary function returning t when parser i (from among the k parsers) suggests constituent c should be in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'140'", "'131'", "'112'", "'34'"]
'61'
'140'
'131'
'112'
'34'
['61', '140', '131', '112', '34']
Error in Discourse Facet
<S sid ="87" ssid = "16">It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S><S sid ="37" ssid = "23">Here NO counts the number of hypothesized constituents in the development set that match the binary predicate specified as an argument.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="100" ssid = "29">When this metric is less than 0.5  we expect to incur more errors' than we will remove by adding those constituents to the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'", "'57'", "'37'", "'11'", "'100'"]
'87'
'57'
'37'
'11'
'100'
['87', '57', '37', '11', '100']
Error in Discourse Facet
<S sid ="83" ssid = "12">We performed three experiments to evaluate our techniques.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="108" ssid = "37">From this we see that a finer-grained model for parser combination  at least for the features we have examined  will not give us any additional power.</S><S sid ="107" ssid = "36">Again we notice that the isolated constituent precision is larger than 0.5 only in those partitions that contain very few samples.</S><S sid ="118" ssid = "47">The maximum precision oracle is an upper bound on the possible gain we can achieve by parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'11'", "'108'", "'107'", "'118'"]
'83'
'11'
'108'
'107'
'118'
['83', '11', '108', '107', '118']
Error in Discourse Facet
<S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="30" ssid = "16">This is equivalent to the assumption used in probability estimation for nave Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="113" ssid = "42">The next two rows are results of oracle experiments.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'30'", "'52'", "'113'", "'50'"]
'18'
'30'
'52'
'113'
'50'
['18', '30', '52', '113', '50']
Error in Discourse Facet
["In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers."]
['These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.', u'This is equivalent to the assumption used in probability estimation for na\xc3\xafve Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.', 'There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.', 'The next two rows are results of oracle experiments.', 'This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.']
['system', 'ROUGE-S*', 'Average_R:', '0.00106', '(95%-conf.int.', '0.00106', '-', '0.00106)']
['system', 'ROUGE-S*', 'Average_P:', '0.00476', '(95%-conf.int.', '0.00476', '-', '0.00476)']
['system', 'ROUGE-S*', 'Average_F:', '0.00173', '(95%-conf.int.', '0.00173', '-', '0.00173)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:210', 'F:1']
["In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers."]
['Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).', "When this metric is less than 0.5  we expect to incur more errors' than we will remove by adding those constituents to the parse.", 'The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.', 'Here NO counts the number of hypothesized constituents in the development set that match the binary predicate specified as an argument.', 'It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00665', '(95%-conf.int.', '0.00665', '-', '0.00665)']
['system', 'ROUGE-S*', 'Average_P:', '0.05238', '(95%-conf.int.', '0.05238', '-', '0.05238)']
['system', 'ROUGE-S*', 'Average_F:', '0.01181', '(95%-conf.int.', '0.01181', '-', '0.01181)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:210', 'F:11']
['We have presented two general approaches to studying parser combination: parser switching and parse hybridization.']
['In each figure the upper graph shows the isolated constituent precision and the bottom graph shows the corresponding number of hypothesized constituents.', 'We show the results of three of the experiments we conducted to measure isolated constituent precision under various partitioning schemes.', 'We are interested in combining the substructures of the input parses to produce a better parse.', 'There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.', 'If enough parsers suggest that a particular constituent belongs in the parse  we include it.']
['system', 'ROUGE-S*', 'Average_R:', '0.00303', '(95%-conf.int.', '0.00303', '-', '0.00303)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00580', '(95%-conf.int.', '0.00580', '-', '0.00580)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:45', 'F:3']
['We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.']
['Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).', 'We performed three experiments to evaluate our techniques.', 'From this we see that a finer-grained model for parser combination  at least for the features we have examined  will not give us any additional power.', 'Again we notice that the isolated constituent precision is larger than 0.5 only in those partitions that contain very few samples.', 'The maximum precision oracle is an upper bound on the possible gain we can achieve by parse hybridization.']
['system', 'ROUGE-S*', 'Average_R:', '0.00490', '(95%-conf.int.', '0.00490', '-', '0.00490)']
['system', 'ROUGE-S*', 'Average_P:', '0.16667', '(95%-conf.int.', '0.16667', '-', '0.16667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:36', 'F:6']
['The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.']
['These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.', 'The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.', 'The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.', 'We are interested in combining the substructures of the input parses to produce a better parse.', 'There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.']
['system', 'ROUGE-S*', 'Average_R:', '0.01772', '(95%-conf.int.', '0.01772', '-', '0.01772)']
['system', 'ROUGE-S*', 'Average_P:', '0.05333', '(95%-conf.int.', '0.05333', '-', '0.05333)']
['system', 'ROUGE-S*', 'Average_F:', '0.02660', '(95%-conf.int.', '0.02660', '-', '0.02660)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:300', 'F:16']
['The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.']
['We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.', 'We have presented two general approaches to studying parser combination: parser switching and parse hybridization.', 'The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.', "One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.", "The second row is the accuracy of the best of the three parsers.'"]
['system', 'ROUGE-S*', 'Average_R:', '0.04155', '(95%-conf.int.', '0.04155', '-', '0.04155)']
['system', 'ROUGE-S*', 'Average_P:', '0.14333', '(95%-conf.int.', '0.14333', '-', '0.14333)']
['system', 'ROUGE-S*', 'Average_F:', '0.06442', '(95%-conf.int.', '0.06442', '-', '0.06442)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:300', 'F:43']
['The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.']
['The results of this experiment can be seen in Table 5.', 'We have presented two general approaches to studying parser combination: parser switching and parse hybridization.', u'Under certain conditions the constituent voting and na\xefve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.', u'The constituent voting and na\xefve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.', 'We call such a constituent an isolated constituent.']
['system', 'ROUGE-S*', 'Average_R:', '0.03330', '(95%-conf.int.', '0.03330', '-', '0.03330)']
['system', 'ROUGE-S*', 'Average_P:', '0.12000', '(95%-conf.int.', '0.12000', '-', '0.12000)']
['system', 'ROUGE-S*', 'Average_F:', '0.05214', '(95%-conf.int.', '0.05214', '-', '0.05214)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:300', 'F:36']
['In our particular case the majority requires the agreement of only two parsers because we have only three.']
['Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).', 'We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.', 'All of these systems were run on data that was not seen during their development.', 'Again we notice that the isolated constituent precision is larger than 0.5 only in those partitions that contain very few samples.', 'We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.']
['system', 'ROUGE-S*', 'Average_R:', '0.00089', '(95%-conf.int.', '0.00089', '-', '0.00089)']
['system', 'ROUGE-S*', 'Average_P:', '0.10000', '(95%-conf.int.', '0.10000', '-', '0.10000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00176', '(95%-conf.int.', '0.00176', '-', '0.00176)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:10', 'F:1']
['The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.']
['The first row represents the average accuracy of the three parsers we combine.', 'Both of the switching techniques  as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.', 'A receives a votes  and B receives b votes.', 'Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.', 'This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.']
['system', 'ROUGE-S*', 'Average_R:', '0.00427', '(95%-conf.int.', '0.00427', '-', '0.00427)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00802', '(95%-conf.int.', '0.00802', '-', '0.00802)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:45', 'F:3']
0.0859788879336 0.0125966665267 0.0201999997756





input/ref/Task1/W06-2932_swastika.csv
input/res/Task1/W06-2932.csv
parsing: input/ref/Task1/W06-2932_swastika.csv
<S sid="86" ssid="8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S>
original cit marker offset is 
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="54" ssid="2">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



['54']
54
['54']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



['12']
12
['12']
parsed_discourse_facet ['method_citation']
    <S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



['41']
41
['41']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="6">Its power lies in the ability to define a rich set of features over parsing decisions, as well as surface level features relative to these decisions.</S>
original cit marker offset is 0
new cit marker offset is 0



['24']
24
['24']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



['12']
12
['12']
parsed_discourse_facet ['method_citation']
<S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



['43']
43
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S><S sid ="6" ssid = "2">With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'cuk  1988).</S><S sid ="71" ssid = "9">Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S>
original cit marker offset is nan
new cit marker offset is 0



["'64'", "'6'", "'7'", "'71'", "'2'"]
'64'
'6'
'7'
'71'
'2'
['64', '6', '7', '71', '2']
parsed_discourse_facet ['implication_citation']
<S sid ="71" ssid = "9">Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="7" ssid = "3">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'cuk  1988).</S><S sid ="78" ssid = "16">Even with this improvement  the labeling of verb dependents remains the highest source of error.</S><S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'71'", "'20'", "'7'", "'78'", "'61'"]
'71'
'20'
'7'
'78'
'61'
['71', '20', '7', '78', '61']
parsed_discourse_facet ['implication_citation']
<S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="92" ssid = "14">For example  in the test sentence Lo que decia Mae West de si misma podriamos decirlo tambien los hombres:...  decias head is given as decirlo  although the main verbs of relative clauses are normally dependent on what the relative modifies  in this case the article Lo.</S><S sid ="28" ssid = "10">Consider a proposed dependency of a dependent xj on the head xi  each with morphological features Mj and Mi respectively.</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'92'", "'28'", "'88'", "'10'"]
'20'
'92'
'28'
'88'
'10'
['20', '92', '28', '88', '10']
parsed_discourse_facet ['results_citation']
<S sid ="13" ssid = "9">We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajic et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; Bohmova et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S><S sid ="30" ssid = "12">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'", "'36'", "'64'", "'30'", "'72'"]
'13'
'36'
'64'
'30'
'72'
['13', '36', '64', '30', '72']
parsed_discourse_facet ['method_citation']
<S sid ="109" ssid = "6">The current system simply includes all morphological bi-gram features.</S><S sid ="60" ssid = "8">Furthermore  these results show that a twostage system can achieve a relatively high performance.</S><S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S><S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="101" ssid = "23">For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'60'", "'107'", "'86'", "'101'"]
'109'
'60'
'107'
'86'
'101'
['109', '60', '107', '86', '101']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="101" ssid = "23">For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.</S><S sid ="100" ssid = "22">The fact that Arabic has only 1500 training instances might also be problematic.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'72'", "'101'", "'100'", "'64'"]
'86'
'72'
'101'
'100'
'64'
['86', '72', '101', '100', '64']
parsed_discourse_facet ['method_citation']
<S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S><S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216220  New York City  June 2006. c2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="30" ssid = "12">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.</S><S sid ="82" ssid = "4">These weaknesses are not surprising  since these decisions encode the more global aspects of sentence structure: arrangement of clauses and adverbial dependents in multi-clause sentences  and prepositional phrase attachment.</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'66'", "'18'", "'30'", "'82'", "'41'"]
'66'
'18'
'30'
'82'
'41'
['66', '18', '30', '82', '41']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "4">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="51" ssid = "20">Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.</S><S sid ="91" ssid = "13">In doing this preliminary analysis  we noticed some inconsistencies in the reference dependency structures.</S><S sid ="60" ssid = "8">Furthermore  these results show that a twostage system can achieve a relatively high performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'72'", "'51'", "'91'", "'60'"]
'107'
'72'
'51'
'91'
'60'
['107', '72', '51', '91', '60']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="90" ssid = "12">We need to look more carefully at verb features that may be useful here  in particular features that distinguish finite and non-finite forms.</S><S sid ="69" ssid = "7">However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).</S><S sid ="71" ssid = "9">Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'72'", "'90'", "'69'", "'71'"]
'86'
'72'
'90'
'69'
'71'
['86', '72', '90', '69', '71']
parsed_discourse_facet ['method_citation']
<S sid ="72" ssid = "10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S><S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216220  New York City  June 2006. c2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="93" ssid = "15">A quick look at unlabeled attachment accuracies indicate that errors in Arabic parsing are the most common across all languages: prepositions (62%)  conjunctions (69%) and to a lesser extent verbs (73%).</S><S sid ="100" ssid = "22">The fact that Arabic has only 1500 training instances might also be problematic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'", "'18'", "'86'", "'93'", "'100'"]
'72'
'18'
'86'
'93'
'100'
['72', '18', '86', '93', '100']
parsed_discourse_facet ['implication_citation']
<S sid ="109" ssid = "6">The current system simply includes all morphological bi-gram features.</S><S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S><S sid ="0" ssid = "0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S sid ="48" ssid = "17">Is this the left/rightmost dependent for the head?</S><S sid ="93" ssid = "15">A quick look at unlabeled attachment accuracies indicate that errors in Arabic parsing are the most common across all languages: prepositions (62%)  conjunctions (69%) and to a lesser extent verbs (73%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'61'", "'0'", "'48'", "'93'"]
'109'
'61'
'0'
'48'
'93'
['109', '61', '0', '48', '93']
parsed_discourse_facet ['method_citation']
<S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S><S sid ="82" ssid = "4">These weaknesses are not surprising  since these decisions encode the more global aspects of sentence structure: arrangement of clauses and adverbial dependents in multi-clause sentences  and prepositional phrase attachment.</S><S sid ="9" ssid = "5">Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.</S><S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S><S sid ="100" ssid = "22">The fact that Arabic has only 1500 training instances might also be problematic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'66'", "'82'", "'9'", "'61'", "'100'"]
'66'
'82'
'9'
'61'
'100'
['66', '82', '9', '61', '100']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="69" ssid = "7">However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).</S><S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216220  New York City  June 2006. c2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'2'", "'69'", "'18'", "'66'"]
'32'
'2'
'69'
'18'
'66'
['32', '2', '69', '18', '66']
parsed_discourse_facet ['method_citation']
['An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.']
['Furthermore  these results show that a twostage system can achieve a relatively high performance.', 'The current system simply includes all morphological bi-gram features.', 'Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.', 'It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.', 'For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:136', 'F:0']
['An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.']
['In doing this preliminary analysis  we noticed some inconsistencies in the reference dependency structures.', 'Furthermore  these results show that a twostage system can achieve a relatively high performance.', 'Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.', 'It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.', 'Derived morphological features improved accuracy in all these languages by 1-3% absolute.']
['system', 'ROUGE-S*', 'Average_R:', '0.00581', '(95%-conf.int.', '0.00581', '-', '0.00581)']
['system', 'ROUGE-S*', 'Average_P:', '0.03676', '(95%-conf.int.', '0.03676', '-', '0.03676)']
['system', 'ROUGE-S*', 'Average_F:', '0.01003', '(95%-conf.int.', '0.01003', '-', '0.01003)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:136', 'F:5']
['Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.']
['Consider a proposed dependency of a dependent xj on the head xi  each with morphological features Mj and Mi respectively.', 'Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.', u'For example  in the test sentence Lo que decia Mae West de si misma podriamos decirlo tambi\xb4en los hombres:...  decia\u2019s head is given as decirlo  although the main verbs of relative clauses are normally dependent on what the relative modifies  in this case the article Lo.', 'This system is primarily based on the parsing models described by McDonald and Pereira (2006).', 'Dependency graphs also encode much of the deep syntactic information needed for further processing.']
['system', 'ROUGE-S*', 'Average_R:', '0.00198', '(95%-conf.int.', '0.00198', '-', '0.00198)']
['system', 'ROUGE-S*', 'Average_P:', '0.02339', '(95%-conf.int.', '0.02339', '-', '0.02339)']
['system', 'ROUGE-S*', 'Average_F:', '0.00366', '(95%-conf.int.', '0.00366', '-', '0.00366)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:171', 'F:4']
['Its power lies in the ability to define a rich set of features over parsing decisions, as well as surface level features relative to these decisions.']
['The fact that Arabic has only 1500 training instances might also be problematic.', u'We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216\u2013220  New York City  June 2006. c\ufffd2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.', 'A quick look at unlabeled attachment accuracies indicate that errors in Arabic parsing are the most common across all languages: prepositions (62%)  conjunctions (69%) and to a lesser extent verbs (73%).', 'Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.', 'Derived morphological features improved accuracy in all these languages by 1-3% absolute.']
['system', 'ROUGE-S*', 'Average_R:', '0.00132', '(95%-conf.int.', '0.00132', '-', '0.00132)']
['system', 'ROUGE-S*', 'Average_P:', '0.03297', '(95%-conf.int.', '0.03297', '-', '0.03297)']
['system', 'ROUGE-S*', 'Average_F:', '0.00253', '(95%-conf.int.', '0.00253', '-', '0.00253)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:91', 'F:3']
['For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.']
['The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).', 'The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.', 'These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.', u'We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216\u2013220  New York City  June 2006. c\ufffd2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.', 'However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).']
['system', 'ROUGE-S*', 'Average_R:', '0.00200', '(95%-conf.int.', '0.00200', '-', '0.00200)']
['system', 'ROUGE-S*', 'Average_P:', '0.02597', '(95%-conf.int.', '0.02597', '-', '0.02597)']
['system', 'ROUGE-S*', 'Average_F:', '0.00371', '(95%-conf.int.', '0.00371', '-', '0.00371)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:231', 'F:6']
['In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['These weaknesses are not surprising  since these decisions encode the more global aspects of sentence structure: arrangement of clauses and adverbial dependents in multi-clause sentences  and prepositional phrase attachment.', u'We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216\u2013220  New York City  June 2006. c\ufffd2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.', 'These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.', 'These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.', u'To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm\ufffd1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm\u22121) in the tree y.']
['system', 'ROUGE-S*', 'Average_R:', '0.00129', '(95%-conf.int.', '0.00129', '-', '0.00129)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.00254', '(95%-conf.int.', '0.00254', '-', '0.00254)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4656', 'P:66', 'F:6']
['Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.']
['N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.', 'The fact that Arabic has only 1500 training instances might also be problematic.', 'For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.', 'Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.', 'Derived morphological features improved accuracy in all these languages by 1-3% absolute.']
['system', 'ROUGE-S*', 'Average_R:', '0.00169', '(95%-conf.int.', '0.00169', '-', '0.00169)']
['system', 'ROUGE-S*', 'Average_P:', '0.01000', '(95%-conf.int.', '0.01000', '-', '0.01000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00290', '(95%-conf.int.', '0.00290', '-', '0.00290)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:300', 'F:3']
['To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.']
['We need to look more carefully at verb features that may be useful here  in particular features that distinguish finite and non-finite forms.', 'Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.', 'Derived morphological features improved accuracy in all these languages by 1-3% absolute.', 'Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.', 'However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).']
['system', 'ROUGE-S*', 'Average_R:', '0.00339', '(95%-conf.int.', '0.00339', '-', '0.00339)']
['system', 'ROUGE-S*', 'Average_P:', '0.01587', '(95%-conf.int.', '0.01587', '-', '0.01587)']
['system', 'ROUGE-S*', 'Average_F:', '0.00559', '(95%-conf.int.', '0.00559', '-', '0.00559)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:378', 'F:6']
['Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.']
[u'We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Haji\u02c7c et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B\xa8ohmov\xb4a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; D\u02c7zeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).', 'These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.', 'N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.', 'However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.', 'Derived morphological features improved accuracy in all these languages by 1-3% absolute.']
['system', 'ROUGE-S*', 'Average_R:', '0.00028', '(95%-conf.int.', '0.00028', '-', '0.00028)']
['system', 'ROUGE-S*', 'Average_P:', '0.01170', '(95%-conf.int.', '0.01170', '-', '0.01170)']
['system', 'ROUGE-S*', 'Average_F:', '0.00055', '(95%-conf.int.', '0.00055', '-', '0.00055)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:7140', 'P:171', 'F:2']
['Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.']
['In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).', 'A quick look at unlabeled attachment accuracies indicate that errors in Arabic parsing are the most common across all languages: prepositions (62%)  conjunctions (69%) and to a lesser extent verbs (73%).', 'The current system simply includes all morphological bi-gram features.', 'Is this the left/rightmost dependent for the head?', 'Multilingual Dependency Analysis with a Two-Stage Discriminative Parser']
['system', 'ROUGE-S*', 'Average_R:', '0.01773', '(95%-conf.int.', '0.01773', '-', '0.01773)']
['system', 'ROUGE-S*', 'Average_P:', '0.11696', '(95%-conf.int.', '0.11696', '-', '0.11696)']
['system', 'ROUGE-S*', 'Average_F:', '0.03079', '(95%-conf.int.', '0.03079', '-', '0.03079)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:171', 'F:20']
0.0364529996355 0.00354899996451 0.0062299999377





input/ref/Task1/J01-2004_sweta.csv
input/res/Task1/J01-2004.csv
parsing: input/ref/Task1/J01-2004_sweta.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["372'"]
372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="28">The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.</S>
    <S sid="41" ssid="29">There will also be a brief review of previous work using syntactic information for language modeling, before we introduce our model in Section 4.</S>
    <S sid="42" ssid="30">Three parse trees: (a) a complete parse tree; (b) a complete parse tree with an explicit stop symbol; and (c) a partial parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["40'", "'41'", "'42'"]
40'
'41'
'42'
['40', '41', '42']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="13">A parser that is not left to right, but which has rooted derivations, e.g., a headfirst parser, will be able to calculate generative joint probabilities for entire strings; however, it will not be able to calculate probabilities for each word conditioned on previously generated words, unless each derivation generates the words in the string in exactly the same order.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="364" ssid="120">We follow Chelba (2000) in dealing with this problem: for parsing purposes, we use the Penn Treebank tokenization; for interpolation with the provided trigram model, and for evaluation, the lattice tokenization is used.</S>
original cit marker offset is 0
new cit marker offset is 0



["364'"]
364'
['364']
parsed_discourse_facet ['method_citation']
<S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



["302'"]
302'
['302']
parsed_discourse_facet ['method_citation']
 <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="231" ssid="135">Since we do not know the POS for the word, we must sum the LAP for all POS For a PCFG G, a stack S = Ao An$ (which we will write AN and a look-ahead terminal item wi, we define the look-ahead probability as follows: We recursively estimate this with two empirically observed conditional probabilities for every nonterminal A,: 13(A, w,a) and P(A, c).</S>
original cit marker offset is 0
new cit marker offset is 0



["231'"]
231'
['231']
parsed_discourse_facet ['method_citation']
<S sid="297" ssid="53">The differences between a k-best and a beam-search parser (not to mention the use of dynamic programming) make a running time difference unsurprising.</S>
original cit marker offset is 0
new cit marker offset is 0



["297'"]
297'
['297']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="37">Statistically based heuristic best-first or beam-search strategies (Caraballo and Charniak 1998; Charniak, Goldwater, and Johnson 1998; Goodman 1997) have yielded an enormous improvement in the quality and speed of parsers, even without any guarantee that the parse returned is, in fact, that with the maximum likelihood for the probability model.</S>
original cit marker offset is 0
new cit marker offset is 0



["133'"]
133'
['133']
parsed_discourse_facet ['method_citation']
<S sid="291" ssid="47">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["291'"]
291'
['291']
parsed_discourse_facet ['method_citation']
<S sid="355" ssid="111">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.</S>
original cit marker offset is 0
new cit marker offset is 0



["355'"]
355'
['355']
parsed_discourse_facet ['method_citation']
<S sid="59" ssid="17">A PCFG is a CFG with a probability assigned to each rule; specifically, each righthand side has a probability given the left-hand side of the rule.</S>
original cit marker offset is 0
new cit marker offset is 0



["59'"]
59'
['59']
parsed_discourse_facet ['method_citation']
<S sid="100" ssid="4">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="12">Another approach that uses syntactic structure for language modeling has been to use a shift-reduce parser to &amp;quot;surface&amp;quot; c-commanding phrasal headwords or part-of-speech (POS) tags from arbitrarily far back in the prefix string, for use in a trigram-like model.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S sid ="321" ssid = "77">In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.</S><S sid ="387" ssid = "143">Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.</S><S sid ="315" ssid = "71">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;the sum of the probabilities over the vocabulary is less than one.</S><S sid ="344" ssid = "100">However  when we interpolate with the trigram  we see that the additional improvement is greater than the one they experienced.</S><S sid ="403" ssid = "16">Perhaps some compromise between the fully connected structures and extreme underspecification will yield an efficiency improvement.</S>
original cit marker offset is 0
new cit marker offset is 0



["'321'", "'387'", "'315'", "'344'", "'403'"]
'321'
'387'
'315'
'344'
'403'
['321', '387', '315', '344', '403']
parsed_discourse_facet ['implication_citation']
<S sid ="380" ssid = "136">Future work will include more substantial word recognition experiments.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S><S sid ="344" ssid = "100">However  when we interpolate with the trigram  we see that the additional improvement is greater than the one they experienced.</S><S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S>
original cit marker offset is 0
new cit marker offset is 0



["'380'", "'280'", "'391'", "'344'", "'336'"]
'380'
'280'
'391'
'344'
'336'
['380', '280', '391', '344', '336']
parsed_discourse_facet ['implication_citation']
<S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="338" ssid = "94">Table 4 compares the perplexity of our model with Chelba and Jelinek (1998a  1998b) on the same training and testing corpora.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="326" ssid = "82">We obtained the training and testing corpora from them (which we will denote C&J corpus)  and also created intermediate corpora  upon which only the first two modifications were carried out (which we will denote no punct).</S><S sid ="343" ssid = "99">Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'</S>
original cit marker offset is 0
new cit marker offset is 0



["'349'", "'338'", "'280'", "'326'", "'343'"]
'349'
'338'
'280'
'326'
'343'
['349', '338', '280', '326', '343']
parsed_discourse_facet ['results_citation']
<S sid ="372" ssid = "128">The small size of our training data  as well as the fact that we are rescoring n-best lists  rather than working directly on lattices  makes comparison with the other models not particularly informative.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="309" ssid = "65">Let Ht be the priority queue H  before any processing has begun with word w  in the look-ahead.</S><S sid ="266" ssid = "22">From this set of measures  we will also include the crossing bracket scores: average crossing brackets (CB)  percentage of sentences with no crossing brackets (0 CB)  and the percentage of sentences with two crossing brackets or fewer (< 2 CB).</S><S sid ="268" ssid = "24">This is an incremental parser with a pruning strategy and no backtracking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'", "'301'", "'309'", "'266'", "'268'"]
'372'
'301'
'309'
'266'
'268'
['372', '301', '309', '266', '268']
parsed_discourse_facet ['method_citation']
<S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="355" ssid = "111">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task  we performed a very small preliminary experiment on n-best lists.</S><S sid ="340" ssid = "96">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid ="354" ssid = "110">Interestingly  at the word where the failure occurred  the sum of the probabilities was 0.9301.</S>
original cit marker offset is 0
new cit marker offset is 0



["'336'", "'301'", "'355'", "'340'", "'354'"]
'336'
'301'
'355'
'340'
'354'
['336', '301', '355', '340', '354']
parsed_discourse_facet ['method_citation']
<S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="361" ssid = "117">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S><S sid ="319" ssid = "75">The hope  however  is that the improved parsing model provided by our conditional probability model will cause the distribution over structures to be more peaked  thus enabling us to capture more of the total probability mass  and making this a fairly snug upper bound on the perplexity.</S><S sid ="355" ssid = "111">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task  we performed a very small preliminary experiment on n-best lists.</S>
original cit marker offset is 0
new cit marker offset is 0



["'301'", "'349'", "'361'", "'319'", "'355'"]
'301'
'349'
'361'
'319'
'355'
['301', '349', '361', '319', '355']
parsed_discourse_facet ['method_citation']
<S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="324" ssid = "80">Their modifications included: (i) removing orthographic cues to structure (e.g.  punctuation); (ii) replacing all numbers with the single token N; and (iii) closing the vocabulary at 10 000  replacing all other words with the UNK token.</S><S sid ="343" ssid = "99">Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'</S><S sid ="387" ssid = "143">Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.</S><S sid ="346" ssid = "102">These results are particularly remarkable  given that we did not build our model as a language model per se  but rather as a parsing model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'349'", "'324'", "'343'", "'387'", "'346'"]
'349'
'324'
'343'
'387'
'346'
['349', '324', '343', '387', '346']
parsed_discourse_facet ['method_citation']
<S sid ="315" ssid = "71">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;the sum of the probabilities over the vocabulary is less than one.</S><S sid ="377" ssid = "133">The point of this small experiment was to see if our parsing model could provide useful information even in the case that recognition errors occur  as opposed to the (generally) fully grammatical strings upon which the perplexity results were obtained.</S><S sid ="288" ssid = "44">Interestingly  conditioning all POS expansions on two c-commanding heads made no difference in accuracy compared to conditioning only leftmost POS expansions on a single c-commanding head; but it did improve the efficiency.</S><S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="382" ssid = "138">The base beam factor that we have used to this point is 10'  which is quite wide.</S>
original cit marker offset is 0
new cit marker offset is 0



["'315'", "'377'", "'288'", "'358'", "'382'"]
'315'
'377'
'288'
'358'
'382'
['315', '377', '288', '358', '382']
parsed_discourse_facet ['method_citation']
<S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="382" ssid = "138">The base beam factor that we have used to this point is 10'  which is quite wide.</S><S sid ="283" ssid = "39">Unlike the Roark and Johnson parser  however  our coverage did not substantially drop as the amount of conditioning information increased  and in some cases  coverage improved slightly.</S><S sid ="380" ssid = "136">Future work will include more substantial word recognition experiments.</S><S sid ="268" ssid = "24">This is an incremental parser with a pruning strategy and no backtracking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'301'", "'382'", "'283'", "'380'", "'268'"]
'301'
'382'
'283'
'380'
'268'
['301', '382', '283', '380', '268']
parsed_discourse_facet ['method_citation']
<S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S><S sid ="321" ssid = "77">In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.</S><S sid ="262" ssid = "18">Recall is the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD.</S>
original cit marker offset is 0
new cit marker offset is 0



["'336'", "'280'", "'391'", "'321'", "'262'"]
'336'
'280'
'391'
'321'
'262'
['336', '280', '391', '321', '262']
parsed_discourse_facet ['implication_citation']
<S sid ="315" ssid = "71">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;the sum of the probabilities over the vocabulary is less than one.</S><S sid ="336" ssid = "92">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S><S sid ="295" ssid = "51">Our observed times look polynomial  which is to be expected given our pruning strategy: the denser the competitors within a narrow probability range of the best analysis  the more time will be spent working on these competitors; and the farther along in the sentence  the more chance for ambiguities that can lead to such a situation.</S><S sid ="382" ssid = "138">The base beam factor that we have used to this point is 10'  which is quite wide.</S><S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'315'", "'336'", "'295'", "'382'", "'391'"]
'315'
'336'
'295'
'382'
'391'
['315', '336', '295', '382', '391']
parsed_discourse_facet ['method_citation']
<S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="258" ssid = "14">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S><S sid ="372" ssid = "128">The small size of our training data  as well as the fact that we are rescoring n-best lists  rather than working directly on lattices  makes comparison with the other models not particularly informative.</S><S sid ="298" ssid = "54">What is perhaps surprising is that the difference is not greater.</S><S sid ="270" ssid = "26">In such a case  the parser fails to return a complete parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'301'", "'258'", "'372'", "'298'", "'270'"]
'301'
'258'
'372'
'298'
'270'
['301', '258', '372', '298', '270']
parsed_discourse_facet ['method_citation']
<S sid ="391" ssid = "4">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S><S sid ="398" ssid = "11">By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="354" ssid = "110">Interestingly  at the word where the failure occurred  the sum of the probabilities was 0.9301.</S><S sid ="404" ssid = "17">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'391'", "'398'", "'301'", "'354'", "'404'"]
'391'
'398'
'301'
'354'
'404'
['391', '398', '301', '354', '404']
parsed_discourse_facet ['method_citation']
<S sid ="402" ssid = "15">In fact  left-corner parsing can be simulated by a top-down parser by transforming the grammar  as was done in Roark and Johnson (1999)  and so an approach very similar to the one outlined here could be used in that case.</S><S sid ="398" ssid = "11">By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.</S><S sid ="280" ssid = "36">There are a couple of things to notice from these results.</S><S sid ="371" ssid = "127">For our model and the Treebank trigram model  the LM weight that resulted in the lowest error rates is given.</S><S sid ="343" ssid = "99">Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'</S>
original cit marker offset is 0
new cit marker offset is 0



["'402'", "'398'", "'280'", "'371'", "'343'"]
'402'
'398'
'280'
'371'
'343'
['402', '398', '280', '371', '343']
parsed_discourse_facet ['results_citation']
<S sid ="398" ssid = "11">By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="361" ssid = "117">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S><S sid ="258" ssid = "14">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S>
original cit marker offset is 0
new cit marker offset is 0



["'398'", "'301'", "'390'", "'361'", "'258'"]
'398'
'301'
'390'
'361'
'258'
['398', '301', '390', '361', '258']
parsed_discourse_facet ['implication_citation']
<S sid ="258" ssid = "14">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S><S sid ="262" ssid = "18">Recall is the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD.</S><S sid ="361" ssid = "117">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S><S sid ="339" ssid = "95">We built an interpolated trigram model to serve as a baseline (as they did)  and also interpolated our model's perplexity with the trigram  using the same mixing coefficient as they did in their trials (taking 36 percent of the estimate from the trigram).'</S><S sid ="324" ssid = "80">Their modifications included: (i) removing orthographic cues to structure (e.g.  punctuation); (ii) replacing all numbers with the single token N; and (iii) closing the vocabulary at 10 000  replacing all other words with the UNK token.</S>
original cit marker offset is 0
new cit marker offset is 0



["'258'", "'262'", "'361'", "'339'", "'324'"]
'258'
'262'
'361'
'339'
'324'
['258', '262', '361', '339', '324']
parsed_discourse_facet ['method_citation']
<S sid ="349" ssid = "105">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid ="257" ssid = "13">A constituent for evaluation purposes consists of a label (e.g.  NP) and a span (beginning and ending word positions).</S><S sid ="340" ssid = "96">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid ="301" ssid = "57">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid ="387" ssid = "143">Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.</S>
original cit marker offset is 0
new cit marker offset is 0



["'349'", "'257'", "'340'", "'301'", "'387'"]
'349'
'257'
'340'
'301'
'387'
['349', '257', '340', '301', '387']
parsed_discourse_facet ['results_citation']
['Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).']
["By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).", 'Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.', 'The trigram model was also trained on Sections 00-20 of the C&J corpus.', "One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'", 'A constituent for evaluation purposes consists of a label (e.g.  NP) and a span (beginning and ending word positions).']
['system', 'ROUGE-S*', 'Average_R:', '0.00665', '(95%-conf.int.', '0.00665', '-', '0.00665)']
['system', 'ROUGE-S*', 'Average_P:', '0.03667', '(95%-conf.int.', '0.03667', '-', '0.03667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01126', '(95%-conf.int.', '0.01126', '-', '0.01126)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:300', 'F:11']
['The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.']
["By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).", 'These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.', 'By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.', 'Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.', 'Interestingly  at the word where the failure occurred  the sum of the probabilities was 0.9301.']
['system', 'ROUGE-S*', 'Average_R:', '0.01212', '(95%-conf.int.', '0.01212', '-', '0.01212)']
['system', 'ROUGE-S*', 'Average_P:', '0.07407', '(95%-conf.int.', '0.07407', '-', '0.07407)']
['system', 'ROUGE-S*', 'Average_F:', '0.02083', '(95%-conf.int.', '0.02083', '-', '0.02083)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:351', 'F:26']
['Since we do not know the POS for the word, we must sum the LAP for all POS For a PCFG G, a stack S = Ao An$ (which we will write AN and a look-ahead terminal item wi, we define the look-ahead probability as follows: We recursively estimate this with two empirically observed conditional probabilities for every nonterminal A,: 13(A, w,a) and P(A, c).']
['These results are particularly remarkable  given that we did not build our model as a language model per se  but rather as a parsing model.', 'Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.', "One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'", 'Their modifications included: (i) removing orthographic cues to structure (e.g.  punctuation); (ii) replacing all numbers with the single token N; and (iii) closing the vocabulary at 10 000  replacing all other words with the UNK token.', "Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00093', '(95%-conf.int.', '0.00093', '-', '0.00093)']
['system', 'ROUGE-S*', 'Average_P:', '0.00725', '(95%-conf.int.', '0.00725', '-', '0.00725)']
['system', 'ROUGE-S*', 'Average_F:', '0.00165', '(95%-conf.int.', '0.00165', '-', '0.00165)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:276', 'F:2']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['However  when we interpolate with the trigram  we see that the additional improvement is greater than the one they experienced.', 'In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.', 'Perhaps some compromise between the fully connected structures and extreme underspecification will yield an efficiency improvement.', u'Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;\xe2\u20ac\u201dthe sum of the probabilities over the vocabulary is less than one.', 'Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.']
['system', 'ROUGE-S*', 'Average_R:', '0.00078', '(95%-conf.int.', '0.00078', '-', '0.00078)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00146', '(95%-conf.int.', '0.00146', '-', '0.00146)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:91', 'F:1']
['In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.']
['Our observed times look polynomial  which is to be expected given our pruning strategy: the denser the competitors within a narrow probability range of the best analysis  the more time will be spent working on these competitors; and the farther along in the sentence  the more chance for ambiguities that can lead to such a situation.', "The base beam factor that we have used to this point is 10'  which is quite wide.", 'The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.', 'These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.', u'Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;\xe2\u20ac\u201dthe sum of the probabilities over the vocabulary is less than one.']
['system', 'ROUGE-S*', 'Average_R:', '0.00068', '(95%-conf.int.', '0.00068', '-', '0.00068)']
['system', 'ROUGE-S*', 'Average_P:', '0.01905', '(95%-conf.int.', '0.01905', '-', '0.01905)']
['system', 'ROUGE-S*', 'Average_F:', '0.00132', '(95%-conf.int.', '0.00132', '-', '0.00132)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:105', 'F:2']
['Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.']
['There are a couple of things to notice from these results.', 'These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.', 'The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.', 'In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.', 'Recall is the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD.']
['system', 'ROUGE-S*', 'Average_R:', '0.00325', '(95%-conf.int.', '0.00325', '-', '0.00325)']
['system', 'ROUGE-S*', 'Average_P:', '0.01149', '(95%-conf.int.', '0.01149', '-', '0.01149)']
['system', 'ROUGE-S*', 'Average_F:', '0.00506', '(95%-conf.int.', '0.00506', '-', '0.00506)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:435', 'F:5']
['A PCFG is a CFG with a probability assigned to each rule; specifically, each righthand side has a probability given the left-hand side of the rule.']
['In such a case  the parser fails to return a complete parse.', "By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).", 'For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.', 'The small size of our training data  as well as the fact that we are rescoring n-best lists  rather than working directly on lattices  makes comparison with the other models not particularly informative.', 'What is perhaps surprising is that the difference is not greater.']
['system', 'ROUGE-S*', 'Average_R:', '0.00290', '(95%-conf.int.', '0.00290', '-', '0.00290)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00539', '(95%-conf.int.', '0.00539', '-', '0.00539)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:78', 'F:3']
['Three parse trees: (a) a complete parse tree; (b) a complete parse tree with an explicit stop symbol; and (c) a partial parse tree.', 'The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.', 'There will also be a brief review of previous work using syntactic information for language modeling, before we introduce our model in Section 4.']
['Future work will include more substantial word recognition experiments.', 'There are a couple of things to notice from these results.', 'These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.', 'The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.', 'However  when we interpolate with the trigram  we see that the additional improvement is greater than the one they experienced.']
['system', 'ROUGE-S*', 'Average_R:', '0.01056', '(95%-conf.int.', '0.01056', '-', '0.01056)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.01431', '(95%-conf.int.', '0.01431', '-', '0.01431)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:630', 'F:14']
0.0275249996559 0.00473374994083 0.00765999990425





input/ref/Task1/P05-1013_vardha.csv
input/res/Task1/P05-1013.csv
parsing: input/ref/Task1/P05-1013_vardha.csv
 <S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
  <S sid="9" ssid="5">This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
  <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
    <S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
 <S sid="62" ssid="1">In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al., 2004) and English (Nivre and Scholz, 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="23" ssid="19">By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
    <S sid="96" ssid="7">With respect to exact match, the improvement is even more noticeable, which shows quite clearly that even if non-projective dependencies are rare on the token level, they are nevertheless important for getting the global syntactic structure correct.</S>
original cit marker offset is 0
new cit marker offset is 0



["'96'"]
'96'
['96']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
  <S sid="40" ssid="11">Even this may be nondeterministic, in case the graph contains several non-projective arcs whose lifts interact, but we use the following algorithm to construct a minimal projective transformation D0 = (W, A0) of a (nonprojective) dependency graph D = (W, A): The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right).</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
  <S sid="7" ssid="3">From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="14" ssid="10">While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
 <S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'"]
'49'
['49']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P05-1013.csv
<S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S><S sid ="53" ssid = "24">In the third and final scheme  denoted Path  we keep the extra infor2Note that this is a baseline for the parsing experiment only (Experiment 2).</S><S sid ="37" ssid = "8">Here we use a slightly different notion of lift  applying to individual arcs and moving their head upwards one step at a time: Intuitively  lifting an arc makes the word wk dependent on the head wi of its original head wj (which is unique in a well-formed dependency graph)  unless wj is a root in which case the operation is undefined (but then wj * wk is necessarily projective if the dependency graph is well-formed).</S><S sid ="83" ssid = "10">It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'53'", "'37'", "'83'", "'75'"]
'51'
'53'
'37'
'83'
'75'
['51', '53', '37', '83', '75']
parsed_discourse_facet ['implication_citation']
<S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S><S sid ="38" ssid = "9">Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'38'", "'78'", "'34'", "'51'"]
'75'
'38'
'78'
'34'
'51'
['75', '38', '78', '34', '51']
parsed_discourse_facet ['implication_citation']
<S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S><S sid ="23" ssid = "19">By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S><S sid ="110" ssid = "2">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S><S sid ="25" ssid = "21">The rest of the paper is structured as follows.</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'70'", "'23'", "'110'", "'25'", "'11'"]
'70'
'23'
'110'
'25'
'11'
['70', '23', '110', '25', '11']
parsed_discourse_facet ['results_citation']
<S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="77" ssid = "4">The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'24'", "'95'", "'77'", "'11'"]
'34'
'24'
'95'
'77'
'11'
['34', '24', '95', '77', '11']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "14">Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.</S><S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="18" ssid = "14">In addition  there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington  1990; Kahane et al.  1998; Duchier and Debusmann  2001; Holan et al.  2001; Hellwig  2003).</S><S sid ="103" ssid = "14">On the other hand  given that all schemes have similar parsing accuracy overall  this means that the Path scheme is the least likely to introduce errors on projective arcs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'65'", "'14'", "'18'", "'103'"]
'43'
'65'
'14'
'18'
'103'
['43', '65', '14', '18', '103']
parsed_discourse_facet ['method_citation']
<S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S><S sid ="42" ssid = "13">Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="55" ssid = "26">As can be seen from the last column in Table 1  both Head and Head+Path may theoretically lead to a quadratic increase in the number of distinct arc labels (Head+Path being worse than Head only by a constant factor)  while the increase is only linear in the case of Path.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'42'", "'14'", "'55'", "'104'"]
'75'
'42'
'14'
'55'
'104'
['75', '42', '14', '55', '104']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="107" ssid = "18">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="66" ssid = "5">The choice between different actions is in general nondeterministic  and the parser relies on a memorybased classifier  trained on treebank data  to predict the next action based on features of the current parser configuration.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'107'", "'14'", "'95'", "'66'"]
'11'
'107'
'14'
'95'
'66'
['11', '107', '14', '95', '66']
parsed_discourse_facet ['method_citation']
<S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S><S sid ="12" ssid = "8">Prague Dependency Treebank (Hajic et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'70'", "'81'", "'12'", "'78'", "'89'"]
'70'
'81'
'12'
'78'
'89'
['70', '81', '12', '78', '89']
parsed_discourse_facet ['method_citation']
<S sid ="12" ssid = "8">Prague Dependency Treebank (Hajic et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.</S><S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="5" ssid = "1">It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order  where discontinuous syntactic constructions are more common than in languages like English (Melcuk  1988; Covington  1990).</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S><S sid ="66" ssid = "5">The choice between different actions is in general nondeterministic  and the parser relies on a memorybased classifier  trained on treebank data  to predict the next action based on features of the current parser configuration.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'65'", "'5'", "'71'", "'66'"]
'12'
'65'
'5'
'71'
'66'
['12', '65', '5', '71', '66']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S><S sid ="82" ssid = "9">The last four columns in Table 3 show the distribution of nonprojective arcs with respect to the number of lifts required.</S><S sid ="86" ssid = "13">As expected  the most informative encoding  Head+Path  gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S><S sid ="84" ssid = "11">In the second part of the experiment  we applied the inverse transformation based on breadth-first search under the three different encoding schemes.</S><S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'82'", "'86'", "'84'", "'70'"]
'51'
'82'
'86'
'84'
'70'
['51', '82', '86', '84', '70']
parsed_discourse_facet ['implication_citation']
<S sid ="85" ssid = "12">The results are given in Table 4.</S><S sid ="33" ssid = "4">If (wi  r  wj) E A  we say that wi is the head of wj and wj a dependent of wi.</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="43" ssid = "14">Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.</S><S sid ="44" ssid = "15">Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'", "'33'", "'60'", "'43'", "'44'"]
'85'
'33'
'60'
'43'
'44'
['85', '33', '60', '43', '44']
parsed_discourse_facet ['method_citation']
<S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="25" ssid = "21">The rest of the paper is structured as follows.</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'65'", "'14'", "'25'", "'60'", "'75'"]
'65'
'14'
'25'
'60'
'75'
['65', '14', '25', '60', '75']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="104" ssid = "15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="26" ssid = "22">In section 2 we introduce the graph transformation techniques used to projectivize and deprojectivize dependency graphs  and in section 3 we describe the data-driven dependency parser that is the core of our system.</S><S sid ="61" ssid = "32">Before we turn to the evaluation  however  we need to introduce the data-driven dependency parser used in the latter experiments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'104'", "'21'", "'26'", "'61'"]
'11'
'104'
'21'
'26'
'61'
['11', '104', '21', '26', '61']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="43" ssid = "14">Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S><S sid ="63" ssid = "2">The parser builds dependency graphs by traversing the input from left to right  using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'11'", "'43'", "'81'", "'63'"]
'34'
'11'
'43'
'81'
'63'
['34', '11', '43', '81', '63']
parsed_discourse_facet ['results_citation']
<S sid ="86" ssid = "13">As expected  the most informative encoding  Head+Path  gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S><S sid ="57" ssid = "28">In approaching this problem  a variety of different methods are conceivable  including a more or less sophisticated use of machine learning.</S><S sid ="73" ssid = "12">More details on the memory-based prediction can be found in Nivre et al. (2004) and Nivre and Scholz (2004).</S><S sid ="61" ssid = "32">Before we turn to the evaluation  however  we need to introduce the data-driven dependency parser used in the latter experiments.</S><S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'57'", "'73'", "'61'", "'34'"]
'86'
'57'
'73'
'61'
'34'
['86', '57', '73', '61', '34']
parsed_discourse_facet ['implication_citation']
<S sid ="83" ssid = "10">It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S><S sid ="78" ssid = "5">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid ="101" ssid = "12">However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.</S><S sid ="25" ssid = "21">The rest of the paper is structured as follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'106'", "'78'", "'101'", "'25'"]
'83'
'106'
'78'
'101'
'25'
['83', '106', '78', '101', '25']
parsed_discourse_facet ['method_citation']
<S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="24" ssid = "20">We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).</S><S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="77" ssid = "4">The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'", "'24'", "'95'", "'77'", "'11'"]
'34'
'24'
'95'
'77'
'11'
['34', '24', '95', '77', '11']
parsed_discourse_facet ['results_citation']
<S sid ="95" ssid = "6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S><S sid ="65" ssid = "4">More details on the parsing algorithm can be found in Nivre (2003).</S><S sid ="92" ssid = "3">Evaluation metrics used are Attachment Score (AS)  i.e. the proportion of tokens that are attached to the correct head  and Exact Match (EM)  i.e. the proportion of sentences for which the dependency graph exactly matches the gold standard.</S><S sid ="60" ssid = "31">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'65'", "'92'", "'60'", "'71'"]
'95'
'65'
'92'
'60'
'71'
['95', '65', '92', '60', '71']
parsed_discourse_facet ['aim_citation']
<S sid ="67" ssid = "6">Table 2 shows the features used in the current version of the parser.</S><S sid ="70" ssid = "9">Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.</S><S sid ="82" ssid = "9">The last four columns in Table 3 show the distribution of nonprojective arcs with respect to the number of lifts required.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 1525%  the total proportion of non-projective arcs is normally only 12%.</S><S sid ="81" ssid = "8">However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'70'", "'82'", "'14'", "'81'"]
'67'
'70'
'82'
'14'
'81'
['67', '70', '82', '14', '81']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p.</S><S sid ="11" ssid = "7">This is in contrast to dependency treebanks  e.g.</S><S sid ="34" ssid = "5">In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi * wj for the reflexive and transitive closure of the (unlabeled) arc relation.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajic  1998).</S><S sid ="63" ssid = "2">The parser builds dependency graphs by traversing the input from left to right  using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'11'", "'34'", "'75'", "'63'"]
'51'
'11'
'34'
'75'
'63'
['51', '11', '34', '75', '63']
parsed_discourse_facet ['aim_citation']
['The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
['In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.', 'The rest of the paper is structured as follows.', u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'More details on the parsing algorithm can be found in Nivre (2003).', u'While the proportion of sentences containing non-projective dependencies is often 15\u201325%  the total proportion of non-projective arcs is normally only 1\u20132%.']
['system', 'ROUGE-S*', 'Average_R:', '0.00235', '(95%-conf.int.', '0.00235', '-', '0.00235)']
['system', 'ROUGE-S*', 'Average_P:', '0.08333', '(95%-conf.int.', '0.08333', '-', '0.08333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00458', '(95%-conf.int.', '0.00458', '-', '0.00458)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:36', 'F:3']
['In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al., 2004) and English (Nivre and Scholz, 2004).']
['The choice between different actions is in general nondeterministic  and the parser relies on a memorybased classifier  trained on treebank data  to predict the next action based on features of the current parser configuration.', u'Prague Dependency Treebank (Haji\u02c7c et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.', u'It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order  where discontinuous syntactic constructions are more common than in languages like English (Mel\u2019\u02c7cuk  1988; Covington  1990).', 'More details on the parsing algorithm can be found in Nivre (2003).', 'For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.']
['system', 'ROUGE-S*', 'Average_R:', '0.00627', '(95%-conf.int.', '0.00627', '-', '0.00627)']
['system', 'ROUGE-S*', 'Average_P:', '0.09486', '(95%-conf.int.', '0.09486', '-', '0.09486)']
['system', 'ROUGE-S*', 'Average_F:', '0.01176', '(95%-conf.int.', '0.01176', '-', '0.01176)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3828', 'P:253', 'F:24']
['The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
['We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).', 'The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).', 'This is in contrast to dependency treebanks  e.g.', 'The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.', u'In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi \u2014*\u2217 wj for the reflexive and transitive closure of the (unlabeled) arc relation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00395', '(95%-conf.int.', '0.00395', '-', '0.00395)']
['system', 'ROUGE-S*', 'Average_P:', '0.25000', '(95%-conf.int.', '0.25000', '-', '0.25000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00778', '(95%-conf.int.', '0.00778', '-', '0.00778)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:36', 'F:9']
['As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.']
[u'Prague Dependency Treebank (Haji\xcb\u2021c et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.', 'However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.', 'The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.', 'The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.', 'Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.']
['system', 'ROUGE-S*', 'Average_R:', '0.01428', '(95%-conf.int.', '0.01428', '-', '0.01428)']
['system', 'ROUGE-S*', 'Average_P:', '0.07143', '(95%-conf.int.', '0.07143', '-', '0.07143)']
['system', 'ROUGE-S*', 'Average_F:', '0.02380', '(95%-conf.int.', '0.02380', '-', '0.02380)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:378', 'F:27']
['While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.']
['In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.', 'Evaluation metrics used are Attachment Score (AS)  i.e. the proportion of tokens that are attached to the correct head  and Exact Match (EM)  i.e. the proportion of sentences for which the dependency graph exactly matches the gold standard.', 'The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.', 'More details on the parsing algorithm can be found in Nivre (2003).', 'For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.']
['system', 'ROUGE-S*', 'Average_R:', '0.00571', '(95%-conf.int.', '0.00571', '-', '0.00571)']
['system', 'ROUGE-S*', 'Average_P:', '0.16484', '(95%-conf.int.', '0.16484', '-', '0.16484)']
['system', 'ROUGE-S*', 'Average_F:', '0.01103', '(95%-conf.int.', '0.01103', '-', '0.01103)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:91', 'F:15']
['The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.']
['The last four columns in Table 3 show the distribution of nonprojective arcs with respect to the number of lifts required.', 'Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.', 'However  the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.', u'While the proportion of sentences containing non-projective dependencies is often 15\u201325%  the total proportion of non-projective arcs is normally only 1\u20132%.', 'Table 2 shows the features used in the current version of the parser.']
['system', 'ROUGE-S*', 'Average_R:', '0.02775', '(95%-conf.int.', '0.02775', '-', '0.02775)']
['system', 'ROUGE-S*', 'Average_P:', '0.04049', '(95%-conf.int.', '0.04049', '-', '0.04049)']
['system', 'ROUGE-S*', 'Average_F:', '0.03293', '(95%-conf.int.', '0.03293', '-', '0.03293)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:741', 'F:30']
['The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
['The last four columns in Table 3 show the distribution of nonprojective arcs with respect to the number of lifts required.', 'As expected  the most informative encoding  Head+Path  gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.', u'In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p\xe2\u2020\u201c.', 'In the second part of the experiment  we applied the inverse transformation based on breadth-first search under the three different encoding schemes.', 'Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.']
['system', 'ROUGE-S*', 'Average_R:', '0.00055', '(95%-conf.int.', '0.00055', '-', '0.00055)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00107', '(95%-conf.int.', '0.00107', '-', '0.00107)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:36', 'F:1']
['The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
['This is in contrast to dependency treebanks  e.g.', u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'The parser builds dependency graphs by traversing the input from left to right  using a stack to store tokens that are not yet complete with respect to their dependents.', u'In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p\u2193.', u'In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi \u2014*\u2217 wj for the reflexive and transitive closure of the (unlabeled) arc relation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:36', 'F:0']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['In addition  there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington  1990; Kahane et al.  1998; Duchier and Debusmann  2001; Holan et al.  2001; Hellwig  2003).', 'On the other hand  given that all schemes have similar parsing accuracy overall  this means that the Path scheme is the least likely to introduce errors on projective arcs.', 'Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.', u'While the proportion of sentences containing non-projective dependencies is often 15\u201325%  the total proportion of non-projective arcs is normally only 1\u20132%.', 'More details on the parsing algorithm can be found in Nivre (2003).']
['system', 'ROUGE-S*', 'Average_R:', '0.00820', '(95%-conf.int.', '0.00820', '-', '0.00820)']
['system', 'ROUGE-S*', 'Average_P:', '0.14286', '(95%-conf.int.', '0.14286', '-', '0.14286)']
['system', 'ROUGE-S*', 'Average_F:', '0.01550', '(95%-conf.int.', '0.01550', '-', '0.01550)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:105', 'F:15']
['The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
['This is in contrast to dependency treebanks  e.g.', 'The rest of the paper is structured as follows.', 'By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.', 'The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.', 'Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.']
['system', 'ROUGE-S*', 'Average_R:', '0.00196', '(95%-conf.int.', '0.00196', '-', '0.00196)']
['system', 'ROUGE-S*', 'Average_P:', '0.13889', '(95%-conf.int.', '0.13889', '-', '0.13889)']
['system', 'ROUGE-S*', 'Average_F:', '0.00386', '(95%-conf.int.', '0.00386', '-', '0.00386)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:36', 'F:5']
['This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004).']
[u'In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi \u2014*\u2217 wj for the reflexive and transitive closure of the (unlabeled) arc relation.', u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.', u'In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p\u2193.', 'The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.']
['system', 'ROUGE-S*', 'Average_R:', '0.00105', '(95%-conf.int.', '0.00105', '-', '0.00105)']
['system', 'ROUGE-S*', 'Average_P:', '0.00794', '(95%-conf.int.', '0.00794', '-', '0.00794)']
['system', 'ROUGE-S*', 'Average_F:', '0.00186', '(95%-conf.int.', '0.00186', '-', '0.00186)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:378', 'F:3']
['Even this may be nondeterministic, in case the graph contains several non-projective arcs whose lifts interact, but we use the following algorithm to construct a minimal projective transformation D0 = (W, A0) of a (nonprojective) dependency graph D = (W, A): The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right).']
['It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.', 'However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.', 'However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).', 'The rest of the paper is structured as follows.', 'The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.']
['system', 'ROUGE-S*', 'Average_R:', '0.01538', '(95%-conf.int.', '0.01538', '-', '0.01538)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.02499', '(95%-conf.int.', '0.02499', '-', '0.02499)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:465', 'F:31']
['In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
['This is in contrast to dependency treebanks  e.g.', 'The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.', 'Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.', 'The choice between different actions is in general nondeterministic  and the parser relies on a memorybased classifier  trained on treebank data  to predict the next action based on features of the current parser configuration.', u'While the proportion of sentences containing non-projective dependencies is often 15\u201325%  the total proportion of non-projective arcs is normally only 1\u20132%.']
['system', 'ROUGE-S*', 'Average_R:', '0.00552', '(95%-conf.int.', '0.00552', '-', '0.00552)']
['system', 'ROUGE-S*', 'Average_P:', '0.18681', '(95%-conf.int.', '0.18681', '-', '0.18681)']
['system', 'ROUGE-S*', 'Average_F:', '0.01072', '(95%-conf.int.', '0.01072', '-', '0.01072)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:91', 'F:17']
['In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
[u'Here we use a slightly different notion of lift  applying to individual arcs and moving their head upwards one step at a time: Intuitively  lifting an arc makes the word wk dependent on the head wi of its original head wj (which is unique in a well-formed dependency graph)  unless wj is a root in which case the operation is undefined (but then wj \u2014* wk is necessarily projective if the dependency graph is well-formed).', u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', u'In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p\u2193.', 'In the third and final scheme  denoted Path  we keep the extra infor2Note that this is a baseline for the parsing experiment only (Experiment 2).', 'It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.']
['system', 'ROUGE-S*', 'Average_R:', '0.00155', '(95%-conf.int.', '0.00155', '-', '0.00155)']
['system', 'ROUGE-S*', 'Average_P:', '0.08791', '(95%-conf.int.', '0.08791', '-', '0.08791)']
['system', 'ROUGE-S*', 'Average_F:', '0.00305', '(95%-conf.int.', '0.00305', '-', '0.00305)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5151', 'P:91', 'F:8']
['From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.']
['We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).', 'The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).', 'This is in contrast to dependency treebanks  e.g.', 'The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.', u'In the following  we use the notation wi wj to mean that (wi  r  wj) E A; r we also use wi wj to denote an arc with unspecified label and wi \u2014*\u2217 wj for the reflexive and transitive closure of the (unlabeled) arc relation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00044', '(95%-conf.int.', '0.00044', '-', '0.00044)']
['system', 'ROUGE-S*', 'Average_P:', '0.00735', '(95%-conf.int.', '0.00735', '-', '0.00735)']
['system', 'ROUGE-S*', 'Average_F:', '0.00083', '(95%-conf.int.', '0.00083', '-', '0.00083)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:136', 'F:1']
['The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.']
['As can be seen from the last column in Table 1  both Head and Head+Path may theoretically lead to a quadratic increase in the number of distinct arc labels (Head+Path being worse than Head only by a constant factor)  while the increase is only linear in the case of Path.', u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.', u'While the proportion of sentences containing non-projective dependencies is often 15\u201325%  the total proportion of non-projective arcs is normally only 1\u20132%.', 'The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.00216', '(95%-conf.int.', '0.00216', '-', '0.00216)']
['system', 'ROUGE-S*', 'Average_P:', '0.04412', '(95%-conf.int.', '0.04412', '-', '0.04412)']
['system', 'ROUGE-S*', 'Average_F:', '0.00412', '(95%-conf.int.', '0.00412', '-', '0.00412)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:136', 'F:6']
['By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.']
['If (wi  r  wj) E A  we say that wi is the head of wj and wj a dependent of wi.', 'In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.', 'Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.', 'Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.', 'The results are given in Table 4.']
['system', 'ROUGE-S*', 'Average_R:', '0.03987', '(95%-conf.int.', '0.03987', '-', '0.03987)']
['system', 'ROUGE-S*', 'Average_P:', '0.07258', '(95%-conf.int.', '0.07258', '-', '0.07258)']
['system', 'ROUGE-S*', 'Average_F:', '0.05147', '(95%-conf.int.', '0.05147', '-', '0.05147)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:496', 'F:36']
0.0875211759558 0.00805823524672 0.0123147058099





input/ref/Task1/W99-0613_aakansha.csv
input/res/Task1/W99-0613.csv
parsing: input/ref/Task1/W99-0613_aakansha.csv
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="30">Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'"]
'137'
['137']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="12">2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="4">The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="12">But we will show that the use of unlabeled data can drastically reduce the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
    <S sid="237" ssid="4">The numbers falling into the location, person, organization categories were 186, 289 and 402 respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'", "'237'"]
'236'
'237'
['236', '237']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
    <S sid="10" ssid="4">The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'10'"]
'9'
'10'
['9', '10']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S sid="139" ssid="6">This section describes AdaBoost, which is the basis for the CoBoost algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'39'"]
'137'
'39'
['137', '39']
parsed_discourse_facet ['method_citation']
<S sid="26" ssid="20">We present two algorithms.</S>
    <S sid="27" ssid="21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'27'"]
'26'
'27'
['26', '27']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="12">But we will show that the use of unlabeled data can drastically reduce the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="2">Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S>
    <S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'9'"]
'8'
'9'
['8', '9']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="125" ssid = "58">It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'178'", "'125'", "'41'"]
'256'
'198'
'178'
'125'
'41'
['256', '198', '178', '125', '41']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'97'", "'70'", "'198'"]
'256'
'178'
'97'
'70'
'198'
['256', '178', '97', '70', '198']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="128" ssid = "61">At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'128'", "'198'", "'178'", "'70'"]
'256'
'128'
'198'
'178'
'70'
['256', '128', '198', '178', '70']
parsed_discourse_facet ['results_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'97'", "'70'", "'198'", "'178'"]
'256'
'97'
'70'
'198'
'178'
['256', '97', '70', '198', '178']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="70" ssid = "3">.</S><S sid ="220" ssid = "87">(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'198'", "'70'", "'220'"]
'256'
'178'
'198'
'70'
'220'
['256', '178', '198', '70', '220']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'178'", "'70'", "'236'"]
'256'
'198'
'178'
'70'
'236'
['256', '198', '178', '70', '236']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="70" ssid = "3">.</S><S sid ="128" ssid = "61">At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'70'", "'128'", "'97'"]
'256'
'198'
'70'
'128'
'97'
['256', '198', '70', '128', '97']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="70" ssid = "3">.</S><S sid ="220" ssid = "87">(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'70'", "'220'", "'198'"]
'256'
'178'
'70'
'220'
'198'
['256', '178', '70', '220', '198']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'198'", "'236'", "'41'", "'97'"]
'256'
'198'
'236'
'41'
'97'
['256', '198', '236', '41', '97']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="70" ssid = "3">.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'97'", "'236'", "'70'"]
'256'
'178'
'97'
'236'
'70'
['256', '178', '97', '236', '70']
parsed_discourse_facet ['implication_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="125" ssid = "58">It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.</S><S sid ="41" ssid = "35">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'178'", "'198'", "'125'", "'41'"]
'256'
'178'
'198'
'125'
'41'
['256', '178', '198', '125', '41']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="97" ssid = "30">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'97'", "'178'", "'198'", "'236'"]
'256'
'97'
'178'
'198'
'236'
['256', '97', '178', '198', '236']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "7">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid ="70" ssid = "3">.</S><S sid ="198" ssid = "65">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid ="178" ssid = "45">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'70'", "'198'", "'178'", "'236'"]
'256'
'70'
'198'
'178'
'236'
['256', '70', '198', '178', '236']
parsed_discourse_facet ['method_citation']
['The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.']
['At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.', '.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00189', '(95%-conf.int.', '0.00189', '-', '0.00189)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00355', '(95%-conf.int.', '0.00355', '-', '0.00355)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:528', 'P:36', 'F:1']
['This section describes AdaBoost, which is the basis for the CoBoost algorithm.', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.']
['We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', '(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:105', 'F:0']
['The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.']
['(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).', '.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00215', '(95%-conf.int.', '0.00215', '-', '0.00215)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00368', '(95%-conf.int.', '0.00368', '-', '0.00368)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:78', 'F:1']
['This paper discusses the use of unlabeled examples for the problem of named entity classification.', 'Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.']
['We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', '.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:253', 'F:0']
['We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.', 'The numbers falling into the location, person, organization categories were 186, 289 and 402 respectively.']
['At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', '.']
['system', 'ROUGE-S*', 'Average_R:', '0.01652', '(95%-conf.int.', '0.01652', '-', '0.01652)']
['system', 'ROUGE-S*', 'Average_P:', '0.06433', '(95%-conf.int.', '0.06433', '-', '0.06433)']
['system', 'ROUGE-S*', 'Average_F:', '0.02628', '(95%-conf.int.', '0.02628', '-', '0.02628)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:171', 'F:11']
['We present two algorithms.', 'The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).']
['We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', '.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00317', '(95%-conf.int.', '0.00317', '-', '0.00317)']
['system', 'ROUGE-S*', 'Average_P:', '0.03636', '(95%-conf.int.', '0.03636', '-', '0.03636)']
['system', 'ROUGE-S*', 'Average_F:', '0.00584', '(95%-conf.int.', '0.00584', '-', '0.00584)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:55', 'F:2']
['This paper discusses the use of unlabeled examples for the problem of named entity classification.', 'The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.']
['(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).', '.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00430', '(95%-conf.int.', '0.00430', '-', '0.00430)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00593', '(95%-conf.int.', '0.00593', '-', '0.00593)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:210', 'F:2']
['Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.']
['.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00430', '(95%-conf.int.', '0.00430', '-', '0.00430)']
['system', 'ROUGE-S*', 'Average_P:', '0.01905', '(95%-conf.int.', '0.01905', '-', '0.01905)']
['system', 'ROUGE-S*', 'Average_F:', '0.00702', '(95%-conf.int.', '0.00702', '-', '0.00702)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:105', 'F:2']
['(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.']
['We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00122', '(95%-conf.int.', '0.00122', '-', '0.00122)']
['system', 'ROUGE-S*', 'Average_P:', '0.00159', '(95%-conf.int.', '0.00159', '-', '0.00159)']
['system', 'ROUGE-S*', 'Average_F:', '0.00138', '(95%-conf.int.', '0.00138', '-', '0.00138)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:630', 'F:1']
['But we will show that the use of unlabeled data can drastically reduce the need for supervision.']
['We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', '.', 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.', 'In fact  during the first rounds many of the predictions of Th.  g2 are zero.', 'Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:15', 'F:0']
0.0171449998286 0.00335499996645 0.00536799994632





input/ref/Task1/P08-1043_swastika.csv
input/res/Task1/P08-1043.csv
parsing: input/ref/Task1/P08-1043_swastika.csv
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['result_citation']
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="37">3An English sentence with ambiguous PoS assignment can be trivially represented as a lattice similar to our own, where every pair of consecutive nodes correspond to a word, and every possible PoS assignment for this word is a connecting arc.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="163" ssid="1">The accuracy results for segmentation, tagging and parsing using our different models and our standard data split are summarized in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



['163']
163
['163']
parsed_discourse_facet ['result_citation']
    <S sid="100" ssid="32">This means that the rules in our grammar are of two kinds: (a) syntactic rules relating nonterminals to a sequence of non-terminals and/or PoS tags, and (b) lexical rules relating PoS tags to lattice arcs (lexemes).</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['method_citation']
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['result_citation']
<S sid="188" ssid="2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



['188']
188
['188']
parsed_discourse_facet ['result_citation']
<S sid="86" ssid="18">A morphological analyzer M : W&#8212;* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S>
original cit marker offset is 0
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['result_citation']
<S sid="97" ssid="29">Thus our proposed model is a proper model assigning probability mass to all (7r, L) pairs, where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S>
original cit marker offset is 0
new cit marker offset is 0



['97']
97
['97']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S sid ="54" ssid = "1">A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.</S><S sid ="89" ssid = "21">Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree  s.t.</S><S sid ="179" ssid = "17">On the surface  our model may seem as a special case of Cohen and Smith in which  = 0.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the  hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="148" ssid = "26">Removing the leaves from the resulting tree yields a parse for L under G  with the desired probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'89'", "'179'", "'183'", "'148'"]
'54'
'89'
'179'
'183'
'148'
['54', '89', '179', '183', '148']
parsed_discourse_facet ['implication_citation']
<S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="33" ssid = "12">The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="169" ssid = "7">Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.</S><S sid ="188" ssid = "2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'33'", "'156'", "'169'", "'188'"]
'80'
'33'
'156'
'169'
'188'
['80', '33', '156', '169', '188']
parsed_discourse_facet ['implication_citation']
<S sid ="130" ssid = "8">To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and malformed7 were removed.</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="48" ssid = "6">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S><S sid ="176" ssid = "14">Furthermore  the combination of pruning and vertical markovization of the grammar outperforms the Oracle results reported by Cohen and Smith.</S><S sid ="97" ssid = "29">Thus our proposed model is a proper model assigning probability mass to all (7r  L) pairs  where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'80'", "'48'", "'176'", "'97'"]
'130'
'80'
'48'
'176'
'97'
['130', '80', '48', '176', '97']
parsed_discourse_facet ['results_citation']
<S sid ="49" ssid = "7">Tsarfaty and Simaan (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S><S sid ="163" ssid = "1">The accuracy results for segmentation  tagging and parsing using our different models and our standard data split are summarized in Table 1.</S><S sid ="71" ssid = "3">This is by now a fairly standard representation for multiple morphological segmentation of Hebrew utterances (Adler  2001; Bar-Haim et al.  2005; Smith et al.  2005; Cohen and Smith  2007; Adler  2007).</S><S sid ="33" ssid = "12">The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).</S><S sid ="148" ssid = "26">Removing the leaves from the resulting tree yields a parse for L under G  with the desired probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'163'", "'71'", "'33'", "'148'"]
'49'
'163'
'71'
'33'
'148'
['49', '163', '71', '33', '148']
parsed_discourse_facet ['method_citation']
<S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="45" ssid = "3">Morphological disambiguators that consider a token in context (an utterance) and propose the most likely morphological analysis of an utterance (including segmentation) were presented by Bar-Haim et al. (2005)  Adler and Elhadad (2006)  Shacham and Wintner (2007)  and achieved good results (the best segmentation result so far is around 98%).</S><S sid ="157" ssid = "35">SEGTok(noH) is the segmentation accuracy ignoring mistakes involving the implicit definite article h.11 To evaluate our performance on the tagging task we report CPOS and FPOS corresponding to coarse- and fine-grained PoS tagging results (F1) measure.</S><S sid ="169" ssid = "7">Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'156'", "'45'", "'157'", "'169'"]
'180'
'156'
'45'
'157'
'169'
['180', '156', '45', '157', '169']
parsed_discourse_facet ['method_citation']
<S sid ="169" ssid = "7">Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="161" ssid = "39">We report the F1 value of both measures.</S><S sid ="67" ssid = "14">Hence  we take the probability of the event fmnh analyzed as REL VB to be This means that we generate f and mnh independently depending on their corresponding PoS tags  and the context (as well as the syntactic relation between the two) is modeled via the derivation resulting in a sequence REL VB spanning the form fmnh. based on linear context.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'", "'156'", "'80'", "'161'", "'67'"]
'169'
'156'
'80'
'161'
'67'
['169', '156', '80', '161', '67']
parsed_discourse_facet ['method_citation']
<S sid ="88" ssid = "20">M(wi) = Li).</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="36" ssid = "15">Furthermore  the systematic way in which particles are prefixed to one another and onto an open-class category gives rise to a distinct sort of morphological ambiguity: space-delimited tokens may be ambiguous between several different segmentation possibilities.</S><S sid ="26" ssid = "5">The relativizer f(that) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.</S><S sid ="73" ssid = "5">We use double-circles to indicate the space-delimited token boundaries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'156'", "'36'", "'26'", "'73'"]
'88'
'156'
'36'
'26'
'73'
['88', '156', '36', '26', '73']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="38" ssid = "17">The form mnh itself can be read as at least three different verbs (counted  appointed  was appointed)  a noun (a portion)  and a possessed noun (her kind).</S><S sid ="108" ssid = "40">Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p * (s  p)) > 0  while other segments have never been observed as a lexical event before.</S><S sid ="49" ssid = "7">Tsarfaty and Simaan (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S><S sid ="158" ssid = "36">Evaluating parsing results in our joint framework  as argued by Tsarfaty (2006)  is not trivial under the joint disambiguation task  as the hypothesized yield need not coincide with the correct one.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'38'", "'108'", "'49'", "'158'"]
'80'
'38'
'108'
'49'
'158'
['80', '38', '108', '49', '158']
parsed_discourse_facet ['method_citation']
<S sid ="182" ssid = "20">In addition  as the CRF and PCFG look at similar sorts of information from within two inherently different models  they are far from independent and optimizing their product is meaningless.</S><S sid ="101" ssid = "33">The possible analyses of a surface token pose constraints on the analyses of specific segments.</S><S sid ="163" ssid = "1">The accuracy results for segmentation  tagging and parsing using our different models and our standard data split are summarized in Table 1.</S><S sid ="58" ssid = "5">Such tag sequences are often treated as complex tags (e.g.</S><S sid ="44" ssid = "2">Such analyzers propose multiple segmentation possibilities and their corresponding analyses for a token in isolation but have no means to determine the most likely ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["'182'", "'101'", "'163'", "'58'", "'44'"]
'182'
'101'
'163'
'58'
'44'
['182', '101', '163', '58', '44']
parsed_discourse_facet ['method_citation']
<S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="195" ssid = "9">We further thank Khalil Simaan (ILLCUvA) for his careful advise concerning the formal details of the proposal.</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="98" ssid = "30">The Grammar Our parser looks for the most likely tree spanning a single path through the lattice of which the yield is a sequence of lexemes.</S><S sid ="191" ssid = "5">In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'195'", "'80'", "'98'", "'191'"]
'180'
'195'
'80'
'98'
'191'
['180', '195', '80', '98', '191']
parsed_discourse_facet ['implication_citation']
<S sid ="120" ssid = "52">From now on all lattice arcs are tagged segments and the assignment of probability P(p * (s  p)) to lattice arcs proceeds as usual.4 A rather pathological case is when our lexical heuristics prune away all segmentation possibilities and we remain with an empty lattice.</S><S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="158" ssid = "36">Evaluating parsing results in our joint framework  as argued by Tsarfaty (2006)  is not trivial under the joint disambiguation task  as the hypothesized yield need not coincide with the correct one.</S><S sid ="157" ssid = "35">SEGTok(noH) is the segmentation accuracy ignoring mistakes involving the implicit definite article h.11 To evaluate our performance on the tagging task we report CPOS and FPOS corresponding to coarse- and fine-grained PoS tagging results (F1) measure.</S><S sid ="49" ssid = "7">Tsarfaty and Simaan (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'", "'180'", "'158'", "'157'", "'49'"]
'120'
'180'
'158'
'157'
'49'
['120', '180', '158', '157', '49']
parsed_discourse_facet ['method_citation']
<S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="26" ssid = "5">The relativizer f(that) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.</S><S sid ="54" ssid = "1">A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.</S><S sid ="130" ssid = "8">To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and malformed7 were removed.</S><S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'", "'26'", "'54'", "'130'", "'80'"]
'14'
'26'
'54'
'130'
'80'
['14', '26', '54', '130', '80']
parsed_discourse_facet ['method_citation']
<S sid ="154" ssid = "32">For all grammars  we use fine-grained PoS tags indicating various morphological features annotated therein.</S><S sid ="194" ssid = "8">Acknowledgments We thank Meni Adler and Michael Elhadad (BGU) for helpful comments and discussion.</S><S sid ="97" ssid = "29">Thus our proposed model is a proper model assigning probability mass to all (7r  L) pairs  where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S><S sid ="130" ssid = "8">To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and malformed7 were removed.</S><S sid ="149" ssid = "27">We use a patched version of BitPar allowing for direct input of probabilities instead of counts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'154'", "'194'", "'97'", "'130'", "'149'"]
'154'
'194'
'97'
'130'
'149'
['154', '194', '97', '130', '149']
parsed_discourse_facet ['method_citation']
<S sid ="5" ssid = "1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  tokens) that constitute the unanalyzed surface forms (utterances).</S><S sid ="76" ssid = "8">Furthermore  some of the arcs represent lexemes not present in the input tokens (e.g. h/DT  fl/POS)  however these are parts of valid analyses of the token (cf. super-segmental morphology section 2).</S><S sid ="180" ssid = "18">However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.</S><S sid ="108" ssid = "40">Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p * (s  p)) > 0  while other segments have never been observed as a lexical event before.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'76'", "'180'", "'108'", "'133'"]
'5'
'76'
'180'
'108'
'133'
['5', '76', '180', '108', '133']
parsed_discourse_facet ['results_citation']
<S sid ="80" ssid = "12">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S><S sid ="48" ssid = "6">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S><S sid ="181" ssid = "19">Many morphological decisions are based on long distance dependencies  and when the global syntactic evidence disagrees with evidence based on local linear context  the two models compete with one another  despite the fact that the PCFG takes also local context into account.</S><S sid ="95" ssid = "27">A compatible view is presented by Charniak et al. (1996) who consider the kind of probabilities a generative parser should get from a PoS tagger  and concludes that these should be P(w|t) and nothing fancier.3 In our setting  therefore  the Lattice is not used to induce a probability distribution on a linear context  but rather  it is used as a common-denominator of state-indexation of all segmentations possibilities of a surface form.</S><S sid ="54" ssid = "1">A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'48'", "'181'", "'95'", "'54'"]
'80'
'48'
'181'
'95'
'54'
['80', '48', '181', '95', '54']
parsed_discourse_facet ['implication_citation']
<S sid ="164" ssid = "2">In addition we report for each model its performance on goldsegmented input (GS) to indicate the upper bound 11Overt definiteness errors may be seen as a wrong feature rather than as wrong constituent and it is by now an accepted standard to report accuracy with and without such errors. for the grammars performance on the parsing task.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S><S sid ="22" ssid = "1">Segmental morphology Hebrew consists of seven particles m(from) f(when/who/that) h(the) w(and) k(like) l(to) and b(in). which may never appear in isolation and must always attach as prefixes to the following open-class category item we refer to as stem.</S><S sid ="126" ssid = "4">When a comparison against previous results requires additional pre-processing  we state it explicitly to allow for the reader to replicate the reported results.</S><S sid ="173" ssid = "11">The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter () which is tuned separately for each of the tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'164'", "'133'", "'22'", "'126'", "'173'"]
'164'
'133'
'22'
'126'
'173'
['164', '133', '22', '126', '173']
parsed_discourse_facet ['method_citation']
['Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.']
['A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.', u'Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree \u03c0 s.t.', 'Removing the leaves from the resulting tree yields a parse for L under G  with the desired probabilities.', u'On the surface  our model may seem as a special case of Cohen and Smith in which \u03b1 = 0.', u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00528', '(95%-conf.int.', '0.00528', '-', '0.00528)']
['system', 'ROUGE-S*', 'Average_P:', '0.04575', '(95%-conf.int.', '0.04575', '-', '0.04575)']
['system', 'ROUGE-S*', 'Average_F:', '0.00947', '(95%-conf.int.', '0.00947', '-', '0.00947)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:153', 'F:7']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['We further thank Khalil Simaan (ILLCUvA) for his careful advise concerning the formal details of the proposal.', 'The Grammar Our parser looks for the most likely tree spanning a single path through the lattice of which the yield is a sequence of lexemes.', 'However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.', 'In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.', 'In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.']
['system', 'ROUGE-S*', 'Average_R:', '0.00443', '(95%-conf.int.', '0.00443', '-', '0.00443)']
['system', 'ROUGE-S*', 'Average_P:', '0.04167', '(95%-conf.int.', '0.04167', '-', '0.04167)']
['system', 'ROUGE-S*', 'Average_F:', '0.00801', '(95%-conf.int.', '0.00801', '-', '0.00801)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:120', 'F:5']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).', 'The accuracy results for segmentation  tagging and parsing using our different models and our standard data split are summarized in Table 1.', 'Removing the leaves from the resulting tree yields a parse for L under G  with the desired probabilities.', 'This is by now a fairly standard representation for multiple morphological segmentation of Hebrew utterances (Adler  2001; Bar-Haim et al.  2005; Smith et al.  2005; Cohen and Smith  2007; Adler  2007).', u'Tsarfaty and Sima\xe2\u20ac\u2122an (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00095', '(95%-conf.int.', '0.00095', '-', '0.00095)']
['system', 'ROUGE-S*', 'Average_P:', '0.02500', '(95%-conf.int.', '0.02500', '-', '0.02500)']
['system', 'ROUGE-S*', 'Average_F:', '0.00183', '(95%-conf.int.', '0.00183', '-', '0.00183)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:120', 'F:3']
['Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.']
['Furthermore  the combination of pruning and vertical markovization of the grammar outperforms the Oracle results reported by Cohen and Smith.', 'In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.', u'To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and \u201cmalformed\u201d7 were removed.', 'Thus our proposed model is a proper model assigning probability mass to all (7r  L) pairs  where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.', 'Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.']
['system', 'ROUGE-S*', 'Average_R:', '0.00090', '(95%-conf.int.', '0.00090', '-', '0.00090)']
['system', 'ROUGE-S*', 'Average_P:', '0.01307', '(95%-conf.int.', '0.01307', '-', '0.01307)']
['system', 'ROUGE-S*', 'Average_F:', '0.00169', '(95%-conf.int.', '0.00169', '-', '0.00169)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:153', 'F:2']
['Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).', 'In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.', 'The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.', 'Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.', 'To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).']
['system', 'ROUGE-S*', 'Average_R:', '0.00708', '(95%-conf.int.', '0.00708', '-', '0.00708)']
['system', 'ROUGE-S*', 'Average_P:', '0.11640', '(95%-conf.int.', '0.11640', '-', '0.11640)']
['system', 'ROUGE-S*', 'Average_F:', '0.01335', '(95%-conf.int.', '0.01335', '-', '0.01335)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6216', 'P:378', 'F:44']
['3An English sentence with ambiguous PoS assignment can be trivially represented as a lattice similar to our own, where every pair of consecutive nodes correspond to a word, and every possible PoS assignment for this word is a connecting arc.']
['M(wi) = Li).', u'The relativizer f(\u201cthat\u201d) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.', 'We use double-circles to indicate the space-delimited token boundaries.', 'Furthermore  the systematic way in which particles are prefixed to one another and onto an open-class category gives rise to a distinct sort of morphological ambiguity: space-delimited tokens may be ambiguous between several different segmentation possibilities.', 'To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).']
['system', 'ROUGE-S*', 'Average_R:', '0.00085', '(95%-conf.int.', '0.00085', '-', '0.00085)']
['system', 'ROUGE-S*', 'Average_P:', '0.01053', '(95%-conf.int.', '0.01053', '-', '0.01053)']
['system', 'ROUGE-S*', 'Average_F:', '0.00158', '(95%-conf.int.', '0.00158', '-', '0.00158)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:190', 'F:2']
['Thus our proposed model is a proper model assigning probability mass to all (7r, L) pairs, where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.']
[u'In addition we report for each model its performance on goldsegmented input (GS) to indicate the upper bound 11Overt definiteness errors may be seen as a wrong feature rather than as wrong constituent and it is by now an accepted standard to report accuracy with and without such errors. for the grammars\u2019 performance on the parsing task.', 'Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.', u'The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter (\u03b1) which is tuned separately for each of the tasks.', 'When a comparison against previous results requires additional pre-processing  we state it explicitly to allow for the reader to replicate the reported results.', u'Segmental morphology Hebrew consists of seven particles m(\u201cfrom\u201d) f(\u201cwhen\u201d/\u201cwho\u201d/\u201cthat\u201d) h(\u201cthe\u201d) w(\u201cand\u201d) k(\u201clike\u201d) l(\u201cto\u201d) and b(\u201cin\u201d). which may never appear in isolation and must always attach as prefixes to the following open-class category item we refer to as stem.']
['system', 'ROUGE-S*', 'Average_R:', '0.00065', '(95%-conf.int.', '0.00065', '-', '0.00065)']
['system', 'ROUGE-S*', 'Average_P:', '0.01170', '(95%-conf.int.', '0.01170', '-', '0.01170)']
['system', 'ROUGE-S*', 'Average_F:', '0.00123', '(95%-conf.int.', '0.00123', '-', '0.00123)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:171', 'F:2']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.', 'Morphological disambiguators that consider a token in context (an utterance) and propose the most likely morphological analysis of an utterance (including segmentation) were presented by Bar-Haim et al. (2005)  Adler and Elhadad (2006)  Shacham and Wintner (2007)  and achieved good results (the best segmentation result so far is around 98%).', 'Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.', 'SEGTok(noH) is the segmentation accuracy ignoring mistakes involving the implicit definite article h.11 To evaluate our performance on the tagging task we report CPOS and FPOS corresponding to coarse- and fine-grained PoS tagging results (F1) measure.', 'To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).']
['system', 'ROUGE-S*', 'Average_R:', '0.00145', '(95%-conf.int.', '0.00145', '-', '0.00145)']
['system', 'ROUGE-S*', 'Average_P:', '0.07500', '(95%-conf.int.', '0.07500', '-', '0.07500)']
['system', 'ROUGE-S*', 'Average_F:', '0.00284', '(95%-conf.int.', '0.00284', '-', '0.00284)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6216', 'P:120', 'F:9']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['In addition  as the CRF and PCFG look at similar sorts of information from within two inherently different models  they are far from independent and optimizing their product is meaningless.', 'Such analyzers propose multiple segmentation possibilities and their corresponding analyses for a token in isolation but have no means to determine the most likely ones.', 'The possible analyses of a surface token pose constraints on the analyses of specific segments.', u'Such tag sequences are often treated as \u201ccomplex tags\u201d (e.g.', 'The accuracy results for segmentation  tagging and parsing using our different models and our standard data split are summarized in Table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00093', '(95%-conf.int.', '0.00093', '-', '0.00093)']
['system', 'ROUGE-S*', 'Average_P:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00167', '(95%-conf.int.', '0.00167', '-', '0.00167)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:120', 'F:1']
0.0386055551266 0.00250222219442 0.00462999994856





input/ref/Task1/W11-2123_vardha.csv
input/res/Task1/W11-2123.csv
parsing: input/ref/Task1/W11-2123_vardha.csv
    <S sid="12" ssid="7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
 <S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
 <S sid="131" ssid="3">Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N &#8722; 1 preceding words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'"]
'131'
['131']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="16">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="16">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="12">Compared with SRILM, IRSTLM adds several features: lower memory consumption, a binary file format with memory mapping, caching to increase speed, and quantization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'"]
'108'
['108']
parsed_discourse_facet ['method_citation']
    <S sid="129" ssid="1">In addition to the optimizations specific to each datastructure described in Section 2, we implement several general optimizations for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'129'"]
'129'
['129']
parsed_discourse_facet ['method_citation']
    <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
    <S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
    <S sid="263" ssid="5">Quantization can be improved by jointly encoding probability and backoff.</S>
original cit marker offset is 0
new cit marker offset is 0



["'263'"]
'263'
['263']
parsed_discourse_facet ['method_citation']
    <S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
    <S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="17">If the context wnf will never extend to the right (i.e. wnf v is not present in the model for all words v) then no subsequent query will match the full context.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'"]
'145'
['145']
parsed_discourse_facet ['method_citation']
 <S sid="182" ssid="1">This section measures performance on shared tasks in order of increasing complexity: sparse lookups, evaluating perplexity of a large file, and translation with Moses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'182'"]
'182'
['182']
parsed_discourse_facet ['method_citation']
    <S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
    <S sid="12" ssid="7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
    <S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="12">We have modified Moses (Koehn et al., 2007) to keep our state with hypotheses; to conserve memory, phrases do not keep state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
 <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
 <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W11-2123.csv
<S sid ="270" ssid = "12">This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S>
original cit marker offset is 0
new cit marker offset is 0



["'270'", "'276'", "'265'", "'286'", "'284'"]
'270'
'276'
'265'
'286'
'284'
['270', '276', '265', '286', '284']
parsed_discourse_facet ['implication_citation']
<S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S>
original cit marker offset is 0
new cit marker offset is 0



["'262'", "'280'", "'265'", "'277'", "'284'"]
'262'
'280'
'265'
'277'
'284'
['262', '280', '265', '277', '284']
parsed_discourse_facet ['implication_citation']
<S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="285" ssid = "6">Juri Ganitkevitch answered questions about Joshua.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'265'", "'284'", "'278'", "'285'", "'276'"]
'265'
'284'
'278'
'285'
'276'
['265', '284', '278', '285', '276']
parsed_discourse_facet ['results_citation']
<S sid ="260" ssid = "2">For speed  we plan to implement the direct-mapped cache from BerkeleyLM.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>
original cit marker offset is 0
new cit marker offset is 0



["'260'", "'262'", "'280'", "'283'", "'266'"]
'260'
'262'
'280'
'283'
'266'
['260', '262', '280', '283', '266']
parsed_discourse_facet ['method_citation']
<S sid ="272" ssid = "14">Generalizing state minimization  the model could also provide explicit bounds on probability for both backward and forward extension.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="268" ssid = "10">For example  syntactic decoders (Koehn et al.  2007; Dyer et al.  2010; Li et al.  2009) perform dynamic programming parametrized by both backward- and forward-looking state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'272'", "'265'", "'283'", "'286'", "'268'"]
'272'
'265'
'283'
'286'
'268'
['272', '265', '283', '286', '268']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="264" ssid = "6">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'284'", "'265'", "'264'", "'267'"]
'256'
'284'
'265'
'264'
'267'
['256', '284', '265', '264', '267']
parsed_discourse_facet ['method_citation']
<S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S>
original cit marker offset is 0
new cit marker offset is 0



["'283'", "'284'", "'280'", "'265'", "'275'"]
'283'
'284'
'280'
'265'
'275'
['283', '284', '280', '265', '275']
parsed_discourse_facet ['method_citation']
<S sid ="261" ssid = "3">Much could be done to further reduce memory consumption.</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'261'", "'277'", "'286'", "'284'", "'283'"]
'261'
'277'
'286'
'284'
'283'
['261', '277', '286', '284', '283']
parsed_discourse_facet ['method_citation']
<S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S><S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'286'", "'280'", "'267'", "'279'", "'283'"]
'286'
'280'
'267'
'279'
'283'
['286', '280', '267', '279', '283']
parsed_discourse_facet ['method_citation']
<S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'287'", "'266'", "'262'", "'265'"]
'256'
'287'
'266'
'262'
'265'
['256', '287', '266', '262', '265']
parsed_discourse_facet ['implication_citation']
<S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="264" ssid = "6">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'262'", "'278'", "'277'", "'265'", "'264'"]
'262'
'278'
'277'
'265'
'264'
['262', '278', '277', '265', '264']
parsed_discourse_facet ['method_citation']
<S sid ="261" ssid = "3">Much could be done to further reduce memory consumption.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S><S sid ="269" ssid = "11">If they knew that the first four words in a hypergraph node would never extend to the left and form a 5-gram  then three or even fewer words could be kept in the backward state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'261'", "'265'", "'266'", "'267'", "'269'"]
'261'
'265'
'266'
'267'
'269'
['261', '265', '266', '267', '269']
parsed_discourse_facet ['method_citation']
<S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="266" ssid = "8">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="274" ssid = "1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'287'", "'266'", "'286'", "'262'", "'274'"]
'287'
'266'
'286'
'262'
'274'
['287', '266', '286', '262', '274']
parsed_discourse_facet ['method_citation']
<S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="284" ssid = "5">Chris Dyer integrated the code into cdec.</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'286'", "'256'", "'265'", "'284'", "'276'"]
'286'
'256'
'265'
'284'
'276'
['286', '256', '265', '284', '276']
parsed_discourse_facet ['results_citation']
<S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="275" ssid = "2">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S><S sid ="270" ssid = "12">This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.</S>
original cit marker offset is 0
new cit marker offset is 0



["'278'", "'262'", "'265'", "'275'", "'270'"]
'278'
'262'
'265'
'275'
'270'
['278', '262', '265', '275', '270']
parsed_discourse_facet ['implication_citation']
<S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'280'", "'262'", "'265'", "'279'", "'256'"]
'280'
'262'
'265'
'279'
'256'
['280', '262', '265', '279', '256']
parsed_discourse_facet ['method_citation']
<S sid ="279" ssid = "6">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="262" ssid = "4">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="276" ssid = "3">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S>
original cit marker offset is 0
new cit marker offset is 0



["'279'", "'256'", "'262'", "'265'", "'276'"]
'279'
'256'
'262'
'265'
'276'
['279', '256', '262', '265', '276']
parsed_discourse_facet ['results_citation']
<S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="277" ssid = "4">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid ="286" ssid = "7">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="264" ssid = "6">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'256'", "'277'", "'286'", "'287'", "'264'"]
'256'
'277'
'286'
'287'
'264'
['256', '277', '286', '287', '264']
parsed_discourse_facet ['aim_citation']
<S sid ="263" ssid = "5">Quantization can be improved by jointly encoding probability and backoff.</S><S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="287" ssid = "8">0750271 and by the DARPA GALE program.</S><S sid ="283" ssid = "4">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid ="258" ssid = "77">However  the point of RandLM is to scale to even larger data  compensating for this loss in quality.</S>
original cit marker offset is 0
new cit marker offset is 0



["'263'", "'265'", "'287'", "'283'", "'258'"]
'263'
'265'
'287'
'283'
'258'
['263', '265', '287', '283', '258']
parsed_discourse_facet ['method_citation']
<S sid ="265" ssid = "7">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid ="280" ssid = "1">Alon Lavie advised on this work.</S><S sid ="256" ssid = "75">We elected run Moses single-threaded to minimize the impact of RandLMs cache on memory use.</S><S sid ="278" ssid = "5">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid ="267" ssid = "9">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'265'", "'280'", "'256'", "'278'", "'267'"]
'265'
'280'
'256'
'278'
'267'
['265', '280', '256', '278', '267']
parsed_discourse_facet ['aim_citation']
['If the context wnf will never extend to the right (i.e. wnf v is not present in the model for all words v) then no subsequent query will match the full context.']
['Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', 'We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.', 'Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.', '0750271 and by the DARPA GALE program.', 'This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:66', 'F:0']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['If they knew that the first four words in a hypergraph node would never extend to the left and form a 5-gram  then three or even fewer words could be kept in the backward state.', 'Much could be done to further reduce memory consumption.', 'While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.', 'Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).']
['system', 'ROUGE-S*', 'Average_R:', '0.00490', '(95%-conf.int.', '0.00490', '-', '0.00490)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.00921', '(95%-conf.int.', '0.00921', '-', '0.00921)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:78', 'F:6']
['We have modified Moses (Koehn et al., 2007) to keep our state with hypotheses; to conserve memory, phrases do not keep state.']
['For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.', 'This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.', u'We elected run Moses single-threaded to minimize the impact of RandLM\xe2\u20ac\u2122s cache on memory use.', '0750271 and by the DARPA GALE program.', 'These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.']
['system', 'ROUGE-S*', 'Average_R:', '0.00145', '(95%-conf.int.', '0.00145', '-', '0.00145)']
['system', 'ROUGE-S*', 'Average_P:', '0.03636', '(95%-conf.int.', '0.03636', '-', '0.03636)']
['system', 'ROUGE-S*', 'Average_F:', '0.00279', '(95%-conf.int.', '0.00279', '-', '0.00279)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:55', 'F:2']
['Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.']
['Generalizing state minimization  the model could also provide explicit bounds on probability for both backward and forward extension.', 'For example  syntactic decoders (Koehn et al.  2007; Dyer et al.  2010; Li et al.  2009) perform dynamic programming parametrized by both backward- and forward-looking state.', 'Nicola Bertoldi and Marcello Federico assisted with IRSTLM.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.']
['system', 'ROUGE-S*', 'Average_R:', '0.02403', '(95%-conf.int.', '0.02403', '-', '0.02403)']
['system', 'ROUGE-S*', 'Average_P:', '0.21637', '(95%-conf.int.', '0.21637', '-', '0.21637)']
['system', 'ROUGE-S*', 'Average_F:', '0.04325', '(95%-conf.int.', '0.04325', '-', '0.04325)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:171', 'F:37']
['Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N &#8722; 1 preceding words.']
['We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.', 'Juri Ganitkevitch answered questions about Joshua.', 'Chris Dyer integrated the code into cdec.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:105', 'F:0']
['Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.']
['Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', 'Nicola Bertoldi and Marcello Federico assisted with IRSTLM.', 'For speed  we plan to implement the direct-mapped cache from BerkeleyLM.', 'Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.', 'Alon Lavie advised on this work.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:171', 'F:0']
['Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.']
['We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', 'For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.']
['system', 'ROUGE-S*', 'Average_R:', '0.00164', '(95%-conf.int.', '0.00164', '-', '0.00164)']
['system', 'ROUGE-S*', 'Average_P:', '0.04545', '(95%-conf.int.', '0.04545', '-', '0.04545)']
['system', 'ROUGE-S*', 'Average_F:', '0.00316', '(95%-conf.int.', '0.00316', '-', '0.00316)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:66', 'F:3']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.', u'We elected run Moses single-threaded to minimize the impact of RandLM\xe2\u20ac\u2122s cache on memory use.', 'The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:55', 'F:0']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', 'These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.', 'Chris Dyer integrated the code into cdec.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'Alon Lavie advised on this work.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:55', 'F:0']
['Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.']
['The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.', 'Alon Lavie advised on this work.', u'We elected run Moses single-threaded to minimize the impact of RandLM\xe2\u20ac\u2122s cache on memory use.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:78', 'F:0']
['Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.']
['This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.', 'Chris Dyer integrated the code into cdec.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.', 'This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.']
['system', 'ROUGE-S*', 'Average_R:', '0.00111', '(95%-conf.int.', '0.00111', '-', '0.00111)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00204', '(95%-conf.int.', '0.00204', '-', '0.00204)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:78', 'F:1']
['In addition to the optimizations specific to each datastructure described in Section 2, we implement several general optimizations for language modeling.']
['Alon Lavie advised on this work.', 'Nicola Bertoldi and Marcello Federico assisted with IRSTLM.', 'Chris Dyer integrated the code into cdec.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.']
['system', 'ROUGE-S*', 'Average_R:', '0.00336', '(95%-conf.int.', '0.00336', '-', '0.00336)']
['system', 'ROUGE-S*', 'Average_P:', '0.03636', '(95%-conf.int.', '0.03636', '-', '0.03636)']
['system', 'ROUGE-S*', 'Average_F:', '0.00615', '(95%-conf.int.', '0.00615', '-', '0.00615)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:55', 'F:2']
['Compared with SRILM, IRSTLM adds several features: lower memory consumption, a binary file format with memory mapping, caching to increase speed, and quantization.']
['While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.', u'We elected run Moses single-threaded to minimize the impact of RandLM\xe2\u20ac\u2122s cache on memory use.', 'Chris Dyer integrated the code into cdec.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.']
['system', 'ROUGE-S*', 'Average_R:', '0.00075', '(95%-conf.int.', '0.00075', '-', '0.00075)']
['system', 'ROUGE-S*', 'Average_P:', '0.00735', '(95%-conf.int.', '0.00735', '-', '0.00735)']
['system', 'ROUGE-S*', 'Average_F:', '0.00137', '(95%-conf.int.', '0.00137', '-', '0.00137)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:136', 'F:1']
['We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.']
['We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.', 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', 'This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.', 'The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).']
['system', 'ROUGE-S*', 'Average_R:', '0.00452', '(95%-conf.int.', '0.00452', '-', '0.00452)']
['system', 'ROUGE-S*', 'Average_P:', '0.13333', '(95%-conf.int.', '0.13333', '-', '0.13333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00875', '(95%-conf.int.', '0.00875', '-', '0.00875)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:45', 'F:6']
['For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.']
['We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.', 'Alon Lavie advised on this work.', 'While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.', u'We elected run Moses single-threaded to minimize the impact of RandLM\xe2\u20ac\u2122s cache on memory use.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).']
['system', 'ROUGE-S*', 'Average_R:', '0.00078', '(95%-conf.int.', '0.00078', '-', '0.00078)']
['system', 'ROUGE-S*', 'Average_P:', '0.00308', '(95%-conf.int.', '0.00308', '-', '0.00308)']
['system', 'ROUGE-S*', 'Average_F:', '0.00125', '(95%-conf.int.', '0.00125', '-', '0.00125)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:325', 'F:1']
['For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.']
['Nicola Bertoldi and Marcello Federico assisted with IRSTLM.', 'Much could be done to further reduce memory consumption.', 'This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.', 'Chris Dyer integrated the code into cdec.', 'These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:325', 'F:0']
['This section measures performance on shared tasks in order of increasing complexity: sparse lookups, evaluating perplexity of a large file, and translation with Moses.']
['The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.', u'We elected run Moses single-threaded to minimize the impact of RandLM\xe2\u20ac\u2122s cache on memory use.', 'Chris Dyer integrated the code into cdec.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).', 'This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:120', 'F:0']
['For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.']
['Quantization can be improved by jointly encoding probability and backoff.', 'Nicola Bertoldi and Marcello Federico assisted with IRSTLM.', 'However  the point of RandLM is to scale to even larger data  compensating for this loss in quality.', '0750271 and by the DARPA GALE program.', 'Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:325', 'F:0']
0.0315577776025 0.0023633333202 0.0043316666426
creating setting: abstract
else part task 2
eval: abstract
['system', 'ROUGE-1', 'Average_R:', '0.70833', '(95%-conf.int.', '0.60415', '-', '0.78005)']
['system', 'ROUGE-1', 'Average_P:', '0.06851', '(95%-conf.int.', '0.04924', '-', '0.08573)']
['system', 'ROUGE-1', 'Average_F:', '0.12469', '(95%-conf.int.', '0.09209', '-', '0.15366)']
['system', 'ROUGE-1', 'Eval', 'A00-2018.system', 'R:82', 'P:549', 'F:69']
['system', 'ROUGE-1', 'Eval', 'A00-2030.system', 'R:31', 'P:532', 'F:23']
['system', 'ROUGE-1', 'Eval', 'A97-1014.system', 'R:36', 'P:448', 'F:18']
['system', 'ROUGE-1', 'Eval', 'D09-1092.system', 'R:50', 'P:573', 'F:38']
['system', 'ROUGE-1', 'Eval', 'D10-1044.system', 'R:44', 'P:687', 'F:40']
['system', 'ROUGE-1', 'Eval', 'E03-1005.system', 'R:53', 'P:583', 'F:38']
['system', 'ROUGE-1', 'Eval', 'J01-2004.system', 'R:90', 'P:518', 'F:54']
['system', 'ROUGE-1', 'Eval', 'P04-1036.system', 'R:87', 'P:577', 'F:63']
['system', 'ROUGE-1', 'Eval', 'P05-1013.system', 'R:52', 'P:697', 'F:41']
['system', 'ROUGE-1', 'Eval', 'P08-1028.system', 'R:40', 'P:528', 'F:32']
['system', 'ROUGE-1', 'Eval', 'P08-1043.system', 'R:67', 'P:750', 'F:44']
['system', 'ROUGE-1', 'Eval', 'P08-1102.system', 'R:62', 'P:647', 'F:52']
['system', 'ROUGE-1', 'Eval', 'P11-1060.system', 'R:63', 'P:624', 'F:51']
['system', 'ROUGE-1', 'Eval', 'P11-1061.system', 'R:70', 'P:642', 'F:57']
['system', 'ROUGE-1', 'Eval', 'P87-1015.system', 'R:60', 'P:719', 'F:44']
['system', 'ROUGE-1', 'Eval', 'W06-2932.system', 'R:53', 'P:493', 'F:49']
['system', 'ROUGE-1', 'Eval', 'W06-3114.system', 'R:24', 'P:1855', 'F:21']
['system', 'ROUGE-1', 'Eval', 'W11-2123.system', 'R:79', 'P:241', 'F:50']
['system', 'ROUGE-1', 'Eval', 'W99-0613.system', 'R:77', 'P:68', 'F:11']
['system', 'ROUGE-1', 'Eval', 'W99-0623.system', 'R:32', 'P:460', 'F:25']
['system', 'ROUGE-2', 'Average_R:', '0.33516', '(95%-conf.int.', '0.24483', '-', '0.42066)']
yay
['system', 'ROUGE-2', 'Average_P:', '0.03192', '(95%-conf.int.', '0.02061', '-', '0.04336)']
yay
['system', 'ROUGE-2', 'Average_F:', '0.05816', '(95%-conf.int.', '0.03812', '-', '0.07779)']
yay
['system', 'ROUGE-2', 'Eval', 'A00-2018.system', 'R:81', 'P:548', 'F:45']
yay
['system', 'ROUGE-2', 'Eval', 'A00-2030.system', 'R:30', 'P:531', 'F:12']
yay
['system', 'ROUGE-2', 'Eval', 'A97-1014.system', 'R:35', 'P:447', 'F:2']
yay
['system', 'ROUGE-2', 'Eval', 'D09-1092.system', 'R:49', 'P:572', 'F:15']
yay
['system', 'ROUGE-2', 'Eval', 'D10-1044.system', 'R:43', 'P:686', 'F:32']
yay
['system', 'ROUGE-2', 'Eval', 'E03-1005.system', 'R:52', 'P:582', 'F:13']
yay
['system', 'ROUGE-2', 'Eval', 'J01-2004.system', 'R:89', 'P:517', 'F:11']
yay
['system', 'ROUGE-2', 'Eval', 'P04-1036.system', 'R:86', 'P:576', 'F:22']
yay
['system', 'ROUGE-2', 'Eval', 'P05-1013.system', 'R:51', 'P:696', 'F:20']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1028.system', 'R:39', 'P:527', 'F:15']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1043.system', 'R:66', 'P:749', 'F:10']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1102.system', 'R:61', 'P:646', 'F:41']
yay
['system', 'ROUGE-2', 'Eval', 'P11-1060.system', 'R:62', 'P:623', 'F:32']
yay
['system', 'ROUGE-2', 'Eval', 'P11-1061.system', 'R:69', 'P:641', 'F:40']
yay
['system', 'ROUGE-2', 'Eval', 'P87-1015.system', 'R:59', 'P:718', 'F:19']
yay
['system', 'ROUGE-2', 'Eval', 'W06-2932.system', 'R:52', 'P:492', 'F:24']
yay
['system', 'ROUGE-2', 'Eval', 'W06-3114.system', 'R:23', 'P:1854', 'F:4']
yay
['system', 'ROUGE-2', 'Eval', 'W11-2123.system', 'R:78', 'P:240', 'F:17']
yay
['system', 'ROUGE-2', 'Eval', 'W99-0613.system', 'R:76', 'P:67', 'F:3']
yay
['system', 'ROUGE-2', 'Eval', 'W99-0623.system', 'R:31', 'P:459', 'F:7']
yay
['system', 'ROUGE-SU*', 'Average_R:', '0.47323', '(95%-conf.int.', '0.36254', '-', '0.56046)']
['system', 'ROUGE-SU*', 'Average_P:', '0.00417', '(95%-conf.int.', '0.00182', '-', '0.00688)']
['system', 'ROUGE-SU*', 'Average_F:', '0.00825', '(95%-conf.int.', '0.00362', '-', '0.01359)']
['system', 'ROUGE-SU*', 'Eval', 'A00-2018.system', 'R:3402', 'P:150974', 'F:2084']
['system', 'ROUGE-SU*', 'Eval', 'A00-2030.system', 'R:495', 'P:141777', 'F:197']
['system', 'ROUGE-SU*', 'Eval', 'A97-1014.system', 'R:665', 'P:100575', 'F:140']
['system', 'ROUGE-SU*', 'Eval', 'D09-1092.system', 'R:1274', 'P:164450', 'F:700']
['system', 'ROUGE-SU*', 'Eval', 'D10-1044.system', 'R:989', 'P:236327', 'F:745']
['system', 'ROUGE-SU*', 'Eval', 'E03-1005.system', 'R:1430', 'P:170235', 'F:690']
['system', 'ROUGE-SU*', 'Eval', 'J01-2004.system', 'R:4094', 'P:134420', 'F:1368']
['system', 'ROUGE-SU*', 'Eval', 'P04-1036.system', 'R:3827', 'P:166752', 'F:1968']
['system', 'ROUGE-SU*', 'Eval', 'P05-1013.system', 'R:1377', 'P:243252', 'F:830']
['system', 'ROUGE-SU*', 'Eval', 'P08-1028.system', 'R:819', 'P:139655', 'F:490']
['system', 'ROUGE-SU*', 'Eval', 'P08-1043.system', 'R:2277', 'P:281624', 'F:922']
['system', 'ROUGE-SU*', 'Eval', 'P08-1102.system', 'R:1952', 'P:209627', 'F:1378']
['system', 'ROUGE-SU*', 'Eval', 'P11-1060.system', 'R:2015', 'P:194999', 'F:1123']
['system', 'ROUGE-SU*', 'Eval', 'P11-1061.system', 'R:2484', 'P:206402', 'F:1633']
['system', 'ROUGE-SU*', 'Eval', 'P87-1015.system', 'R:1829', 'P:258839', 'F:948']
['system', 'ROUGE-SU*', 'Eval', 'W06-2932.system', 'R:1430', 'P:121770', 'F:1070']
['system', 'ROUGE-SU*', 'Eval', 'W06-3114.system', 'R:299', 'P:1721439', 'F:218']
['system', 'ROUGE-SU*', 'Eval', 'W11-2123.system', 'R:3159', 'P:29160', 'F:874']
['system', 'ROUGE-SU*', 'Eval', 'W99-0613.system', 'R:3002', 'P:2345', 'F:66']
['system', 'ROUGE-SU*', 'Eval', 'W99-0623.system', 'R:527', 'P:106029', 'F:276']
creating setting: community
else part task 2
eval: community
['system', 'ROUGE-1', 'Average_R:', '0.53886', '(95%-conf.int.', '0.47014', '-', '0.59616)']
['system', 'ROUGE-1', 'Average_P:', '0.25420', '(95%-conf.int.', '0.19040', '-', '0.30358)']
['system', 'ROUGE-1', 'Average_F:', '0.34407', '(95%-conf.int.', '0.27643', '-', '0.39343)']
['system', 'ROUGE-1', 'Eval', 'A00-2018.system', 'R:1269', 'P:1647', 'F:558']
['system', 'ROUGE-1', 'Eval', 'A00-2030.system', 'R:1053', 'P:1596', 'F:465']
['system', 'ROUGE-1', 'Eval', 'A97-1014.system', 'R:603', 'P:1344', 'F:258']
['system', 'ROUGE-1', 'Eval', 'D09-1092.system', 'R:1296', 'P:1719', 'F:651']
['system', 'ROUGE-1', 'Eval', 'D10-1044.system', 'R:1125', 'P:2061', 'F:594']
['system', 'ROUGE-1', 'Eval', 'E03-1005.system', 'R:801', 'P:1749', 'F:528']
['system', 'ROUGE-1', 'Eval', 'J01-2004.system', 'R:558', 'P:1554', 'F:288']
['system', 'ROUGE-1', 'Eval', 'P04-1036.system', 'R:1004', 'P:2308', 'F:616']
['system', 'ROUGE-1', 'Eval', 'P05-1013.system', 'R:633', 'P:2091', 'F:444']
['system', 'ROUGE-1', 'Eval', 'P08-1028.system', 'R:612', 'P:1584', 'F:372']
['system', 'ROUGE-1', 'Eval', 'P08-1043.system', 'R:1323', 'P:2250', 'F:885']
['system', 'ROUGE-1', 'Eval', 'P08-1102.system', 'R:972', 'P:1941', 'F:666']
['system', 'ROUGE-1', 'Eval', 'P11-1060.system', 'R:762', 'P:1872', 'F:453']
['system', 'ROUGE-1', 'Eval', 'P11-1061.system', 'R:768', 'P:1926', 'F:468']
['system', 'ROUGE-1', 'Eval', 'P87-1015.system', 'R:897', 'P:2157', 'F:537']
['system', 'ROUGE-1', 'Eval', 'W06-2932.system', 'R:1014', 'P:1479', 'F:495']
['system', 'ROUGE-1', 'Eval', 'W06-3114.system', 'R:543', 'P:5565', 'F:336']
['system', 'ROUGE-1', 'Eval', 'W11-2123.system', 'R:300', 'P:723', 'F:111']
['system', 'ROUGE-1', 'Eval', 'W99-0613.system', 'R:824', 'P:272', 'F:56']
['system', 'ROUGE-1', 'Eval', 'W99-0623.system', 'R:852', 'P:1380', 'F:516']
['system', 'ROUGE-2', 'Average_R:', '0.26984', '(95%-conf.int.', '0.21870', '-', '0.32123)']
yay
['system', 'ROUGE-2', 'Average_P:', '0.12710', '(95%-conf.int.', '0.09135', '-', '0.16183)']
yay
['system', 'ROUGE-2', 'Average_F:', '0.17212', '(95%-conf.int.', '0.13237', '-', '0.21154)']
yay
['system', 'ROUGE-2', 'Eval', 'A00-2018.system', 'R:1266', 'P:1644', 'F:216']
yay
['system', 'ROUGE-2', 'Eval', 'A00-2030.system', 'R:1050', 'P:1593', 'F:198']
yay
['system', 'ROUGE-2', 'Eval', 'A97-1014.system', 'R:600', 'P:1341', 'F:153']
yay
['system', 'ROUGE-2', 'Eval', 'D09-1092.system', 'R:1293', 'P:1716', 'F:279']
yay
['system', 'ROUGE-2', 'Eval', 'D10-1044.system', 'R:1122', 'P:2058', 'F:270']
yay
['system', 'ROUGE-2', 'Eval', 'E03-1005.system', 'R:798', 'P:1746', 'F:324']
yay
['system', 'ROUGE-2', 'Eval', 'J01-2004.system', 'R:555', 'P:1551', 'F:81']
yay
['system', 'ROUGE-2', 'Eval', 'P04-1036.system', 'R:1000', 'P:2304', 'F:384']
yay
['system', 'ROUGE-2', 'Eval', 'P05-1013.system', 'R:630', 'P:2088', 'F:228']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1028.system', 'R:609', 'P:1581', 'F:135']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1043.system', 'R:1320', 'P:2247', 'F:534']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1102.system', 'R:969', 'P:1938', 'F:462']
yay
['system', 'ROUGE-2', 'Eval', 'P11-1060.system', 'R:759', 'P:1869', 'F:225']
yay
['system', 'ROUGE-2', 'Eval', 'P11-1061.system', 'R:765', 'P:1923', 'F:210']
yay
['system', 'ROUGE-2', 'Eval', 'P87-1015.system', 'R:894', 'P:2154', 'F:201']
yay
['system', 'ROUGE-2', 'Eval', 'W06-2932.system', 'R:1011', 'P:1476', 'F:231']
yay
['system', 'ROUGE-2', 'Eval', 'W06-3114.system', 'R:540', 'P:5562', 'F:132']
yay
['system', 'ROUGE-2', 'Eval', 'W11-2123.system', 'R:297', 'P:720', 'F:48']
yay
['system', 'ROUGE-2', 'Eval', 'W99-0613.system', 'R:820', 'P:268', 'F:16']
yay
['system', 'ROUGE-2', 'Eval', 'W99-0623.system', 'R:849', 'P:1377', 'F:297']
yay
['system', 'ROUGE-SU*', 'Average_R:', '0.29201', '(95%-conf.int.', '0.24127', '-', '0.34503)']
['system', 'ROUGE-SU*', 'Average_P:', '0.05964', '(95%-conf.int.', '0.02733', '-', '0.09582)']
['system', 'ROUGE-SU*', 'Average_F:', '0.09749', '(95%-conf.int.', '0.04971', '-', '0.14548)']
['system', 'ROUGE-SU*', 'Eval', 'A00-2018.system', 'R:269025', 'P:452922', 'F:52206']
['system', 'ROUGE-SU*', 'Eval', 'A00-2030.system', 'R:185325', 'P:425331', 'F:31404']
['system', 'ROUGE-SU*', 'Eval', 'A97-1014.system', 'R:60900', 'P:301725', 'F:7527']
['system', 'ROUGE-SU*', 'Eval', 'D09-1092.system', 'R:280581', 'P:493350', 'F:70767']
['system', 'ROUGE-SU*', 'Eval', 'D10-1044.system', 'R:211497', 'P:708981', 'F:58572']
['system', 'ROUGE-SU*', 'Eval', 'E03-1005.system', 'R:107331', 'P:510705', 'F:44667']
['system', 'ROUGE-SU*', 'Eval', 'J01-2004.system', 'R:52170', 'P:403260', 'F:13611']
['system', 'ROUGE-SU*', 'Eval', 'P04-1036.system', 'R:126500', 'P:667008', 'F:41704']
['system', 'ROUGE-SU*', 'Eval', 'P05-1013.system', 'R:67095', 'P:729756', 'F:31320']
['system', 'ROUGE-SU*', 'Eval', 'P08-1028.system', 'R:62727', 'P:418965', 'F:21360']
['system', 'ROUGE-SU*', 'Eval', 'P08-1043.system', 'R:292380', 'P:844872', 'F:126747']
['system', 'ROUGE-SU*', 'Eval', 'P08-1102.system', 'R:157947', 'P:628881', 'F:66738']
['system', 'ROUGE-SU*', 'Eval', 'P11-1060.system', 'R:97152', 'P:584997', 'F:29286']
['system', 'ROUGE-SU*', 'Eval', 'P11-1061.system', 'R:98685', 'P:619206', 'F:36780']
['system', 'ROUGE-SU*', 'Eval', 'P87-1015.system', 'R:134547', 'P:776517', 'F:43419']
['system', 'ROUGE-SU*', 'Eval', 'W06-2932.system', 'R:171870', 'P:365310', 'F:36717']
['system', 'ROUGE-SU*', 'Eval', 'W06-3114.system', 'R:49410', 'P:5164317', 'F:18000']
['system', 'ROUGE-SU*', 'Eval', 'W11-2123.system', 'R:15147', 'P:87480', 'F:1374']
['system', 'ROUGE-SU*', 'Eval', 'W99-0613.system', 'R:85280', 'P:9380', 'F:392']
['system', 'ROUGE-SU*', 'Eval', 'W99-0623.system', 'R:121407', 'P:318087', 'F:40761']
creating setting: human
else part task 2
eval: human
['system', 'ROUGE-1', 'Average_R:', '0.70582', '(95%-conf.int.', '0.61007', '-', '0.77910)']
['system', 'ROUGE-1', 'Average_P:', '0.10124', '(95%-conf.int.', '0.07732', '-', '0.12219)']
['system', 'ROUGE-1', 'Average_F:', '0.17676', '(95%-conf.int.', '0.14009', '-', '0.20910)']
['system', 'ROUGE-1', 'Eval', 'A00-2018.system', 'R:336', 'P:1647', 'F:222']
['system', 'ROUGE-1', 'Eval', 'A00-2030.system', 'R:252', 'P:1596', 'F:204']
['system', 'ROUGE-1', 'Eval', 'A97-1014.system', 'R:228', 'P:1344', 'F:108']
['system', 'ROUGE-1', 'Eval', 'D09-1092.system', 'R:201', 'P:1719', 'F:138']
['system', 'ROUGE-1', 'Eval', 'D10-1044.system', 'R:348', 'P:2061', 'F:306']
['system', 'ROUGE-1', 'Eval', 'E03-1005.system', 'R:309', 'P:1749', 'F:270']
['system', 'ROUGE-1', 'Eval', 'J01-2004.system', 'R:201', 'P:1554', 'F:156']
['system', 'ROUGE-1', 'Eval', 'P04-1036.system', 'R:344', 'P:2308', 'F:276']
['system', 'ROUGE-1', 'Eval', 'P05-1013.system', 'R:243', 'P:2091', 'F:198']
['system', 'ROUGE-1', 'Eval', 'P08-1028.system', 'R:333', 'P:1584', 'F:189']
['system', 'ROUGE-1', 'Eval', 'P08-1043.system', 'R:198', 'P:2250', 'F:156']
['system', 'ROUGE-1', 'Eval', 'P08-1102.system', 'R:318', 'P:1941', 'F:279']
['system', 'ROUGE-1', 'Eval', 'P11-1060.system', 'R:270', 'P:1872', 'F:207']
['system', 'ROUGE-1', 'Eval', 'P11-1061.system', 'R:306', 'P:1926', 'F:240']
['system', 'ROUGE-1', 'Eval', 'P87-1015.system', 'R:180', 'P:2157', 'F:135']
['system', 'ROUGE-1', 'Eval', 'W06-2932.system', 'R:219', 'P:1479', 'F:177']
['system', 'ROUGE-1', 'Eval', 'W06-3114.system', 'R:213', 'P:5565', 'F:168']
['system', 'ROUGE-1', 'Eval', 'W11-2123.system', 'R:252', 'P:723', 'F:123']
['system', 'ROUGE-1', 'Eval', 'W99-0613.system', 'R:256', 'P:272', 'F:36']
['system', 'ROUGE-1', 'Eval', 'W99-0623.system', 'R:261', 'P:1380', 'F:147']
['system', 'ROUGE-2', 'Average_R:', '0.34443', '(95%-conf.int.', '0.26349', '-', '0.42067)']
yay
['system', 'ROUGE-2', 'Average_P:', '0.04896', '(95%-conf.int.', '0.03388', '-', '0.06435)']
yay
['system', 'ROUGE-2', 'Average_F:', '0.08559', '(95%-conf.int.', '0.05982', '-', '0.11076)']
yay
['system', 'ROUGE-2', 'Eval', 'A00-2018.system', 'R:333', 'P:1644', 'F:93']
yay
['system', 'ROUGE-2', 'Eval', 'A00-2030.system', 'R:249', 'P:1593', 'F:105']
yay
['system', 'ROUGE-2', 'Eval', 'A97-1014.system', 'R:225', 'P:1341', 'F:21']
yay
['system', 'ROUGE-2', 'Eval', 'D09-1092.system', 'R:198', 'P:1716', 'F:54']
yay
['system', 'ROUGE-2', 'Eval', 'D10-1044.system', 'R:345', 'P:2058', 'F:201']
yay
['system', 'ROUGE-2', 'Eval', 'E03-1005.system', 'R:306', 'P:1746', 'F:171']
yay
['system', 'ROUGE-2', 'Eval', 'J01-2004.system', 'R:198', 'P:1551', 'F:87']
yay
['system', 'ROUGE-2', 'Eval', 'P04-1036.system', 'R:340', 'P:2304', 'F:116']
yay
['system', 'ROUGE-2', 'Eval', 'P05-1013.system', 'R:240', 'P:2088', 'F:132']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1028.system', 'R:330', 'P:1581', 'F:30']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1043.system', 'R:195', 'P:2247', 'F:27']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1102.system', 'R:315', 'P:1938', 'F:195']
yay
['system', 'ROUGE-2', 'Eval', 'P11-1060.system', 'R:267', 'P:1869', 'F:102']
yay
['system', 'ROUGE-2', 'Eval', 'P11-1061.system', 'R:303', 'P:1923', 'F:132']
yay
['system', 'ROUGE-2', 'Eval', 'P87-1015.system', 'R:177', 'P:2154', 'F:60']
yay
['system', 'ROUGE-2', 'Eval', 'W06-2932.system', 'R:216', 'P:1476', 'F:90']
yay
['system', 'ROUGE-2', 'Eval', 'W06-3114.system', 'R:210', 'P:5562', 'F:63']
yay
['system', 'ROUGE-2', 'Eval', 'W11-2123.system', 'R:249', 'P:720', 'F:51']
yay
['system', 'ROUGE-2', 'Eval', 'W99-0613.system', 'R:252', 'P:268', 'F:12']
yay
['system', 'ROUGE-2', 'Eval', 'W99-0623.system', 'R:258', 'P:1377', 'F:69']
yay
['system', 'ROUGE-SU*', 'Average_R:', '0.50029', '(95%-conf.int.', '0.40762', '-', '0.57925)']
['system', 'ROUGE-SU*', 'Average_P:', '0.00897', '(95%-conf.int.', '0.00435', '-', '0.01433)']
['system', 'ROUGE-SU*', 'Average_F:', '0.01760', '(95%-conf.int.', '0.00863', '-', '0.02788)']
['system', 'ROUGE-SU*', 'Eval', 'A00-2018.system', 'R:18981', 'P:452922', 'F:8058']
['system', 'ROUGE-SU*', 'Eval', 'A00-2030.system', 'R:10707', 'P:425331', 'F:5988']
['system', 'ROUGE-SU*', 'Eval', 'A97-1014.system', 'R:8775', 'P:301725', 'F:1527']
['system', 'ROUGE-SU*', 'Eval', 'D09-1092.system', 'R:6831', 'P:493350', 'F:2991']
['system', 'ROUGE-SU*', 'Eval', 'D10-1044.system', 'R:20355', 'P:708981', 'F:14364']
['system', 'ROUGE-SU*', 'Eval', 'E03-1005.system', 'R:16065', 'P:510705', 'F:11790']
['system', 'ROUGE-SU*', 'Eval', 'J01-2004.system', 'R:6831', 'P:403260', 'F:3732']
['system', 'ROUGE-SU*', 'Eval', 'P04-1036.system', 'R:14960', 'P:667008', 'F:8032']
['system', 'ROUGE-SU*', 'Eval', 'P05-1013.system', 'R:9960', 'P:729756', 'F:6195']
['system', 'ROUGE-SU*', 'Eval', 'P08-1028.system', 'R:18645', 'P:418965', 'F:5580']
['system', 'ROUGE-SU*', 'Eval', 'P08-1043.system', 'R:6630', 'P:844872', 'F:4026']
['system', 'ROUGE-SU*', 'Eval', 'P08-1102.system', 'R:17010', 'P:628881', 'F:11514']
['system', 'ROUGE-SU*', 'Eval', 'P11-1060.system', 'R:12282', 'P:584997', 'F:6780']
['system', 'ROUGE-SU*', 'Eval', 'P11-1061.system', 'R:15756', 'P:619206', 'F:10275']
['system', 'ROUGE-SU*', 'Eval', 'P87-1015.system', 'R:5487', 'P:776517', 'F:2901']
['system', 'ROUGE-SU*', 'Eval', 'W06-2932.system', 'R:8100', 'P:365310', 'F:4554']
['system', 'ROUGE-SU*', 'Eval', 'W06-3114.system', 'R:7665', 'P:5164317', 'F:4596']
['system', 'ROUGE-SU*', 'Eval', 'W11-2123.system', 'R:10707', 'P:87480', 'F:2223']
['system', 'ROUGE-SU*', 'Eval', 'W99-0613.system', 'R:8316', 'P:9380', 'F:196']
['system', 'ROUGE-SU*', 'Eval', 'W99-0623.system', 'R:11481', 'P:318087', 'F:3459']
