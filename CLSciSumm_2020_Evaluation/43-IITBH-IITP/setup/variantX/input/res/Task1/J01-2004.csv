Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,J01-2004,W05-0104,0,2001,0,"Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))","Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))","['321', '387', '315', '344', '403']","<S sid =""321"" ssid = ""77"">In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.</S><S sid =""387"" ssid = ""143"">Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.</S><S sid =""315"" ssid = ""71"">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;—the sum of the probabilities over the vocabulary is less than one.</S><S sid =""344"" ssid = ""100"">However  when we interpolate with the trigram  we see that the additional improvement is greater than the one they experienced.</S><S sid =""403"" ssid = ""16"">Perhaps some compromise between the fully connected structures and extreme underspecification will yield an efficiency improvement.</S>",['Implication_Citation']
2,J01-2004,P08-1013,0,2001,0,"Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a tree bank, with the exception of Chelba and Jelinek (2000)",Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition,"['380', '280', '391', '344', '336']","<S sid =""380"" ssid = ""136"">Future work will include more substantial word recognition experiments.</S><S sid =""280"" ssid = ""36"">There are a couple of things to notice from these results.</S><S sid =""391"" ssid = ""4"">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S><S sid =""344"" ssid = ""100"">However  when we interpolate with the trigram  we see that the additional improvement is greater than the one they experienced.</S><S sid =""336"" ssid = ""92"">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S>",['Implication_Citation']
4,J01-2004,P04-1015,0,"Roark, 2001a",0,"Theperceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn tree bank","The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank","['349', '338', '280', '326', '343']","<S sid =""349"" ssid = ""105"">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid =""338"" ssid = ""94"">Table 4 compares the perplexity of our model with Chelba and Jelinek (1998a  1998b) on the same training and testing corpora.</S><S sid =""280"" ssid = ""36"">There are a couple of things to notice from these results.</S><S sid =""326"" ssid = ""82"">We obtained the training and testing corpora from them (which we will denote C&J corpus)  and also created intermediate corpora  upon which only the first two modifications were carried out (which we will denote no punct).</S><S sid =""343"" ssid = ""99"">Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'</S>",['Results_Citation']
5,J01-2004,P04-1015,0,"Roark, 2001a",0,"We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the per ceptron model gives performance competitive to that of the generative model on parsing the Penn tree bank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search","We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search","['372', '301', '309', '266', '268']","<S sid =""372"" ssid = ""128"">The small size of our training data  as well as the fact that we are rescoring n-best lists  rather than working directly on lattices  makes comparison with the other models not particularly informative.</S><S sid =""301"" ssid = ""57"">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid =""309"" ssid = ""65"">Let Ht be the priority queue H  before any processing has begun with word w  in the look-ahead.</S><S sid =""266"" ssid = ""22"">From this set of measures  we will also include the crossing bracket scores: average crossing brackets (CB)  percentage of sentences with no crossing brackets (0 CB)  and the percentage of sentences with two crossing brackets or fewer (< 2 CB).</S><S sid =""268"" ssid = ""24"">This is an incremental parser with a pruning strategy and no backtracking.</S>",['Method_Citation']
6,J01-2004,P04-1015,0,2001a,0,"One way around this problem is to adopt a two-pass approach, where GEN (x) is the top N analyses under some initial model, as in the re ranking approach of Collins (2000) .In the current paper we explore alternatives to rerank ing approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)","In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the argmax, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)","['336', '301', '355', '340', '354']","<S sid =""336"" ssid = ""92"">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S><S sid =""301"" ssid = ""57"">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid =""355"" ssid = ""111"">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task  we performed a very small preliminary experiment on n-best lists.</S><S sid =""340"" ssid = ""96"">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid =""354"" ssid = ""110"">Interestingly  at the word where the failure occurred  the sum of the probabilities was 0.9301.</S>",['Method_Citation']
7,J01-2004,P04-1015,0,2001a,0,"approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights","The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights","['301', '349', '361', '319', '355']","<S sid =""301"" ssid = ""57"">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid =""349"" ssid = ""105"">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid =""361"" ssid = ""117"">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S><S sid =""319"" ssid = ""75"">The hope  however  is that the improved parsing model provided by our conditional probability model will cause the distribution over structures to be more peaked  thus enabling us to capture more of the total probability mass  and making this a fairly snug upper bound on the perplexity.</S><S sid =""355"" ssid = ""111"">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task  we performed a very small preliminary experiment on n-best lists.</S>",['Method_Citation']
9,J01-2004,P04-1015,0,2001a,0,"Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word","Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word","['349', '324', '343', '387', '346']","<S sid =""349"" ssid = ""105"">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid =""324"" ssid = ""80"">Their modifications included: (i) removing orthographic cues to structure (e.g.  punctuation); (ii) replacing all numbers with the single token N; and (iii) closing the vocabulary at 10 000  replacing all other words with the UNK token.</S><S sid =""343"" ssid = ""99"">Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'</S><S sid =""387"" ssid = ""143"">Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.</S><S sid =""346"" ssid = ""102"">These results are particularly remarkable  given that we did not build our model as a language model per se  but rather as a parsing model.</S>",['Method_Citation']
10,J01-2004,P05-1022,0,"Roark, 2001",0,"A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning","A good example of this is the Roark parser (Roark, 2001) which works left-to-right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning","['315', '377', '288', '358', '382']","<S sid =""315"" ssid = ""71"">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;—the sum of the probabilities over the vocabulary is less than one.</S><S sid =""377"" ssid = ""133"">The point of this small experiment was to see if our parsing model could provide useful information even in the case that recognition errors occur  as opposed to the (generally) fully grammatical strings upon which the perplexity results were obtained.</S><S sid =""288"" ssid = ""44"">Interestingly  conditioning all POS expansions on two c-commanding heads made no difference in accuracy compared to conditioning only leftmost POS expansions on a single c-commanding head; but it did improve the efficiency.</S><S sid =""358"" ssid = ""114"">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid =""382"" ssid = ""138"">The base beam factor that we have used to this point is 10'  which is quite wide.</S>",['Method_Citation']
11,J01-2004,P05-1022,0,"Roark, 2001",0,"At the end one has a beam-width? s number of best parses (Roark, 2001) .The Collins parser (Collins, 1997) does use dynamic programming in its search","At the end one has a beam-width's number of best parses (Roark, 2001)","['301', '382', '283', '380', '268']","<S sid =""301"" ssid = ""57"">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid =""382"" ssid = ""138"">The base beam factor that we have used to this point is 10'  which is quite wide.</S><S sid =""283"" ssid = ""39"">Unlike the Roark and Johnson parser  however  our coverage did not substantially drop as the amount of conditioning information increased  and in some cases  coverage improved slightly.</S><S sid =""380"" ssid = ""136"">Future work will include more substantial word recognition experiments.</S><S sid =""268"" ssid = ""24"">This is an incremental parser with a pruning strategy and no backtracking.</S>",['Method_Citation']
12,J01-2004,P05-1022,0,"Roark, 2001",0,"To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses","To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses","['336', '280', '391', '321', '262']","<S sid =""336"" ssid = ""92"">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S><S sid =""280"" ssid = ""36"">There are a couple of things to notice from these results.</S><S sid =""391"" ssid = ""4"">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S><S sid =""321"" ssid = ""77"">In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.</S><S sid =""262"" ssid = ""18"">Recall is the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD.</S>",['Implication_Citation']
13,J01-2004,P04-1006,0,"Roark, 2001",0,"We ran the first stage parser with 4-timesoverparsing for each string in 7The n? best lists were provided by Brian Roark (Roark, 2001) 8A local-tree is an explicit expansion of an edge and its children","The n-best lists were provided by Brian Roark (Roark, 2001)","['315', '336', '295', '382', '391']","<S sid =""315"" ssid = ""71"">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;—the sum of the probabilities over the vocabulary is less than one.</S><S sid =""336"" ssid = ""92"">The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.</S><S sid =""295"" ssid = ""51"">Our observed times look polynomial  which is to be expected given our pruning strategy: the denser the competitors within a narrow probability range of the best analysis  the more time will be spent working on these competitors; and the farther along in the sentence  the more chance for ambiguities that can lead to such a situation.</S><S sid =""382"" ssid = ""138"">The base beam factor that we have used to this point is 10'  which is quite wide.</S><S sid =""391"" ssid = ""4"">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S>",['Method_Citation']
14,J01-2004,P05-1063,0,"Roark, 2001a",0,"Incremental top down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models","Incremental top-down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models","['301', '258', '372', '298', '270']","<S sid =""301"" ssid = ""57"">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid =""258"" ssid = ""14"">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S><S sid =""372"" ssid = ""128"">The small size of our training data  as well as the fact that we are rescoring n-best lists  rather than working directly on lattices  makes comparison with the other models not particularly informative.</S><S sid =""298"" ssid = ""54"">What is perhaps surprising is that the difference is not greater.</S><S sid =""270"" ssid = ""26"">In such a case  the parser fails to return a complete parse.</S>",['Method_Citation']
15,J01-2004,W10-2009,0,"Roark, 2001",0,"Levy, on the other hand ,argued that studies of probabilistic parsing reveal that typically a small number of analyses are as signed the majority of probability mass (Roark, 2001)","Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001)","['391', '398', '301', '354', '404']","<S sid =""391"" ssid = ""4"">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S><S sid =""398"" ssid = ""11"">By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.</S><S sid =""301"" ssid = ""57"">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid =""354"" ssid = ""110"">Interestingly  at the word where the failure occurred  the sum of the probabilities was 0.9301.</S><S sid =""404"" ssid = ""17"">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S>",['Method_Citation']
17,J01-2004,D09-1034,0,2001,0,"For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lex icalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)","For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)","['402', '398', '280', '371', '343']","<S sid =""402"" ssid = ""15"">In fact  left-corner parsing can be simulated by a top-down parser by transforming the grammar  as was done in Roark and Johnson (1999)  and so an approach very similar to the one outlined here could be used in that case.</S><S sid =""398"" ssid = ""11"">By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.</S><S sid =""280"" ssid = ""36"">There are a couple of things to notice from these results.</S><S sid =""371"" ssid = ""127"">For our model and the Treebank trigram model  the LM weight that resulted in the lowest error rates is given.</S><S sid =""343"" ssid = ""99"">Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'</S>",['Results_Citation']
18,J01-2004,D09-1034,0,2001,0,"We modified the Roark (2001) parser to calculate the discussed measures 1, and the empirical results in? 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time","We modified the Roark (2001) parser to calculate the discussed measures, and the empirical results in ?4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time","['398', '301', '390', '361', '258']","<S sid =""398"" ssid = ""11"">By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.</S><S sid =""301"" ssid = ""57"">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid =""390"" ssid = ""3"">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid =""361"" ssid = ""117"">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S><S sid =""258"" ssid = ""14"">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S>",['Implication_Citation']
19,J01-2004,D09-1034,0,2001,0,"In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here","In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here","['258', '262', '361', '339', '324']","<S sid =""258"" ssid = ""14"">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S><S sid =""262"" ssid = ""18"">Recall is the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD.</S><S sid =""361"" ssid = ""117"">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S><S sid =""339"" ssid = ""95"">We built an interpolated trigram model to serve as a baseline (as they did)  and also interpolated our model's perplexity with the trigram  using the same mixing coefficient as they did in their trials (taking 36 percent of the estimate from the trigram).'</S><S sid =""324"" ssid = ""80"">Their modifications included: (i) removing orthographic cues to structure (e.g.  punctuation); (ii) replacing all numbers with the single token N; and (iii) closing the vocabulary at 10 000  replacing all other words with the UNK token.</S>",['Method_Citation']
20,J01-2004,D09-1034,0,2001,0,"At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is re quired to calculate such measures. Let H (D) be the entropy over a set of derivations D, calculated as follows: H (D)=? X D? D? (D) P D?? D? (D?) log? (D) P D?? D? (D?) (10) If the set of derivations D= D (G, W [1, i]) is a set of partial derivations for string W [1, i], then H (D) is a measure of uncertainty over the partial derivations ,i.e., the uncertainty regarding the correct analysis of what has already been processed","At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures","['349', '257', '340', '301', '387']","<S sid =""349"" ssid = ""105"">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S><S sid =""257"" ssid = ""13"">A constituent for evaluation purposes consists of a label (e.g.  NP) and a span (beginning and ending word positions).</S><S sid =""340"" ssid = ""96"">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid =""301"" ssid = ""57"">By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).</S><S sid =""387"" ssid = ""143"">Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.</S>",['Results_Citation']
