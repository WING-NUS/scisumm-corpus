Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W06-3114,W06-3120,0,"Koehn and Monz, 2006",0,"The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","['134', '126', '80', '136', '0']","<S sid =""134"" ssid = ""27"">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid =""126"" ssid = ""19"">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid =""80"" ssid = ""19"">We collected around 300–400 judgements per judgement type (adequacy or fluency)  per system  per language pair.</S><S sid =""136"" ssid = ""29"">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid =""0"" ssid = ""0"">Manual and Automatic Evaluation of Machine Translation between European Languages</S>",['Results_Citation']
2,W06-3114,D07-1092,0,"Koehn and Monz, 2006",0,"We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","['159', '0', '84', '17', '170']","<S sid =""159"" ssid = ""52"">(b) does the translation have the same meaning  including connotations?</S><S sid =""0"" ssid = ""0"">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid =""84"" ssid = ""23"">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid =""17"" ssid = ""10"">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid =""170"" ssid = ""1"">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>",['Method_Citation']
3,W06-3114,C08-1074,0,"Koehn and Monz, 2006",0,"For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","['170', '18', '16', '126', '84']","<S sid =""170"" ssid = ""1"">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid =""18"" ssid = ""11"">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid =""16"" ssid = ""9"">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid =""126"" ssid = ""19"">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid =""84"" ssid = ""23"">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>",['Method_Citation']
4,W06-3114,W07-0718,0,"Koehn and Monz, 2006",0,"The results of last year? s workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","The results of last year's workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","['126', '17', '170', '84', '38']","<S sid =""126"" ssid = ""19"">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid =""17"" ssid = ""10"">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid =""170"" ssid = ""1"">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid =""84"" ssid = ""23"">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid =""38"" ssid = ""4"">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S>",['Method_Citation']
5,W06-3114,P07-1083,0,"Koehn and Monz, 2006",0,"For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and GermanEnglish (De) (Koehn and Monz, 2006)","For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and German-English (De) (Koehn and Monz, 2006)","['170', '38', '126', '17', '84']","<S sid =""170"" ssid = ""1"">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid =""38"" ssid = ""4"">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid =""126"" ssid = ""19"">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid =""17"" ssid = ""10"">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid =""84"" ssid = ""23"">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>",['Method_Citation']
6,W06-3114,W07-0738,0,2006,0,"Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","['52', '21', '28', '129', '35']","<S sid =""52"" ssid = ""18"">Pairwise comparison: We can use the same method to assess the statistical significance of one system outperforming another.</S><S sid =""21"" ssid = ""14"">The out-of-domain test set differs from the Europarl data in various ways.</S><S sid =""28"" ssid = ""21"">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid =""129"" ssid = ""22"">All systems (except for Systran  which was not tuned to Europarl) did considerably worse on outof-domain training data.</S><S sid =""35"" ssid = ""1"">For the automatic evaluation  we used BLEU  since it is the most established metric in the field.</S>",['Method_Citation']
7,W06-3114,W07-0738,0,2006,0,"For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","['16', '113', '126', '28', '0']","<S sid =""16"" ssid = ""9"">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid =""113"" ssid = ""6"">The confidence intervals are computed by bootstrap resampling for BLEU  and by standard significance testing for the manual scores  as described earlier in the paper.</S><S sid =""126"" ssid = ""19"">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid =""28"" ssid = ""21"">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid =""0"" ssid = ""0"">Manual and Automatic Evaluation of Machine Translation between European Languages</S>",['Method_Citation']
8,W06-3114,W07-0738,0,2006,0,Wepresenta comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),"['18', '16', '38', '84', '109']","<S sid =""18"" ssid = ""11"">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid =""16"" ssid = ""9"">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid =""38"" ssid = ""4"">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid =""84"" ssid = ""23"">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid =""109"" ssid = ""2"">The scores and confidence intervals are detailed first in the Figures 7–10 in table form (including ranks)  and then in graphical form in Figures 11–16.</S>",['Method_Citation']
9,W06-3114,W07-0738,0,2006,0,Weanalyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),"['9', '44', '143', '25', '35']","<S sid =""9"" ssid = ""2"">Training and testing is based on the Europarl corpus.</S><S sid =""44"" ssid = ""10"">We computed BLEU scores for each submission with a single reference translation.</S><S sid =""143"" ssid = ""36"">For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.</S><S sid =""25"" ssid = ""18"">We received submissions from 14 groups from 11 institutions  as listed in Figure 2.</S><S sid =""35"" ssid = ""1"">For the automatic evaluation  we used BLEU  since it is the most established metric in the field.</S>",['Method_Citation']
10,W06-3114,D07-1030,0,"Koehn and Monz, 2006",0,"We use the same method described in (Koehn and Monz, 2006) to perform the significance test","We use the same method described in (Koehn and Monz, 2006) to perform the significance test","['134', '68', '0', '28', '136']","<S sid =""134"" ssid = ""27"">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid =""68"" ssid = ""7"">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid =""0"" ssid = ""0"">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid =""28"" ssid = ""21"">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid =""136"" ssid = ""29"">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S>",['Method_Citation']
11,W06-3114,D07-1030,0,"Koehn and Monz, 2016",0,"We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","['134', '0', '68', '126', '17']","<S sid =""134"" ssid = ""27"">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid =""0"" ssid = ""0"">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid =""68"" ssid = ""7"">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid =""126"" ssid = ""19"">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid =""17"" ssid = ""10"">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S>",['Method_Citation']
12,W06-3114,W08-0406,0,"Koehn and Monz, 2017",0,"The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","['126', '134', '17', '84', '159']","<S sid =""126"" ssid = ""19"">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid =""134"" ssid = ""27"">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid =""17"" ssid = ""10"">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid =""84"" ssid = ""23"">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid =""159"" ssid = ""52"">(b) does the translation have the same meaning  including connotations?</S>",['Method_Citation']
13,W06-3114,W11-1002,0,2006,0,"Callison-Burch et al (2006 )andKoehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality","Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality","['84', '38', '155', '170', '6']","<S sid =""84"" ssid = ""23"">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid =""38"" ssid = ""4"">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid =""155"" ssid = ""48"">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S><S sid =""170"" ssid = ""1"">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid =""6"" ssid = ""4"">English was again paired with German  French  and Spanish.</S>",['Results_Citation']
14,W06-3114,D07-1091,0,"Koehn and Monz, 2006",0,"The English? German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","The English German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","['126', '134', '0', '17', '84']","<S sid =""126"" ssid = ""19"">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid =""134"" ssid = ""27"">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid =""0"" ssid = ""0"">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid =""17"" ssid = ""10"">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid =""84"" ssid = ""23"">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>",['Method_Citation']
15,W06-3114,D07-1091,0,"Koehn and Monz, 2006",0,"We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","['134', '0', '126', '68', '17']","<S sid =""134"" ssid = ""27"">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid =""0"" ssid = ""0"">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid =""126"" ssid = ""19"">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid =""68"" ssid = ""7"">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid =""17"" ssid = ""10"">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S>",['Method_Citation']
16,W06-3114,P07-1108,0,"Koehn and Monz, 2006",0,"A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Ma chine Translation (Koehn and Monz, 2006)","A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006)","['0', '170', '84', '126', '17']","<S sid =""0"" ssid = ""0"">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid =""170"" ssid = ""1"">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid =""84"" ssid = ""23"">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid =""126"" ssid = ""19"">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid =""17"" ssid = ""10"">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S>",['Method_Citation']
18,W06-3114,E12-3010,0,"Koehn and Monz, 2006",0,"For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","['80', '84', '150', '16', '155']","<S sid =""80"" ssid = ""19"">We collected around 300–400 judgements per judgement type (adequacy or fluency)  per system  per language pair.</S><S sid =""84"" ssid = ""23"">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid =""150"" ssid = ""43"">For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French)  at least as measured by BLEU.</S><S sid =""16"" ssid = ""9"">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid =""155"" ssid = ""48"">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S>",['Method_Citation']
19,W06-3114,W09-0402,0,"Koehn and Monz, 2006",0,"The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","['126', '84', '38', '17', '170']","<S sid =""126"" ssid = ""19"">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid =""84"" ssid = ""23"">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid =""38"" ssid = ""4"">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid =""17"" ssid = ""10"">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid =""170"" ssid = ""1"">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>",['Method_Citation']
